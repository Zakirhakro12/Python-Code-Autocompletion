#!/usr/bin/env python
# -*- coding: utf-8 -*-

""" Provides ``mapping`` of url paths to request handlers.
"""

from bootstrap import Bootstrap
from fund import InstantPaymentNotificationHandler
from fund import ThankYouHandler
from view import *

mapping = [(
        r'/',
        Index
    ), (
        r'/ipn',
        InstantPaymentNotificationHandler
    ), (
        r'/thank-you',
        ThankYouHandler
    ), (
        r'/about\/?',
        About
    ), (
        r'/guide\/?',
        Guide
    ), (
        r'/guide/download\/?',
        Download
    ), (
        r'/guide/standards\/?',
        Standards
    ), (
        r'/community\/?',
        Community
    ), (
        r'/news\/?',
        News
    ), (
        r'/support\/?',
        Support
    ), (
        r'/contact\/?',
        Contact
    ), (
        r'/press\/?',
        Press
    ), (
        r'/legal/terms',
        Terms
    ), (
        r'/library\/?',
        Library
    ), (
        r'/library/sketchup\/?',
        Library
    ), (
        r'/library/series/(\w+)\/?',
        Library
    ), (
        r'/library/users\/?',
        Users
    ), (
        r'/library/users/([0-9]+)\/?',
        User
    ), (
        r'/library/designs/([0-9]+)\/?',
        Design
    ), (
        r'/library/designs/([0-9]+)/(edit)\/?',
        Design
    ), (
        r'/library/designs\/?',
        Design
    ), (
        r'/library/designs/add\/?',
        Design
    ), (
        r'/library/designs/add/sketchup\/?',
        Design
    ), (
        r'/redirect/success/([0-9]+)\/?',
        RedirectSuccess
    ), (
        r'/redirect/error\/?',
        RedirectError
    ), (
        r'/redirect/after/delete\/?',
        RedirectAfterDelete
    ),(
        r'/admin/moderate\/?',
        Moderate
    ), (
        r'/admin/bootstrap\/?',
        Bootstrap
    ), (
        r'/activity',
        ActivityScreen
    ), (
        r'/txns',
        TxnList
    ), (
        r'/blob64/([^/]+)/([^/]+)\/?',
        Base64Blob
    ), (
        r'/blob64/([^/]+)\/?',
        Base64Blob
    ), (
        r'/i18n/message_strings.json',
        MessageStrings
    ), (
        r'/.*',
        NotFound
    )
]

# -*- coding: utf-8 -*-
# Open Source Initiative OSI - The MIT License (MIT):Licensing
#
# The MIT License (MIT)
# Copyright (c) 2015 François-Xavier Bourlet (bombela+zerorpc@gmail.com)
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of
# this software and associated documentation files (the "Software"), to deal in
# the Software without restriction, including without limitation the rights to
# use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
# of the Software, and to permit persons to whom the Software is furnished to do
# so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all
# copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.


import msgpack
import gevent.pool
import gevent.queue
import gevent.event
import gevent.local
import gevent.lock
import logging
import sys

import gevent_zmq as zmq
from .exceptions import TimeoutExpired
from .context import Context
from .channel_base import ChannelBase


if sys.version_info < (2, 7):
    def get_pyzmq_frame_buffer(frame):
        return frame.buffer[:]
else:
    def get_pyzmq_frame_buffer(frame):
        return frame.buffer


logger = logging.getLogger(__name__)


class SequentialSender(object):

    def __init__(self, socket):
        self._socket = socket

    def _send(self, parts):
        e = None
        for i in xrange(len(parts) - 1):
            try:
                self._socket.send(parts[i], copy=False, flags=zmq.SNDMORE)
            except (gevent.GreenletExit, gevent.Timeout) as e:
                if i == 0:
                    raise
                self._socket.send(parts[i], copy=False, flags=zmq.SNDMORE)
        try:
            self._socket.send(parts[-1], copy=False)
        except (gevent.GreenletExit, gevent.Timeout) as e:
            self._socket.send(parts[-1], copy=False)
        if e:
            raise e

    def __call__(self, parts, timeout=None):
        if timeout:
            with gevent.Timeout(timeout):
                self._send(parts)
        else:
            self._send(parts)


class SequentialReceiver(object):

    def __init__(self, socket):
        self._socket = socket

    def _recv(self):
        e = None
        parts = []
        while True:
            try:
                part = self._socket.recv(copy=False)
            except (gevent.GreenletExit, gevent.Timeout) as e:
                if len(parts) == 0:
                    raise
                part = self._socket.recv(copy=False)
            parts.append(part)
            if not part.more:
                break
        if e:
            raise e
        return parts

    def __call__(self, timeout=None):
        if timeout:
            with gevent.Timeout(timeout):
                return self._recv()
        else:
            return self._recv()


class Sender(SequentialSender):

    def __init__(self, socket):
        self._socket = socket
        self._send_queue = gevent.queue.Channel()
        self._send_task = gevent.spawn(self._sender)

    def close(self):
        if self._send_task:
            self._send_task.kill()

    def _sender(self):
        for parts in self._send_queue:
            super(Sender, self)._send(parts)

    def __call__(self, parts, timeout=None):
        try:
            self._send_queue.put(parts, timeout=timeout)
        except gevent.queue.Full:
            raise TimeoutExpired(timeout)


class Receiver(SequentialReceiver):

    def __init__(self, socket):
        self._socket = socket
        self._recv_queue = gevent.queue.Channel()
        self._recv_task = gevent.spawn(self._recver)

    def close(self):
        if self._recv_task:
            self._recv_task.kill()
        self._recv_queue = None

    def _recver(self):
        while True:
            parts = super(Receiver, self)._recv()
            self._recv_queue.put(parts)

    def __call__(self, timeout=None):
        try:
            return self._recv_queue.get(timeout=timeout)
        except gevent.queue.Empty:
            raise TimeoutExpired(timeout)


class Event(object):

    __slots__ = ['_name', '_args', '_header', '_identity']

    def __init__(self, name, args, context, header=None):
        self._name = name
        self._args = args
        if header is None:
            self._header = {'message_id': context.new_msgid(), 'v': 3}
        else:
            self._header = header
        self._identity = None

    @property
    def header(self):
        return self._header

    @property
    def name(self):
        return self._name

    @name.setter
    def name(self, v):
        self._name = v

    @property
    def args(self):
        return self._args

    @property
    def identity(self):
        return self._identity

    @identity.setter
    def identity(self, v):
        self._identity = v

    def pack(self):
        return msgpack.Packer(use_bin_type=True).pack((self._header, self._name, self._args))

    @staticmethod
    def unpack(blob):
        unpacker = msgpack.Unpacker(encoding='utf-8')
        unpacker.feed(blob)
        unpacked_msg = unpacker.unpack()

        try:
            (header, name, args) = unpacked_msg
        except Exception as e:
            raise Exception('invalid msg format "{0}": {1}'.format(
                unpacked_msg, e))

        # Backward compatibility
        if not isinstance(header, dict):
            header = {}

        return Event(name, args, None, header)

    def __str__(self, ignore_args=False):
        if ignore_args:
            args = '[...]'
        else:
            args = self._args
            try:
                args = '<<{0}>>'.format(str(self.unpack(self._args)))
            except Exception:
                pass
        if self._identity:
            identity = ', '.join(repr(x.bytes) for x in self._identity)
            return '<{0}> {1} {2} {3}'.format(identity, self._name,
                    self._header, args)
        return '{0} {1} {2}'.format(self._name, self._header, args)


class Events(ChannelBase):
    def __init__(self, zmq_socket_type, context=None):
        self._debug = False
        self._zmq_socket_type = zmq_socket_type
        self._context = context or Context.get_instance()
        self._socket = self._context.socket(zmq_socket_type)

        if zmq_socket_type in (zmq.PUSH, zmq.PUB, zmq.DEALER, zmq.ROUTER):
            self._send = Sender(self._socket)
        elif zmq_socket_type in (zmq.REQ, zmq.REP):
            self._send = SequentialSender(self._socket)
        else:
            self._send = None

        if zmq_socket_type in (zmq.PULL, zmq.SUB, zmq.DEALER, zmq.ROUTER):
            self._recv = Receiver(self._socket)
        elif zmq_socket_type in (zmq.REQ, zmq.REP):
            self._recv = SequentialReceiver(self._socket)
        else:
            self._recv = None

    @property
    def recv_is_supported(self):
        return self._recv is not None

    @property
    def emit_is_supported(self):
        return self._send is not None

    def __del__(self):
        try:
            if not self._socket.closed:
                self.close()
        except (AttributeError, TypeError):
            pass

    def close(self):
        try:
            self._send.close()
        except AttributeError:
            pass
        try:
            self._recv.close()
        except AttributeError:
            pass
        self._socket.close()

    @property
    def debug(self):
        return self._debug

    @debug.setter
    def debug(self, v):
        if v != self._debug:
            self._debug = v
            if self._debug:
                logger.debug('debug enabled')
            else:
                logger.debug('debug disabled')

    def _resolve_endpoint(self, endpoint, resolve=True):
        if resolve:
            endpoint = self._context.hook_resolve_endpoint(endpoint)
        if isinstance(endpoint, (tuple, list)):
            r = []
            for sub_endpoint in endpoint:
                r.extend(self._resolve_endpoint(sub_endpoint, resolve))
            return r
        return [endpoint]

    def connect(self, endpoint, resolve=True):
        r = []
        for endpoint_ in self._resolve_endpoint(endpoint, resolve):
            r.append(self._socket.connect(endpoint_))
            logger.debug('connected to %s (status=%s)', endpoint_, r[-1])
        return r

    def bind(self, endpoint, resolve=True):
        r = []
        for endpoint_ in self._resolve_endpoint(endpoint, resolve):
            r.append(self._socket.bind(endpoint_))
            logger.debug('bound to %s (status=%s)', endpoint_, r[-1])
        return r

    def disconnect(self, endpoint, resolve=True):
        r = []
        for endpoint_ in self._resolve_endpoint(endpoint, resolve):
            r.append(self._socket.disconnect(endpoint_))
            logging.debug('disconnected from %s (status=%s)', endpoint_, r[-1])
        return r

    def new_event(self, name, args, xheader=None):
        event = Event(name, args, context=self._context)
        if xheader:
            event.header.update(xheader)
        return event

    def emit_event(self, event, timeout=None):
        if self._debug:
            logger.debug('--> %s', event)
        if event.identity:
            parts = list(event.identity or list())
            parts.extend(['', event.pack()])
        elif self._zmq_socket_type in (zmq.DEALER, zmq.ROUTER):
            parts = ('', event.pack())
        else:
            parts = (event.pack(),)
        self._send(parts, timeout)

    def recv(self, timeout=None):
        parts = self._recv(timeout=timeout)
        if len(parts) > 2:
            identity = parts[0:-2]
            blob = parts[-1]
        elif len(parts) == 2:
            identity = parts[0:-1]
            blob = parts[-1]
        else:
            identity = None
            blob = parts[0]
        event = Event.unpack(get_pyzmq_frame_buffer(blob))
        event.identity = identity
        if self._debug:
            logger.debug('<-- %s', event)
        return event

    def setsockopt(self, *args):
        return self._socket.setsockopt(*args)

    @property
    def context(self):
        return self._context

#!/usr/bin/env python
"""Django's command line utility."""

import os
import sys

if __name__ == "__main__":
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "project.settings")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)

"""Installer for hippybot
"""

import os
cwd = os.path.dirname(__file__)
__version__ = open(os.path.join(cwd, 'hippybot', 'version.txt'), 'r').read().strip()

try:
        from setuptools import setup, find_packages
except ImportError:
        from ez_setup import use_setuptools
        use_setuptools()
        from setuptools import setup, find_packages
setup(
    name='hippybot',
    description='Python Hipchat bot',
    long_description=open('README.rst').read(),
    version=__version__,
    author='Wes Mason',
    author_email='wes[at]1stvamp[dot]org',
    url='http://github.com/1stvamp/hippybot',
    packages=find_packages(exclude=['ez_setup']),
    install_requires=open('requirements.txt').readlines(),
    package_data={'hippybot': ['version.txt']},
    include_package_data=True,
    extras_require={
        'plugins': open('extras_requirements.txt').readlines(),
    },
    entry_points={
        'console_scripts': ['hippybot = hippybot.bot:main',],
    },
    license='BSD'
)

#!/usr/bin/env python
import os
import sys

if __name__ == "__main__":
    os.environ.setdefault("DJANGO_SETTINGS_MODULE", "twobuntu.settings")

    from django.core.management import execute_from_command_line

    execute_from_command_line(sys.argv)

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
    ]

    operations = [
        migrations.CreateModel(
            name='Category',
            fields=[
                ('id', models.AutoField(verbose_name='ID', serialize=False, auto_created=True, primary_key=True)),
                ('name', models.CharField(help_text=b'The name of the category.', max_length=40)),
                ('image', models.ImageField(help_text=b'A representative image.', null=True, upload_to=b'categories', blank=True)),
            ],
            options={
                'ordering': ('name',),
                'verbose_name_plural': 'Categories',
            },
            bases=(models.Model,),
        ),
    ]

import twitter
from django.contrib import messages
from django.contrib.auth.decorators import user_passes_test
from django.db import transaction
from django.shortcuts import redirect, render

from twobuntu.news.forms import AddItemForm


@user_passes_test(lambda u: u.is_staff)
def add(request):
    """
    Add news items to the home page.
    """
    if request.method == 'POST':
        form = AddItemForm(data=request.POST)
        if form.is_valid():
            item = form.save(commit=False)
            item.reporter = request.user
            try:
                with transaction.atomic():
                    item.save()
            except twitter.TwitterError as e:
                messages.error(request, "Twitter error: \"%s\" Please try again." % e.message[0]['message'])
            else:
                messages.info(request, "Your news item has been published!")
                return redirect('home')
    else:
        form = AddItemForm()
    return render(request, 'form.html', {
        'title': 'Add Item',
        'form': form,
        'description': "Enter the details for the news item below.",
        'action': 'Add',
    })

# -*- coding: utf-8 -*-
##############################################################################
#
# Copyright (c) 2010-2015, 2degrees Limited.
# All Rights Reserved.
#
# This file is part of django-wsgi <https://github.com/2degrees/django-wsgi/>,
# which is subject to the provisions of the BSD at
# <http://dev.2degreesnetwork.com/p/2degrees-license.html>. A copy of the
# license should accompany this distribution. THIS SOFTWARE IS PROVIDED "AS IS"
# AND ANY AND ALL EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT
# NOT LIMITED TO, THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST
# INFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.
#
##############################################################################
"""
Exceptions raised by :mod:`django_wsgi.`

"""

__all__ = ("DjangoWSGIException", "ApplicationCallError")


class DjangoWSGIException(Exception):
    """Base class for exceptions raised by :mod:`django_wsgi`."""
    pass


class ApplicationCallError(DjangoWSGIException):
    """
    Exception raised when an embedded WSGI application was not called properly.
    
    """
    pass

import boto
import boto.s3.connection

from django.conf import settings

import logging
log = logging.getLogger(__name__)


def get_s3_connection():

    if settings.S3_ACCESS_KEY and settings.S3_SECRET_KEY and settings.S3_HOST:
        log.debug('Connecting to {}, with secure connection is {}'.
                  format(settings.S3_HOST, settings.S3_SECURE_CONNECTION))
        return boto.connect_s3(
            aws_access_key_id=settings.S3_ACCESS_KEY,
            aws_secret_access_key=settings.S3_SECRET_KEY,
            host=settings.S3_HOST,
            is_secure=settings.S3_SECURE_CONNECTION,
            calling_format=boto.s3.connection.OrdinaryCallingFormat())
    return None


def get_or_create_bucket(s3_connection):
    bucket = s3_connection.get_bucket(settings.S3_BUCKET_NAME)
    if bucket is None:
        bucket = s3_connection.create_bucket(settings.S3_BUCKET_NAME)
    return bucket

from django.db import models
import datetime

from common.models import Project


class Stage(models.Model):
    name = models.CharField(max_length=128)
    project = models.ForeignKey(Project)
    text = models.TextField(default='', blank=True)
    link = models.URLField(default=None, blank=True, null=True)
    state = models.CharField(max_length=24, default='info', blank=True)
    weight = models.IntegerField(default=0)
    updated = models.DateTimeField(default=datetime.datetime.now())

    def save(self, *args, **kwargs):
        self.updated = datetime.datetime.now()
        return super(Stage, self).save(*args, **kwargs)

    def __str__(self):
        return self.name

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('testreport', '0026_testresult_launch_item_id'),
    ]

    operations = [
        migrations.AddField(
            model_name='testplan',
            name='filter',
            field=models.TextField(default=b'', max_length=128, verbose_name='Started by filter', blank=True),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='testplan',
            name='main',
            field=models.BooleanField(default=False, verbose_name='Show in short statistic'),
            preserve_default=True,
        ),
    ]

#!/usr/bin/env python

import sys
import json

if sys.version_info < (3,):
    def b(x):
        return x

    def s(x):
        return x
else:
    def b(x):
        return bytes(x, 'utf-8')

    def s(x):
        return x.decode('utf-8')


def parse_payload(payload):
    if not isinstance(payload, str):
        payload = ' '.join(payload)

    try:
        json.loads(payload)
    except ValueError:
        kv = payload.split(' ', 1)
        if len(kv) > 1:
          payload = '{"%s": "%s"}' % (kv[0], kv[1])
        else:
          payload = '%s' % kv[0]

    return payload

def requires_elements(xs, dictionary):
    missing_values = []
    for x in xs:
        if x not in dictionary:
            missing_values.append(x)
    if missing_values:
        err_msg = ', '.join(missing_values)
        raise KeyError('Missing values %s' % (err_msg))


from flask_resty import Api, GenericModelView
from marshmallow import fields, Schema
import pytest
from sqlalchemy import Column, Integer, String

import helpers

# -----------------------------------------------------------------------------


@pytest.yield_fixture
def models(db):
    class Widget(db.Model):
        __tablename__ = 'widgets'

        id_1 = Column(Integer, primary_key=True)
        id_2 = Column(Integer, primary_key=True)
        name = Column(String, nullable=False)

    db.create_all()

    yield {
        'widget': Widget,
    }

    db.drop_all()


@pytest.fixture
def schemas():
    class WidgetSchema(Schema):
        id_1 = fields.Integer(as_string=True)
        id_2 = fields.Integer(as_string=True)
        name = fields.String(required=True)

    return {
        'widget': WidgetSchema(),
    }


@pytest.fixture(autouse=True)
def routes(app, models, schemas):
    class WidgetViewBase(GenericModelView):
        model = models['widget']
        schema = schemas['widget']
        id_fields = ('id_1', 'id_2')

    class WidgetListView(WidgetViewBase):
        def get(self):
            return self.list()

        def post(self):
            return self.create(allow_client_id=True)

    class WidgetView(WidgetViewBase):
        def get(self, id_1, id_2):
            return self.retrieve((id_1, id_2))

        def patch(self, id_1, id_2):
            return self.update((id_1, id_2), partial=True)

        def delete(self, id_1, id_2):
            return self.destroy((id_1, id_2))

    api = Api(app)
    api.add_resource(
        '/widgets', WidgetListView, WidgetView,
        id_rule='<int:id_1>/<int:id_2>',
    )


@pytest.fixture(autouse=True)
def data(db, models):
    db.session.add_all((
        models['widget'](id_1=1, id_2=2, name="Foo"),
        models['widget'](id_1=1, id_2=3, name="Bar"),
        models['widget'](id_1=4, id_2=5, name="Baz"),
    ))
    db.session.commit()


# -----------------------------------------------------------------------------


def test_list(client):
    response = client.get('/widgets')
    assert response.status_code == 200

    assert helpers.get_data(response) == [
        {
            'id_1': '1',
            'id_2': '2',
            'name': "Foo",
        },
        {
            'id_1': '1',
            'id_2': '3',
            'name': "Bar",
        },
        {
            'id_1': '4',
            'id_2': '5',
            'name': "Baz",
        },
    ]


def test_retrieve(client):
    response = client.get('/widgets/1/2')
    assert response.status_code == 200

    assert helpers.get_data(response) == {
        'id_1': '1',
        'id_2': '2',
        'name': "Foo",
    }


def test_create(client):
    response = helpers.request(
        client,
        'POST', '/widgets',
        {
            'id_1': '4',
            'id_2': '6',
            'name': "Qux",
        },
    )
    assert response.status_code == 201
    assert response.headers['Location'] == 'http://localhost/widgets/4/6'

    assert helpers.get_data(response) == {
        'id_1': '4',
        'id_2': '6',
        'name': "Qux",
    }


def test_update(client):
    update_response = helpers.request(
        client,
        'PATCH', '/widgets/1/2',
        {
            'id_1': '1',
            'id_2': '2',
            'name': "Qux",
        },
    )
    assert update_response.status_code == 204

    retrieve_response = client.get('/widgets/1/2')
    assert retrieve_response.status_code == 200

    assert helpers.get_data(retrieve_response) == {
        'id_1': '1',
        'id_2': '2',
        'name': "Qux",
    }


def test_destroy(client):
    destroy_response = client.delete('/widgets/1/2')
    assert destroy_response.status_code == 204

    retrieve_response = client.get('/widgets/1/2')
    assert retrieve_response.status_code == 404

from .dogpile import Dogpile

#coding:utf8
'''
Created on 2013-7-10
memcached client
@author: lan (www.9miao.com)
'''
import memcache

class MemConnError(Exception): 
    """
    """
    def __str__(self):
        return "memcache connect error"

class MemClient:
    '''memcached
    '''
    
    def __init__(self,timeout = 0):
        '''
        '''
        self._hostname = ""
        self._urls = []
        self.connection = None
        
    def connect(self,urls,hostname):
        '''memcached connect
        '''
        self._hostname = hostname
        self._urls = urls
        self.connection = memcache.Client(self._urls,debug=0)
        if not self.connection.set("__testkey__",1):
            raise MemConnError()
        
    def produceKey(self,keyname):
        '''
        '''
        if isinstance(keyname, basestring):
            return ''.join([self._hostname,':',keyname])
        else:
            raise "type error"
        
    def get(self,key):
        '''
        '''
        key = self.produceKey(key)
        return self.connection.get(key)
    
    def get_multi(self,keys):
        '''
        '''
        keynamelist = [self.produceKey(keyname) for keyname in keys]
        olddict = self.connection.get_multi(keynamelist)
        newdict = dict(zip([keyname.split(':')[-1] for keyname in olddict.keys()],
                              olddict.values()))
        return newdict
        
    def set(self,keyname,value):
        '''
        '''
        key = self.produceKey(keyname)
        result = self.connection.set(key,value)
        if not result:#如果写入失败
            self.connect(self._urls,self._hostname)#重新连接
            return self.connection.set(key,value)
        return result
    
    def set_multi(self,mapping):
        '''
        '''
        newmapping = dict(zip([self.produceKey(keyname) for keyname in mapping.keys()],
                              mapping.values()))
        result = self.connection.set_multi(newmapping)
        if result:#如果写入失败
            self.connect(self._urls,self._hostname)#重新连接
            return self.connection.set_multi(newmapping)
        return result
        
    def incr(self,key,delta):
        '''
        '''
        key = self.produceKey(key)
        return self.connection.incr(key, delta)
        
    def delete(self,key):
        '''
        '''
        key = self.produceKey(key)
        return self.connection.delete(key)
    
    def delete_multi(self,keys):
        """
        """
        keys = [self.produceKey(key) for key in keys]
        return self.connection.delete_multi(keys)
        
    def flush_all(self):
        '''
        '''
        self.connection.flush_all()
        
mclient = MemClient()



""" Really basic gatttool (BlueZ) wrapper

Based on https://github.com/stratosinc/pygatt
Part of https://github.com/ALPSquid/thebutton-monitor
"""

import pexpect


class connect():
    """ Use to initiate a connection to a GATT device
    Example: bt_device = gatt.connect('AB:CD:EF:01:23:45')
    """
    def __init__(self, address):
        self.address = ""  # Connected bluetooth device address. Assigned from connect()
        self.conn = None  # pexpect.spawn() object for the gatttool command
        self.connect(address)

    def connect(self, address, adapter='hci0'):
        """ Open an interactive connection to a bluetooth device

        :param address: Bluetooth device address
        :param adapter: Bluetooth adapter to use. Default: hci0
        """
        if self.conn is None:
            self.address = address
            cmd = ' '.join(['gatttool', '-b', address, '-i', adapter, '-I'])
            self.conn = pexpect.spawn(cmd)
            self.conn.expect(r'\[LE\]>', timeout=1)
            self.conn.sendline('connect')
            try:
                self.conn.expect(r'Connection successful', timeout=10)
                print("Connected to " + address)
            except pexpect.TIMEOUT:
                raise Exception("Unable to connect to device")
        else:
            raise Exception("Device already connected! Call disconnect before attempting a new connection")

    def reconnect(self):
        """ Check and attempt to reconnect to device if necessary
        :return: True if a reconnect was performed
        """
        try:
            self.conn.expect(r'Disconnected', timeout=0.1)
            self.conn.sendline('connect')
            try:
                self.conn.expect(r'Connection successful', timeout=10)
                print("Reconnected to device: " + self.address)
            except pexpect.TIMEOUT:
                # Continue and try to reconnect next time
                print("Lost connection to device: " + self.address)
            return True
        except pexpect.TIMEOUT:
            # No need to reconnect
            return False

    def disconnect(self):
        """ Disconnect from current bluetooth device """
        if self.conn is not None:
            self.conn.sendline('exit')
            self.conn = None
            print("Disconnected from " + self.address)

    def write(self, handle, value):
        """ Write a value to the specified handle

        :param handle: address to write to. e.g. 0016
        :param value: value to write
        """
        self.send(' '.join(['char-write-cmd', '0x'+handle, value]))

    def read(self, handle):
        """ Read from the specified handle

        :param handle: address to read from. e.g. 0016
        """
        self.send('char-read-hnd 0x' + handle, r'descriptor: .* \r', timeout=5)
        val = ' '.join(self.conn.after.decode("utf-8").split()[1:])
        return val

    def send(self, cmd, expect=None, timeout=5):
        """ Send command to device. Attempt a reconnect if disconnected

        :param cmd: Command to send
        """
        self.conn.sendline(cmd)
        if expect is not None:
            try:
                self.conn.expect(expect, timeout)
            except pexpect.TIMEOUT:
                if self.reconnect():
                    self.conn.sendline(cmd)
        else:
            if self.reconnect():
                self.conn.sendline(cmd)
# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations
import wagtail.wagtailcore.fields


class Migration(migrations.Migration):

    dependencies = [
        ('puput', '0001_initial'),
    ]

    operations = [
        migrations.AlterField(
            model_name='blogpage',
            name='description',
            field=models.CharField(max_length=255, help_text='The blog description that will appear under the title.', verbose_name='Description', blank=True),
        ),
        migrations.AlterField(
            model_name='category',
            name='description',
            field=models.CharField(max_length=500, verbose_name='Description', blank=True),
        ),
        migrations.AlterField(
            model_name='category',
            name='name',
            field=models.CharField(max_length=80, unique=True, verbose_name='Category name'),
        ),
        migrations.AlterField(
            model_name='category',
            name='parent',
            field=models.ForeignKey(to='puput.Category', related_name='children', null=True, verbose_name='Parent category', blank=True),
        ),
        migrations.AlterField(
            model_name='entrypage',
            name='excerpt',
            field=wagtail.wagtailcore.fields.RichTextField(help_text='Entry excerpt to be displayed on entries list. If this field is not filled, a truncate version of body text will be used.', verbose_name='excerpt', blank=True),
        ),
    ]

"""
==================================
Map two radars to a Cartesian grid
==================================

Map the reflectivity field of two nearby ARM XSARP radars from antenna
coordinates to a Cartesian grid.

"""
print(__doc__)

# Author: Jonathan J. Helmus (jhelmus@anl.gov)
# License: BSD 3 clause

import matplotlib.pyplot as plt
import pyart

# read in the data from both XSAPR radars
XSAPR_SW_FILE = 'swx_20120520_0641.nc'
XSAPR_SE_FILE = 'sex_20120520_0641.nc'
radar_sw = pyart.io.read_cfradial(XSAPR_SW_FILE)
radar_se = pyart.io.read_cfradial(XSAPR_SE_FILE)

# filter out gates with reflectivity > 100 from both radars
gatefilter_se = pyart.filters.GateFilter(radar_se)
gatefilter_se.exclude_above('corrected_reflectivity_horizontal', 100)
gatefilter_sw = pyart.filters.GateFilter(radar_sw)
gatefilter_sw.exclude_above('corrected_reflectivity_horizontal', 100)

# perform Cartesian mapping, limit to the reflectivity field.
grid = pyart.map.grid_from_radars(
    (radar_se, radar_sw), gatefilters=(gatefilter_se, gatefilter_sw),
    grid_shape=(1, 201, 201),
    grid_limits=((1000, 1000), (-50000, 40000), (-60000, 40000)),
    grid_origin = (36.57861, -97.363611),
    fields=['corrected_reflectivity_horizontal'])

# create the plot
fig = plt.figure()
ax = fig.add_subplot(111)
ax.imshow(grid.fields['corrected_reflectivity_horizontal']['data'][0],
          origin='lower', extent=(-60, 40, -50, 40), vmin=0, vmax=48)
plt.show()

"""
pyart.aux_io.radx
=================

Reading files using Radx to first convert the file to Cf.Radial format

.. autosummary::
    :toctree: generated/

    read_radx

"""

import os
import tempfile
import subprocess

from ..io.cfradial import read_cfradial
from ..io.common import _test_arguments


def read_radx(filename, **kwargs):
    """
    Read a file by first converting it to Cf/Radial using RadxConvert.

    Parameters
    ----------
    filename : str
        Name of file to read using RadxConvert.

    Returns
    -------
    radar : Radar
        Radar object.

    """
    # test for non empty kwargs
    _test_arguments(kwargs)

    tmpfile = tempfile.mkstemp(suffix='.nc', dir='.')[1]
    head, tail = os.path.split(tmpfile)
    try:
        subprocess.check_call(
            ['RadxConvert', '-const_ngates',
             '-outdir', head, '-outname', tail, '-f', filename])
        if not os.path.isfile(tmpfile):
            raise IOError(
                'RadxConvert failed to create a file, upgrading to the '
                ' latest version of Radx may be necessary.')
        radar = read_cfradial(tmpfile)
    finally:
        os.remove(tmpfile)
    return radar

"""
pyart.exceptions
================

Custom Py-ART exceptions.

.. autosummary::
    :toctree: generated/

    MissingOptionalDependency
    DeprecatedAttribute
    DeprecatedFunctionName
    _deprecated_alias

"""

import warnings


class MissingOptionalDependency(Exception):
    """ Exception raised when a optional dependency is needed by not found. """
    pass


class DeprecatedAttribute(DeprecationWarning):
    """ Warning category for an attribute which has been renamed/moved.  """
    pass


class DeprecatedFunctionName(DeprecationWarning):
    """ Warning category for a function which has been renamed/moved. """
    pass


def _deprecated_alias(func, old_name, new_name):
    """

    A function for creating an alias to a renamed or moved function.

    Parameters
    ----------
    func : func
        The function which has been renamed or moved.
    old_name, new_name : str
        Name of the function before and after it was moved or renamed
        (with namespace if changed).

    Returns
    -------
    wrapper : func
        A wrapper version of func, which issues a DeprecatedFunctionName
        warning when the called.

    """
    def wrapper(*args, **kwargs):
        warnings.warn(
            ("{0} has been deprecated and will be removed in future " +
             "versions of Py-ART, pleases use {1}. ").format(
                old_name, new_name), category=DeprecatedFunctionName)
        return func(*args, **kwargs)
    return wrapper

"""
pyart.io.nexrad_archive
=======================

Functions for reading NEXRAD Level II Archive files.

.. autosummary::
    :toctree: generated/
    :template: dev_template.rst

    _NEXRADLevel2StagedField

.. autosummary::
    :toctree: generated/

    read_nexrad_archive
    _find_range_params
    _find_scans_to_interp
    _interpolate_scan

"""

import warnings

import numpy as np

from ..config import FileMetadata, get_fillvalue
from ..core.radar import Radar
from .common import make_time_unit_str, _test_arguments, prepare_for_read
from .nexrad_level2 import NEXRADLevel2File
from ..lazydict import LazyLoadDict
from .nexrad_common import get_nexrad_location


def read_nexrad_archive(filename, field_names=None, additional_metadata=None,
                        file_field_names=False, exclude_fields=None,
                        delay_field_loading=False, station=None, scans=None,
                        linear_interp=True, **kwargs):
    """
    Read a NEXRAD Level 2 Archive file.

    Parameters
    ----------
    filename : str
        Filename of NEXRAD Level 2 Archive file.  The files hosted by
        at the NOAA National Climate Data Center [1]_ as well as on the
        UCAR THREDDS Data Server [2]_ have been tested.  Other NEXRAD
        Level 2 Archive files may or may not work.  Message type 1 file
        and message type 31 files are supported.
    field_names : dict, optional
        Dictionary mapping NEXRAD moments to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        metadata configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduct any addition metadata and the file specific or default
        metadata as specified by the metadata configuration file will be used.
    file_field_names : bool, optional
        True to use the NEXRAD field names for the field names. If this
        case the field_names parameter is ignored. The field dictionary will
        likely only have a 'data' key, unless the fields are defined in
        `additional_metadata`.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters.
    delay_field_loading : bool, optional
        True to delay loading of field data from the file until the 'data'
        key in a particular field dictionary is accessed.  In this case
        the field attribute of the returned Radar object will contain
        LazyLoadDict objects not dict objects.
    station : str or None, optional
        Four letter ICAO name of the NEXRAD station used to determine the
        location in the returned radar object.  This parameter is only
        used when the location is not contained in the file, which occur
        in older NEXRAD message 1 files.
    scans : list or None, optional
        Read only specified scans from the file.  None (the default) will read
        all scans.
    linear_interp : bool, optional
        True (the default) to perform linear interpolation between valid pairs
        of gates in low resolution rays in files mixed resolution rays.
        False will perform a nearest neighbor interpolation.  This parameter is
        not used if the resolution of all rays in the file or requested sweeps
        is constant.

    Returns
    -------
    radar : Radar
        Radar object containing all moments and sweeps/cuts in the volume.
        Gates not collected are masked in the field data.

    References
    ----------
    .. [1] http://www.ncdc.noaa.gov/
    .. [2] http://thredds.ucar.edu/thredds/catalog.html

    """
    # test for non empty kwargs
    _test_arguments(kwargs)

    # create metadata retrieval object
    filemetadata = FileMetadata('nexrad_archive', field_names,
                                additional_metadata, file_field_names,
                                exclude_fields)

    # open the file and retrieve scan information
    nfile = NEXRADLevel2File(prepare_for_read(filename))
    scan_info = nfile.scan_info(scans)

    # time
    time = filemetadata('time')
    time_start, _time = nfile.get_times(scans)
    time['data'] = _time
    time['units'] = make_time_unit_str(time_start)

    # range
    _range = filemetadata('range')
    first_gate, gate_spacing, last_gate = _find_range_params(
        scan_info, filemetadata)
    _range['data'] = np.arange(first_gate, last_gate, gate_spacing, 'float32')
    _range['meters_to_center_of_first_gate'] = float(first_gate)
    _range['meters_between_gates'] = float(gate_spacing)

    # metadata
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'NEXRAD Level II'

    # scan_type
    scan_type = 'ppi'

    # latitude, longitude, altitude
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')

    if nfile._msg_type == '1' and station is not None:
        lat, lon, alt = get_nexrad_location(station)
    else:
        lat, lon, alt = nfile.location()
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([alt], dtype='float64')

    # sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index
    # sweep_end_ray_index
    sweep_number = filemetadata('sweep_number')
    sweep_mode = filemetadata('sweep_mode')
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')

    if scans is None:
        nsweeps = int(nfile.nscans)
    else:
        nsweeps = len(scans)
    sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    sweep_mode['data'] = np.array(
        nsweeps * ['azimuth_surveillance'], dtype='S')

    rays_per_scan = [s['nrays'] for s in scan_info]
    sweep_end_ray_index['data'] = np.cumsum(rays_per_scan, dtype='int32') - 1

    rays_per_scan.insert(0, 0)
    sweep_start_ray_index['data'] = np.cumsum(
        rays_per_scan[:-1], dtype='int32')

    # azimuth, elevation, fixed_angle
    azimuth = filemetadata('azimuth')
    elevation = filemetadata('elevation')
    fixed_angle = filemetadata('fixed_angle')
    azimuth['data'] = nfile.get_azimuth_angles(scans)
    elevation['data'] = nfile.get_elevation_angles(scans).astype('float32')
    fixed_angle['data'] = nfile.get_target_angles(scans)

    # fields
    max_ngates = len(_range['data'])
    available_moments = set([m for scan in scan_info for m in scan['moments']])
    interpolate = _find_scans_to_interp(
        scan_info, first_gate, gate_spacing, filemetadata)

    fields = {}
    for moment in available_moments:
        field_name = filemetadata.get_field_name(moment)
        if field_name is None:
            continue
        dic = filemetadata(field_name)
        dic['_FillValue'] = get_fillvalue()
        if delay_field_loading and moment not in interpolate:
            dic = LazyLoadDict(dic)
            data_call = _NEXRADLevel2StagedField(
                nfile, moment, max_ngates, scans)
            dic.set_lazy('data', data_call)
        else:
            mdata = nfile.get_data(moment, max_ngates, scans=scans)
            if moment in interpolate:
                interp_scans = interpolate[moment]
                warnings.warn(
                    "Gate spacing is not constant, interpolating data in " +
                    "scans %s for moment %s." % (interp_scans, moment),
                    UserWarning)
                for scan in interp_scans:
                    idx = scan_info[scan]['moments'].index(moment)
                    moment_ngates = scan_info[scan]['ngates'][idx]
                    start = sweep_start_ray_index['data'][scan]
                    end = sweep_end_ray_index['data'][scan]
                    _interpolate_scan(mdata, start, end, moment_ngates,
                                      linear_interp)
            dic['data'] = mdata
        fields[field_name] = dic

    # instrument_parameters
    nyquist_velocity = filemetadata('nyquist_velocity')
    unambiguous_range = filemetadata('unambiguous_range')
    nyquist_velocity['data'] = nfile.get_nyquist_vel(scans).astype('float32')
    unambiguous_range['data'] = (
        nfile.get_unambigous_range(scans).astype('float32'))

    instrument_parameters = {'unambiguous_range': unambiguous_range,
                             'nyquist_velocity': nyquist_velocity, }

    nfile.close()
    return Radar(
        time, _range, fields, metadata, scan_type,
        latitude, longitude, altitude,
        sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index,
        sweep_end_ray_index,
        azimuth, elevation,
        instrument_parameters=instrument_parameters)


def _find_range_params(scan_info, filemetadata):
    """ Return range parameters, first_gate, gate_spacing, last_gate. """
    min_first_gate = 999999
    min_gate_spacing = 999999
    max_last_gate = 0
    for scan_params in scan_info:
        ngates = scan_params['ngates'][0]
        for i, moment in enumerate(scan_params['moments']):
            if filemetadata.get_field_name(moment) is None:
                # moment is not read, skip
                continue
            first_gate = scan_params['first_gate'][i]
            gate_spacing = scan_params['gate_spacing'][i]
            last_gate = first_gate + gate_spacing * (ngates - 0.5)

            min_first_gate = min(min_first_gate, first_gate)
            min_gate_spacing = min(min_gate_spacing, gate_spacing)
            max_last_gate = max(max_last_gate, last_gate)
    return min_first_gate, min_gate_spacing, max_last_gate


def _find_scans_to_interp(scan_info, first_gate, gate_spacing, filemetadata):
    """ Return a dict indicating what moments/scans need interpolation.  """
    moments = set([m for scan in scan_info for m in scan['moments']])
    interpolate = dict([(moment, []) for moment in moments])
    for scan_num, scan in enumerate(scan_info):
        for moment in moments:
            if moment not in scan['moments']:
                continue
            if filemetadata.get_field_name(moment) is None:
                # moment is not read, skip
                continue
            index = scan['moments'].index(moment)
            first = scan['first_gate'][index]
            spacing = scan['gate_spacing'][index]
            if first != first_gate or spacing != gate_spacing:
                interpolate[moment].append(scan_num)
                # for proper interpolation the gate spacing of the scan to be
                # interpolated should be 1/4th the spacing of the radar
                assert spacing == gate_spacing * 4
                # and the first gate for the scan should be one and half times
                # the radar spacing past the radar first gate
                assert first_gate + 1.5 * gate_spacing == first
    # remove moments with no scans needing interpolation
    interpolate = dict([(k, v) for k, v in interpolate.items() if len(v) != 0])
    return interpolate


def _interpolate_scan(mdata, start, end, moment_ngates, linear_interp=True):
    """ Interpolate a single NEXRAD moment scan from 1000 m to 250 m. """
    # This interpolation scheme is only valid for NEXRAD data where a 4:1
    # (1000 m : 250 m) interpolation is needed.
    #
    # The scheme here performs a linear interpolation between pairs of gates
    # in a ray when the both of the gates are not masked (below threshold).
    # When one of the gates is masked the interpolation changes to a nearest
    # neighbor interpolation. Nearest neighbor is also performed at the end
    # points until the new range bin would be centered beyond half of the range
    # spacing of the original range.
    #
    # Nearest neighbor interpolation is performed when linear_interp is False,
    # this is equivalent to repeating each gate four times in each ray.
    #
    # No transformation of the raw data is performed prior to interpolation, so
    # reflectivity will be interpolated in dB units, velocity in m/s, etc,
    # this may not be the best method for interpolation.
    #
    # This method was adapted from Radx
    for ray_num in range(start, end+1):
        ray = mdata[ray_num].copy()

        # repeat each gate value 4 times
        interp_ngates = 4 * moment_ngates
        ray[:interp_ngates] = np.repeat(ray[:moment_ngates], 4)

        if linear_interp:
            # linear interpolate
            for i in range(2, interp_ngates - 4, 4):
                gate_val = ray[i]
                next_val = ray[i+4]
                if np.ma.is_masked(gate_val) or np.ma.is_masked(next_val):
                    continue
                delta = (next_val - gate_val) / 4.
                ray[i+0] = gate_val + delta * 0.5
                ray[i+1] = gate_val + delta * 1.5
                ray[i+2] = gate_val + delta * 2.5
                ray[i+3] = gate_val + delta * 3.5

        mdata[ray_num] = ray[:]


class _NEXRADLevel2StagedField(object):
    """
    A class to facilitate on demand loading of field data from a Level 2 file.
    """

    def __init__(self, nfile, moment, max_ngates, scans):
        """ initialize. """
        self.nfile = nfile
        self.moment = moment
        self.max_ngates = max_ngates
        self.scans = scans

    def __call__(self):
        """ Return the array containing the field data. """
        return self.nfile.get_data(
            self.moment, self.max_ngates, scans=self.scans)

"""
pyart.io.uf
===========

Reading of Universal format (UF) files

.. autosummary::
    :toctree: generated/

    read_uf
    _get_instrument_parameters

"""

import warnings

import numpy as np
from netCDF4 import date2num

from ..config import FileMetadata, get_fillvalue
from ..core.radar import Radar
from .common import make_time_unit_str, _test_arguments, prepare_for_read
from .uffile import UFFile

_LIGHT_SPEED = 2.99792458e8  # speed of light in meters per second
_UF_SWEEP_MODES = {
    0: 'calibration',
    1: 'ppi',
    2: 'coplane',
    3: 'rhi',
    4: 'vpt',
    5: 'target',
    6: 'manual',
    7: 'idle',
}

_SWEEP_MODE_STR = {
    'calibration': 'calibration',
    'ppi': 'azimuth_surveillance',
    'coplane': 'coplane',
    'rhi': 'rhi',
    'vpt': 'vertical_pointing',
    'target': 'pointing',
    'manual': 'manual',
    'idle': 'idle',
}


def read_uf(filename, field_names=None, additional_metadata=None,
            file_field_names=False, exclude_fields=None,
            delay_field_loading=False, **kwargs):
    """
    Read a UF File.

    Parameters
    ----------
    filename : str or file-like
        Name of Universal format file to read data from.
    field_names : dict, optional
        Dictionary mapping UF data type names to radar field names. If a
        data type found in the file does not appear in this dictionary or has
        a value of None it will not be placed in the radar.fields dictionary.
        A value of None, the default, will use the mapping defined in the
        Py-ART configuration file.
    additional_metadata : dict of dicts, optional
        Dictionary of dictionaries to retrieve metadata from during this read.
        This metadata is not used during any successive file reads unless
        explicitly included.  A value of None, the default, will not
        introduce any addition metadata and the file specific or default
        metadata as specified by the Py-ART configuration file will be used.
    file_field_names : bool, optional
        True to force the use of the field names from the file in which
        case the `field_names` parameter is ignored. False will use to
        `field_names` parameter to rename fields.
    exclude_fields : list or None, optional
        List of fields to exclude from the radar object. This is applied
        after the `file_field_names` and `field_names` parameters.
    delay_field_loading : bool
        This option is not implemented in the function but included for
        compatibility.

    Returns
    -------
    radar : Radar
        Radar object.

    """
    # test for non empty kwargs
    _test_arguments(kwargs)

    # create metadata retrieval object
    filemetadata = FileMetadata('uf', field_names, additional_metadata,
                                file_field_names, exclude_fields)

    # Open UF file and get handle
    ufile = UFFile(prepare_for_read(filename))
    first_ray = ufile.rays[0]

    # time
    dts = ufile.get_datetimes()
    units = make_time_unit_str(min(dts))
    time = filemetadata('time')
    time['units'] = units
    time['data'] = date2num(dts, units).astype('float32')

    # range
    _range = filemetadata('range')
    # assume that the number of gates and spacing from the first ray is
    # representative of the entire volume
    field_header = first_ray.field_headers[0]
    ngates = field_header['nbins']
    step = field_header['range_spacing_m']
    # this gives distances to the center of each gate, remove step/2 for start
    start = (field_header['range_start_km'] * 1000. +
             field_header['range_start_m'] + step / 2.)
    _range['data'] = np.arange(ngates, dtype='float32') * step + start
    _range['meters_to_center_of_first_gate'] = start
    _range['meters_between_gates'] = step

    # latitude, longitude and altitude
    latitude = filemetadata('latitude')
    longitude = filemetadata('longitude')
    altitude = filemetadata('altitude')
    lat, lon, height = first_ray.get_location()
    latitude['data'] = np.array([lat], dtype='float64')
    longitude['data'] = np.array([lon], dtype='float64')
    altitude['data'] = np.array([height], dtype='float64')

    # metadata
    metadata = filemetadata('metadata')
    metadata['original_container'] = 'UF'
    metadata['site_name'] = first_ray.mandatory_header['site_name']
    metadata['radar_name'] = first_ray.mandatory_header['radar_name']

    # sweep_start_ray_index, sweep_end_ray_index
    sweep_start_ray_index = filemetadata('sweep_start_ray_index')
    sweep_end_ray_index = filemetadata('sweep_end_ray_index')
    sweep_start_ray_index['data'] = ufile.first_ray_in_sweep
    sweep_end_ray_index['data'] = ufile.last_ray_in_sweep

    # sweep number
    sweep_number = filemetadata('sweep_number')
    sweep_number['data'] = np.arange(ufile.nsweeps, dtype='int32')

    # sweep_type
    scan_type = _UF_SWEEP_MODES[first_ray.mandatory_header['sweep_mode']]

    # sweep_mode
    sweep_mode = filemetadata('sweep_mode')
    sweep_mode['data'] = np.array(
        ufile.nsweeps * [_SWEEP_MODE_STR[scan_type]], dtype='S')

    # elevation
    elevation = filemetadata('elevation')
    elevation['data'] = ufile.get_elevations()

    # azimuth
    azimuth = filemetadata('azimuth')
    azimuth['data'] = ufile.get_azimuths()

    # fixed_angle
    fixed_angle = filemetadata('fixed_angle')
    fixed_angle['data'] = ufile.get_sweep_fixed_angles()

    # fields
    fields = {}
    for uf_field_number, uf_field_dic in enumerate(first_ray.field_positions):
        uf_field_name = uf_field_dic['data_type'].decode('ascii')
        field_name = filemetadata.get_field_name(uf_field_name)
        if field_name is None:
            continue
        field_dic = filemetadata(field_name)
        field_dic['data'] = ufile.get_field_data(uf_field_number)
        field_dic['_FillValue'] = get_fillvalue()
        fields[field_name] = field_dic

    # instrument_parameters
    instrument_parameters = _get_instrument_parameters(ufile, filemetadata)

    # scan rate
    scan_rate = filemetadata('scan_rate')
    scan_rate['data'] = ufile.get_sweep_rates()

    ufile.close()
    return Radar(
        time, _range, fields, metadata, scan_type,
        latitude, longitude, altitude,
        sweep_number, sweep_mode, fixed_angle, sweep_start_ray_index,
        sweep_end_ray_index,
        azimuth, elevation,
        scan_rate=scan_rate,
        instrument_parameters=instrument_parameters)


def _get_instrument_parameters(ufile, filemetadata):
    """ Return a dictionary containing instrument parameters. """

    # pulse width
    pulse_width = filemetadata('pulse_width')
    pulse_width['data'] = ufile.get_pulse_widths() / _LIGHT_SPEED  # m->sec

    # assume that the parameters in the first ray represent the beam widths,
    # bandwidth and frequency in the entire volume
    first_ray = ufile.rays[0]
    field_header = first_ray.field_headers[0]
    beam_width_h = field_header['beam_width_h'] / 64.
    beam_width_v = field_header['beam_width_v'] / 64.
    bandwidth = field_header['bandwidth'] / 16. * 1.e6
    wavelength_cm = field_header['wavelength_cm'] / 64.
    if wavelength_cm == 0:
        warnings.warn('Invalid wavelength, frequency set to default value.')
        wavelength_hz = 9999.0
    else:
        wavelength_hz = _LIGHT_SPEED / (wavelength_cm / 100.)

    # radar_beam_width_h
    radar_beam_width_h = filemetadata('radar_beam_width_h')
    radar_beam_width_h['data'] = np.array([beam_width_h], dtype='float32')

    # radar_beam_width_v
    radar_beam_width_v = filemetadata('radar_beam_width_w')
    radar_beam_width_v['data'] = np.array([beam_width_v], dtype='float32')

    # radar_receiver_bandwidth
    radar_receiver_bandwidth = filemetadata('radar_receiver_bandwidth')
    radar_receiver_bandwidth['data'] = np.array([bandwidth], dtype='float32')

    # polarization_mode
    polarization_mode = filemetadata('polarization_mode')
    polarization_mode['data'] = ufile.get_sweep_polarizations()

    # frequency
    frequency = filemetadata('frequency')
    frequency['data'] = np.array([wavelength_hz], dtype='float32')

    # prt
    prt = filemetadata('prt')
    prt['data'] = ufile.get_prts() / 1e6  # us->sec

    instrument_parameters = {
        'pulse_width': pulse_width,
        'radar_beam_width_h': radar_beam_width_h,
        'radar_beam_width_v': radar_beam_width_v,
        'radar_receiver_bandwidth': radar_receiver_bandwidth,
        'polarization_mode': polarization_mode,
        'frequency': frequency,
        'prt': prt,
    }

    # nyquist velocity if defined
    nyquist_velocity = filemetadata('nyquist_velocity')
    nyquist_velocity['data'] = ufile.get_nyquists()
    if nyquist_velocity['data'] is not None:
        instrument_parameters['nyquist_velocity'] = nyquist_velocity

    return instrument_parameters

#! /usr/bin/env python
"""
Make a small netCDF CF/Radial file containing a single RHI scan.

Single field and scan is converted from sigmet file XSW110520113537.RAW7HHL
"""

import pyart

radar = pyart.io.read_rsl('XSW110520113537.RAW7HHL')

time_slice = slice(None, 713, 18)
range_slice = slice(None, None, 12)
sweep_slice = slice(None, 1)

# remove all but the reflectivity_horizontal fields
rf_field = radar.fields['reflectivity']
rf_data = rf_field['data']
rf_field['data'] = rf_data[time_slice, range_slice]
radar.fields = {'reflectivity_horizontal': rf_field}

radar.nsweeps = 1
radar.nray = 40
radar.ngates = 45

# truncate the range based variables
radar.range['data'] = radar.range['data'][range_slice]

# truncate the time based variables
radar.time['data'] = radar.time['data'][time_slice]
radar.azimuth['data'] = radar.azimuth['data'][time_slice]
radar.elevation['data'] = radar.elevation['data'][time_slice]
radar.instrument_parameters['prt']['data'] = \
    radar.instrument_parameters['prt']['data'][time_slice]

radar.instrument_parameters['unambiguous_range']['data'] = \
    radar.instrument_parameters['unambiguous_range']['data'][time_slice]

radar.instrument_parameters['nyquist_velocity']['data'] = \
    radar.instrument_parameters['nyquist_velocity']['data'][time_slice]

# truncate the sweep based variables
radar.sweep_number['data'] = radar.sweep_number['data'][sweep_slice]
radar.fixed_angle['data'] = radar.fixed_angle['data'][sweep_slice]
radar.sweep_start_ray_index['data'] = \
    radar.sweep_start_ray_index['data'][sweep_slice]
radar.sweep_end_ray_index['data'] = \
    radar.sweep_end_ray_index['data'][sweep_slice]
radar.sweep_end_ray_index['data'][0] = 39
radar.sweep_mode['data'] = radar.sweep_mode['data'][sweep_slice]

radar.sweep_number['data'] = radar.sweep_number['data'][sweep_slice]

radar.instrument_parameters['prt_mode']['data'] = \
    radar.instrument_parameters['prt_mode']['data'][sweep_slice]

# adjust metadata
radar.metadata = {
    'Conventions': 'CF/Radial instrument_parameters',
    'version': '1.2',
    'title': 'Py-ART Example RHI CF/Radial file',
    'institution': ('United States Department of Energy - Atmospheric '
                    'Radiation Measurement (ARM) program'),
    'references': 'none',
    'source': 'ARM SGP XSAPR Radar',
    'history': 'created by jhelmus on evs348532 at 2013-05-22T12:34:56',
    'comment': 'none',
    'instrument_name': 'xsapr-sgp'}

pyart.io.write_cfradial('example_cfradial_rhi.nc', radar)

"""
pyart.util.radar_utils
======================

Functions for working radar instances.

.. autosummary::
    :toctree: generated/

    is_vpt
    to_vpt
    join_radar
"""

from __future__ import print_function

import copy

import numpy as np
from netCDF4 import num2date, date2num

from . import datetime_utils


def is_vpt(radar, offset=0.5):
    """
    Determine if a Radar appears to be a vertical pointing scan.

    This function only verifies that the object is a vertical pointing scan,
    use the :py:func:`to_vpt` function to convert the radar to a vpt scan
    if this function returns True.

    Parameters
    ----------
    radar : Radar
        Radar object to determine if
    offset : float
        Maximum offset of the elevation from 90 degrees to still consider
        to be vertically pointing.

    Returns
    -------
    flag : bool
        True if the radar appear to be verticle pointing, False if not.

    """
    # check that the elevation is within offset of 90 degrees.
    elev = radar.elevation['data']
    return np.all((elev < 90.0 + offset) & (elev > 90.0 - offset))


def to_vpt(radar, single_scan=True):
    """
    Convert an existing Radar object to represent a vertical pointing scan.

    This function does not verify that the Radar object contains a vertical
    pointing scan.  To perform such a check use :py:func:`is_vpt`.

    Parameters
    ----------
    radar : Radar
        Mislabeled vertical pointing scan Radar object to convert to be
        properly labeled.  This object is converted in place, no copy of
        the existing data is made.
    single_scan : bool, optional
        True to convert the volume to a single scan, any azimuth angle data
        is lost.  False will convert the scan to contain the same number of
        scans as rays, azimuth angles are retained.

    """
    if single_scan:
        nsweeps = 1
        radar.azimuth['data'][:] = 0.0
        seri = np.array([radar.nrays - 1], dtype='int32')
        radar.sweep_end_ray_index['data'] = seri
    else:
        nsweeps = radar.nrays
        # radar.azimuth not adjusted
        radar.sweep_end_ray_index['data'] = np.arange(nsweeps, dtype='int32')

    radar.scan_type = 'vpt'
    radar.nsweeps = nsweeps
    radar.target_scan_rate = None       # no scanning
    radar.elevation['data'][:] = 90.0

    radar.sweep_number['data'] = np.arange(nsweeps, dtype='int32')
    radar.sweep_mode['data'] = np.array(['vertical_pointing'] * nsweeps)
    radar.fixed_angle['data'] = np.ones(nsweeps, dtype='float32') * 90.0
    radar.sweep_start_ray_index['data'] = np.arange(nsweeps, dtype='int32')

    if radar.instrument_parameters is not None:
        for key in ['prt_mode', 'follow_mode', 'polarization_mode']:
            if key in radar.instrument_parameters:
                ip_dic = radar.instrument_parameters[key]
                ip_dic['data'] = np.array([ip_dic['data'][0]] * nsweeps)

    # Attributes that do not need any changes
    # radar.altitude
    # radar.altitude_agl
    # radar.latitude
    # radar.longitude

    # radar.range
    # radar.ngates
    # radar.nrays

    # radar.metadata
    # radar.radar_calibration

    # radar.time
    # radar.fields
    # radar.antenna_transition
    # radar.scan_rate
    return


def join_radar(radar1, radar2):
    """
    Combine two radar instances into one.

    Parameters
    ----------
    radar1 : Radar
        Radar object.
    radar2 : Radar
        Radar object.
    """
    # must have same gate spacing
    new_radar = copy.deepcopy(radar1)
    new_radar.azimuth['data'] = np.append(radar1.azimuth['data'],
                                          radar2.azimuth['data'])
    new_radar.elevation['data'] = np.append(radar1.elevation['data'],
                                            radar2.elevation['data'])

    if len(radar1.range['data']) >= len(radar2.range['data']):
        new_radar.range['data'] = radar1.range['data']
    else:
        new_radar.range['data'] = radar2.range['data']

    # to combine times we need to reference them to a standard
    # for this we'll use epoch time
    estring = "seconds since 1970-01-01T00:00:00Z"
    r1dt = num2date(radar1.time['data'], radar1.time['units'])
    r2dt = num2date(radar2.time['data'], radar2.time['units'])
    r1num = datetime_utils.datetimes_from_radar(radar1, epoch=True)
    r2num = datetime_utils.datetimes_from_radar(radar2, epoch=True)
    new_radar.time['data'] = np.append(r1num, r2num)
    new_radar.time['units'] = datetime_utils.EPOCH_UNITS

    for var in new_radar.fields.keys():
        sh1 = radar1.fields[var]['data'].shape
        sh2 = radar2.fields[var]['data'].shape
        new_field = np.ma.zeros([sh1[0] + sh2[0],
                                max([sh1[1], sh2[1]])]) - 9999.0
        new_field[0:sh1[0], 0:sh1[1]] = radar1.fields[var]['data']
        new_field[sh1[0]:, 0:sh2[1]] = radar2.fields[var]['data']
        new_radar.fields[var]['data'] = new_field

    # radar locations
    # TODO moving platforms - any more?
    if (len(radar1.latitude['data']) == 1 &
            len(radar2.latitude['data']) == 1 &
            len(radar1.longitude['data']) == 1 &
            len(radar2.longitude['data']) == 1 &
            len(radar1.altitude['data']) == 1 &
            len(radar2.altitude['data']) == 1):

        lat1 = float(radar1.latitude['data'])
        lon1 = float(radar1.longitude['data'])
        alt1 = float(radar1.altitude['data'])
        lat2 = float(radar2.latitude['data'])
        lon2 = float(radar2.longitude['data'])
        alt2 = float(radar2.altitude['data'])

        if (lat1 != lat2) or (lon1 != lon2) or (alt1 != alt2):
            ones1 = np.ones(len(radar1.time['data']), dtype='float32')
            ones2 = np.ones(len(radar2.time['data']), dtype='float32')
            new_radar.latitude['data'] = np.append(ones1 * lat1, ones2 * lat2)
            new_radar.longitude['data'] = np.append(ones1 * lon1, ones2 * lon2)
            new_radar.latitude['data'] = np.append(ones1 * alt1, ones2 * alt2)
        else:
            new_radar.latitude['data'] = radar1.latitude['data']
            new_radar.longitude['data'] = radar1.longitude['data']
            new_radar.altitude['data'] = radar1.altitude['data']

    else:
        new_radar.latitude['data'] = np.append(radar1.latitude['data'],
                                               radar2.latitude['data'])
        new_radar.longitude['data'] = np.append(radar1.longitude['data'],
                                                radar2.longitude['data'])
        new_radar.altitude['data'] = np.append(radar1.altitude['data'],
                                               radar2.altitude['data'])
    return new_radar

"""
Default config for Workload Automation. DO NOT MODIFY this file. This file
gets copied to ~/.workload_automation/config.py on initial run of run_workloads.
Add your configuration to that file instead.

"""
#  *** WARNING: ***
# Configuration listed in this file is NOT COMPLETE. This file sets the default
# configuration for WA and gives EXAMPLES of other configuration available. It
# is not supposed to be an exhaustive list.
# PLEASE REFER TO WA DOCUMENTATION FOR THE COMPLETE LIST OF AVAILABLE
# EXTENSIONS AND THEIR CONFIGURATION.


# This defines when the device will be rebooted during Workload Automation execution.              #
#                                                                                                  #
# Valid policies are:                                                                              #
#   never:  The device will never be rebooted.                                                     #
#   as_needed: The device will only be rebooted if the need arises (e.g. if it                     #
#              becomes unresponsive                                                                #
#   initial: The device will be rebooted when the execution first starts, just before executing    #
#            the first workload spec.                                                              #
#   each_spec: The device will be rebooted before running a new workload spec.                     #
#   each_iteration: The device will be rebooted before each new iteration.                         #
#                                                                                                  #
reboot_policy = 'as_needed'

#  Defines the order in which the agenda spec will be executed. At the moment,                     #
#  the following execution orders are supported:                                                   #
#                                                                                                  #
#   by_iteration: The first iteration of each workload spec is executed one ofter the other,       #
#                 so all workloads are executed before proceeding on to the second iteration.      #
#                 This is the default if no order is explicitly specified.                         #
#                 If multiple sections were specified, this will also split them up, so that specs #
#                 in the same section are further apart in the execution order.                    #
#   by_section:   Same as "by_iteration", but runn specs from the same section one after the other #
#   by_spec:      All iterations of the first spec are executed before moving on to the next       #
#                 spec. This may also be specified as ``"classic"``, as this was the way           #
#                 workloads were executed in earlier versions of WA.                               #
#   random:       Randomisizes the order in which specs run.                                       #
execution_order = 'by_iteration'


# This indicates when a job will be re-run.
# Possible values:
#     OK: This iteration has completed and no errors have been detected
#     PARTIAL: One or more instruments have failed (the iteration may still be running).
#     FAILED: The workload itself has failed.
#     ABORTED: The user interupted the workload
#
# If set to an empty list, a job will not be re-run ever.
retry_on_status = ['FAILED', 'PARTIAL']

# How many times a job will be re-run before giving up
max_retries = 3

####################################################################################################
######################################### Device Settings ##########################################
####################################################################################################
# Specify the device you want to run workload automation on. This must be a                        #
# string with the ID of the device. At the moment, only 'TC2' is supported.                        #
#                                                                                                  #
device = 'generic_android'

# Configuration options that will be passed onto the device. These are obviously device-specific,  #
# so check the documentation for the particular device to find out which options and values are    #
# valid. The settings listed below are common to all devices                                       #
#                                                                                                  #
device_config = dict(
    # The name used by adb to identify the device. Use "adb devices" in bash to list
    # the devices currently seen by adb.
    #adb_name='10.109.173.2:5555',

    # The directory on the device that WA will use to push files to
    #working_directory='/sdcard/wa-working',

    # This specifies the device's CPU cores. The order must match how they
    # appear in cpufreq. The example below is for TC2.
    # core_names = ['a7', 'a7', 'a7', 'a15', 'a15']

    # Specifies cluster mapping for the device's cores.
    # core_clusters = [0, 0, 0, 1, 1]
)


####################################################################################################
################################### Instrumention Configuration ####################################
####################################################################################################
# This defines the additionnal instrumentation that will be enabled during workload execution,     #
# which in turn determines what additional data (such as /proc/interrupts content or Streamline    #
# traces) will be available in the results directory.                                              #
#                                                                                                  #
instrumentation = [
    # Records the time it took to run the workload
    'execution_time',

    # Collects /proc/interrupts before and after execution and does a diff.
    'interrupts',

    # Collects the contents of/sys/devices/system/cpu before and after execution and does a diff.
    'cpufreq',

    # Gets energy usage from the workload form HWMON devices
    # NOTE: the hardware needs to have the right sensors in order for this to work
    #'hwmon',

    # Run perf in the background during workload execution and then collect the results. perf is a
    # standard Linux performance analysis tool.
    #'perf',

    # Collect Streamline traces during workload execution. Streamline is part of DS-5
    #'streamline',

    # Collects traces by interacting with Ftrace Linux kernel internal tracer
    #'trace-cmd',

    # Obtains the power consumption of the target device's core measured by National Instruments
    # Data Acquisition(DAQ) device.
    #'daq',

    # Collects CCI counter data.
    #'cci_pmu_logger',

    # Collects FPS (Frames Per Second) and related metrics (such as jank) from
    # the View of the workload (Note: only a single View per workload is
    # supported at the moment, so this is mainly useful for games).
    #'fps',
]


####################################################################################################
################################# Result Processors Configuration ##################################
####################################################################################################
# Specifies how results will be processed and presented.                                           #
#                                                                                                  #
result_processors = [
    # Creates a status.txt that provides a summary status for the run
    'status',

    # Creates a results.txt file for each iteration that lists all collected metrics
    # in "name = value (units)" format
    'standard',

    # Creates a results.csv that contains metrics for all iterations of all workloads
    # in the .csv format.
    'csv',

    # Creates a summary.csv that contains summary metrics for all iterations of all
    # all in the .csv format. Summary metrics are defined on per-worklod basis
    # are typically things like overall scores. The contents of summary.csv are
    # always a subset of the contents of results.csv (if it is generated).
    #'summary_csv',

    # Creates a results.csv that contains metrics for all iterations of all workloads
    # in the JSON format
    #'json',

    # Write results to an sqlite3 database. By default, a new database will be
    # generated for each run, however it is possible to specify a path to an
    # existing DB file (see result processor configuration below), in which
    # case results from multiple runs may be stored in the one file.
    #'sqlite',
]


####################################################################################################
################################### Logging output Configuration ###################################
####################################################################################################
# Specify the format of logging messages. The format uses the old formatting syntax:               #
#                                                                                                  #
#   http://docs.python.org/2/library/stdtypes.html#string-formatting-operations                    #
#                                                                                                  #
# The attributes that can be used in formats are listested here:                                   #
#                                                                                                  #
#   http://docs.python.org/2/library/logging.html#logrecord-attributes                             #
#                                                                                                  #
logging = {
    # Log file format
    'file format': '%(asctime)s %(levelname)-8s %(name)s: %(message)s',
    # Verbose console output format
    'verbose format': '%(asctime)s %(levelname)-8s %(name)s: %(message)s',
    # Regular console output format
    'regular format': '%(levelname)-8s %(message)s',
    # Colouring the console output
    'colour_enabled': True,
}


####################################################################################################
#################################### Instruments Configuration #####################################
####################################################################################################
# Instrumention Configuration is related to specific insturment's settings. Some of the            #
# instrumentations require specific settings in order for them to work. These settings are         #
# specified here.                                                                                  #
# Note that these settings only take effect if the corresponding instrument is
# enabled above.

####################################################################################################
######################################## perf configuration ########################################

# The hardware events such as instructions executed, cache-misses suffered, or branches
# mispredicted to be reported by perf. Events can be obtained from the device by tpying
# 'perf list'.
#perf_events = ['migrations', 'cs']

# The perf options which can be obtained from man page for perf-record
#perf_options = '-a -i'

####################################################################################################
####################################### hwmon configuration ########################################

# The kinds of sensors hwmon instrument will look for
#hwmon_sensors = ['energy', 'temp']

####################################################################################################
###################################### trace-cmd configuration #####################################

# trace-cmd events to be traced. The events can be found by rooting on the device then type
# 'trace-cmd list -e'
#trace_events = ['power*']

####################################################################################################
######################################### DAQ configuration ########################################

# The host address of the machine that runs the daq Server which the insturment communicates with
#daq_server_host = '10.1.17.56'

# The port number for daq Server in which daq insturment communicates with
#daq_server_port = 56788

# The values of resistors 1 and 2 (in Ohms) across which the voltages are measured
#daq_resistor_values = [0.002, 0.002]

####################################################################################################
################################### cci_pmu_logger configuration ###################################

# The events to be counted by PMU
# NOTE: The number of events must not exceed the number of counters available (which is 4 for CCI-400)
#cci_pmu_events = ['0x63', '0x83']

# The name of the events which will be used when reporting PMU counts
#cci_pmu_event_labels = ['event_0x63', 'event_0x83']

# The period (in jiffies) between counter reads
#cci_pmu_period = 15

####################################################################################################
################################### fps configuration ##############################################

# Data points below this FPS will dropped as not constituting "real" gameplay. The assumption
# being that while actually running, the FPS in the game will not drop below X frames per second,
# except on loading screens, menus, etc, which should not contribute to FPS calculation.
#fps_drop_threshold=5

# If set to True, this will keep the raw dumpsys output in the results directory (this is maily
# used for debugging). Note: frames.csv with collected frames data will always be generated
# regardless of this setting.
#fps_keep_raw=False

####################################################################################################
################################# Result Processor Configuration ###################################
####################################################################################################

# Specifies an alternative database to store results in. If the file does not
# exist, it will be created (the directiory of the file must exist however). If
# the file does exist, the results will be added to the existing data set (each
# run as a UUID, so results won't clash even if identical agendas were used).
# Note that in order for this to work, the version of the schema used to generate
# the DB file must match that of the schema used for the current run. Please
# see "What's new" secition in WA docs to check if the schema has changed in
# recent releases of WA.
#sqlite_database = '/work/results/myresults.sqlite'

# If the file specified by sqlite_database exists, setting this to True will
# cause that file to be overwritten rather than updated -- existing results in
# the file will be lost.
#sqlite_overwrite = False

# distribution: internal

####################################################################################################
#################################### Resource Getter configuration #################################
####################################################################################################

# The location on your system where /arm/scratch is mounted. Used by
# Scratch resource getter.
#scratch_mount_point = '/arm/scratch'

# end distribution

#    Copyright 2014-2015 ARM Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Original implementation by Rene de Jong. Updated by Sascha Bischoff.

import logging

from wlauto import LinuxDevice, Parameter
from wlauto.common.gem5.device import BaseGem5Device
from wlauto.utils import types


class Gem5LinuxDevice(BaseGem5Device, LinuxDevice):
    """
    Implements gem5 Linux device.

    This class allows a user to connect WA to a simulation using gem5. The
    connection to the device is made using the telnet connection of the
    simulator, and is used for all commands. The simulator does not have ADB
    support, and therefore we need to fall back to using standard shell
    commands.

    Files are copied into the simulation using a VirtIO 9P device in gem5. Files
    are copied out of the simulated environment using the m5 writefile command
    within the simulated system.

    When starting the workload run, the simulator is automatically started by
    Workload Automation, and a connection to the simulator is established. WA
    will then wait for Android to boot on the simulated system (which can take
    hours), prior to executing any other commands on the device. It is also
    possible to resume from a checkpoint when starting the simulation. To do
    this, please append the relevant checkpoint commands from the gem5
    simulation script to the gem5_discription argument in the agenda.

    Host system requirements:
        * VirtIO support. We rely on diod on the host system. This can be
          installed on ubuntu using the following command:

                sudo apt-get install diod

    Guest requirements:
        * VirtIO support. We rely on VirtIO to move files into the simulation.
          Please make sure that the following are set in the kernel
          configuration:

                CONFIG_NET_9P=y

                CONFIG_NET_9P_VIRTIO=y

                CONFIG_9P_FS=y

                CONFIG_9P_FS_POSIX_ACL=y

                CONFIG_9P_FS_SECURITY=y

                CONFIG_VIRTIO_BLK=y

        * m5 binary. Please make sure that the m5 binary is on the device and
          can by found in the path.
    """

    name = 'gem5_linux'
    platform = 'linux'

    parameters = [
        Parameter('core_names', default=[], override=True),
        Parameter('core_clusters', default=[], override=True),
        Parameter('host', default='localhost', override=True,
                  description='Host name or IP address for the device.'),
        Parameter('login_prompt', kind=types.list_of_strs,
                  default=['login:', 'AEL login:', 'username:'],
                  mandatory=False),
        Parameter('login_password_prompt', kind=types.list_of_strs,
                  default=['password:'], mandatory=False),
    ]

    # Overwritten from Device. For documentation, see corresponding method in
    # Device.

    def __init__(self, **kwargs):
        self.logger = logging.getLogger('Gem5LinuxDevice')
        LinuxDevice.__init__(self, **kwargs)
        BaseGem5Device.__init__(self)

    def login_to_device(self):
        # Wait for the login prompt
        prompt = self.login_prompt + [self.sckt.UNIQUE_PROMPT]
        i = self.sckt.expect(prompt, timeout=10)
        # Check if we are already at a prompt, or if we need to log in.
        if i < len(prompt) - 1:
            self.sckt.sendline("{}".format(self.username))
            password_prompt = self.login_password_prompt + [r'# ', self.sckt.UNIQUE_PROMPT]
            j = self.sckt.expect(password_prompt, timeout=self.delay)
            if j < len(password_prompt) - 2:
                self.sckt.sendline("{}".format(self.password))
                self.sckt.expect([r'# ', self.sckt.UNIQUE_PROMPT], timeout=self.delay)

    def capture_screen(self, filepath):
        if BaseGem5Device.capture_screen(self, filepath):
            return

        # If we didn't manage to do the above, call the parent class.
        self.logger.warning("capture_screen: falling back to parent class implementation")
        LinuxDevice.capture_screen(self, filepath)

    def initialize(self, context):
        self.resize_shell()
        self.deploy_m5(context, force=False)

"""Louie version information."""


NAME = 'Louie'
DESCRIPTION = 'Signal dispatching mechanism'
VERSION = '1.1'



#    Copyright 2013-2015 ARM Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# pylint: disable=attribute-defined-outside-init

import os
import sqlite3
import json
import uuid
from datetime import datetime, timedelta
from contextlib import contextmanager

from wlauto import ResultProcessor, settings, Parameter
from wlauto.exceptions import ResultProcessorError
from wlauto.utils.types import boolean


# IMPORTANT: when updating this schema, make sure to bump the version!
SCHEMA_VERSION = '0.0.2'
SCHEMA = [
    '''CREATE TABLE  runs (
        uuid text,
        start_time datetime,
        end_time datetime,
        duration integer
    )''',
    '''CREATE TABLE  workload_specs (
        id text,
        run_oid text,
        number_of_iterations integer,
        label text,
        workload_name text,
        boot_parameters text,
        runtime_parameters text,
        workload_parameters text
    )''',
    '''CREATE TABLE  metrics (
        spec_oid int,
        iteration integer,
        metric text,
        value text,
        units text,
        lower_is_better integer
    )''',
    '''CREATE VIEW results AS
       SELECT uuid as run_uuid, spec_id, label as workload, iteration, metric, value, units, lower_is_better
       FROM metrics AS m INNER JOIN (
            SELECT ws.OID as spec_oid, ws.id as spec_id, uuid, label
            FROM workload_specs AS ws INNER JOIN runs AS r ON ws.run_oid = r.OID
       ) AS wsr ON wsr.spec_oid = m.spec_oid
    ''',
    '''CREATE TABLE  __meta (
        schema_version text
    )''',
    '''INSERT INTO __meta VALUES ("{}")'''.format(SCHEMA_VERSION),
]


sqlite3.register_adapter(datetime, lambda x: x.isoformat())
sqlite3.register_adapter(timedelta, lambda x: x.total_seconds())
sqlite3.register_adapter(uuid.UUID, str)


class SqliteResultProcessor(ResultProcessor):

    name = 'sqlite'
    description = """
    Stores results in an sqlite database.

    This may be used accumulate results of multiple runs in a single file.

    """

    name = 'sqlite'
    parameters = [
        Parameter('database', default=None,
                  global_alias='sqlite_database',
                  description=""" Full path to the sqlite database to be used.  If this is not specified then
                                a new database file will be created in the output directory. This setting can be
                                used to accumulate results from multiple runs in a single database. If the
                                specified file does not exist, it will be created, however the directory of the
                                file must exist.

                                .. note:: The value must resolve to an absolute path,
                                            relative paths are not allowed; however the
                                            value may contain environment variables and/or
                                            the home reference ~.
                                """),
        Parameter('overwrite', kind=boolean, default=False,
                  global_alias='sqlite_overwrite',
                  description="""If ``True``, this will overwrite the database file
                                 if it already exists. If ``False`` (the default) data
                                 will be added to the existing file (provided schema
                                 versions match -- otherwise an error will be raised).
                              """),

    ]

    def initialize(self, context):
        self._last_spec = None
        self._run_oid = None
        self._spec_oid = None
        if not os.path.exists(self.database):
            self._initdb()
        elif self.overwrite:  # pylint: disable=no-member
            os.remove(self.database)
            self._initdb()
        else:
            self._validate_schema_version()
        self._update_run(context.run_info.uuid)

    def process_iteration_result(self, result, context):
        if self._last_spec != context.spec:
            self._update_spec(context.spec)
        metrics = [(self._spec_oid, context.current_iteration, m.name, str(m.value), m.units, int(m.lower_is_better))
                   for m in result.metrics]
        with self._open_connecton() as conn:
            conn.executemany('INSERT INTO metrics VALUES (?,?,?,?,?,?)', metrics)

    def process_run_result(self, result, context):
        info = context.run_info
        with self._open_connecton() as conn:
            conn.execute('''UPDATE runs SET start_time=?, end_time=?, duration=?
                            WHERE OID=?''', (info.start_time, info.end_time, info.duration, self._run_oid))

    def validate(self):
        if not self.database:  # pylint: disable=access-member-before-definition
            self.database = os.path.join(settings.output_directory, 'results.sqlite')
        self.database = os.path.expandvars(os.path.expanduser(self.database))

    def _initdb(self):
        with self._open_connecton() as conn:
            for command in SCHEMA:
                conn.execute(command)

    def _validate_schema_version(self):
        with self._open_connecton() as conn:
            try:
                c = conn.execute('SELECT schema_version FROM __meta')
                found_version = c.fetchone()[0]
            except sqlite3.OperationalError:
                message = '{} does not appear to be a valid WA results database.'.format(self.database)
                raise ResultProcessorError(message)
            if found_version != SCHEMA_VERSION:
                message = 'Schema version in {} ({}) does not match current version ({}).'
                raise ResultProcessorError(message.format(self.database, found_version, SCHEMA_VERSION))

    def _update_run(self, run_uuid):
        with self._open_connecton() as conn:
            conn.execute('INSERT INTO runs (uuid) VALUES (?)', (run_uuid,))
            conn.commit()
            c = conn.execute('SELECT OID FROM runs WHERE uuid=?', (run_uuid,))
            self._run_oid = c.fetchone()[0]

    def _update_spec(self, spec):
        self._last_spec = spec
        spec_tuple = (spec.id, self._run_oid, spec.number_of_iterations, spec.label, spec.workload_name,
                      json.dumps(spec.boot_parameters), json.dumps(spec.runtime_parameters),
                      json.dumps(spec.workload_parameters))
        with self._open_connecton() as conn:
            conn.execute('INSERT INTO workload_specs VALUES (?,?,?,?,?,?,?,?)', spec_tuple)
            conn.commit()
            c = conn.execute('SELECT OID FROM workload_specs WHERE run_oid=? AND id=?', (self._run_oid, spec.id))
            self._spec_oid = c.fetchone()[0]

    @contextmanager
    def _open_connecton(self):
        conn = sqlite3.connect(self.database)
        try:
            yield conn
        finally:
            conn.commit()

#    Copyright 2014-2015 ARM Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#


"""
This module contains utilities for implemening device hard reset
using Netio 230 series power switches. This utilizes the KSHELL connection.

"""

import telnetlib
import socket
import re
import time
import logging


logger = logging.getLogger('NetIO')


class NetioError(Exception):
    pass


class KshellConnection(object):

    response_regex = re.compile(r'^(\d+) (.*?)\r\n')
    delay = 0.5

    def __init__(self, host='ippowerbar', port=1234, timeout=None):
        """Parameters are passed into ``telnetlib.Telnet`` -- see Python docs."""
        self.host = host
        self.port = port
        self.conn = telnetlib.Telnet(host, port, timeout)
        time.sleep(self.delay)  # give time to respond
        output = self.conn.read_very_eager()
        if 'HELLO' not in output:
            raise NetioError('Could not connect: did not see a HELLO. Got: {}'.format(output))

    def login(self, user, password):
        code, out = self.send_command('login {} {}\r\n'.format(user, password))
        if code != 250:
            raise NetioError('Login failed. Got: {} {}'.format(code, out))

    def enable_port(self, port):
        """Enable the power supply at the specified port."""
        self.set_port(port, 1)

    def disable_port(self, port):
        """Enable the power supply at the specified port."""
        self.set_port(port, 0)

    def set_port(self, port, value):
        code, out = self.send_command('port {} {}'.format(port, value))
        if code != 250:
            raise NetioError('Could not set {} on port {}. Got: {} {}'.format(value, port, code, out))

    def send_command(self, command):
        try:
            if command.startswith('login'):
                parts = command.split()
                parts[2] = '*' * len(parts[2])
                logger.debug(' '.join(parts))
            else:
                logger.debug(command)
            self.conn.write('{}\n'.format(command))
            time.sleep(self.delay)  # give time to respond
            out = self.conn.read_very_eager()
            match = self.response_regex.search(out)
            if not match:
                raise NetioError('Invalid response: {}'.format(out.strip()))
            logger.debug('response: {} {}'.format(match.group(1), match.group(2)))
            return int(match.group(1)), match.group(2)
        except socket.error as err:
            try:
                time.sleep(self.delay)  # give time to respond
                out = self.conn.read_very_eager()
                if out.startswith('130 CONNECTION TIMEOUT'):
                    raise NetioError('130 Timed out.')
            except EOFError:
                pass
            raise err

    def close(self):
        self.conn.close()

#    Copyright 2012-2015 ARM Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
# pylint: disable=no-member
# pylint: disable=attribute-defined-outside-init

import os
import time

from wlauto import settings, Workload, Executable, Parameter
from wlauto.exceptions import ConfigError, WorkloadError
from wlauto.utils.types import boolean

TXT_RESULT_NAME = 'cyclictest_result.txt'
RESULT_INTERPRETATION = {
    'T': 'Thread',
    'P': 'Priority',
    'C': 'Clock',
}


class Cyclictest(Workload):

    name = 'cyclictest'
    description = """
    Measures the amount of time that passes between when a timer expires and
    when the thread which set the timer actually runs.

    Cyclic test works by taking a time snapshot just prior to waiting for a specific
    time interval (t1), then taking another time snapshot after the timer
    finishes (t2), then comparing the theoretical wakeup time with the actual
    wakeup time (t2 -(t1 + sleep_time)). This value is the latency for that
    timers wakeup.

    """

    parameters = [
        Parameter('clock', allowed_values=['monotonic', 'realtime'], default='realtime',
                  description=('specify the clock to be used during the test.')),
        Parameter('duration', kind=int, default=30,
                  description=('Specify the length for the test to run in seconds.')),
        Parameter('quiet', kind=boolean, default=True,
                  description=('Run the tests quiet and print only a summary on exit.')),
        Parameter('thread', kind=int, default=8,
                  description=('Set the number of test threads')),
        Parameter('latency', kind=int, default=1000000,
                  description=('Write the value to /dev/cpu_dma_latency')),
        Parameter('extra_parameters', kind=str, default="",
                  description=('Any additional command line parameters to append to the '
                               'existing parameters above. A list can be found at '
                               'https://rt.wiki.kernel.org/index.php/Cyclictest or '
                               'in the help page ``cyclictest -h``')),
        Parameter('clear_file_cache', kind=boolean, default=True,
                  description=('Clear file caches before starting test')),
        Parameter('screen_off', kind=boolean, default=True,
                  description=('If true it will turn the screen off so that onscreen '
                               'graphics do not effect the score. This is predominantly '
                               'for devices without a GPU')),

    ]

    def setup(self, context):
        self.cyclictest_on_device = 'cyclictest'
        self.cyclictest_result = os.path.join(self.device.working_directory, TXT_RESULT_NAME)
        self.cyclictest_command = '{} --clock={} --duration={}s --thread={} --latency={} {} {} > {}'
        self.device_binary = None

        if not self.device.is_rooted:
            raise WorkloadError("This workload requires a device with root premissions to run")

        host_binary = context.resolver.get(Executable(self, self.device.abi, 'cyclictest'))
        self.device_binary = self.device.install(host_binary)

        self.cyclictest_command = self.cyclictest_command.format(self.device_binary,
                                                                 0 if self.clock == 'monotonic' else 1,
                                                                 self.duration,
                                                                 self.thread,
                                                                 self.latency,
                                                                 "--quiet" if self.quiet else "",
                                                                 self.extra_parameters,
                                                                 self.cyclictest_result)

        if self.clear_file_cache:
            self.device.execute('sync')
            self.device.set_sysfile_value('/proc/sys/vm/drop_caches', 3)

        if self.device.platform == 'android':
            if self.screen_off and self.device.is_screen_on:
                self.device.execute('input keyevent 26')

    def run(self, context):
        self.device.execute(self.cyclictest_command, self.duration * 2, as_root=True)

    def update_result(self, context):
        self.device.pull_file(self.cyclictest_result, context.output_directory)

        # Parsing the output
        # Standard Cyclictest Output:
        # T: 0 (31974) P:95 I:1000 C:4990 Min:9 Act:37 Avg:31 Max:59
        with open(os.path.join(context.output_directory, TXT_RESULT_NAME)) as f:
            for line in f:
                if line.find('C:') is not -1:
                    # Key = T: 0 (31974) P:95 I:1000
                    # Remaing = 49990 Min:9 Act:37 Avg:31 Max:59
                    # sperator = C:
                    (key, sperator, remaing) = line.partition('C:')

                    index = key.find('T')
                    key = key.replace(key[index], RESULT_INTERPRETATION['T'])
                    index = key.find('P')
                    key = key.replace(key[index], RESULT_INTERPRETATION['P'])

                    index = sperator.find('C')
                    sperator = sperator.replace(sperator[index], RESULT_INTERPRETATION['C'])

                    metrics = (sperator + remaing).split()
                    # metrics is now in the from of ['Min:', '9', 'Act:', '37', 'Avg:', '31' , 'Max', '59']
                    for i in range(0, len(metrics), 2):
                        full_key = key + ' ' + metrics[i][:-1]
                        value = int(metrics[i + 1])
                        context.result.add_metric(full_key, value, 'microseconds')

    def teardown(self, context):
        if self.device.platform == 'android':
            if self.screen_off:
                self.device.ensure_screen_is_on()
        self.device.execute('rm -f {}'.format(self.cyclictest_result))

#    Copyright 2013-2015 ARM Limited
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

#pylint: disable=E1101,W0201

import os
import re
from collections import defaultdict

from wlauto import Workload, Parameter, File
from wlauto.utils.types import caseless_string
from wlauto.exceptions import WorkloadError


class Recentfling(Workload):

    name = 'recentfling'
    description = """
    Tests UI jank on android devices.

    For this workload to work, ``recentfling.sh`` and ``defs.sh`` must be placed
    in ``~/.workload_automation/dependencies/recentfling/``. These can be found
    in the [AOSP Git repository](https://android.googlesource.com/platform/system/extras/+/master/tests/).

    To change the apps that are opened at the start of the workload you will need
    to modify the ``defs.sh`` file. You will need to add your app to ``dfltAppList``
    and then add a variable called ``{app_name}Activity`` with the name of the
    activity to launch (where ``{add_name}`` is the name you put into ``dfltAppList``).

    You can get a list of activities available on your device by running
    ``adb shell pm list packages -f``
    """
    supported_platforms = ['android']

    parameters = [
        Parameter('loops', kind=int, default=3,
                  description="The number of test iterations."),
    ]

    def initialise(self, context):  # pylint: disable=no-self-use
        if context.device.get_sdk_version() < 23:
            raise WorkloadError("This workload relies on ``dumpsys gfxinfo`` \
                                 only present in Android M and onwards")

    def setup(self, context):
        self.defs_host = context.resolver.get(File(self, "defs.sh"))
        self.recentfling_host = context.resolver.get(File(self, "recentfling.sh"))
        self.device.push_file(self.recentfling_host, self.device.working_directory)
        self.device.push_file(self.defs_host, self.device.working_directory)
        self._kill_recentfling()
        self.device.ensure_screen_is_on()

    def run(self, context):
        cmd = "echo $$>{dir}/pidfile; exec {dir}/recentfling.sh -i {}; rm {dir}/pidfile"
        cmd = cmd.format(self.loops, dir=self.device.working_directory)
        try:
            self.output = self.device.execute(cmd, timeout=120)
        except KeyboardInterrupt:
            self._kill_recentfling()
            raise

    def update_result(self, context):
        group_names = ["90th Percentile", "95th Percentile", "99th Percentile", "Jank", "Jank%"]
        count = 0
        for line in self.output.strip().splitlines():
            p = re.compile("Frames: \d+ latency: (?P<pct90>\d+)/(?P<pct95>\d+)/(?P<pct99>\d+) Janks: (?P<jank>\d+)\((?P<jank_pct>\d+)%\)")
            match = p.search(line)
            if match:
                count += 1
                if line.startswith("AVE: "):
                    group_names = ["Average " + g for g in group_names]
                    count = 0
                for metric in zip(group_names, match.groups()):
                    context.result.add_metric(metric[0],
                                              metric[1],
                                              None,
                                              classifiers={"loop": count or "Average"})

    def teardown(self, context):
        self.device.delete_file(self.device.path.join(self.device.working_directory,
                                                      "recentfling.sh"))
        self.device.delete_file(self.device.path.join(self.device.working_directory,
                                                      "defs.sh"))

    def _kill_recentfling(self):
        pid = self.device.execute('cat {}/pidfile'.format(self.device.working_directory))
        if pid:
            self.device.kill(pid.strip(), signal='SIGKILL')

#!/usr/bin/env python
"""
mbed SDK
Copyright (c) 2011-2016 ARM Limited

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""


import sys
from time import time


class HtrunLogger(object):
    """! Yet another logger flavour """
    def __init__(self, prn_lock, name):
        self.__prn_lock = prn_lock
        self.__name = name

    def __prn_func(self, text, nl=True):
        """! Prints and flushes data to stdout """
        with self.__prn_lock:
            if nl and not text.endswith('\n'):
                text += '\n'
            sys.stdout.write(text)
            sys.stdout.flush()

    def __prn_log_human(self, level, text, timestamp=None):
        if not timestamp:
            timestamp = time()
        timestamp_str = strftime("%y-%m-%d %H:%M:%S", gmtime(timestamp))
        frac, whole = modf(timestamp)
        s = "[%s.%d][%s][%s] %s"% (timestamp_str, frac, self.__name, level, text)
        self.__prn_func(s, nl=True)

    def __prn_log(self, level, text, timestamp=None):
        if not timestamp:
            timestamp = time()
        s = "[%.2f][%s][%s] %s"% (timestamp, self.__name, level, text)
        self.__prn_func(s, nl=True)

    def prn_dbg(self, text, timestamp=None):
        self.__prn_log('DBG', text, timestamp)

    def prn_wrn(self, text, timestamp=None):
        self.__prn_log('WRN', text, timestamp)

    def prn_err(self, text, timestamp=None):
        self.__prn_log('ERR', text, timestamp)

    def prn_inf(self, text, timestamp=None):
        self.__prn_log('INF', text, timestamp)

    def prn_txt(self, text, timestamp=None):
        self.__prn_log('TXT', text, timestamp)

    def prn_txd(self, text, timestamp=None):
        self.__prn_log('TXD', text, timestamp)

    def prn_rxd(self, text, timestamp=None):
        self.__prn_log('RXD', text, timestamp)

#!/usr/bin/env python
"""
mbed SDK
Copyright (c) 2011-2015 ARM Limited

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import unittest

from mbed_host_tests import is_host_test
from mbed_host_tests import get_host_test
from mbed_host_tests import get_plugin_caps
from mbed_host_tests import get_host_test_list

class BasicHostTestsTestCase(unittest.TestCase):

    def setUp(self):
        pass

    def tearDown(self):
        pass

    def test_basic_get_host_test(self):
        self.assertNotEqual(None, get_host_test('default'))
        self.assertNotEqual(None, get_host_test('default_auto'))

    def test_basic_is_host_test(self):
        self.assertFalse(is_host_test(''))
        self.assertFalse(is_host_test(None))

        self.assertTrue(is_host_test('default'))
        self.assertTrue(is_host_test('default_auto'))

    def test_get_host_test_list(self):
        d = get_host_test_list()
        self.assertIs(type(d), dict)
        self.assertIn('default', d)
        self.assertIn('default_auto', d)

    def test_get_plugin_caps(self):
        d = get_plugin_caps()
        self.assertIs(type(d), dict)


if __name__ == '__main__':
    unittest.main()

#!/usr/bin/env python
# small RNA oriented bowtie wrapper
# version 1.5 17-7-2014: arg parser implementation
# Usage sRbowtie.py <1 input_fasta_file> <2 alignment method> <3 -v mismatches> <4 out_type> <5 buildIndexIfHistory> <6 fasta/bowtie index> <7 bowtie output> <8 ali_fasta> <9 unali_fasta> <10 --num-threads \${GALAXY_SLOTS:-4}>
# current rev: for bowtie __norc, move from --supress 2,6,7,8 to --supress 6,7,8. Future Parser must be updated to take into account this standardisation
# Christophe Antoniewski <drosofff@gmail.com>

import sys
import os
import subprocess
import tempfile
import shutil
import argparse


def Parser():
    the_parser = argparse.ArgumentParser(
        description="bowtie wrapper for small fasta reads")
    the_parser.add_argument(
        '--input', action="store", type=str, help="input file")
    the_parser.add_argument(
        '--input-format', dest="input_format", action="store", type=str, help="fasta or fastq")
    the_parser.add_argument('--method', action="store", type=str,
                            help="RNA, unique, multiple, k_option, n_option, a_option")
    the_parser.add_argument('--v-mismatches', dest="v_mismatches", action="store",
                            type=str, help="number of mismatches allowed for the alignments")
    the_parser.add_argument(
        '--output-format', dest="output_format", action="store", type=str, help="tabular, sam, bam")
    the_parser.add_argument(
        '--output', action="store", type=str, help="output file path")
    the_parser.add_argument(
        '--index-from', dest="index_from", action="store", type=str, help="indexed or history")
    the_parser.add_argument('--index-source', dest="index_source",
                            action="store", type=str, help="file path to the index source")
    the_parser.add_argument(
        '--aligned', action="store", type=str, help="aligned read file path, maybe None")
    the_parser.add_argument('--unaligned', action="store",
                            type=str, help="unaligned read file path, maybe None")
    the_parser.add_argument('--num-threads', dest="num_threads",
                            action="store", type=str, help="number of bowtie threads")
    args = the_parser.parse_args()
    return args


def stop_err(msg):
    sys.stderr.write('%s\n' % msg)
    sys.exit()


def bowtieCommandLiner(alignment_method="RNA", v_mis="1", out_type="tabular",
                       aligned="None", unaligned="None", input_format="fasta", input="path",
                       index="path", output="path", pslots="4"):
    if input_format == "fasta":
        input_format = "-f"
    elif (input_format == "fastq") or (input_format == "fastqsanger"):
        input_format = "-q"
    else:
        raise Exception('input format must be one of fasta or fastq')
    if alignment_method == "RNA":
        x = "-v %s -M 1 --best --strata -p %s --norc --suppress 6,7,8" % (
            v_mis, pslots)
    elif alignment_method == "unique":
        x = "-v %s -m 1 -p %s --suppress 6,7,8" % (v_mis, pslots)
    elif alignment_method == "multiple":
        x = "-v %s -M 1 --best --strata -p %s --suppress 6,7,8" % (
            v_mis, pslots)
    elif alignment_method == "k_option":
        x = "-v %s -k 1 --best -p %s --suppress 6,7,8" % (v_mis, pslots)
    elif alignment_method == "n_option":
        x = "-n %s -M 1 --best -p %s --suppress 6,7,8" % (v_mis, pslots)
    elif alignment_method == "a_option":
        x = "-v %s -a --best -p %s --suppress 6,7,8" % (v_mis, pslots)
    if aligned == "None" and unaligned == "None":
        fasta_command = ""
    elif aligned != "None" and unaligned == "None":
        fasta_command = " --al %s" % aligned
    elif aligned == "None" and unaligned != "None":
        fasta_command = " --un %s" % unaligned
    else:
        fasta_command = " --al %s --un %s" % (aligned, unaligned)
    x = x + fasta_command
    if out_type == "tabular":
        return "bowtie %s %s %s %s > %s" % (x, index, input_format, input, output)
    elif out_type == "sam":
        return "bowtie %s -S %s %s %s > %s" % (x, index, input_format, input, output)
    elif out_type == "bam":
        return "bowtie %s -S %s %s %s |samtools view -bS - > %s" % (
            x, index, input_format, input, output)


def bowtie_squash(fasta):
    # make temp directory for bowtie indexes
    tmp_index_dir = tempfile.mkdtemp()
    ref_file = tempfile.NamedTemporaryFile(dir=tmp_index_dir)
    ref_file_name = ref_file.name
    # by default, delete the temporary file, but ref_file.name is now stored
    # in ref_file_name
    ref_file.close()
    # symlink between the fasta source file and the deleted ref_file name
    os.symlink(fasta, ref_file_name)
    # bowtie command line, which will work after changing dir
    # (cwd=tmp_index_dir)
    cmd1 = 'bowtie-build -f %s %s' % (ref_file_name, ref_file_name)
    try:
        FNULL = open(os.devnull, 'w')
        # a path string for a temp file in tmp_index_dir. Just a string
        tmp = tempfile.NamedTemporaryFile(dir=tmp_index_dir).name
        # creates and open a file handler pointing to the temp file
        tmp_stderr = open(tmp, 'wb')
        # both stderr and stdout of bowtie-build are redirected in  dev/null
        proc = subprocess.Popen(
            args=cmd1, shell=True, cwd=tmp_index_dir, stderr=FNULL, stdout=FNULL)
        returncode = proc.wait()
        tmp_stderr.close()
        FNULL.close()
        sys.stdout.write(cmd1 + "\n")
    except Exception as e:
        # clean up temp dir
        if os.path.exists(tmp_index_dir):
            shutil.rmtree(tmp_index_dir)
            stop_err('Error indexing reference sequence\n' + str(e))
    # no Cleaning if no Exception, tmp_index_dir has to be cleaned after
    # bowtie_alignment()
    # bowtie fashion path without extention
    index_full_path = os.path.join(tmp_index_dir, ref_file_name)
    return tmp_index_dir, index_full_path


def bowtie_alignment(command_line, flyPreIndexed=''):
    # make temp directory just for stderr
    tmp_index_dir = tempfile.mkdtemp()
    tmp = tempfile.NamedTemporaryFile(dir=tmp_index_dir).name
    tmp_stderr = open(tmp, 'wb')
    # conditional statement for sorted bam generation viewable in Trackster
    if "samtools" in command_line:
        # recover the final output file name
        target_file = command_line.split()[-1]
        path_to_unsortedBam = os.path.join(tmp_index_dir, "unsorted.bam")
        path_to_sortedBam = os.path.join(tmp_index_dir, "unsorted.bam.sorted")
        first_command_line = " ".join(
            command_line.split()[:-3]) + " -o " + path_to_unsortedBam + " - "
        # example: bowtie -v 0 -M 1 --best --strata -p 12 --suppress 6,7,8 -S
        # /home/galaxy/galaxy-dist/bowtie/Dmel/dmel-all-chromosome-r5.49 -f
        # /home/galaxy/galaxy-dist/database/files/003/dataset_3460.dat
        # |samtools view -bS -o /tmp/tmp_PgMT0/unsorted.bam -
        # generates an "unsorted.bam.sorted.bam file", NOT an
        # "unsorted.bam.sorted" file
        second_command_line = "samtools sort  %s %s" % (
            path_to_unsortedBam, path_to_sortedBam)
        # fileno() method return the file descriptor number of tmp_stderr
        p = subprocess.Popen(
            args=first_command_line, cwd=tmp_index_dir, shell=True, stderr=tmp_stderr.fileno())
        returncode = p.wait()
        sys.stdout.write("%s\n" % first_command_line + str(returncode))
        p = subprocess.Popen(
            args=second_command_line, cwd=tmp_index_dir, shell=True, stderr=tmp_stderr.fileno())
        returncode = p.wait()
        sys.stdout.write("\n%s\n" % second_command_line + str(returncode))
        if os.path.isfile(path_to_sortedBam + ".bam"):
            shutil.copy2(path_to_sortedBam + ".bam", target_file)
    else:
        p = subprocess.Popen(
            args=command_line, shell=True, stderr=tmp_stderr.fileno())
        returncode = p.wait()
        sys.stdout.write(command_line + "\n")
    tmp_stderr.close()
    # cleaning if the index was created in the fly
    if os.path.exists(flyPreIndexed):
        shutil.rmtree(flyPreIndexed)
    # cleaning tmp files and directories
    if os.path.exists(tmp_index_dir):
        shutil.rmtree(tmp_index_dir)
    return


def __main__():
    args = Parser()
    F = open(args.output, "w")
    if args.index_from == "history":
        tmp_dir, index_path = bowtie_squash(args.index_source)
    else:
        tmp_dir, index_path = "dummy/dymmy", args.index_source
    command_line = bowtieCommandLiner(args.method, args.v_mismatches, args.output_format,
                                      args.aligned, args.unaligned, args.input_format, args.input, 
                                      index_path, args.output, args.num_threads)
    bowtie_alignment(command_line, flyPreIndexed=tmp_dir)
    F.close()
if __name__ == "__main__":
    __main__()

#!/usr/bin/python
#
import sys

input = open(sys.argv[1], "r")
output = open(sys.argv[2], "w")

for line in input:
  if line[0]==">":
    print >> output, "@HTW-"+line[1:-1]
    continue
  else:
    print >> output, line[:-1]
    print >> output, "+"
    print >> output, "H"*len(line[:-1])
    
input.close()
output.close()

"""
Verbose demonstration of how to set up a server and run a remote game.

For all practical needs, using the simplesetup module should be sufficient.
"""

import sys
import subprocess

from pelita.simplesetup import SimpleServer, SimplePublisher, SimpleController
import logging
from pelita.ui.tk_viewer import TkViewer

try:
    import colorama
    MAGENTA = colorama.Fore.MAGENTA
    RESET = colorama.Fore.RESET
except ImportError:
    MAGENTA = ""
    RESET = ""

def get_python_process():
    py_proc = sys.executable
    if not py_proc:
        raise RuntimeError("Cannot retrieve current Python executable.")
    return py_proc

FORMAT = '[%(asctime)s,%(msecs)03d][%(name)s][%(levelname)s][%(funcName)s]' + MAGENTA + ' %(message)s' + RESET
logging.basicConfig(format=FORMAT, datefmt="%H:%M:%S", level=logging.INFO)

layout = (
        """ ##################
            #0#.  . 2# .   3 #
            # #####    ##### #
            #     . #  .  .#1#
            ################## """)

server = SimpleServer(layout_string=layout, rounds=200, bind_addrs=("tcp://*:50007", "tcp://*:50008"))

publisher = SimplePublisher("tcp://*:50012")
server.game_master.register_viewer(publisher)

subscribe_sock = server
tk_open = "TkViewer(%r, %r).run()" % ("tcp://localhost:50012", "tcp://localhost:50013")
tkprocess = subprocess.Popen([get_python_process(),
                              "-c",
                              "from pelita.ui.tk_viewer import TkViewer\n" + tk_open])

try:
    print(server.bind_addresses)
    server.register_teams()
    controller = SimpleController(server.game_master, "tcp://*:50013")
    controller.run()
    server.exit_teams()
except KeyboardInterrupt:
    tkprocess.kill()


# Main entry point for the plugin.
# Author: Yuri van Geffen

import sublime, sublime_plugin

import os
import threading
import queue
import asyncore
import socket
from itertools import chain
import re

settings = sublime.load_settings("subdebug")

TCP_IP = '127.0.0.1'
TCP_PORT = 8172
BUFFER_SIZE = 1024

BASEDIR = settings.get("basedir", "")
STEP_ON_CONNECT = settings.get("step_on_connect", False)

# Handles incoming and outgoing messages for the MobDebug client
class SubDebugHandler(asyncore.dispatcher):
	def __init__(self, socket, handler_id):
		asyncore.dispatcher.__init__(self, socket)
		self.handler_id = handler_id
		msg_queue.put(b"STEP\n" if STEP_ON_CONNECT else b"RUN\n")
		for view_name,row in state_handler.breakpoints():
			msg_queue.put("SETB {0} {1}\n".format(view_name, row).encode('latin-1'))

	# Reads the message-code of incomming messages and passes 
	# them to the right function
	def handle_read(self):
		data = self.recv(BUFFER_SIZE)
		if data:
			print(self.handler_id, "Received: ", data)
			split = data.split()
			if split[0] in message_parsers:
				message_parsers[split[0]](split)

	def handle_write(self):
		if not msg_queue.empty():
			msg = msg_queue.get()
			print("Sending: ", msg)
			self.send(msg)

	def handle_error(self):
		raise

# Starts listening on TCP_PORT and accepts incoming connections
# before passing them to an instance of SubDebugHandler
class SubDebugServer(asyncore.dispatcher):

	def __init__(self, host, port):
		asyncore.dispatcher.__init__(self)
		self.handler_id = 0
		self.create_socket(socket.AF_INET, socket.SOCK_STREAM)
		self.set_reuse_addr()
		self.bind((host, port))
		self.listen(1)
		print("Started listening on: ", host, ":", port)

	def handle_accept(self):
		pair = self.accept()
		if pair is not None:
			(conn_sock, client_address) = pair
			print("Incoming connection: ", client_address)
			SubDebugHandler(conn_sock, ++self.handler_id)

	def handle_close(self):
		print("Closing server.")
		self.close()

	def handle_error(self):
		self.close()

# Lets the user run the script (until breakpoint)
class RunCommand(sublime_plugin.WindowCommand):
	def run(self):
		print("Running until breakpoint...")
		msg_queue.put(b"RUN\n")
		state_handler.remove_line_marker()

# Lets the user step to the next line
class StepCommand(sublime_plugin.WindowCommand):
	def run(self):
		print("Stepping to next line...")
		msg_queue.put(b"STEP\n")

# Lets the user step to the next line
class ToggleBreakpointCommand(sublime_plugin.TextCommand):
	def run(self, edit):
		view_name = simplify_path(self.view.file_name())
		row,_ = self.view.rowcol(self.view.sel()[0].begin())
		print("Toggling breakpoint:", view_name, row)
		state_handler.toggle_breakpoint(view_name, row + 1)

# Lets the user pick a base directory from where the lua is executed
class SetBasedirCommand(sublime_plugin.WindowCommand):
	def run(self):
		# Ran if the user want to choose their own base directory
		def choose_other(path):
			global BASEDIR
			BASEDIR = path.replace('\\','/')
			if(BASEDIR[-1] != "/"):
				BASEDIR += "/"
			print("BASEDIR:", BASEDIR)

		# Ran if the user has chosen a base directory option
		def selected_folder(index):
			global BASEDIR
			if index != -1: # The last option lets the user choose a base dir themself
				if(index == len(folders)-1):
					sublime.active_window().show_input_panel("Give the base directory path.", BASEDIR, choose_other, None, None)
				else:
					BASEDIR = folders[index] + "/"
					state_handler.clear_state()
					print("BASEDIR:", BASEDIR)
		folders = list(chain.from_iterable([w.folders() for w in sublime.windows()]))
		folders = [f.replace("\\", "/") for f in folders]
		folders.insert(len(folders), "Choose other directory...")
		sublime.active_window().show_quick_panel(folders, selected_folder)

# Lets the user step to the next line
class ToggleStepOnConnectCommand(sublime_plugin.WindowCommand):
	def run(self):
		global STEP_ON_CONNECT
		STEP_ON_CONNECT = not STEP_ON_CONNECT
		print("Step on connect:", STEP_ON_CONNECT)

	def is_checked(self):
		return STEP_ON_CONNECT or False

#=========Incomming message parsers=========#
# Called when the "202 Paused" message is received
def paused_command(args):
	state_handler.set_line_marker(args[2].decode("utf-8"), int(args[3]))

# Mapping from incomming messages to the functions that parse them
message_parsers = { 
	b"202": paused_command,
}
#===========================================#


class StateHandler():

	# Initiates object by checking which views are available and 
	# clearing the state
	def __init__(self):
		self.clear_state()
		self.update_regions()

	def clear_state(self):
		self.state = {}
		self.update_regions()

	# Gets all available views in sublime and adds the missing ones to the state
	def add_missing_views(self):
		views = [v for v in sum([w.views() for w in sublime.windows()], [])]
		self.views = {simplify_path(v.file_name()):v for v in views if v.file_name() != None}
		print(self.views)
		for view_name, view in self.views.items():
			if view_name not in self.state:
				self.state[view_name] = []

	# Updates all views with the available state-objects using the
	# assigned functions
	def update_regions(self):
		self.add_missing_views()

		# Iterate over all files in the state
		for view_name,regions in self.state.items():
			# Remove all old regions
			for reg_type_name in self.region_types:
				self.views[view_name].erase_regions(reg_type_name)

			region_sets = {}
			# Iterate over all regions in that file
			for (reg_type,line) in regions:
				if reg_type == "line_marker" or ("line_marker", line) not in regions:
					if reg_type not in region_sets:
						region_sets[reg_type] = []
					region_sets[reg_type].append(sublime.Region(self.views[view_name].text_point(line-1, 0)))
			
			# Register all new regions except the line-marker with sublime
			for reg_name,v in region_sets.items():
				print("Adding region:", view_name, reg_name, v)
				self.views[view_name].add_regions(reg_name, v, *self.region_types[reg_name])
	
	def set_line_marker(self, view_name, line_number):
		view_name = simplify_path(view_name)
		print("Setting line marker:", view_name, line_number)
		self.add_missing_views()
		if view_name in self.views:
			self.state.setdefault(view_name, [])
			self.state[view_name] = [(k,v) for k, v in self.state[view_name] if k != "line_marker"]
			self.state[view_name].append(("line_marker", line_number))
			self.update_regions()

	def remove_line_marker(self):
		for name,view in self.state.items():
			self.state[name] = [(t,n) for t,n in view if t != "line_marker"]
		self.update_regions()

	def toggle_breakpoint(self, view_name, line_number):
		self.add_missing_views()
		if view_name in self.views and ("breakpoint", line_number) in self.state[view_name]:
			self.remove_breakpoint(view_name, line_number)
		else:
			self.set_breakpoint(view_name, line_number)
		self.update_regions()

	def set_breakpoint(self, view_name, line_number):
		self.state.setdefault(view_name, [])
		self.state[view_name].append(("breakpoint", line_number))
		msg_queue.put("SETB {0} {1}\n".format(view_name, line_number).encode('latin-1'))

	def remove_breakpoint(self, view_name, line_number):
		self.state[view_name].remove(("breakpoint", line_number))
		msg_queue.put("DELB {0} {1}\n".format(view_name, line_number).encode('latin-1'))

	def breakpoints(self):
		ret = []
		for k,v in self.state.items():
			for t in v:
				if t[0] == "breakpoint":
					ret.append((k,t[1]))
		return ret

	views = {}
	state = {}
	region_types = {
		"breakpoint": ("keyword", "circle"),
		"line_marker": ("keyword", "bookmark"),
	}

def plugin_unloaded():
	settings.set("basedir", BASEDIR)
	settings.set("step_on_connect", STEP_ON_CONNECT)
	print("Closing down the server...")
	server.close()

def simplify_path(path):
	path = path.replace("\\","/").replace(BASEDIR,"")
	path = re.sub('\.lua$', '', path) # Strip ".lua" from the path
	return path

# Open a threadsafe message queue
msg_queue = queue.Queue()

state_handler = StateHandler()

# Start listening and open the asyncore loop
server = SubDebugServer(TCP_IP, TCP_PORT)

if os.name == "posix":
	thread = threading.Thread(target=asyncore.loop, kwargs={"use_poll": True})
else:
	thread = threading.Thread(target=asyncore.loop)
thread.start()


from django.contrib import sitemaps
from django.core.urlresolvers import reverse

class StaticViewSitemap(sitemaps.Sitemap):
    priority = 0.5
    changefreq = 'monthly'
    
    def items(self):
        return ['landpage','robots','humans','google_plus_verify','terms','privacy',]
    
    def location(self, item):
        return reverse(item)

# https://docs.djangoproject.com/en/1.8/ref/contrib/sitemaps/
from django.conf.urls import patterns, include, url
from publisher.views import catalog
from publisher.views import my_publication
from publisher.views import publication

urlpatterns = patterns('',
    # Publications(s)
    url(r'^publish$', catalog.catalog_page),
    url(r'^publication/(\d+)$', publication.publication_page),
    url(r'^publication/(\d+)/peer_review_modal$', publication.peer_review_modal),
    url(r'^publication/(\d+)/save_peer_review$', publication.save_peer_review),
    url(r'^publication/(\d+)/delete_peer_review$', publication.delete_peer_review),
                       
    # My Publications
    url(r'^my_publications$', my_publication.my_publications_page),
    url(r'^refresh_publications_table$', my_publication.refresh_publications_table),
    url(r'^my_publication_modal$', my_publication.my_publication_modal),
    url(r'^save_publication$', my_publication.save_publication),
    url(r'^delete_publication$', my_publication.delete_publication),
)

from django.core.urlresolvers import resolve
from django.http import HttpRequest
from django.http import QueryDict
from django.test import TestCase
from django.test import Client
from django.contrib.auth.models import User
from django.contrib.auth import authenticate, login, logout
from django.contrib.auth.decorators import login_required
from django.conf.urls.static import static, settings
import json
from registrar.models import Course
from registrar.models import Teacher
from registrar.models import Student
from registrar.models import Assignment
from registrar.models import AssignmentSubmission
from registrar.models import Quiz
from registrar.models import QuizSubmission
from registrar.models import Exam
from registrar.models import ExamSubmission
from registrar.models import EssayQuestion
from registrar.models import EssaySubmission
from registrar.models import MultipleChoiceQuestion
from registrar.models import MultipleChoiceSubmission
from registrar.models import ResponseQuestion
from registrar.models import ResponseSubmission
from registrar.models import TrueFalseQuestion
from registrar.models import TrueFalseSubmission
from registrar.models import PeerReview
from student.views import assignment
from student.views import quiz
from student.views import exam
from student.views import credit


TEST_USER_EMAIL = "ledo@gah.com"
TEST_USER_USERNAME = "Ledo"
TEST_USER_PASSWORD = "password"


class CreditTestCase(TestCase):
    def tearDown(self):
        courses = Course.objects.all()
        for course in courses:
            course.delete()
        User.objects.get(email=TEST_USER_EMAIL).delete()
    
    def setUp(self):
        # Create our Student.
        User.objects.create_user(
            email=TEST_USER_EMAIL,
            username=TEST_USER_USERNAME,
            password=TEST_USER_PASSWORD
        )
        user = User.objects.get(email=TEST_USER_EMAIL)
        teacher = Teacher.objects.create(user=user)
        student = Student.objects.create(user=user)
        
        # Create a test course.
        course = Course.objects.create(
            id=1,
            title="Comics Book Course",
            sub_title="The definitive course on comics!",
            category="",
            teacher=teacher,
        )

        # Create our assignment(s)
        assignment = Assignment.objects.create(
            assignment_id=1,
            assignment_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=25,
            course=course,
        )

        # Create questions
        EssayQuestion.objects.create(
            question_id=1,
            assignment=assignment,
            title="Evolvers",
            description="Write an essay about the Evolvers.",
        )
        MultipleChoiceQuestion.objects.create(
            question_id=2,
            assignment=assignment,
            title="Hideauze",
            description="Who where the Hideauze?",
            a="Former Humans",
            a_is_correct=True,
            b="Aliens",
            b_is_correct=False,
            c="Magical or Supernatural Creatures",
            c_is_correct=False,
            d="Dark Elves",
            d_is_correct=False,
            e="Heavenly Creatures",
            e_is_correct=False,
        )
        TrueFalseQuestion.objects.create(
            question_id=3,
            assignment=assignment,
            title="Hideauze",
            description="Where the Hideauze human?",
            true_choice="Yes, former humans",
            false_choice="No, aliens",
            answer=True,
        )
        ResponseQuestion.objects.create(
            question_id=4,
            assignment=assignment,
            title="Hideauze",
            description="Why did humanity migrate off-world?",
            answer="Because of solar hibernation causing Global Cooling on Earth.",
        )

        # Create our quiz
        Quiz.objects.create(
            quiz_id=1,
            quiz_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=25,
            course=course,
        )
        quiz = Quiz.objects.get(quiz_id=1)
        TrueFalseQuestion.objects.create(
            question_id=5,
            quiz=quiz,
            title="Hideauze",
            description="Where the Hideauze human?",
            true_choice="Yes, former humans",
            false_choice="No, aliens",
            answer=True,
        )
            
        # Create our Exam
        Exam.objects.create(
            exam_id=1,
            exam_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=50,
            course=course,
            is_final=True,
        )
        exam = Exam.objects.get(exam_id=1)
        MultipleChoiceQuestion.objects.create(
            question_id=6,
            exam=exam,
            title="Hideauze",
            description="Who where the Hideauze?",
            a="Former Humans",
            a_is_correct=True,
            b="Aliens",
            b_is_correct=False,
            c="Magical or Supernatural Creatures",
            c_is_correct=False,
            d="Orcs",
            d_is_correct=False,
            e="Heavenly Creatures",
            e_is_correct=False,
        )

    def get_logged_in_client(self):
        client = Client()
        client.login(
            username=TEST_USER_USERNAME,
            password=TEST_USER_PASSWORD
        )
        return client

    def test_url_resolves_to_credit_page_view(self):
        found = resolve('/course/1/credit')
        self.assertEqual(found.func, credit.credit_page)

    def test_credit_page_with_no_submissions(self):
        client = self.get_logged_in_client()
        response = client.post('/course/1/credit')

        self.assertEqual(response.status_code, 200)
        self.assertIn(b'Comics Book Course',response.content)
        self.assertIn(b'ajax_submit_credit_application();',response.content)

    def test_url_resolves_to_submit_json(self):
        found = resolve('/course/1/submit_credit_application')
        self.assertEqual(found.func, credit.submit_credit_application)

    def test_submit_credit_application_on_no_failing_criteria(self):
        kwargs = {'HTTP_X_REQUESTED_WITH':'XMLHttpRequest'}
        client = self.get_logged_in_client()
        response = client.post('/course/1/submit_credit_application',{
            'assignment_id': 1,
        }, **kwargs)
        json_string = response.content.decode(encoding='UTF-8')
        array = json.loads(json_string)
        self.assertEqual(response.status_code, 200)
        self.assertEqual(array['status'], 'failure')
        self.assertEqual(array['message'], 'you need to pass with at minimum 50%')

    def test_submit_credit_application_on_passing_criteria_without_peer_reviews(self):
        kwargs = {'HTTP_X_REQUESTED_WITH':'XMLHttpRequest'}
        client = self.get_logged_in_client()
        
        # Setup Failing
        # Assignment
        file_path = settings.MEDIA_ROOT + '/sample.pdf'
        with open(file_path, 'rb') as fp:
            self.assertTrue(fp is not None)
            client.post('/course/1/assignment/1/submit_e_assignment_answer',{
                'question_id': 1,
                'file': fp
            }, **kwargs)
        client.post('/course/1/assignment/1/submit_mc_assignment_answer',{
            'question_id': 2,
            'answer': 'A',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_tf_assignment_answer',{
            'question_id': 3,
            'answer': 'true',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_r_assignment_answer',{
            'question_id': 4,
            'answer': 'Because of Global Cooling caused by abnormal solar hibernation.',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_assignment',{}, **kwargs)

        # Quiz
        client.post('/course/1/quiz/1/submit_tf_quiz_answer',{
            'question_id': 5,
            'answer': 'true',
        }, **kwargs)
        client.post('/course/1/quiz/1/submit_quiz',{}, **kwargs)

        # Exam
        response = client.post('/course/1/exam/1/submit_mc_exam_answer',{
            'question_id': 6,
            'answer': 'A',
        }, **kwargs)
        client.post('/course/1/exam/1/submit_exam',{}, **kwargs)

        # Test
        response = client.post('/course/1/submit_credit_application',{
            'assignment_id': 1,
        }, **kwargs)
        json_string = response.content.decode(encoding='UTF-8')
        array = json.loads(json_string)
        self.assertEqual(response.status_code, 200)
        self.assertEqual(array['status'], 'success')
        self.assertEqual(array['message'], 'credit granted')

        # Cleanup
        try:
            EssaySubmission.objects.get(submission_id=1).delete()
        except EssaySubmission.DoesNotExist:
            pass
        try:
            EssaySubmission.objects.get(submission_id=2).delete()
        except EssaySubmission.DoesNotExist:
            pass

    def test_submit_credit_application_on_passing_criteria_with_peer_reviews(self):
        kwargs = {'HTTP_X_REQUESTED_WITH':'XMLHttpRequest'}
        client = self.get_logged_in_client()
        
        # Setup Failing
        # Assignment
        file_path = settings.MEDIA_ROOT + '/sample.pdf'
        with open(file_path, 'rb') as fp:
            self.assertTrue(fp is not None)
            client.post('/course/1/assignment/1/submit_e_assignment_answer',{
                'question_id': 1,
                'file': fp
            }, **kwargs)
        client.post('/course/1/assignment/1/submit_mc_assignment_answer',{
            'question_id': 2,
            'answer': 'A',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_tf_assignment_answer',{
            'question_id': 3,
            'answer': 'true',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_r_assignment_answer',{
            'question_id': 4,
            'answer': 'Because of Global Cooling caused by abnormal solar hibernation.',
        }, **kwargs)
        client.post('/course/1/assignment/1/submit_assignment',{}, **kwargs)
                        
        # Quiz
        client.post('/course/1/quiz/1/submit_tf_quiz_answer',{
            'question_id': 5,
            'answer': 'true',
        }, **kwargs)
        client.post('/course/1/quiz/1/submit_quiz',{}, **kwargs)
                                
        # Exam
        response = client.post('/course/1/exam/1/submit_mc_exam_answer',{
            'question_id': 6,
            'answer': 'A',
        }, **kwargs)
        client.post('/course/1/exam/1/submit_exam',{}, **kwargs)
        
        # Peer Reviews
        client.post('/course/1/peer_review/1/save_peer_review',{
            'question_id': 1,
            'question_type': settings.ESSAY_QUESTION_TYPE,
            'submission_id': 1,
            'marks': 5,
        },**kwargs)
        client.post('/course/1/peer_review/1/save_peer_review',{
            'question_id': 4,
            'question_type': settings.RESPONSE_QUESTION_TYPE,
            'submission_id': 1,
            'marks': 5,
        },**kwargs)
        
        # Test
        response = client.post('/course/1/submit_credit_application',{
            'assignment_id': 1,
        }, **kwargs)
        json_string = response.content.decode(encoding='UTF-8')
        array = json.loads(json_string)
        self.assertEqual(response.status_code, 200)
        self.assertEqual(array['status'], 'success')
        self.assertEqual(array['message'], 'credit granted')
                                                                
        # Cleanup
        try:
            EssaySubmission.objects.get(submission_id=1).delete()
        except EssaySubmission.DoesNotExist:
            pass
        try:
            EssaySubmission.objects.get(submission_id=2).delete()
        except EssaySubmission.DoesNotExist:
            pass
# Django & Python
from django.core.urlresolvers import resolve
from django.http import HttpRequest
from django.http import QueryDict
from django.test import TestCase
from django.test import Client
from django.contrib.auth.models import User
from django.contrib.auth import authenticate, login, logout
from django.contrib.auth.decorators import login_required
from django.conf.urls.static import static, settings
import json

# Modal
from registrar.models import Teacher
from registrar.models import Course
from registrar.models import Announcement
from registrar.models import Syllabus
from registrar.models import Policy
from registrar.models import Lecture
from registrar.models import Assignment
from registrar.models import Quiz
from registrar.models import Exam
from registrar.models import CourseSubmission

# View
from teacher.views import overview

# Contants
TEST_USER_EMAIL = "ledo@gah.com"
TEST_USER_USERNAME = "Ledo"
TEST_USER_PASSWORD = "ContinentalUnion"
TEST_USER_EMAIL2 = "whalesquid@hideauze.com"
TEST_USER_USERNAME2 = "whalesquid"
TEST_USER_PASSWORD2 = "Evolvers"

class OverviewTestCase(TestCase):
    def tearDown(self):
        syllabuses = Syllabus.objects.all()
        for syllabus in syllabuses:
            syllabus.delete()
        policies = Policy.objects.all()
        for policy in policies:
            policy.delete()
        courses = Course.objects.all()
        for course in courses:
            course.delete()
        User.objects.all().delete()

    def setUp(self):
        # Create our Trudy user.
        User.objects.create_user(
            email=TEST_USER_EMAIL2,
            username=TEST_USER_USERNAME2,
            password=TEST_USER_PASSWORD2
        )
        user = User.objects.get(email=TEST_USER_EMAIL2)
        teacher = Teacher.objects.create(user=user)
                                 
        # Create our Teacher.
        user = User.objects.create_user(
            email=TEST_USER_EMAIL,
            username=TEST_USER_USERNAME,
            password=TEST_USER_PASSWORD
        )
        teacher = Teacher.objects.create(user=user)
        course = Course.objects.create(
            id=1,
            title="Comics Book Course",
            sub_title="The definitive course on comics!",
            category="",
            teacher=teacher,
        )

    def populate_course_content(self, client, kwargs):
        course = Course.objects.get(id=1)
        Announcement.objects.create(
            announcement_id=1,
            course=course,
            title='Hello world!',
            body='This is the body of the message.',
        )
        course = Course.objects.get(id=1)
        file_path = settings.MEDIA_ROOT + '/sample.pdf'
        with open(file_path, 'rb') as fp:
            self.assertTrue(fp is not None)
            Syllabus.objects.create(
                syllabus_id=1,
                file='',
                course=course,
            )
        with open(file_path, 'rb') as fp:
            self.assertTrue(fp is not None)
            Policy.objects.create(
                policy_id=1,
                file='',
                course=course,
            )
            
        Lecture.objects.create(
            lecture_id=1,
            lecture_num=1,
            week_num=1,
            title="Blade vs Evil",
            description="Fighting for the destiny of the Earth.",
            course=course,
        )
        Lecture.objects.create(
            lecture_id=2,
            lecture_num=2,
            week_num=1,
            title="Blade vs Evil",
            description="Fighting for the destiny of the Earth.",
            course=course,
        )
        Assignment.objects.create(
            assignment_id=1,
            assignment_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=25,
            course=course,
        )
        Quiz.objects.create(
            quiz_id=1,
            quiz_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=25,
            course=course,
        )
        Exam.objects.create(
            exam_id=1,
            exam_num=1,
            title="Hideauze",
            description="Anime related assignment.",
            worth=50,
            course=course,
            is_final=True,
        )

    def delete_course_content(self):
        for id in range(1, 10):
            # Syllabus
            try:
                Syllabus.objects.get(syllabus_id=id).delete()
            except Syllabus.DoesNotExist:
                pass
            # Policy
            try:
                Policy.objects.get(policy_id=id).delete()
            except Policy.DoesNotExist:
                pass
        
        # Announcement
        try:
            Announcement.objects.get(announcement_id=1).delete()
        except Announcement.DoesNotExist:
            pass


    def get_logged_in_client(self):
        client = Client()
        client.login(
            username=TEST_USER_USERNAME,
            password=TEST_USER_PASSWORD
        )
        return client

    def test_url_resolves_to_overview_page_view(self):
        found = resolve('/teacher/course/1/overview')
        self.assertEqual(found.func, overview.overview_page)

    def test_overview_page(self):
        client = self.get_logged_in_client()
        response = client.post('/teacher/course/1/overview')
        self.assertEqual(response.status_code, 200)
        self.assertIn(b'Comics Book Course',response.content)
        self.assertIn(b'ajax_submit_course()',response.content)

    def test_submit_course_for_review(self):
        client = self.get_logged_in_client()
        kwargs = {'HTTP_X_REQUESTED_WITH':'XMLHttpRequest'}
        
        # Create course content.
        self.populate_course_content(client, kwargs)
        
        response = client.post('/teacher/course/1/submit_course_for_review',{}, **kwargs)
        self.assertEqual(response.status_code, 200)
        json_string = response.content.decode(encoding='UTF-8')
        array = json.loads(json_string)
        self.assertEqual(array['message'], 'submitted course review')
        self.assertEqual(array['status'], 'success')

        # Delete course content.
        self.delete_course_content()

"""added goal properties

Revision ID: 5018059c5c8f
Revises: 16b4a243d41d
Create Date: 2015-09-23 11:56:01.897992

"""

# revision identifiers, used by Alembic.
revision = '5018059c5c8f'
down_revision = '16b4a243d41d'
branch_labels = None
depends_on = None

from alembic import op
import sqlalchemy as sa


def upgrade():
    ### commands auto generated by Alembic - please adjust! ###
    op.create_table('goalproperties',
    sa.Column('id', sa.Integer(), nullable=False),
    sa.Column('name', sa.String(length=255), nullable=False),
    sa.Column('is_variable', sa.Boolean(), nullable=False),
    sa.PrimaryKeyConstraint('id')
    )
    op.create_table('goals_goalproperties',
    sa.Column('goal_id', sa.Integer(), nullable=False),
    sa.Column('property_id', sa.Integer(), nullable=False),
    sa.Column('value', sa.String(length=255), nullable=True),
    sa.Column('value_translation_id', sa.Integer(), nullable=True),
    sa.Column('from_level', sa.Integer(), nullable=False),
    sa.ForeignKeyConstraint(['goal_id'], ['goals.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['property_id'], ['goalproperties.id'], ondelete='CASCADE'),
    sa.ForeignKeyConstraint(['value_translation_id'], ['translationvariables.id'], ondelete='RESTRICT'),
    sa.PrimaryKeyConstraint('goal_id', 'property_id', 'from_level')
    )
    op.add_column(u'goals', sa.Column('name', sa.String(length=255), nullable=False, server_default=""))
    ### end Alembic commands ###


def downgrade():
    ### commands auto generated by Alembic - please adjust! ###
    op.drop_column(u'goals', 'name')
    op.drop_table('goals_goalproperties')
    op.drop_table('goalproperties')
    ### end Alembic commands ###

# Demonstration of `applib` features

import logging

from applib.base import Cmdln, Application
from applib.misc import require_option
from applib import textui, sh, _cmdln as cmdln

LOG = logging.getLogger(__name__)

application = Application('demo-app', 'CompanyNameHere', '1.2')


@cmdln.option('', '--foo', action='store_true', help='*must pass --foo')
class Commands(Cmdln):
    name = "demo-app"

    def initialize(self):
        require_option(self.options, 'foo')

    @cmdln.alias('cd')
    @cmdln.option('-t', '--show-time', action='store_true',
                  help='Also show the current time')
    def do_currentdate(self, subcmd, opts):
        """${cmd_name}: Show the current date
        
        ${cmd_usage}
        ${cmd_option_list}
        """
        with self.bootstrapped():
            from datetime import datetime
            now = datetime.now()
            LOG.debug('datetime.now = %s', now)
            if opts.show_time:
                print(now)
            else:
                print(now.date())
                
    def do_ls(self, subcmd, opts):
        """${cmd_name}: Show directory listing (runs 'ls')
        
        ${cmd_usage}
        ${cmd_option_list}
        """
        with self.bootstrapped():
            print(sh.run('ls')[0].decode('utf-8'))
                
    def do_makeerror(self, subcmd, opts, what):
        """${cmd_name}: Make an error. Use -v to see full traceback
        
        ${cmd_usage}
        ${cmd_option_list}
        """
        with self.bootstrapped():
            LOG.debug('About to make an error! %s', what)
            textui.askyesno('Press enter to proceed:', default=True)
            1/0
            
    @cmdln.option('', '--no-break', action='store_true',
                  help='Don\'t break from loop')
    def do_think(self, subcmd, opts, length=200):
        """${cmd_name}: Progress bar example
        
        ${cmd_usage}
        ${cmd_option_list}
        """
        with self.bootstrapped():
            import time
            length = int(length)
            for x in textui.ProgressBar.iterate(range(length),
                                                post='Thought {total} thoughts in time {elapsed}'):
                if x == length-1 and not opts.no_break:
                    break # test that break doesn't mess up output
                time.sleep(0.1)
            
    def do_multable(self, subcmd, opts, number=10, times=25):
        """${cmd_name}: Print multiplication table
        
        To demonstrate `colprint` feature
        
        ${cmd_usage}
        ${cmd_option_list}
        """
        with self.bootstrapped():
            textui.colprint([
                [str(x*y) for y in range(1, 1+int(times))]
                for x in range(1, 1+int(number))
            ])


if __name__ == '__main__':
    application.run(Commands)


# Copyright (c) 2015-2016, Activision Publishing, Inc.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification,
# are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
# list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
# this list of conditions and the following disclaimer in the documentation
# and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its contributors
# may be used to endorse or promote products derived from this software without
# specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
# ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
# ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

from assertpy import assert_that,fail

class TestType(object):

    def test_is_type_of(self):
        assert_that('foo').is_type_of(str)
        assert_that(123).is_type_of(int)
        assert_that(0.456).is_type_of(float)
        #assert_that(234L).is_type_of(long)
        assert_that(['a','b']).is_type_of(list)
        assert_that(('a','b')).is_type_of(tuple)
        assert_that({ 'a':1,'b':2 }).is_type_of(dict)
        assert_that(set(['a','b'])).is_type_of(set)
        assert_that(None).is_type_of(type(None))
        assert_that(Foo()).is_type_of(Foo)
        assert_that(Bar()).is_type_of(Bar)

    def test_is_type_of_failure(self):
        try:
            assert_that('foo').is_type_of(int)
            fail('should have raised error')
        except AssertionError as ex:
            assert_that(str(ex)).is_equal_to('Expected <foo:str> to be of type <int>, but was not.')

    def test_is_type_of_bad_arg_failure(self):
        try:
            assert_that('foo').is_type_of('bad')
            fail('should have raised error')
        except TypeError as ex:
            assert_that(str(ex)).is_equal_to('given arg must be a type')

    def test_is_type_of_subclass_failure(self):
        try:
            assert_that(Bar()).is_type_of(Foo)
            fail('should have raised error')
        except AssertionError as ex:
            assert_that(str(ex)).starts_with('Expected <')
            assert_that(str(ex)).ends_with(':Bar> to be of type <Foo>, but was not.')

    def test_is_instance_of(self):
        assert_that('foo').is_instance_of(str)
        assert_that(123).is_instance_of(int)
        assert_that(0.456).is_instance_of(float)
        #assert_that(234L).is_instance_of(long)
        assert_that(['a','b']).is_instance_of(list)
        assert_that(('a','b')).is_instance_of(tuple)
        assert_that({ 'a':1,'b':2 }).is_instance_of(dict)
        assert_that(set(['a','b'])).is_instance_of(set)
        assert_that(None).is_instance_of(type(None))
        assert_that(Foo()).is_instance_of(Foo)
        assert_that(Bar()).is_instance_of(Bar)
        assert_that(Bar()).is_instance_of(Foo)

    def test_is_instance_of_failure(self):
        try:
            assert_that('foo').is_instance_of(int)
            fail('should have raised error')
        except AssertionError as ex:
            assert_that(str(ex)).is_equal_to('Expected <foo:str> to be instance of class <int>, but was not.')

    def test_is_instance_of_bad_arg_failure(self):
        try:
            assert_that('foo').is_instance_of('bad')
            fail('should have raised error')
        except TypeError as ex:
            assert_that(str(ex)).is_equal_to('given arg must be a class')

class Foo(object):
    pass

class Bar(Foo):
    pass


__author__ = 'Ahmed Hani Ibrahim'


class Action(object):

    def GetActionName(self):
        return self.__name

    def SetActionName(self, name):
        self.__name = name

    def __init__(self, name):
        self.__name = name

# auto-generated file
import _cffi_backend

ffi = _cffi_backend.FFI('_simple_example',
    _version = 0x2601,
    _types = b'\x00\x00\x04\x0D\x00\x00\x03\x03\x00\x00\x01\x0F\x00\x00\x02\x01\x00\x00\x07\x01',
    _globals = (b'\x00\x00\x00\x23printf',0,),
)

from app import app

if __name__ == "__main__":
    app.run()
# coding: utf-8
from flask import render_template, Blueprint, redirect, request, url_for
from ..forms import SigninForm, SignupForm
from ..utils.account import signin_user, signout_user
from ..utils.permissions import VisitorPermission, UserPermission
from ..models import db, User

bp = Blueprint('account', __name__)


@bp.route('/signin', methods=['GET', 'POST'])
@VisitorPermission()
def signin():
    """Signin"""
    form = SigninForm()
    if form.validate_on_submit():
        signin_user(form.user)
        return redirect(url_for('site.index'))
    return render_template('account/signin/signin.html', form=form)


@bp.route('/signup', methods=['GET', 'POST'])
@VisitorPermission()
def signup():
    """Signup"""
    form = SignupForm()
    if form.validate_on_submit():
        params = form.data.copy()
        params.pop('repassword')
        user = User(**params)
        db.session.add(user)
        db.session.commit()
        signin_user(user)
        return redirect(url_for('site.index'))
    return render_template('account/signup/signup.html', form=form)


@bp.route('/signout')
def signout():
    """Signout"""
    signout_user()
    return redirect(request.referrer or url_for('site.index'))

from app import app, db
import unittest
import os
import tempfile
from flask import json

TEST_DB = 'test.db'


class BasicTestCase(unittest.TestCase):
    def test_index(self):
        """inital test. ensure flask was set up correctly"""
        tester = app.test_client(self)
        response = tester.get('/', content_type='html/text')
        self.assertEqual(response.status_code, 200)

    def test_database(self):
        """inital test. ensure that the database exists"""
        tester = os.path.exists("flaskr.db")
        self.assertTrue(tester)


class FlaskrTestCase(unittest.TestCase):
    def setUp(self):
        """Set up a blank temp database before each test"""
        basedir = os.path.abspath(os.path.dirname(__file__))
        app.config['TESTING'] = True
        app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + \
                                                os.path.join(basedir, TEST_DB)
        self.app = app.test_client()
        db.create_all()

    def tearDown(self):
        """Destroy blank temp database after each test"""
        db.drop_all()

    def login(self, username, password):
        """Login helper function"""
        return self.app.post('/login', data=dict(
            username=username,
            password=password
        ), follow_redirects=True)

    def logout(self):
        """Logout helper function"""
        return self.app.get('/logout', follow_redirects=True)

    # assert functions

    def test_empty_db(self):
        """Ensure database is blank"""
        rv = self.app.get('/')
        self.assertIn(b'No entries yet. Add some!', rv.data)

    def test_login_logout(self):
        """Test login and logout using helper functions"""
        rv = self.login(app.config['USERNAME'], app.config['PASSWORD'])
        self.assertIn(b'You were logged in', rv.data)
        rv = self.logout()
        self.assertIn(b'You were logged out', rv.data)
        rv = self.login(app.config['USERNAME'] + 'x', app.config['PASSWORD'])
        self.assertIn(b'Invalid username', rv.data)
        rv = self.login(app.config['USERNAME'], app.config['PASSWORD'] + 'x')
        self.assertIn(b'Invalid password', rv.data)

    def test_messages(self):
        """Ensure that user can post messages"""
        self.login(app.config['USERNAME'], app.config['PASSWORD'])
        rv = self.app.post('/add', data=dict(
            title='<Hello>',
            text='<strong>HTML</strong> allowed here'
        ), follow_redirects=True)
        self.assertNotIn(b'No entries here so far', rv.data)
        self.assertIn(b'&lt;Hello&gt;', rv.data)
        self.assertIn(b'<strong>HTML</strong> allowed here', rv.data)

    def test_delete_message(self):
        """Ensure the messages are being deleted"""
        rv = self.app.get('/delete/1')
        data = json.loads(rv.data)
        self.assertEqual(data['status'], 1)


if __name__ == '__main__':
    unittest.main()

#!/usr/bin/env python2

# -*- coding: utf-8 -*-

import thread
import time

mylock = thread.allocate_lock() # Allocate a lock
num = 0 # Shared resource

def add_num(name):
    global num
    while True:
        mylock.acquire() # Get the lock
        # Do something to the shared resource
        print('Thread %s locked! num=%s' % (name, str(num)))
        if num >= 5:
            print('Thread %s released! num=%s' % (name, str(num)))
            mylock.release()
            thread.exit()
        num += 1
        print('Thread %s released! num=%s' % (name, str(num)))
        mylock.release() # Release the lock.

def test():
    thread.start_new_thread(add_num, ('A',))
    thread.start_new_thread(add_num, ('B',))
    time.sleep(30)

if __name__ == '__main__':
    test()

#!/usr/bin/env python

from __future__ import print_function

import pyglet
from pyglet.window import key
from pyglet.window import mouse

window = pyglet.window.Window()


@window.event
def on_key_press(symbol, modifiers):
    print("key %s was pressed" % symbol)
    if symbol == key.A:
        print('The "A" key was pressed.')
    elif symbol == key.LEFT:
        print('The left arrow key was pressed.')
    elif symbol == key.ENTER:
        print('The enter key was pressed.')


@window.event
def on_mouse_press(x, y, button, modifiers):
    print("location: (%s, %s), button: %s" % (x, y, button))
    if button == mouse.LEFT:
        print('The left mouse button was pressed.')


@window.event
def on_draw():
    window.clear()


pyglet.app.run()

import os

from setuptools import setup, find_packages

here = os.path.abspath(os.path.dirname(__file__))
with open(os.path.join(here, 'README.txt')) as f:
    README = f.read()
with open(os.path.join(here, 'CHANGES.txt')) as f:
    CHANGES = f.read()

requires = [
    'pyramid',
    'pyramid_chameleon',
    'pyramid_debugtoolbar',
    'pyramid_tm',
    'SQLAlchemy',
    'transaction',
    'zope.sqlalchemy',
    'waitress',
    ]

setup(name='pyramid_pycharm',
      version='0.0',
      description='pyramid_pycharm',
      long_description=README + '\n\n' + CHANGES,
      classifiers=[
        "Programming Language :: Python",
        "Framework :: Pyramid",
        "Topic :: Internet :: WWW/HTTP",
        "Topic :: Internet :: WWW/HTTP :: WSGI :: Application",
        ],
      author='',
      author_email='',
      url='',
      keywords='web wsgi bfg pylons pyramid',
      packages=find_packages(),
      include_package_data=True,
      zip_safe=False,
      test_suite='pyramid_pycharm',
      install_requires=requires,
      entry_points="""\
      [paste.app_factory]
      main = pyramid_pycharm:main
      [console_scripts]
      initialize_pyramid_pycharm_db = pyramid_pycharm.scripts.initializedb:main
      """,
      )

#!/usr/bin/env python

from mako.template import Template
from mako.runtime import Context
from StringIO import StringIO

mytemplate = Template("hello, ${name}!")
buf = StringIO()
ctx = Context(buf, name="Akagi201")
mytemplate.render_context(ctx)
print(buf.getvalue())

#!/usr/bin/env python

"""Test for inequality

"""

import unittest


class InequalityTest(unittest.TestCase):
    def testEqual(self):
        self.failIfEqual(1, 3 - 2)

    def testNotEqual(self):
        self.failUnlessEqual(2, 3 - 2)


if __name__ == '__main__':
    unittest.main()

from flask import Flask
from flask.ext.fragment import Fragment
from flask.ext.login import LoginManager
from flask.ext.sqlalchemy import SQLAlchemy
app = Flask(__name__)
db = SQLAlchemy(app)
fragment = Fragment(app)
login = LoginManager(app)

from models import User, Post, Comment, LoginForm, RegisterForm, PostForm, CommentForm
from flask.ext.login import current_user, login_required, login_user, logout_user
from flask import render_template, redirect, url_for, request, flash


#### VIEWS
from models import User, Post, Comment, LoginForm, RegisterForm, PostForm, CommentForm
from flask.ext.login import current_user, login_required, login_user, logout_user
from flask import render_template, redirect, url_for, request, flash

POSTS_ON_PAGE = 20
COMMENTS_ON_PAGE = 20


## Handlers

@login.user_loader
def load_user(userid):
    return User.get(userid)

@app.errorhandler(404)
def page_not_found(e):
    return render_template('page404.html'), 404

@login.unauthorized_handler
def unauthorized():
    flash('Only authorized users can do requested action or see requested page.', 'warning')
    return redirect(url_for('index'))


### Login/Logout/Register pages

@fragment(app)
def login_form():
    return render_template('login.html', form=LoginForm())

@app.route('/login', methods=['POST'])
def login():
    form = LoginForm()
    if form.validate_on_submit():
        login_user(form.user)
        flash('You are logged successfully.', 'info')
        return redirect(request.args.get('next') or url_for('index'))
    return redirect(url_for('index'))


@app.route("/logout")
@login_required
def logout():
    logout_user()
    return redirect(url_for('index'))

@app.route('/register', methods=['GET', 'POST'])
def register():
    form = RegisterForm()
    if form.validate_on_submit():
        db.session.add(form.user)
        db.session.commit()
        login_user(form.user)
        flash('You are registered successfully.', 'info')
        return redirect(url_for('index'))
    return render_template('register.html', form=form)


### Index page

@fragment(app, cache=300)
def user_info(userid):
    return render_template('fragments/userinfo.html')


@fragment(app, cache=300)
def posts_list(page):
    page = int(page)
    page_size = POSTS_ON_PAGE
    pagination = Post.query.filter_by().paginate(page, page_size)
    posts = Post.query.filter_by().offset((page-1)*page_size).limit(page_size).all()
    return render_template('fragments/posts_list.html', pagination=pagination, posts=posts)


@fragment.resethandler(posts_list)
def reset_posts_list():
    page_size = POSTS_ON_PAGE
    pagination = Post.query.filter_by().paginate(1, page_size)
    for N in range(pagination.pages):
        fragment.reset_url(url_for('posts_list', page=N+1))


@app.route('/posts/<int:page>')
@app.route('/', endpoint='index', defaults={'page':1})
def posts(page):
    return render_template('index.html', page=page)


### Post page

@fragment(app, cache=300)
def post_show(post_id):
    post = Post.query.filter_by(id=post_id).first()
    return render_template('fragments/post_show.html', post=post)


@fragment(app, cache=300)
def comments_list(post_id, page):
    page = int(page)
    page_size = COMMENTS_ON_PAGE
    pagination = Comment.query.filter_by(post_id=post_id).paginate(page, page_size)
    comments = Comment.query.filter_by(post_id=post_id).offset((page-1)*page_size).limit(page_size).all()
    return render_template('fragments/comments_list.html', post_id=post_id, page=page,
                                                           pagination=pagination, comments=comments)


@fragment.resethandler(comments_list)
def reset_comments_list(post_id):
    page_size = COMMENTS_ON_PAGE
    pagination = Comment.query.filter_by(post_id=post_id).paginate(1, page_size)
    for N in range(pagination.pages):
        fragment.reset_url(url_for('comments_list', post_id=post_id, page=N+1))


@app.route('/post/<int:post_id>/<int:page>', methods=['GET', 'POST'])
def post(post_id, page):
    form = CommentForm()
    if (current_user.is_authenticated() and form.validate_on_submit()):
        form.comment.author_id = current_user.id
        form.comment.post_id = post_id
        db.session.add(form.comment)
        db.session.commit()
        fragment.reset(posts_list)
        fragment.reset(comments_list, post_id)
        fragment.reset(user_info, current_user.id)
        flash('Your comment has saved successfully.', 'info')
    return render_template('post.html', form=form, post_id=post_id, page=page)


### New Post page

@app.route('/new/post', methods=['GET', 'POST'])
@login_required
def new_post():
    form = PostForm()
    if form.validate_on_submit():
        form.post.author_id = current_user.id
        db.session.add(form.post)
        db.session.commit()
        fragment.reset(posts_list)
        fragment.reset(user_info, current_user.id)
        flash('Your post has saved successfully.', 'info')
        return redirect(url_for('index'))
    return render_template('newpost.html', form=form)


### Config ###

class DefaultConfig(object):
    FRAGMENT_CACHING = True
    SQLALCHEMY_DATABASE_URI = 'sqlite:///ssiblog.db'
    SECRET_KEY = 'Development_Secret_Key_Must_Be_Overwritten'
    

### Console command ###

import sys
import os.path
PY2 = sys.version_info[0] == 2

from flask.ext.script import Manager
manager = Manager(app, with_default_commands=False)

@manager.command
def debug():
    """Runs application within debug environment."""
    app.config['DEBUG'] = True
    if PY2:
        from flask_debugtoolbar import DebugToolbarExtension
        DebugToolbarExtension(app)
    app.run(debug=True)

@manager.command
def nginx_conf():
    """Creates application config for nginx."""
    file_name = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'nginx.conf')
    fragment._create_nginx_config(file_name)
    
@manager.command
def create_db():
    """Creates application DB."""
    from models import DB
    url = app.config.get('SQLALCHEMY_DATABASE_URI', 'sqlite://')
    if url.startswith('sqlite:////'):
        path = url[10:]
        if not os.path.exists(path):
            os.makedirs(path)
    DB.create_all()
    DB.session.commit()


if __name__ == '__main__':
        app.config.from_object(DefaultConfig)
        manager.run()

# -*- coding: utf-8 -*-
from tests.common import parent_id, parent_name, child_id, child_parent_id, relation, child, parent
from eralchemy.main import _intermediary_to_markdown

import re
import pytest
column_re = re.compile('(?P<key>\*?)(?P<name>[^*].+) \{label:\"(?P<type>.+)\"\}')


def test_all_to_er():
    tables = [child, parent]
    relations = [relation]
    output = _intermediary_to_markdown(tables, relations)
    for element in relations + tables:
        assert element.to_markdown() in output


def assert_column_well_rendered_to_er(col):
    col_er = col.to_markdown().strip()
    col_parsed = column_re.match(col_er)
    assert col_parsed.group('key') == ('*' if col.is_key else '')
    assert col_parsed.group('name') == col.name
    assert col_parsed.group('type') == col.type


def test_column_to_er():
    assert_column_well_rendered_to_er(parent_id)
    assert_column_well_rendered_to_er(parent_name)
    assert_column_well_rendered_to_er(child_id)
    assert_column_well_rendered_to_er(child_parent_id)


def test_relation():
    assert relation.to_markdown() in ['parent *--? child', 'child ?--* parent']


def assert_table_well_rendered_to_er(table):
    assert table.header_markdown == '[' + table.name + ']'
    table_er = table.to_markdown()
    for col in table.columns:
        assert col.to_markdown() in table_er


def test_table():
    assert_table_well_rendered_to_er(child)
    assert_table_well_rendered_to_er(parent)
from django.http import Http404
from django.shortcuts import render_to_response
from django.core.paginator import Paginator, EmptyPage, PageNotAnInteger

###########
# CHOICES #
###########

def choice_list(request, app_label, module_name, field_name, models):
    m, f = lookup_field(app_label, module_name, field_name, models)
    return render_to_response(
        'databrowse/choice_list.html',
        {'model': m, 'field': f}
    )

def choice_detail(request, app_label, module_name, field_name,
                  field_val, models):
    m, f = lookup_field(app_label, module_name, field_name, models)
    try:
        label = dict(f.field.choices)[field_val]
    except KeyError:
        raise Http404('Invalid choice value given')
    obj_list = m.objects(**{f.field.name: field_val})
    numitems = request.GET.get('items')
    items_per_page = [25,50,100]
    if numitems and numitems.isdigit() and int(numitems)>0:
        paginator = Paginator(obj_list, numitems)
    else:
        # fall back to default
        paginator = Paginator(obj_list, items_per_page[0])
    
    page = request.GET.get('page')
    try:
        obj_list_page = paginator.page(page)
    except PageNotAnInteger:
        # If page is not an integer, deliver first page.
        obj_list_page = paginator.page(1)
    except EmptyPage:
        # If page is out of range (e.g. 9999), deliver last page.
        obj_list_page = paginator.page(paginator.num_pages)

    return render_to_response(
        'databrowse/choice_detail.html',
        {
            'model': m,
            'field': f,
            'value': label,
            'object_list': obj_list_page,
            'items_per_page': items_per_page,
        }
    )

"""
    This is testing project for KeyKeeper application.
"""

"""Dynamic REST (or DREST) is an extension of Django REST Framework.

DREST offers the following features on top of the standard DRF kit:

- Linked/embedded/sideloaded relationships
- Field inclusions/exlusions
- Field-based filtering/sorting
- Directory panel for the browsable API
- Optimizations
"""

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import migrations, models


class Migration(migrations.Migration):

    dependencies = [
        ('contenttypes', '0001_initial'),
        ('tests', '0002_auto_20160310_1052'),
    ]

    operations = [
        migrations.AddField(
            model_name='user',
            name='favorite_pet_id',
            field=models.TextField(null=True, blank=True),
            preserve_default=True,
        ),
        migrations.AddField(
            model_name='user',
            name='favorite_pet_type',
            field=models.ForeignKey(blank=True, to='contenttypes.ContentType', null=True),  # noqa
            preserve_default=True,
        ),
    ]

"""FamilySearch User submodule"""
# Python imports



# Magic

class User(object):
    """https://familysearch.org/developers/docs/api/resources#user"""
    def __init__(self):
        """https://familysearch.org/developers/docs/api/examples#user"""
        pass

    def current_user(self):
        """https://familysearch.org/developers/docs/api/users/Current_User_resource"""
        url = self.root_collection['response']['collections'][0]['links']\
        ['current-user']['href']
        return url
        
    def current_user_person(self):
        """https://familysearch.org/developers/docs/api/tree/Current_Tree_Person_resource"""
        try:
            url = self.collections["FSFT"]["response"]["collections"][0][
            "links"]["current-user-person"]["href"]
        except KeyError:
            self.update_collection("FSFT")
            url = self.collections["FSFT"]["response"]["collections"][0][
            "links"]["current-user-person"]["href"]
        return url

    def agent(self, uid):
        """https://familysearch.org/developers/docs/api/users/Agent_resource"""
        return self.user_base + "agents/" + uid

    def current_user_history(self):
        """https://familysearch.org/developers/docs/api/users/Current_User_History_resource"""
        try:
            url = self.collections["FSFT"]["response"]["collections"][0][
            "links"]["current-user-history"]["href"]
        except KeyError:
            self.update_collection("FSFT")
            url = self.collections["FSFT"]["response"]["collections"][0][
            "links"]["current-user-history"]["href"]
        return url
'''
[Advanced] [In-development]

Export a program list to a single yaml file.

The export may contain machine specific paths.
and may need to be edited for portability
'''

from __future__ import unicode_literals, print_function

from argparse import FileType
import logging
import sys

import yaml

from chalmers.utils.cli import add_selection_group, select_programs


log = logging.getLogger('chalmers.export')



def main(args):

    export_data = []

    programs = select_programs(args, filter_paused=False)

    for prog in programs:
        export_data.append({'program': dict(prog.raw_data)})

    yaml.safe_dump(export_data, args.output, default_flow_style=False)

def add_parser(subparsers):
    parser = subparsers.add_parser('export',
                                      help='[IN DEVELOPMENT] Export current configuration to be installed with the "import" command',
                                      description=__doc__)

    add_selection_group(parser)

    parser.add_argument('-o', '--output', type=FileType('w'), default=sys.stdout)
    parser.set_defaults(main=main)

"""
Linux services, this module checks the existence of linux command line 
programs on import

 * systemd_service
 * upstart_service
 * sysv_service
 * cron_service

In that order  
"""
from __future__ import unicode_literals, print_function

import logging
import platform
import sys

from . import cron_service, sysv_service, upstart_service, systemd_service
from chalmers import errors


# Fix for AWS Linux
if sys.version_info.major == 3:
    system_dist = ('system',)
else:
    system_dist = (b'system',)

platform._supported_dists += system_dist


log = logging.getLogger('chalmers.service')

class NoPosixSystemService(object):

    def __init__(self, target_user=None):
        supported_dists = platform._supported_dists + system_dist
        linux = platform.linux_distribution(supported_dists=supported_dists)
        raise errors.ChalmersError("Could not detect system service for platform %s (tried systemd, sysv init and upstart)" % linux[0])

if systemd_service.check():
    PosixSystemService = systemd_service.SystemdService
elif sysv_service.check():
    PosixSystemService = sysv_service.SysVService
elif upstart_service.check():
    PosixSystemService = upstart_service.UpstartService
else:
    PosixSystemService = NoPosixSystemService

PosixLocalService = cron_service.CronService


import abc
import logging
import traceback

import servicemanager
import win32event, win32service, win32api
from win32serviceutil import ServiceFramework


log = logging.getLogger(__name__)

class WindowsService(object, ServiceFramework):
    """
    Base windows service class that provides all the nice things that a python
    service needs
    """
    __metaclass__ = abc.ABCMeta

    def __init__(self, args):
        try:

            self._svc_name_ = args[0]
            self._svc_display_name_ = args[0]

            ServiceFramework.__init__(self, args)
            self.stop_event = win32event.CreateEvent(None, 0, 0, None)

        except Exception:
            self.log("Error in WindowsService.__init__")
            self.log(traceback.format_exc())
            raise

    def log(self, msg):
        'Log to the NTEventlog'
        servicemanager.LogInfoMsg(str(msg))

    def sleep(self, sec):
        win32api.Sleep(sec * 1000, True)


    def SvcDoRun(self):
        self.log('start')

        self.ReportServiceStatus(win32service.SERVICE_START_PENDING)
        try:
            self.ReportServiceStatus(win32service.SERVICE_RUNNING)
            self.log('start')
            self.start()

            self.ReportServiceStatus(win32service.SERVICE_STOPPED)
#             self.log('wait')
#             win32event.WaitForSingleObject(self.stop_event, win32event.INFINITE)
            self.log('done')
        except Exception:
            self.log("Error in WindowsService.SvcDoRun")
            self.log(traceback.format_exc())
            self.SvcStop()


    def SvcStop(self):
        pass
        self.ReportServiceStatus(win32service.SERVICE_STOP_PENDING)
        self.log('stopping')
        self.stop()
        self.log('stopped')
        win32event.SetEvent(self.stop_event)
        self.ReportServiceStatus(win32service.SERVICE_STOPPED)



# Copyright (c) 2014 Johan Burke
# Distributed under the MIT software license.  See http://www.opensource.org/licenses/mit-license.php.

from ..pyelliptic.ecc import *
from ..threads.threadutils import *
from ..constants import *
from .key import *
import hashlib
from struct import *
import sys

def encodeInt(val, alphabet = ALPHABET):
    base = len(alphabet)
    result = ""
    while val > 0:
        rem = val % base
        result = str(alphabet[rem]) + result
        val = val // base
    return result

class Address:
    def __init__(self, hashValue, version=VERSION):
        self.version = version
        self.hashValue = hashValue
        self.encodedValue = ""

    def encodeVersion(self):
        # return the version as a big-endian unsigned byte.
        return pack('>B', self.version)

    def encode(self):
        a = self.encodeVersion() + self.hashValue
        sha = hashlib.new('sha512')
        sha.update(a)
        sha.update(sha.digest())
        checksum = sha.digest()[0:2]
        intValue = int.from_bytes(a + checksum, 'big')
        # this value is in base 64
        self.encodedValue = encodeInt(intValue)

def genKey():
    curve = ECC()
    pubKey = curve.get_pubkey()
    sha = hashlib.new('sha512')
    sha.update(pubKey)
    ripemd = hashlib.new('ripemd160')
    ripemd.update(sha.digest())
    sha.update(ripemd.digest())
    ripemd.update(sha.digest())
    #safePrint(ripemd.digest())
    a = Address(ripemd.digest())
    a.encode()
    key = Key(pubKey, curve.get_privkey(), a.encodedValue)
    return key
from anymesh import AnyMesh, AnyMeshDelegateProtocol

class LeftDelegate(AnyMeshDelegateProtocol):
    def connected_to(self, device_info):
        print('left connected to ' + device_info.name)

    def disconnected_from(self, name):
        pass

    def received_msg(self, message):
        print('left received message from ' + message.sender)
        print('message: ' + message.data['msg'])
        leftMesh.request('right', {'msg': 'back at ya righty!'})


class RightDelegate(AnyMeshDelegateProtocol):
    def connected_to(self, device_info):
        print('right connected to ' + device_info.name)
        rightMesh.request('left', {'msg': 'hey lefty!'})

    def disconnected_from(self, name):
        pass

    def received_msg(self, message):
        print('right received message from ' + message.sender)
        print('message: ' + message.data['msg'])


leftMesh =AnyMesh('left', 'global', LeftDelegate())

rightMesh = AnyMesh('right', 'global', RightDelegate())

AnyMesh.run()

import unittest
import doctest

import urwid

def load_tests(loader, tests, ignore):
    module_doctests = [
        urwid.widget,
        urwid.wimp,
        urwid.decoration,
        urwid.display_common,
        urwid.main_loop,
        urwid.monitored_list,
        urwid.raw_display,
        'urwid.split_repr', # override function with same name
        urwid.util,
        urwid.signals,
        ]
    for m in module_doctests:
        tests.addTests(doctest.DocTestSuite(m,
            optionflags=doctest.ELLIPSIS | doctest.IGNORE_EXCEPTION_DETAIL))
    return tests

import logging

log = logging.getLogger(__name__)

EXCLUDED_LOG_VARS = ['threadName', 'name', 'thread', 'created', 'process', 'processName', 'args', 'module', 'filename',
                     'levelno', 'exc_text', 'pathname', 'lineno', 'msg', 'exc_info', 'message', 'funcName',
                     'relativeCreated', 'levelname', 'msecs', 'asctime']



def register_logging(logger, client_config, cls):
    found = False
    for handler in logger.handlers:
        if isinstance(handler, cls):
            found = True
            reg_handler = handler
    if not found:
        reg_handler = cls(client_config=client_config)
        logger.addHandler(reg_handler)
    return reg_handler


def unregister_logger(logger, handler):
    logger.removeHandler(handler)

import uuid
import datetime
from appenlight_client.timing import get_local_storage
from appenlight_client.timing import default_timer
from appenlight_client.client import PY3
import logging

log = logging.getLogger(__name__)


class AppenlightWSGIWrapper(object):
    __version__ = '0.3'

    def __init__(self, app, appenlight_client):
        self.app = app
        self.appenlight_client = appenlight_client

    def __call__(self, environ, start_response):
        """Run the application and conserve the traceback frames.
        also determine if we got 404
        """
        environ['appenlight.request_id'] = str(uuid.uuid4())
        appenlight_storage = get_local_storage()
        # clear out thread stats on request start
        appenlight_storage.clear()
        app_iter = None
        detected_data = []
        create_report = False
        traceback = None
        http_status = 200
        start_time = default_timer()

        def detect_headers(status, headers, *k, **kw):
            detected_data[:] = status[:3], headers
            return start_response(status, headers, *k, **kw)

        # inject client instance reference to environ
        if 'appenlight.client' not in environ:
            environ['appenlight.client'] = self.appenlight_client
            # some bw. compat stubs

            def local_report(message, include_traceback=True, http_status=200):
                environ['appenlight.force_send'] = True

            def local_log(level, message):
                environ['appenlight.force_send'] = True

            environ['appenlight.report'] = local_report
            environ['appenlight.log'] = local_log
        if 'appenlight.tags' not in environ:
            environ['appenlight.tags'] = {}
        if 'appenlight.extra' not in environ:
            environ['appenlight.extra'] = {}

        try:
            app_iter = self.app(environ, detect_headers)
            return app_iter
        except Exception:
            if hasattr(app_iter, 'close'):
                app_iter.close()
                # we need that here

            traceback = self.appenlight_client.get_current_traceback()
            # by default reraise exceptions for app/FW to handle
            if self.appenlight_client.config['reraise_exceptions']:
                raise
            try:
                start_response('500 INTERNAL SERVER ERROR',
                               [('Content-Type', 'text/html; charset=utf-8')])
            except Exception:
                environ['wsgi.errors'].write(
                    'AppenlightWSGIWrapper middleware catched exception '
                    'in streamed response at a point where response headers '
                    'were already sent.\n')
            else:
                return 'Server Error'
        finally:
            # report 500's and 404's
            # report slowness
            end_time = default_timer()
            appenlight_storage.thread_stats['main'] = end_time - start_time
            delta = datetime.timedelta(seconds=(end_time - start_time))
            stats, slow_calls = appenlight_storage.get_thread_stats()
            if 'appenlight.view_name' not in environ:
                environ['appenlight.view_name'] = getattr(appenlight_storage, 'view_name', '')
            if detected_data and detected_data[0]:
                http_status = int(detected_data[0])
            if self.appenlight_client.config['slow_requests'] and not environ.get('appenlight.ignore_slow'):
                # do we have slow calls/request ?
                if (delta >= self.appenlight_client.config['slow_request_time'] or slow_calls):
                    create_report = True
            if 'appenlight.__traceback' in environ and not environ.get('appenlight.ignore_error'):
                # get traceback gathered by pyramid tween
                traceback = environ['appenlight.__traceback']
                del environ['appenlight.__traceback']
                http_status = 500
                create_report = True
            if traceback and self.appenlight_client.config['report_errors'] and not environ.get('appenlight.ignore_error'):
                http_status = 500
                create_report = True
            elif (self.appenlight_client.config['report_404'] and http_status == 404):
                create_report = True
            if create_report:
                self.appenlight_client.py_report(environ, traceback,
                                                 message=None,
                                                 http_status=http_status,
                                                 start_time=datetime.datetime.utcfromtimestamp(start_time),
                                                 end_time=datetime.datetime.utcfromtimestamp(end_time),
                                                 request_stats=stats,
                                                 slow_calls=slow_calls)
                # dereference
                del traceback
            self.appenlight_client.save_request_stats(stats, view_name=environ.get('appenlight.view_name', ''))
            if self.appenlight_client.config['logging']:
                records = self.appenlight_client.log_handlers_get_records()
                self.appenlight_client.log_handlers_clear_records()
                self.appenlight_client.py_log(environ,
                                              records=records,
                                              r_uuid=environ['appenlight.request_id'],
                                              created_report=create_report)
                # send all data we gathered immediately at the end of request
            self.appenlight_client.check_if_deliver(self.appenlight_client.config['force_send'] or
                                                    environ.get('appenlight.force_send'))

#!/usr/bin/env python
# Programmer: Chris Bunch (chris@appscale.com)


# General-purpose Python library imports
import json
import os
import re
import shutil
import subprocess
import sys
import unittest
import yaml


# Third party testing libraries
import boto.ec2
from flexmock import flexmock


# AppScale import, the library that we're testing here
lib = os.path.dirname(__file__) + os.sep + ".." + os.sep + "lib"
sys.path.append(lib)
from agents.ec2_agent import EC2Agent
from appscale import AppScale
from appscale_tools import AppScaleTools
from custom_exceptions import AppScaleException
from custom_exceptions import AppScalefileException
from custom_exceptions import BadConfigurationException
from local_state import LocalState
from remote_helper import RemoteHelper


class TestAppScale(unittest.TestCase):


  def setUp(self):
    os.environ['EC2_ACCESS_KEY'] = ''
    os.environ['EC2_SECRET_KEY'] = ''

  
  def tearDown(self):
    os.environ['EC2_ACCESS_KEY'] = ''
    os.environ['EC2_SECRET_KEY'] = ''


  def addMockForNoAppScalefile(self, appscale):
    flexmock(os)
    os.should_receive('getcwd').and_return('/boo')

    mock = flexmock(sys.modules['__builtin__'])
    mock.should_call('open')  # set the fall-through
    (mock.should_receive('open')
      .with_args('/boo/' + appscale.APPSCALEFILE)
      .and_raise(IOError))


  def addMockForAppScalefile(self, appscale, contents):
    flexmock(os)
    os.should_receive('getcwd').and_return('/boo')

    mock = flexmock(sys.modules['__builtin__'])
    mock.should_call('open')  # set the fall-through
    (mock.should_receive('open')
     .with_args('/boo/' + appscale.APPSCALEFILE)
     .and_return(flexmock(read=lambda: contents)))

    return mock


  def test_get_nodes(self):
    appscale = flexmock(AppScale())
    builtin = flexmock(sys.modules['__builtin__'])
    builtin.should_call('open')
    nodes = [{'public_ip': 'blarg'}]
    appscale_yaml = {'keyname': 'boo'}
    appscale.should_receive('get_locations_json_file').\
      and_return('locations.json')

    # If the locations JSON file exists, it should return the locations as a
    # dictionary.
    builtin.should_receive('open').with_args('locations.json').\
      and_return(flexmock(read=lambda: json.dumps(nodes)))
    self.assertEqual(nodes, appscale.get_nodes(appscale_yaml['keyname']))

    # If the locations JSON file does not exist, it should throw an
    # AppScaleException.
    builtin.should_receive('open').with_args('locations.json').\
      and_raise(IOError)
    with self.assertRaises(AppScaleException):
      appscale.get_nodes(appscale_yaml['keyname'])


  def test_get_head_node(self):
    shadow_node_1 = {'public_ip': 'public2', 'jobs': ['shadow']}
    appengine_node = {'public_ip': 'public1', 'jobs': ['appengine']}
    shadow_node_2 = {'public_ip': 'public3', 'jobs': ['shadow']}
    appscale = AppScale()

    # If the list of nodes does not have a node with the shadow role, the
    # tools should raise an AppScaleException.
    with self.assertRaises(AppScaleException):
      appscale.get_head_node([appengine_node])

    # If the list of nodes contains any nodes with the shadow role, the tools
    # should return the public IP address of the first node which has that
    # role.
    self.assertEqual(shadow_node_1['public_ip'],
      appscale.get_head_node([shadow_node_1, appengine_node, shadow_node_2]))


  def testInitWithNoAppScalefile(self):
    # calling 'appscale init cloud' if there's no AppScalefile in the local
    # directory should write a new cloud config file there
    appscale = AppScale()

    flexmock(os)
    os.should_receive('getcwd').and_return('/boo')

    flexmock(os.path)
    os.path.should_receive('exists').with_args(
      '/boo/' + appscale.APPSCALEFILE).and_return(False)

    # mock out the actual writing of the template file
    flexmock(shutil)
    shutil.should_receive('copy').with_args(
      appscale.TEMPLATE_CLOUD_APPSCALEFILE, '/boo/' + appscale.APPSCALEFILE) \
      .and_return()

    appscale.init('cloud')


  def testInitWithAppScalefile(self):
    # calling 'appscale init cloud' if there is an AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()

    flexmock(os)
    os.should_receive('getcwd').and_return('/boo')

    flexmock(os.path)
    os.path.should_receive('exists').with_args('/boo/' + appscale.APPSCALEFILE).and_return(True)

    self.assertRaises(AppScalefileException, appscale.init, 'cloud')


  def testUpWithNoAppScalefile(self):
    # calling 'appscale up' if there is no AppScalefile present
    # should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.up)


  def testUpWithClusterAppScalefile(self):
    # calling 'appscale up' if there is an AppScalefile present
    # should call appscale-run-instances with the given config
    # params. here, we assume that the file is intended for use
    # on a virtualized cluster
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'ips_layout': {'master': 'ip1', 'appengine': 'ip1',
                     'database': 'ip2', 'zookeeper': 'ip2'},
      'keyname': 'boobazblarg',
      'group': 'boobazblarg'
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    flexmock(os.path)
    os.path.should_call('exists')
    os.path.should_receive('exists').with_args(
      '/boo/' + appscale.APPSCALEFILE).and_return(True)

    # for this test, let's say that we don't have an SSH key already
    # set up for ip1 and ip2
    # TODO(cgb): Add in tests where we have a key for ip1 but not ip2,
    # and the case where we have a key but it doesn't work
    key_path = os.path.expanduser('~/.appscale/boobazblarg.key')
    os.path.should_receive('exists').with_args(key_path).and_return(False)

    # finally, mock out the actual appscale tools calls. since we're running
    # via a cluster, this means we call add-keypair to set up SSH keys, then
    # run-instances to start appscale
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('add_keypair')
    AppScaleTools.should_receive('run_instances')

    appscale.up()


  def testUpWithMalformedClusterAppScalefile(self):
    # if we try to use an IPs layout that isn't a dictionary, we should throw up
    # and die
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file, with an IPs layout that is a str
    contents = {
      'ips_layout': "'master' 'ip1' 'appengine' 'ip1'",
      'keyname': 'boobazblarg', 'group' : 'boobazblarg'
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    flexmock(os.path)
    os.path.should_call('exists')
    os.path.should_receive('exists').with_args(
      '/boo/' + appscale.APPSCALEFILE).and_return(True)

    # finally, mock out the actual appscale tools calls. since we're running
    # via a cluster, this means we call add-keypair to set up SSH keys, then
    # run-instances to start appscale
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('add_keypair')

    self.assertRaises(BadConfigurationException, appscale.up)


  def testUpWithCloudAppScalefile(self):
    # calling 'appscale up' if there is an AppScalefile present
    # should call appscale-run-instances with the given config
    # params. here, we assume that the file is intended for use
    # on EC2
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'min' : 1,
      'max' : 1,
      'zone' : 'my-zone-1b'
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    flexmock(os.path)
    os.path.should_call('exists')
    os.path.should_receive('exists').with_args(
      '/boo/' + appscale.APPSCALEFILE).and_return(True)

    # throw in some mocks for the argument parsing
    for credential in EC2Agent.REQUIRED_CREDENTIALS:
      os.environ[credential] = "baz"

    # finally, pretend that our ec2 zone and image exists
    fake_ec2 = flexmock(name="fake_ec2")
    fake_ec2.should_receive('get_all_instances')

    fake_ec2.should_receive('get_all_zones').with_args('my-zone-1b') \
      .and_return('anything')

    fake_ec2.should_receive('get_image').with_args('ami-ABCDEFG') \
      .and_return()
    flexmock(boto.ec2)
    boto.ec2.should_receive('connect_to_region').with_args('my-zone-1',
      aws_access_key_id='baz', aws_secret_access_key='baz').and_return(fake_ec2)

    # finally, mock out the actual appscale-run-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('run_instances')
    appscale.up()


  def testUpWithEC2EnvironmentVariables(self):
    # if the user wants us to use their EC2 credentials when running AppScale,
    # we should make sure they get set
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'min' : 1,
      'max' : 1,
      'EC2_ACCESS_KEY' : 'access key',
      'EC2_SECRET_KEY' : 'secret key',
      'zone' : 'my-zone-1b'
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    flexmock(os.path)
    os.path.should_call('exists')
    os.path.should_receive('exists').with_args(
      '/boo/' + appscale.APPSCALEFILE).and_return(True)

    # finally, pretend that our ec2 zone/image to use exist
    fake_ec2 = flexmock(name="fake_ec2")
    fake_ec2.should_receive('get_all_instances')

    fake_ec2.should_receive('get_all_zones').with_args('my-zone-1b') \
      .and_return('anything')

    fake_ec2.should_receive('get_image').with_args('ami-ABCDEFG') \
      .and_return()
    flexmock(boto.ec2)
    boto.ec2.should_receive('connect_to_region').with_args('my-zone-1',
      aws_access_key_id='access key',
      aws_secret_access_key='secret key').and_return(fake_ec2)

    # finally, mock out the actual appscale-run-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('run_instances')
    appscale.up()

    self.assertEquals('access key', os.environ['EC2_ACCESS_KEY'])
    self.assertEquals('secret key', os.environ['EC2_SECRET_KEY'])


  def testSshWithNoAppScalefile(self):
    # calling 'appscale ssh' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.ssh, 1)


  def testSshWithNotIntArg(self):
    # calling 'appscale ssh not-int' should throw up and die
    appscale = AppScale()
    self.addMockForAppScalefile(appscale, "")
    self.assertRaises(TypeError, appscale.ssh, "boo")


  def testSshWithNoNodesJson(self):
    # calling 'appscale ssh' when there isn't a locations.json
    # file should throw up and die
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_raise(IOError))

    self.assertRaises(AppScaleException, appscale.ssh, 0)


  def testSshWithIndexOutOfBounds(self):
    # calling 'appscale ssh 1' should ssh to the second node
    # (nodes[1]). If there's only one node in this deployment,
    # we should throw up and die
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    one = {
      'public_ip' : 'blarg'
    }
    nodes = [one]
    nodes_contents = json.dumps(nodes)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_return(flexmock(read=lambda: nodes_contents)))

    self.assertRaises(AppScaleException, appscale.ssh, 1)


  def testSshWithIndexInBounds(self):
    # calling 'appscale ssh 1' should ssh to the second node
    # (nodes[1]). If there are two nodes in this deployment,
    # we should ssh into it successfully
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    one = {
      'public_ip' : 'blarg'
    }
    two = {
      'public_ip' : 'blarg2'
    }
    nodes = [one, two]
    nodes_contents = json.dumps(nodes)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_return(flexmock(read=lambda: nodes_contents)))

    flexmock(subprocess)
    subprocess.should_receive('call').with_args(["ssh", "-o", "StrictHostkeyChecking=no", "-i", appscale.get_key_location('boo'), "root@blarg2"]).and_return().once()
    appscale.ssh(1)


  def testStatusWithNoAppScalefile(self):
    # calling 'appscale status' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.status)


  def testStatusWithCloudAppScalefile(self):
    # calling 'appscale status' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-describe-instances' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-describe-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('describe_instances')
    appscale.status()


  def testDeployWithNoAppScalefile(self):
    # calling 'appscale deploy' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    app = "/bar/app"
    self.assertRaises(AppScalefileException, appscale.deploy, app)


  def testDeployWithCloudAppScalefile(self):
    # calling 'appscale deploy app' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-upload-app' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-run-instances call
    fake_port = 8080
    fake_host = 'fake_host'
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('upload_app').and_return(
      (fake_host, fake_port))
    app = '/bar/app'
    (host, port) = appscale.deploy(app)
    self.assertEquals(fake_host, host)
    self.assertEquals(fake_port, port)


  def testUndeployWithNoAppScalefile(self):
    # calling 'appscale undeploy' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    appid = "barapp"
    self.assertRaises(AppScalefileException, appscale.undeploy, appid)


  def testUndeployWithCloudAppScalefile(self):
    # calling 'appscale undeploy app' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-remove-app' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-run-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('remove_app')
    app = 'barapp'
    appscale.undeploy(app)


  def testDeployWithCloudAppScalefileAndTestFlag(self):
    # same as before, but with the 'test' flag in our AppScalefile
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1,
      'test' : True
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-run-instances call
    fake_port = 8080
    fake_host = 'fake_host'
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('upload_app').and_return(
      (fake_host, fake_port))
    app = '/bar/app'
    (host, port) = appscale.deploy(app)
    self.assertEquals(fake_host, host)
    self.assertEquals(fake_port, port)


  def testTailWithNoAppScalefile(self):
    # calling 'appscale tail' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.tail, 0, '')


  def testTailWithNotIntArg(self):
    # calling 'appscale tail not-int *' should throw up and die
    appscale = AppScale()
    self.addMockForAppScalefile(appscale, "")
    self.assertRaises(TypeError, appscale.tail, "boo", "")


  def testTailWithNoNodesJson(self):
    # calling 'appscale tail' when there isn't a locations.json
    # file should throw up and die
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_raise(IOError))

    self.assertRaises(AppScaleException, appscale.tail, 0, "")

  def testTailWithIndexOutOfBounds(self):
    # calling 'appscale tail 1 *' should tail from the second node
    # (nodes[1]). If there's only one node in this deployment,
    # we should throw up and die
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    one = {
      'public_ip' : 'blarg'
    }
    nodes = [one]
    nodes_contents = json.dumps(nodes)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_return(flexmock(read=lambda: nodes_contents)))

    self.assertRaises(AppScaleException, appscale.tail, 1, '')

  def testTailWithIndexInBounds(self):
    # calling 'appscale tail 1 *' should tail from the second node
    # (nodes[1]). If there are two nodes in this deployment,
    # we should tail from it successfully
    appscale = AppScale()

    contents = { 'keyname' : 'boo' }
    yaml_dumped_contents = yaml.dump(contents)

    one = {
      'public_ip' : 'blarg'
    }
    two = {
      'public_ip' : 'blarg2'
    }
    nodes = [one, two]
    nodes_contents = json.dumps(nodes)

    mock = self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    (mock.should_receive('open')
      .with_args(appscale.get_locations_json_file('boo'))
      .and_return(flexmock(read=lambda: nodes_contents)))

    flexmock(subprocess)
    subprocess.should_receive('call').with_args(["ssh", "-o",
      "StrictHostkeyChecking=no", "-i", appscale.get_key_location('boo'),
      "root@blarg2", "tail -F /var/log/appscale/c*"]).and_return().once()
    appscale.tail(1, "c*")


  def testGetLogsWithNoAppScalefile(self):
    # calling 'appscale logs' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.logs, '')


  def testGetLogsWithKeyname(self):
    # calling 'appscale logs dir' with a keyname should produce
    # a command to exec with the --keyname flag
    appscale = AppScale()
    contents = {
      "keyname" : "boo"
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # mock out the actual call to appscale-gather-logs
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('run_instances')
    self.assertRaises(BadConfigurationException, appscale.logs, '/baz')

  
  def testRelocateWithNoAppScalefile(self):
    # calling 'appscale relocate' with no AppScalefile in the local directory
    # should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.relocate, 'myapp', 80, 443)


  def testRelocateWithAppScalefile(self):
    # calling 'appscale relocate' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-relocate-app' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-relocate-app call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('relocate_app')
    appscale.relocate('myapp', 80, 443)


  def testGetPropertyWithNoAppScalefile(self):
    # calling 'appscale get' with no AppScalefile in the local directory
    # should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.get, '.*')


  def testGetPropertyWithAppScalefile(self):
    # calling 'appscale get' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-get-property' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-get-property call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('get_property')
    appscale.get('.*')


  def testSetPropertyWithNoAppScalefile(self):
    # calling 'appscale set' with no AppScalefile in the local directory
    # should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.set, 'key', 'value')


  def testSetPropertyWithAppScalefile(self):
    # calling 'appscale set' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-get-property' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-set-property call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('set_property')
    appscale.set('key', 'value')


  def testDestroyWithNoAppScalefile(self):
    # calling 'appscale destroy' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.destroy)


  def testDestroyWithCloudAppScalefile(self):
    # calling 'appscale destroy' with an AppScalefile in the local
    # directory should collect any parameters needed for the
    # 'appscale-terminate-instances' command and then exec it
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-terminate-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('terminate_instances')
    appscale.destroy()


  def testDestroyWithEC2EnvironmentVariables(self):
    # if the user wants us to use their EC2 credentials when running AppScale,
    # we should make sure they get set
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'min' : 1,
      'max' : 1,
      'EC2_ACCESS_KEY' : 'access key',
      'EC2_SECRET_KEY' : 'secret key'
    }
    yaml_dumped_contents = yaml.dump(contents)
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)

    # finally, mock out the actual appscale-terminate-instances call
    flexmock(AppScaleTools)
    AppScaleTools.should_receive('terminate_instances')
    appscale.destroy()

    self.assertEquals('access key', os.environ['EC2_ACCESS_KEY'])
    self.assertEquals('secret key', os.environ['EC2_SECRET_KEY'])


  def testCleanWithNoAppScalefile(self):
    # calling 'appscale clean' with no AppScalefile in the local
    # directory should throw up and die
    appscale = AppScale()
    self.addMockForNoAppScalefile(appscale)
    self.assertRaises(AppScalefileException, appscale.clean)


  def testCleanInCloudDeployment(self):
    # calling 'appscale clean' in a cloud deployment should throw up and die
    appscale = AppScale()

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'infrastructure' : 'ec2',
      'machine' : 'ami-ABCDEFG',
      'keyname' : 'bookey',
      'group' : 'boogroup',
      'verbose' : True,
      'min' : 1,
      'max' : 1
    }
    yaml_dumped_contents = yaml.dump(contents)

    self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    self.assertRaises(BadConfigurationException, appscale.clean)


  def testCleanInClusterDeployment(self):
    # calling 'appscale clean' in a cluster deployment should ssh into each of
    # the boxes specified in the ips_layout and run the terminate script

    # Mock out the actual file reading itself, and slip in a YAML-dumped
    # file
    contents = {
      'ips_layout' : {
        'controller': 'public1',
        'servers': ['public2', 'public3']
      },
      'test' : True
    }
    yaml_dumped_contents = yaml.dump(contents)

    flexmock(RemoteHelper)
    RemoteHelper.should_receive('ssh') \
      .with_args(re.compile('public[123]'), 'appscale', str, False)

    flexmock(LocalState)
    LocalState.should_receive('cleanup_appscale_files').with_args('appscale')

    appscale = AppScale()
    self.addMockForAppScalefile(appscale, yaml_dumped_contents)
    expected = ['public1', 'public2', 'public3']
    self.assertEquals(expected, appscale.clean())

#!/usr/bin/env python
# Programmer: Navraj Chohan <nlake44@gmail.com>

import os
import sys
import time
import unittest

from flexmock import flexmock
import kazoo.client
import kazoo.exceptions
import kazoo.protocol
import kazoo.protocol.states

sys.path.append(os.path.join(os.path.dirname(__file__), "../../"))  
from dbconstants import *

sys.path.append(os.path.join(os.path.dirname(__file__), "../../"))  
from zkappscale import zktransaction as zk
from zkappscale.zktransaction import ZKTransactionException


class TestZookeeperTransaction(unittest.TestCase):
  """
  """

  def setUp(self):
    self.appid = 'appid'
    self.handle = None

  def test_increment_and_get_counter(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').with_args(
      self.appid).and_return('/rootpath')
    
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', create='create',
      delete_async='delete_async', connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').and_return(None)

    fake_counter = flexmock(name='fake_counter', value='value')
    fake_counter.value = 1
    fake_counter.should_receive('__add__').and_return(2)
    fake_zookeeper.should_receive("Counter").and_return(fake_counter)
    # mock out deleting the zero id we get the first time around

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # assert, make sure we got back our id
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals((0, 1), transaction.increment_and_get_counter(
      self.appid, 1))


  def test_create_sequence_node(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').with_args(
      self.appid).and_return('/rootpath')
    
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', create='create',
      delete='delete', connected=lambda: True)
    fake_zookeeper.should_receive('start')

    # mock out zookeeper.create for txn id
    path_to_create = "/rootpath/" + self.appid
    zero_path = path_to_create + "/0"
    nonzero_path = path_to_create + "/1"


    fake_zookeeper.should_receive('retry').with_args('create', str, value=str,
      acl=None, makepath=bool, sequence=bool, ephemeral=bool).\
      and_return(zero_path).and_return(nonzero_path)

    # mock out deleting the zero id we get the first time around
    fake_zookeeper.should_receive('retry').with_args('delete', zero_path)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # assert, make sure we got back our id
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(1, transaction.create_sequence_node('/rootpath/' + \
      self.appid, 'now'))

  def test_create_node(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').with_args(
      self.appid).and_return('/rootpath')
    
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', create='create',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('create', str, value=str,
      acl=None, makepath=bool, sequence=bool, ephemeral=bool)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # mock out zookeeper.create for txn id
    path_to_create = "/rootpath/" + self.appid
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(None, transaction.create_node('/rootpath/' + self.appid,
      'now'))


  def test_get_transaction_id(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').with_args(
      self.appid).and_return('/rootpath/' + self.appid)
    path_to_create = "/rootpath/" + self.appid + "/" + zk.APP_TX_PREFIX
    zk.ZKTransaction.should_receive('get_txn_path_before_getting_id') \
      .with_args(self.appid).and_return(path_to_create)
    
    # mock out time.time
    flexmock(time)
    time.should_receive('time').and_return(1000)
    
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry')

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # mock out making the txn id
    zk.ZKTransaction.should_receive('create_sequence_node').with_args(
      path_to_create, '1000').and_return(1)

    # mock out zookeeper.create for is_xg
    xg_path = path_to_create + "/1/" + zk.XG_PREFIX
    zk.ZKTransaction.should_receive('get_xg_path').and_return(xg_path)
    zk.ZKTransaction.should_receive('create_node').with_args(xg_path, '1000')

    # assert, make sure we got back our id
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(1, transaction.get_transaction_id(self.appid, is_xg=True))

  def test_get_txn_path_before_getting_id(self):
    # mock out initializing a ZK connection
    flexmock(zk.ZKTransaction)

    fake_zookeeper = flexmock(name='fake_zoo')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry')

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    zk.ZKTransaction.should_receive('get_app_root_path').and_return("app_root_path")

    expected = zk.PATH_SEPARATOR.join(["app_root_path", zk.APP_TX_PATH, zk.APP_TX_PREFIX])
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(expected,
      transaction.get_txn_path_before_getting_id(self.appid))

  def test_get_xg_path(self):
    # mock out initializing a ZK connection
    flexmock(zk.ZKTransaction)

    fake_zookeeper = flexmock(name='fake_zoo')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry')

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)
 
    tx_id = 100
    tx_str = zk.APP_TX_PREFIX + "%010d" % tx_id
    zk.ZKTransaction.should_receive('get_app_root_path') \
      .and_return("app_root_path")

    expected = zk.PATH_SEPARATOR.join(["app_root_path", zk.APP_TX_PATH,
      tx_str, zk.XG_PREFIX]) 

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(expected, transaction.get_xg_path("xxx", 100))

  def test_is_in_transaction(self):
    # shared mocks
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_path') \
      .and_return('/transaction/path')

    fake_zookeeper = flexmock(name='fake_zoo', exists='exists',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # test when the transaction is running
    zk.ZKTransaction.should_receive('is_blacklisted').and_return(False)
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.is_in_transaction(self.appid, 1))

    # and when it's not
    zk.ZKTransaction.should_receive('is_blacklisted').and_return(False)
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(False)
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(False, transaction.is_in_transaction(self.appid, 1))

    # and when it's blacklisted
    zk.ZKTransaction.should_receive('is_blacklisted').and_return(True)
    fake_transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertRaises(zk.ZKTransactionException, transaction.is_in_transaction,
      self.appid, 1)

  def test_acquire_lock(self):
    # mock out waitForConnect
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_lock_root_path').\
       and_return('/lock/root/path')
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').\
       and_return('/rootpath/' + self.appid)
    fake_zookeeper = flexmock(name='fake_zoo', get='get',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry')

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # first, test out getting a lock for a regular transaction, that we don't
    # already have the lock for
    zk.ZKTransaction.should_receive('is_in_transaction').and_return(False)
    zk.ZKTransaction.should_receive('acquire_additional_lock').and_return(True)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.acquire_lock(self.appid, "txid",
      "somekey"))

    # next, test when we're in a transaction and we already have the lock
    zk.ZKTransaction.should_receive('is_in_transaction').and_return(True)
    zk.ZKTransaction.should_receive('get_transaction_lock_list_path').\
       and_return('/rootpath/' + self.appid + "/tx1")
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return(['/lock/root/path'])

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.acquire_lock(self.appid, "txid",
      "somekey"))

    # next, test when we're in a non-XG transaction and we're not in the lock
    # root path
    zk.ZKTransaction.should_receive('is_in_transaction').and_return(True)
    zk.ZKTransaction.should_receive('get_transaction_lock_list_path').\
       and_return('/rootpath/' + self.appid + "/tx1")
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return(['/lock/root/path2'])
    zk.ZKTransaction.should_receive('is_xg').and_return(False)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertRaises(zk.ZKTransactionException, transaction.acquire_lock, 
      self.appid, "txid", "somekey")

    # next, test when we're in a XG transaction and we're not in the lock
    # root path
    zk.ZKTransaction.should_receive('is_in_transaction').and_return(True)
    zk.ZKTransaction.should_receive('get_transaction_lock_list_path').\
       and_return('/rootpath/' + self.appid + "/tx1")
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return(['/lock/root/path2'])
    zk.ZKTransaction.should_receive('is_xg').and_return(True)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.acquire_lock(self.appid, "txid",
      "somekey"))


  def test_acquire_additional_lock(self):
    # mock out waitForConnect
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('check_transaction')
    zk.ZKTransaction.should_receive('get_transaction_path').\
       and_return('/txn/path')
    zk.ZKTransaction.should_receive('get_lock_root_path').\
       and_return('/lock/root/path')
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').\
       and_return('/rootpath/' + self.appid)

    fake_zookeeper = flexmock(name='fake_zoo', create='create',
      create_async='create_async', get='get', set_async='set_async',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('create', str, makepath=bool, sequence=bool,
      ephemeral=bool, value=str, acl=None).and_return("/some/lock/path")
    fake_zookeeper.should_receive('retry').with_args('create_async', str, value=str,
      acl=None, ephemeral=bool, makepath=bool, sequence=bool)
    fake_zookeeper.should_receive('retry').with_args('create_async', str, value=str,
      acl=str, ephemeral=bool, makepath=bool, sequence=bool)
    lock_list = ['path1', 'path2', 'path3'] 
    lock_list_str = zk.LOCK_LIST_SEPARATOR.join(lock_list)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([lock_list_str])
    fake_zookeeper.should_receive('retry').with_args('set_async', str, str)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.acquire_additional_lock(self.appid,
      "txid", "somekey", False))

    # Test for when we want to create a new ZK node for the lock path
    self.assertEquals(True, transaction.acquire_additional_lock(self.appid,
      "txid", "somekey", True))

    # Test for existing max groups
    lock_list = ['path' + str(num+1) for num in range(zk.MAX_GROUPS_FOR_XG)]
    lock_list_str = zk.LOCK_LIST_SEPARATOR.join(lock_list)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([lock_list_str])

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertRaises(zk.ZKTransactionException,
      transaction.acquire_additional_lock, self.appid, "txid", "somekey", False)

    # Test for when there is a node which already exists.
    fake_zookeeper.should_receive('retry').with_args('create', str, str, None,
      bool, bool, bool).and_raise(kazoo.exceptions.NodeExistsError)
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertRaises(zk.ZKTransactionException,
      transaction.acquire_additional_lock, self.appid, "txid", "somekey", False)


  def test_check_transaction(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_transaction_prefix_path').with_args(
      self.appid).and_return('/rootpath')
    zk.ZKTransaction.should_receive('is_blacklisted').and_return(False)
    
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.check_transaction(self.appid, 1))

    # Check to make sure it raises exception for blacklisted transactions.
    zk.ZKTransaction.should_receive('is_blacklisted').and_return(True)
    self.assertRaises(zk.ZKTransactionException, transaction.check_transaction,
      self.appid, 1)

    zk.ZKTransaction.should_receive('is_blacklisted').and_return(False)
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(False)
    self.assertRaises(zk.ZKTransactionException, transaction.check_transaction,
      self.appid, 1)
  
  def test_is_xg(self):
    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists',
      connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.is_xg(self.appid, 1))

  def test_release_lock(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('check_transaction')
    zk.ZKTransaction.should_receive('get_transaction_path').\
      and_return('/rootpath')
    zk.ZKTransaction.should_receive('get_transaction_lock_list_path').\
      and_return('/rootpath')
    zk.ZKTransaction.should_receive('is_xg').and_return(False)

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists', get='get',
      delete='delete', delete_async='delete_async',
      get_children='get_children', connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return(['/1/2/3'])
    fake_zookeeper.should_receive('retry').with_args('delete_async', str)
    fake_zookeeper.should_receive('retry').with_args('delete', str)
    fake_zookeeper.should_receive('retry').with_args('get_children', str) \
      .and_return(['1','2'])

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.release_lock(self.appid, 1))

    zk.ZKTransaction.should_receive('is_xg').and_return(True)
    self.assertEquals(True, transaction.release_lock(self.appid, 1))

    # Check to make sure it raises exception for blacklisted transactions.
    zk.ZKTransaction.should_receive('is_xg').and_return(False)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_raise(kazoo.exceptions.NoNodeError)
    self.assertRaises(zk.ZKTransactionException, transaction.release_lock,
      self.appid, 1)


  def test_is_blacklisted(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_blacklist_root_path').\
      and_return("bl_root_path")

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', create='create', exists='exists',
      get_children='get_children', connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('create', str, str, None,
      bool, bool, bool).and_return()
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    fake_zookeeper.should_receive('retry').with_args('get_children', str) \
      .and_return(['1','2'])

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.is_blacklisted(self.appid, 1))

  def test_register_updated_key(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('get_valid_transaction_path').\
      and_return('/txn/path')
    zk.ZKTransaction.should_receive('get_transaction_path').\
      and_return('/txn/path')

    zk.ZKTransaction.should_receive('get_blacklist_root_path').\
      and_return("bl_root_path")

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists',
      set_async='set_async', connected=lambda: True)
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    fake_zookeeper.should_receive('retry').with_args('set_async', str, str)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.register_updated_key(self.appid, 
      "1", "2", "somekey"))

    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(False)
    self.assertRaises(ZKTransactionException, 
      transaction.register_updated_key, self.appid, "1", "2", "somekey")

  def test_try_garbage_collection(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('update_node')

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists', get='get',
      get_children='get_children', create='create', delete='delete')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([str(time.time() + 10000)])
    fake_zookeeper.should_receive('retry').with_args('get_children', str) \
      .and_return(['1','2','3'])
    fake_zookeeper.should_receive('retry').with_args('create', str, value=str, 
      acl=None, ephemeral=bool)
    fake_zookeeper.should_receive('retry').with_args('delete', str)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    # Put the last time we ran GC way into the future.
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(False, transaction.try_garbage_collection(self.appid, 
      "/some/path"))

    # Make it so we recently ran the GC
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([str(time.time())])
    self.assertEquals(False, transaction.try_garbage_collection(self.appid, 
      "/some/path"))

    # Make it so we ran the GC a long time ago.
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([str(time.time() - 1000)])
    self.assertEquals(True, transaction.try_garbage_collection(self.appid, 
      "/some/path"))

    # No node means we have not run the GC before, so run it.
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_raise(kazoo.exceptions.NoNodeError)
    self.assertEquals(True, transaction.try_garbage_collection(self.appid, 
      "/some/path"))
    
  def test_notify_failed_transaction(self):
    pass
    #TODO  

  def test_execute_garbage_collection(self):
    # mock out getTransactionRootPath
    flexmock(zk.ZKTransaction)
    zk.ZKTransaction.should_receive('notify_failed_transaction')

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', exists='exists', get='get',
      get_children='get_children')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('exists', str) \
      .and_return(True)
    fake_zookeeper.should_receive('retry').with_args('get', str) \
      .and_return([str(time.time() + 10000)])
    fake_zookeeper.should_receive('retry').with_args('get_children', str) \
      .and_return(['1','2','3'])

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)
    transaction = zk.ZKTransaction(host="something", start_gc=False)
    transaction.execute_garbage_collection(self.appid, "some/path")

  def test_get_lock_with_path(self):
    flexmock(zk.ZKTransaction)

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', create='create')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('create', str, value=str,
      acl=None, ephemeral=bool).and_return(True)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.get_lock_with_path('path'))

    fake_zookeeper.should_receive('retry').with_args('create', str, value=str,
      acl=None, ephemeral=bool).and_raise(kazoo.exceptions.NodeExistsError)
    self.assertEquals(False, transaction.get_lock_with_path('some/path'))
  
  def test_release_lock_with_path(self):
    flexmock(zk.ZKTransaction)

    # mock out initializing a ZK connection
    fake_zookeeper = flexmock(name='fake_zoo', delete='delete')
    fake_zookeeper.should_receive('start')
    fake_zookeeper.should_receive('retry').with_args('delete', str)

    flexmock(kazoo.client)
    kazoo.client.should_receive('KazooClient').and_return(fake_zookeeper)

    transaction = zk.ZKTransaction(host="something", start_gc=False)
    self.assertEquals(True, transaction.release_lock_with_path('some/path'))

    fake_zookeeper.should_receive('retry').with_args('delete', str). \
      and_raise(kazoo.exceptions.NoNodeError)
    self.assertRaises(ZKTransactionException,
      transaction.release_lock_with_path, 'some/path')
     
if __name__ == "__main__":
  unittest.main()    

import cgi
import datetime
import wsgiref.handlers

from google.appengine.ext import webapp

class MainPage(webapp.RequestHandler):
  def get(self):
    self.response.out.write('<html><body>')
    self.response.out.write('<p>Hello</p>')
    self.response.out.write('</body></html>')

    
application = webapp.WSGIApplication([
  ('/', MainPage),
], debug=True)


def main():
  wsgiref.handlers.CGIHandler().run(application)


if __name__ == '__main__':
  main()

#!/usr/bin/env python
from google.appengine._internal.django.core.management import execute_manager
try:
    import settings # Assumed to be in the same directory.
except ImportError:
    import sys
    sys.stderr.write("Error: Can't find the file 'settings.py' in the directory containing %r. It appears you've customized things.\nYou'll have to run django-admin.py, passing it your settings module.\n(If the file settings.py does indeed exist, it's causing an ImportError somehow.)\n" % __file__)
    sys.exit(1)

if __name__ == "__main__":
    execute_manager(settings)

class FileProxyMixin(object):
    """
    A mixin class used to forward file methods to an underlaying file
    object.  The internal file object has to be called "file"::

        class FileProxy(FileProxyMixin):
            def __init__(self, file):
                self.file = file
    """

    encoding = property(lambda self: self.file.encoding)
    fileno = property(lambda self: self.file.fileno)
    flush = property(lambda self: self.file.flush)
    isatty = property(lambda self: self.file.isatty)
    newlines = property(lambda self: self.file.newlines)
    read = property(lambda self: self.file.read)
    readinto = property(lambda self: self.file.readinto)
    readline = property(lambda self: self.file.readline)
    readlines = property(lambda self: self.file.readlines)
    seek = property(lambda self: self.file.seek)
    softspace = property(lambda self: self.file.softspace)
    tell = property(lambda self: self.file.tell)
    truncate = property(lambda self: self.file.truncate)
    write = property(lambda self: self.file.write)
    writelines = property(lambda self: self.file.writelines)
    xreadlines = property(lambda self: self.file.xreadlines)

    def __iter__(self):
        return iter(self.file)

"""
XML serializer.
"""

from google.appengine._internal.django.conf import settings
from google.appengine._internal.django.core.serializers import base
from google.appengine._internal.django.db import models, DEFAULT_DB_ALIAS
from google.appengine._internal.django.utils.xmlutils import SimplerXMLGenerator
from google.appengine._internal.django.utils.encoding import smart_unicode
from xml.dom import pulldom

class Serializer(base.Serializer):
    """
    Serializes a QuerySet to XML.
    """

    def indent(self, level):
        if self.options.get('indent', None) is not None:
            self.xml.ignorableWhitespace('\n' + ' ' * self.options.get('indent', None) * level)

    def start_serialization(self):
        """
        Start serialization -- open the XML document and the root element.
        """
        self.xml = SimplerXMLGenerator(self.stream, self.options.get("encoding", settings.DEFAULT_CHARSET))
        self.xml.startDocument()
        self.xml.startElement("django-objects", {"version" : "1.0"})

    def end_serialization(self):
        """
        End serialization -- end the document.
        """
        self.indent(0)
        self.xml.endElement("django-objects")
        self.xml.endDocument()

    def start_object(self, obj):
        """
        Called as each object is handled.
        """
        if not hasattr(obj, "_meta"):
            raise base.SerializationError("Non-model object (%s) encountered during serialization" % type(obj))

        self.indent(1)
        obj_pk = obj._get_pk_val()
        if obj_pk is None:
            attrs = {"model": smart_unicode(obj._meta),}
        else:
            attrs = {
                "pk": smart_unicode(obj._get_pk_val()),
                "model": smart_unicode(obj._meta),
            }

        self.xml.startElement("object", attrs)

    def end_object(self, obj):
        """
        Called after handling all fields for an object.
        """
        self.indent(1)
        self.xml.endElement("object")

    def handle_field(self, obj, field):
        """
        Called to handle each field on an object (except for ForeignKeys and
        ManyToManyFields)
        """
        self.indent(2)
        self.xml.startElement("field", {
            "name" : field.name,
            "type" : field.get_internal_type()
        })

        # Get a "string version" of the object's data.
        if getattr(obj, field.name) is not None:
            self.xml.characters(field.value_to_string(obj))
        else:
            self.xml.addQuickElement("None")

        self.xml.endElement("field")

    def handle_fk_field(self, obj, field):
        """
        Called to handle a ForeignKey (we need to treat them slightly
        differently from regular fields).
        """
        self._start_relational_field(field)
        related = getattr(obj, field.name)
        if related is not None:
            if self.use_natural_keys and hasattr(related, 'natural_key'):
                # If related object has a natural key, use it
                related = related.natural_key()
                # Iterable natural keys are rolled out as subelements
                for key_value in related:
                    self.xml.startElement("natural", {})
                    self.xml.characters(smart_unicode(key_value))
                    self.xml.endElement("natural")
            else:
                if field.rel.field_name == related._meta.pk.name:
                    # Related to remote object via primary key
                    related = related._get_pk_val()
                else:
                    # Related to remote object via other field
                    related = getattr(related, field.rel.field_name)
                self.xml.characters(smart_unicode(related))
        else:
            self.xml.addQuickElement("None")
        self.xml.endElement("field")

    def handle_m2m_field(self, obj, field):
        """
        Called to handle a ManyToManyField. Related objects are only
        serialized as references to the object's PK (i.e. the related *data*
        is not dumped, just the relation).
        """
        if field.rel.through._meta.auto_created:
            self._start_relational_field(field)
            if self.use_natural_keys and hasattr(field.rel.to, 'natural_key'):
                # If the objects in the m2m have a natural key, use it
                def handle_m2m(value):
                    natural = value.natural_key()
                    # Iterable natural keys are rolled out as subelements
                    self.xml.startElement("object", {})
                    for key_value in natural:
                        self.xml.startElement("natural", {})
                        self.xml.characters(smart_unicode(key_value))
                        self.xml.endElement("natural")
                    self.xml.endElement("object")
            else:
                def handle_m2m(value):
                    self.xml.addQuickElement("object", attrs={
                        'pk' : smart_unicode(value._get_pk_val())
                    })
            for relobj in getattr(obj, field.name).iterator():
                handle_m2m(relobj)

            self.xml.endElement("field")

    def _start_relational_field(self, field):
        """
        Helper to output the <field> element for relational fields
        """
        self.indent(2)
        self.xml.startElement("field", {
            "name" : field.name,
            "rel"  : field.rel.__class__.__name__,
            "to"   : smart_unicode(field.rel.to._meta),
        })

class Deserializer(base.Deserializer):
    """
    Deserialize XML.
    """

    def __init__(self, stream_or_string, **options):
        super(Deserializer, self).__init__(stream_or_string, **options)
        self.event_stream = pulldom.parse(self.stream)
        self.db = options.pop('using', DEFAULT_DB_ALIAS)

    def next(self):
        for event, node in self.event_stream:
            if event == "START_ELEMENT" and node.nodeName == "object":
                self.event_stream.expandNode(node)
                return self._handle_object(node)
        raise StopIteration

    def _handle_object(self, node):
        """
        Convert an <object> node to a DeserializedObject.
        """
        # Look up the model using the model loading mechanism. If this fails,
        # bail.
        Model = self._get_model_from_node(node, "model")

        # Start building a data dictionary from the object.
        # If the node is missing the pk set it to None
        if node.hasAttribute("pk"):
            pk = node.getAttribute("pk")
        else:
            pk = None

        data = {Model._meta.pk.attname : Model._meta.pk.to_python(pk)}

        # Also start building a dict of m2m data (this is saved as
        # {m2m_accessor_attribute : [list_of_related_objects]})
        m2m_data = {}

        # Deseralize each field.
        for field_node in node.getElementsByTagName("field"):
            # If the field is missing the name attribute, bail (are you
            # sensing a pattern here?)
            field_name = field_node.getAttribute("name")
            if not field_name:
                raise base.DeserializationError("<field> node is missing the 'name' attribute")

            # Get the field from the Model. This will raise a
            # FieldDoesNotExist if, well, the field doesn't exist, which will
            # be propagated correctly.
            field = Model._meta.get_field(field_name)

            # As is usually the case, relation fields get the special treatment.
            if field.rel and isinstance(field.rel, models.ManyToManyRel):
                m2m_data[field.name] = self._handle_m2m_field_node(field_node, field)
            elif field.rel and isinstance(field.rel, models.ManyToOneRel):
                data[field.attname] = self._handle_fk_field_node(field_node, field)
            else:
                if field_node.getElementsByTagName('None'):
                    value = None
                else:
                    value = field.to_python(getInnerText(field_node).strip())
                data[field.name] = value

        # Return a DeserializedObject so that the m2m data has a place to live.
        return base.DeserializedObject(Model(**data), m2m_data)

    def _handle_fk_field_node(self, node, field):
        """
        Handle a <field> node for a ForeignKey
        """
        # Check if there is a child node named 'None', returning None if so.
        if node.getElementsByTagName('None'):
            return None
        else:
            if hasattr(field.rel.to._default_manager, 'get_by_natural_key'):
                keys = node.getElementsByTagName('natural')
                if keys:
                    # If there are 'natural' subelements, it must be a natural key
                    field_value = [getInnerText(k).strip() for k in keys]
                    obj = field.rel.to._default_manager.db_manager(self.db).get_by_natural_key(*field_value)
                    obj_pk = getattr(obj, field.rel.field_name)
                    # If this is a natural foreign key to an object that
                    # has a FK/O2O as the foreign key, use the FK value
                    if field.rel.to._meta.pk.rel:
                        obj_pk = obj_pk.pk
                else:
                    # Otherwise, treat like a normal PK
                    field_value = getInnerText(node).strip()
                    obj_pk = field.rel.to._meta.get_field(field.rel.field_name).to_python(field_value)
                return obj_pk
            else:
                field_value = getInnerText(node).strip()
                return field.rel.to._meta.get_field(field.rel.field_name).to_python(field_value)

    def _handle_m2m_field_node(self, node, field):
        """
        Handle a <field> node for a ManyToManyField.
        """
        if hasattr(field.rel.to._default_manager, 'get_by_natural_key'):
            def m2m_convert(n):
                keys = n.getElementsByTagName('natural')
                if keys:
                    # If there are 'natural' subelements, it must be a natural key
                    field_value = [getInnerText(k).strip() for k in keys]
                    obj_pk = field.rel.to._default_manager.db_manager(self.db).get_by_natural_key(*field_value).pk
                else:
                    # Otherwise, treat like a normal PK value.
                    obj_pk = field.rel.to._meta.pk.to_python(n.getAttribute('pk'))
                return obj_pk
        else:
            m2m_convert = lambda n: field.rel.to._meta.pk.to_python(n.getAttribute('pk'))
        return [m2m_convert(c) for c in node.getElementsByTagName("object")]

    def _get_model_from_node(self, node, attr):
        """
        Helper to look up a model from a <object model=...> or a <field
        rel=... to=...> node.
        """
        model_identifier = node.getAttribute(attr)
        if not model_identifier:
            raise base.DeserializationError(
                "<%s> node is missing the required '%s' attribute" % (node.nodeName, attr))
        try:
            Model = models.get_model(*model_identifier.split("."))
        except TypeError:
            Model = None
        if Model is None:
            raise base.DeserializationError(
                "<%s> node has invalid model identifier: '%s'" % (node.nodeName, model_identifier))
        return Model


def getInnerText(node):
    """
    Get all the inner text of a DOM node (recursively).
    """
    # inspired by http://mail.python.org/pipermail/xml-sig/2005-March/011022.html
    inner_text = []
    for child in node.childNodes:
        if child.nodeType == child.TEXT_NODE or child.nodeType == child.CDATA_SECTION_NODE:
            inner_text.append(child.data)
        elif child.nodeType == child.ELEMENT_NODE:
            inner_text.extend(getInnerText(child))
        else:
           pass
    return u"".join(inner_text)

"""
Code used in a couple of places to work with the current thread's environment.
Current users include i18n and request prefix handling.
"""

try:
    import threading
    currentThread = threading.currentThread
except ImportError:
    def currentThread():
        return "no threading"


#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#




"""PageSpeed configuration tools.

Library for parsing pagespeed configuration data from app.yaml and working
with these in memory.
"""







import google

from google.appengine.api import validation
from google.appengine.api import yaml_builder
from google.appengine.api import yaml_listener
from google.appengine.api import yaml_object

_URL_BLACKLIST_REGEX = r'http(s)?://\S{0,499}'
_REWRITER_NAME_REGEX = r'[a-zA-Z0-9_]+'
_DOMAINS_TO_REWRITE_REGEX = r'(http(s)?://)?[-a-zA-Z0-9_.*]+(:\d+)?'

URL_BLACKLIST = 'url_blacklist'
ENABLED_REWRITERS = 'enabled_rewriters'
DISABLED_REWRITERS = 'disabled_rewriters'
DOMAINS_TO_REWRITE = 'domains_to_rewrite'


class MalformedPagespeedConfiguration(Exception):
  """Configuration file for PageSpeed API is malformed."""






class PagespeedEntry(validation.Validated):
  """Describes the format of a pagespeed configuration from a yaml file.

  URL blacklist entries are patterns (with '?' and '*' as wildcards).  Any URLs
  that match a pattern on the blacklist will not be optimized by PageSpeed.

  Rewriter names are strings (like 'CombineCss' or 'RemoveComments') describing
  individual PageSpeed rewriters.  A full list of valid rewriter names can be
  found in the PageSpeed documentation.

  The domains-to-rewrite list is a whitelist of domain name patterns with '*' as
  a wildcard, optionally starting with 'http://' or 'https://'.  If no protocol
  is given, 'http://' is assumed.  A resource will only be rewritten if it is on
  the same domain as the HTML that references it, or if its domain is on the
  domains-to-rewrite list.
  """
  ATTRIBUTES = {
      URL_BLACKLIST: validation.Optional(
          validation.Repeated(validation.Regex(_URL_BLACKLIST_REGEX))),
      ENABLED_REWRITERS: validation.Optional(
          validation.Repeated(validation.Regex(_REWRITER_NAME_REGEX))),
      DISABLED_REWRITERS: validation.Optional(
          validation.Repeated(validation.Regex(_REWRITER_NAME_REGEX))),
      DOMAINS_TO_REWRITE: validation.Optional(
          validation.Repeated(validation.Regex(_DOMAINS_TO_REWRITE_REGEX))),
  }


def LoadPagespeedEntry(pagespeed_entry, open_fn=None):
  """Load a yaml file or string and return a PagespeedEntry.

  Args:
    pagespeed_entry: The contents of a pagespeed entry from a yaml file
      as a string, or an open file object.
    open_fn: Function for opening files. Unused.

  Returns:
    A PagespeedEntry instance which represents the contents of the parsed yaml.

  Raises:
    yaml_errors.EventError: An error occured while parsing the yaml.
    MalformedPagespeedConfiguration: The configuration is parseable but invalid.
  """
  builder = yaml_object.ObjectBuilder(PagespeedEntry)
  handler = yaml_builder.BuilderHandler(builder)
  listener = yaml_listener.EventListener(handler)
  listener.Parse(pagespeed_entry)

  parsed_yaml = handler.GetResults()
  if not parsed_yaml:
    return PagespeedEntry()

  if len(parsed_yaml) > 1:
    raise MalformedPagespeedConfiguration(
        'Multiple configuration sections in the yaml')

  return parsed_yaml[0]

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#




"""Search API module."""

from search import AtomField
from search import Cursor
from search import DateField
from search import DeleteError
from search import DeleteResult
from search import Document
from search import DOCUMENT_ID_FIELD_NAME
from search import Error
from search import ExpressionError
from search import Field
from search import FieldExpression
from search import GeoField
from search import GeoPoint
from search import get_indexes
from search import GetResponse
from search import HtmlField
from search import Index
from search import InternalError
from search import InvalidRequest
from search import LANGUAGE_FIELD_NAME
from search import MatchScorer
from search import MAXIMUM_DOCUMENT_ID_LENGTH
from search import MAXIMUM_DOCUMENTS_PER_PUT_REQUEST
from search import MAXIMUM_DOCUMENTS_RETURNED_PER_SEARCH
from search import MAXIMUM_EXPRESSION_LENGTH
from search import MAXIMUM_FIELD_ATOM_LENGTH
from search import MAXIMUM_FIELD_NAME_LENGTH
from search import MAXIMUM_FIELD_VALUE_LENGTH
from search import MAXIMUM_FIELDS_RETURNED_PER_SEARCH
from search import MAXIMUM_GET_INDEXES_OFFSET
from search import MAXIMUM_INDEX_NAME_LENGTH
from search import MAXIMUM_INDEXES_RETURNED_PER_GET_REQUEST
from search import MAXIMUM_NUMBER_FOUND_ACCURACY
from search import MAXIMUM_QUERY_LENGTH
from search import MAXIMUM_SEARCH_OFFSET
from search import MAXIMUM_SORTED_DOCUMENTS
from search import NumberField
from search import OperationResult
from search import PutError
from search import PutResult
from search import Query
from search import QueryError
from search import QueryOptions
from search import RANK_FIELD_NAME
from search import RescoringMatchScorer
from search import SCORE_FIELD_NAME
from search import ScoredDocument
from search import SearchResults
from search import SortExpression
from search import SortOptions
from search import TextField
from search import TIMESTAMP_FIELD_NAME
from search import TransientError

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#









"""Implementation of scheduling for Groc format schedules.

A Groc schedule looks like '1st,2nd monday 9:00', or 'every 20 mins'. This
module takes a parsed schedule (produced by Antlr) and creates objects that
can produce times that match this schedule.

A parsed schedule is one of two types - an Interval or a Specific Time.
See the class docstrings for more.

Extensions to be considered:

  allowing a comma separated list of times to run
"""


import calendar
import datetime

try:
  import pytz
except ImportError:
  pytz = None

import groc

HOURS = 'hours'
MINUTES = 'minutes'

try:
  from pytz import NonExistentTimeError
  from pytz import AmbiguousTimeError
except ImportError:

  class NonExistentTimeError(Exception):
    pass

  class AmbiguousTimeError(Exception):
    pass


def GrocTimeSpecification(schedule, timezone=None):
  """Factory function.

  Turns a schedule specification into a TimeSpecification.

  Arguments:
    schedule: the schedule specification, as a string
    timezone: the optional timezone as a string for this specification.
        Defaults to 'UTC' - valid entries are things like 'Australia/Victoria'
        or 'PST8PDT'.
  Returns:
    a TimeSpecification instance
  """
  parser = groc.CreateParser(schedule)
  parser.timespec()

  if parser.period_string:
    return IntervalTimeSpecification(parser.interval_mins,
                                     parser.period_string,
                                     parser.synchronized,
                                     parser.start_time_string,
                                     parser.end_time_string,
                                     timezone)
  else:
    return SpecificTimeSpecification(parser.ordinal_set, parser.weekday_set,
                                     parser.month_set,
                                     parser.monthday_set,
                                     parser.time_string,
                                     timezone)


class TimeSpecification(object):
  """Base class for time specifications."""

  def GetMatches(self, start, n):
    """Returns the next n times that match the schedule, starting at time start.

    Arguments:
      start: a datetime to start from. Matches will start from after this time.
      n:     the number of matching times to return

    Returns:
      a list of n datetime objects
    """
    out = []
    for _ in range(n):
      start = self.GetMatch(start)
      out.append(start)
    return out

  def GetMatch(self, start):
    """Returns the next match after time start.

    Must be implemented in subclasses.

    Arguments:
      start: a datetime to start from. Matches will start from after this time.
          This may be in any pytz time zone, or it may be timezone-naive
          (interpreted as UTC).

    Returns:
      a datetime object in the timezone of the input 'start'
    """
    raise NotImplementedError


def _GetTimezone(timezone_string):
  """Converts a timezone string to a pytz timezone object.

  Arguments:
    timezone_string: a string representing a timezone, or None

  Returns:
    a pytz timezone object, or None if the input timezone_string is None

  Raises:
    ValueError: if timezone_string is not None and the pytz module could not be
        loaded
  """
  if timezone_string:
    if pytz is None:
      raise ValueError('need pytz in order to specify a timezone')
    return pytz.timezone(timezone_string)
  else:
    return None


def _ToTimeZone(t, tzinfo):
  """Converts 't' to the time zone 'tzinfo'.

  Arguments:
    t: a datetime object.  It may be in any pytz time zone, or it may be
        timezone-naive (interpreted as UTC).
    tzinfo: a pytz timezone object, or None (interpreted as UTC).

  Returns:
    a datetime object in the time zone 'tzinfo'
  """
  if pytz is None:

    return t.replace(tzinfo=tzinfo)
  elif tzinfo:

    if not t.tzinfo:
      t = pytz.utc.localize(t)
    return tzinfo.normalize(t.astimezone(tzinfo))
  elif t.tzinfo:

    return pytz.utc.normalize(t.astimezone(pytz.utc)).replace(tzinfo=None)
  else:

    return t


def _GetTime(time_string):
  """Converts a string to a datetime.time object.

  Arguments:
    time_string: a string representing a time ('hours:minutes')

  Returns:
    a datetime.time object
  """
  hourstr, minutestr = time_string.split(':')
  return datetime.time(int(hourstr), int(minutestr))


class IntervalTimeSpecification(TimeSpecification):
  """A time specification for a given interval.

  An Interval type spec runs at the given fixed interval. It has the following
  attributes:
  period - the type of interval, either 'hours' or 'minutes'
  interval - the number of units of type period.
  synchronized - whether to synchronize the times to be locked to a fixed
      period (midnight in the specified timezone).
  start_time, end_time - restrict matches to a given range of times every day.
      If these are None, there is no restriction.  Otherwise, they are
      datetime.time objects.
  timezone - the time zone in which start_time and end_time should be
      interpreted, or None (defaults to UTC).  This is a pytz timezone object.
  """

  def __init__(self, interval, period, synchronized=False,
               start_time_string='', end_time_string='', timezone=None):
    super(IntervalTimeSpecification, self).__init__()
    if interval < 1:
      raise groc.GrocException('interval must be greater than zero')
    self.interval = interval
    self.period = period
    self.synchronized = synchronized
    if self.period == HOURS:
      self.seconds = self.interval * 3600
    else:
      self.seconds = self.interval * 60
    self.timezone = _GetTimezone(timezone)


    if self.synchronized:
      if start_time_string:
        raise ValueError(
            'start_time_string may not be specified if synchronized is true')
      if end_time_string:
        raise ValueError(
            'end_time_string may not be specified if synchronized is true')
      if (self.seconds > 86400) or ((86400 % self.seconds) != 0):
        raise groc.GrocException('can only use synchronized for periods that'
                                 ' divide evenly into 24 hours')


      self.start_time = datetime.time(0, 0).replace(tzinfo=self.timezone)
      self.end_time = datetime.time(23, 59).replace(tzinfo=self.timezone)
    elif start_time_string:
      if not end_time_string:
        raise ValueError(
            'end_time_string must be specified if start_time_string is')
      self.start_time = (
          _GetTime(start_time_string).replace(tzinfo=self.timezone))
      self.end_time = _GetTime(end_time_string).replace(tzinfo=self.timezone)
    else:
      if end_time_string:
        raise ValueError(
            'start_time_string must be specified if end_time_string is')
      self.start_time = None
      self.end_time = None

  def GetMatch(self, start):
    """Returns the next match after 'start'.

    Arguments:
      start: a datetime to start from. Matches will start from after this time.
          This may be in any pytz time zone, or it may be timezone-naive
          (interpreted as UTC).

    Returns:
      a datetime object in the timezone of the input 'start'
    """
    if self.start_time is None:

      return start + datetime.timedelta(seconds=self.seconds)


    t = _ToTimeZone(start, self.timezone)


    start_time = self._GetPreviousDateTime(t, self.start_time)



    t_delta = t - start_time
    t_delta_seconds = (t_delta.days * 60 * 24 + t_delta.seconds)
    num_intervals = (t_delta_seconds + self.seconds) / self.seconds
    interval_time = (
        start_time + datetime.timedelta(seconds=(num_intervals * self.seconds)))
    if self.timezone:
      interval_time = self.timezone.normalize(interval_time)



    next_start_time = self._GetNextDateTime(t, self.start_time)
    if (self._TimeIsInRange(t) and
        self._TimeIsInRange(interval_time) and
        interval_time < next_start_time):
      result = interval_time
    else:
      result = next_start_time


    return _ToTimeZone(result, start.tzinfo)

  def _TimeIsInRange(self, t):
    """Returns true if 't' falls between start_time and end_time, inclusive.

    Arguments:
      t: a datetime object, in self.timezone

    Returns:
      a boolean
    """


    previous_start_time = self._GetPreviousDateTime(t, self.start_time)
    previous_end_time = self._GetPreviousDateTime(t, self.end_time)
    if previous_start_time > previous_end_time:
      return True
    else:
      return t == previous_end_time

  @staticmethod
  def _GetPreviousDateTime(t, target_time):
    """Returns the latest datetime <= 't' that has the time target_time.

    Arguments:
      t: a datetime.datetime object, in self.timezone
      target_time: a datetime.time object, in self.timezone

    Returns:
      a datetime.datetime object, in self.timezone
    """

    date = t.date()
    while True:
      result = IntervalTimeSpecification._CombineDateAndTime(date, target_time)
      if result <= t:
        return result
      date -= datetime.timedelta(days=1)

  @staticmethod
  def _GetNextDateTime(t, target_time):
    """Returns the earliest datetime > 't' that has the time target_time.

    Arguments:
      t: a datetime.datetime object, in self.timezone
      target_time: a time object, in self.timezone

    Returns:
      a datetime.datetime object, in self.timezone
    """

    date = t.date()
    while True:
      result = IntervalTimeSpecification._CombineDateAndTime(date, target_time)
      if result > t:
        return result
      date += datetime.timedelta(days=1)

  @staticmethod
  def _CombineDateAndTime(date, time):
    """Creates a datetime object from date and time objects.

    This is similar to the datetime.combine method, but its timezone
    calculations are designed to work with pytz.

    Arguments:
      date: a datetime.date object, in any timezone
      time: a datetime.time object, in any timezone

    Returns:
      a datetime.datetime object, in the timezone of the input 'time'
    """
    if time.tzinfo:
      naive_result = datetime.datetime(
          date.year, date.month, date.day, time.hour, time.minute, time.second)
      try:
        return time.tzinfo.localize(naive_result, is_dst=None)
      except AmbiguousTimeError:


        return min(time.tzinfo.localize(naive_result, is_dst=True),
                   time.tzinfo.localize(naive_result, is_dst=False))
      except NonExistentTimeError:




        while True:
          naive_result += datetime.timedelta(minutes=1)
          try:
            return time.tzinfo.localize(naive_result, is_dst=None)
          except NonExistentTimeError:
            pass
    else:
      return datetime.datetime.combine(date, time)


class SpecificTimeSpecification(TimeSpecification):
  """Specific time specification.

  A Specific interval is more complex, but defines a certain time to run and
  the days that it should run. It has the following attributes:
  time     - the time of day to run, as 'HH:MM'
  ordinals - first, second, third &c, as a set of integers in 1..5
  months   - the months that this should run, as a set of integers in 1..12
  weekdays - the days of the week that this should run, as a set of integers,
             0=Sunday, 6=Saturday
  timezone - the optional timezone as a string for this specification.
             Defaults to UTC - valid entries are things like Australia/Victoria
             or PST8PDT.

  A specific time schedule can be quite complex. A schedule could look like
  this:
  '1st,third sat,sun of jan,feb,mar 09:15'

  In this case, ordinals would be {1,3}, weekdays {0,6}, months {1,2,3} and
  time would be '09:15'.
  """

  def __init__(self, ordinals=None, weekdays=None, months=None, monthdays=None,
               timestr='00:00', timezone=None):
    super(SpecificTimeSpecification, self).__init__()
    if weekdays and monthdays:
      raise ValueError('cannot supply both monthdays and weekdays')
    if ordinals is None:

      self.ordinals = set(range(1, 6))
    else:
      self.ordinals = set(ordinals)
      if self.ordinals and (min(self.ordinals) < 1 or max(self.ordinals) > 5):
        raise ValueError('ordinals must be between 1 and 5 inclusive, '
                         'got %r' % ordinals)

    if weekdays is None:

      self.weekdays = set(range(7))
    else:
      self.weekdays = set(weekdays)
      if self.weekdays and (min(self.weekdays) < 0 or max(self.weekdays) > 6):
        raise ValueError('weekdays must be between '
                         '0 (sun) and 6 (sat) inclusive, '
                         'got %r' % weekdays)

    if months is None:

      self.months = set(range(1, 13))
    else:
      self.months = set(months)
      if self.months and (min(self.months) < 1 or max(self.months) > 12):
        raise ValueError('months must be between '
                         '1 (jan) and 12 (dec) inclusive, '
                         'got %r' % months)

    if not monthdays:
      self.monthdays = set()
    else:
      if min(monthdays) < 1:
        raise ValueError('day of month must be greater than 0')
      if max(monthdays) > 31:
        raise ValueError('day of month must be less than 32')
      if self.months:
        for month in self.months:
          _, ndays = calendar.monthrange(4, month)
          if min(monthdays) <= ndays:
            break
        else:
          raise ValueError('invalid day of month, '
                           'got day %r of month %r' % (max(monthdays), month))
      self.monthdays = set(monthdays)
    self.time = _GetTime(timestr)
    self.timezone = _GetTimezone(timezone)

  def _MatchingDays(self, year, month):
    """Returns matching days for the given year and month.

    For the given year and month, return the days that match this instance's
    day specification, based on either (a) the ordinals and weekdays, or
    (b) the explicitly specified monthdays.  If monthdays are specified,
    dates that fall outside the range of the month will not be returned.

    Arguments:
      year: the year as an integer
      month: the month as an integer, in range 1-12

    Returns:
      a list of matching days, as ints in range 1-31
    """
    start_day, last_day = calendar.monthrange(year, month)
    if self.monthdays:
      return sorted([day for day in self.monthdays if day <= last_day])


    out_days = []
    start_day = (start_day + 1) % 7
    for ordinal in self.ordinals:
      for weekday in self.weekdays:
        day = ((weekday - start_day) % 7) + 1
        day += 7 * (ordinal - 1)
        if day <= last_day:
          out_days.append(day)
    return sorted(out_days)

  def _NextMonthGenerator(self, start, matches):
    """Creates a generator that produces results from the set 'matches'.

    Matches must be >= 'start'. If none match, the wrap counter is incremented,
    and the result set is reset to the full set. Yields a 2-tuple of (match,
    wrapcount).

    Arguments:
      start: first set of matches will be >= this value (an int)
      matches: the set of potential matches (a sequence of ints)

    Yields:
      a two-tuple of (match, wrap counter). match is an int in range (1-12),
      wrapcount is a int indicating how many times we've wrapped around.
    """
    potential = matches = sorted(matches)

    after = start - 1
    wrapcount = 0
    while True:
      potential = [x for x in potential if x > after]
      if not potential:


        wrapcount += 1
        potential = matches
      after = potential[0]
      yield (after, wrapcount)

  def GetMatch(self, start):
    """Returns the next match after time start.

    Must be implemented in subclasses.

    Arguments:
      start: a datetime to start from. Matches will start from after this time.
          This may be in any pytz time zone, or it may be timezone-naive
          (interpreted as UTC).

    Returns:
      a datetime object in the timezone of the input 'start'
    """





    start_time = _ToTimeZone(start, self.timezone).replace(tzinfo=None)
    if self.months:

      months = self._NextMonthGenerator(start_time.month, self.months)
    while True:

      month, yearwraps = months.next()
      candidate_month = start_time.replace(day=1, month=month,
                                           year=start_time.year + yearwraps)


      day_matches = self._MatchingDays(candidate_month.year, month)

      if ((candidate_month.year, candidate_month.month)
          == (start_time.year, start_time.month)):

        day_matches = [x for x in day_matches if x >= start_time.day]

        while (day_matches and day_matches[0] == start_time.day
               and start_time.time() >= self.time):
          day_matches.pop(0)
      while day_matches:

        out = candidate_month.replace(day=day_matches[0], hour=self.time.hour,
                                      minute=self.time.minute, second=0,
                                      microsecond=0)

        if self.timezone and pytz is not None:









          try:
            out = self.timezone.localize(out, is_dst=None)
          except AmbiguousTimeError:

            out = self.timezone.localize(out)
          except NonExistentTimeError:






            for _ in range(24):


              out += datetime.timedelta(minutes=60)
              try:
                out = self.timezone.localize(out)
              except NonExistentTimeError:

                continue
              break
        return _ToTimeZone(out, start.tzinfo)

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#




"""This module supports asynchronous I/O on multiple file descriptors."""


from google.appengine.api.remote_socket._remote_socket import select, error

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#




"""Repository for all builtin handlers information.

On initialization, this file generates a list of builtin handlers that have
associated app.yaml information.  This file can then be called to read that
information and make it available.
"""













import logging
import os



DEFAULT_DIR = os.path.join(os.path.dirname(__file__))
_handler_dir = None



_available_builtins = None
#BUILTINS_NOT_AVAIABLE_IN_PYTHON27 = set(['datastore_admin', 'mapreduce'])


INCLUDE_FILENAME_TEMPLATE = 'include-%s.yaml'
DEFAULT_INCLUDE_FILENAME = 'include.yaml'


class InvalidBuiltinName(Exception):
  """Raised whenever a builtin handler name is specified that is not found."""


def reset_builtins_dir():
  """Public method for resetting builtins directory to default."""
  set_builtins_dir(DEFAULT_DIR)


def set_builtins_dir(path):
  """Sets the appropriate path for testing and reinitializes the module."""
  global _handler_dir, _available_builtins
  _handler_dir = path
  _available_builtins = []
  _initialize_builtins()


def _initialize_builtins():
  """Scan the immediate subdirectories of the builtins module.

  Encountered subdirectories with an app.yaml file are added to
  AVAILABLE_BUILTINS.
  """
  for filename in os.listdir(_handler_dir):
    if os.path.isfile(_get_yaml_path(filename, '')):
      _available_builtins.append(filename)


def _get_yaml_path(builtin_name, runtime):
  """Return expected path to a builtin handler's yaml file without error check.
  """
  runtime_specific = os.path.join(_handler_dir, builtin_name,
                                  INCLUDE_FILENAME_TEMPLATE % runtime)
  if runtime and os.path.exists(runtime_specific):
    return runtime_specific
  return os.path.join(_handler_dir, builtin_name, DEFAULT_INCLUDE_FILENAME)


def get_yaml_path(builtin_name, runtime=''):
  """Returns the full path to a yaml file by giving the builtin module's name.

  Args:
    builtin_name: single word name of builtin handler
    runtime: name of the runtime

  Raises:
    ValueError: if handler does not exist in expected directory

  Returns:
    the absolute path to a valid builtin handler include.yaml file
  """
  if _handler_dir is None:
    set_builtins_dir(DEFAULT_DIR)

  available_builtins = set(_available_builtins)
  #if runtime == 'python27':
  #  available_builtins = available_builtins - BUILTINS_NOT_AVAIABLE_IN_PYTHON27

  if builtin_name not in available_builtins:
    raise InvalidBuiltinName(
        '%s is not the name of a valid builtin.\n'
        'Available handlers are: %s' % (
            builtin_name, ', '.join(sorted(available_builtins))))
  return _get_yaml_path(builtin_name, runtime)




def get_yaml_basepath():
  """Returns the full path of the directory in which builtins are located."""
  if _handler_dir is None:
    set_builtins_dir(DEFAULT_DIR)
  return _handler_dir

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""Tests for google.apphosting.tools.devappserver2.inotify_file_watcher."""


import logging
import os
import os.path
import shutil
import sys
import tempfile
import unittest

from google.appengine.tools.devappserver2 import inotify_file_watcher


@unittest.skipUnless(sys.platform.startswith('linux'), 'requires linux')
class TestInotifyFileWatcher(unittest.TestCase):
  """Tests for inotify_file_watcher.InotifyFileWatcher."""

  def setUp(self):
    self._directory = tempfile.mkdtemp()  # The watched directory
    self._junk_directory = tempfile.mkdtemp()  # A scrap directory.
    self._watcher = inotify_file_watcher.InotifyFileWatcher(self._directory)
    logging.debug('watched directory=%r, junk directory=%r',
                  self._directory, self._junk_directory)

  def tearDown(self):
    self._watcher.quit()
    shutil.rmtree(self._directory)
    shutil.rmtree(self._junk_directory)

  def _create_file(self, relative_path):
    realpath = os.path.realpath(os.path.join(self._directory, relative_path))
    with open(realpath, 'w'):
      pass
    return realpath

  def _create_directory(self, relative_path):
    realpath = os.path.realpath(os.path.join(self._directory, relative_path))
    os.mkdir(realpath)
    return realpath

  def _create_directory_tree(self, path, num_directories):
    """Create exactly num_directories subdirectories in path."""
    assert num_directories >= 0
    if not num_directories:
      return

    self._create_directory(path)
    num_directories -= 1
    # Divide the remaining number of directories to create among 4
    # subdirectories in an approximate even fashion.
    for i in range(4, 0, -1):
      sub_dir_size = num_directories/i
      self._create_directory_tree(os.path.join(path, 'dir%d' % i), sub_dir_size)
      num_directories -= sub_dir_size

  def test_file_created(self):
    self._watcher.start()
    path = self._create_file('test')
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_file_modified(self):
    path = self._create_file('test')
    self._watcher.start()
    with open(path, 'w') as f:
      f.write('testing')
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_file_read(self):
    path = self._create_file('test')
    with open(path, 'w') as f:
      f.write('testing')
    self._watcher.start()
    with open(path, 'r') as f:
      f.read()
    # Reads should not trigger updates.
    self.assertEqual(
        set(),
        self._watcher._get_changed_paths())

  def test_file_deleted(self):
    path = self._create_file('test')
    self._watcher.start()
    os.remove(path)
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_file_renamed(self):
    source = self._create_file('test')
    target = os.path.join(os.path.dirname(source), 'test2')
    self._watcher.start()
    os.rename(source, target)
    self.assertEqual(
        set([source, target]),
        self._watcher._get_changed_paths())

  def test_create_directory(self):
    self._watcher.start()
    directory = self._create_directory('test')
    self.assertEqual(
        set([directory]),
        self._watcher._get_changed_paths())

  def test_file_created_in_directory(self):
    directory = self._create_directory('test')
    self._watcher.start()
    path = self._create_file('test/file')
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_move_directory(self):
    source = self._create_directory('test')
    target = os.path.join(os.path.dirname(source), 'test2')
    self._watcher.start()
    os.rename(source, target)
    self.assertEqual(
        set([source, target]),
        self._watcher._get_changed_paths())

  def test_move_directory_out_of_watched(self):
    source = self._create_directory('test')
    target = os.path.join(self._junk_directory, 'test')
    self._watcher.start()
    os.rename(source, target)
    self.assertEqual(
        set([source]),
        self._watcher._get_changed_paths())
    with open(os.path.join(target, 'file'), 'w'):
      pass
    # Changes to files in subdirectories that have been moved should be ignored.
    self.assertEqual(
        set([]),
        self._watcher._get_changed_paths())

  def test_move_directory_into_watched(self):
    source = os.path.join(self._junk_directory, 'source')
    target = os.path.join(self._directory, 'target')
    os.mkdir(source)
    self._watcher.start()
    os.rename(source, target)
    self.assertEqual(
        set([target]),
        self._watcher._get_changed_paths())
    file_path = os.path.join(target, 'file')
    with open(file_path, 'w+'):
      pass
    self.assertEqual(
        set([file_path]),
        self._watcher._get_changed_paths())

  def test_directory_deleted(self):
    path = self._create_directory('test')
    self._watcher.start()
    os.rmdir(path)
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_subdirectory_deleted(self):
    """Tests that internal _directory_to_subdirs is updated on delete."""
    path = self._create_directory('test')
    sub_path = self._create_directory('test/test2')
    self._watcher.start()

    self.assertEqual(
        set([sub_path]),
        self._watcher._directory_to_subdirs[path])
    os.rmdir(sub_path)
    self.assertEqual(
        set([sub_path]),
        self._watcher._get_changed_paths())
    self.assertEqual(
        set(),
        self._watcher._directory_to_subdirs[path])

    os.rmdir(path)
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

  def test_symlink(self):
    sym_target = os.path.join(self._directory, 'test')
    os.mkdir(os.path.join(self._junk_directory, 'subdir'))
    self._watcher.start()

    # Check that an added symlinked directory is reported.
    os.symlink(self._junk_directory, sym_target)
    self.assertEqual(
        set([sym_target]),
        self._watcher._get_changed_paths())

    # Check that a file added to the symlinked directory is reported.
    with open(os.path.join(self._junk_directory, 'file1'), 'w'):
      pass
    self.assertEqual(
        set([os.path.join(self._directory, 'test', 'file1')]),
        self._watcher._get_changed_paths())

    # Check that a removed symlinked directory is reported.
    os.remove(sym_target)
    self.assertEqual(
        set([sym_target]),
        self._watcher._get_changed_paths())

    # Check that a file added to the removed symlinked directory is *not*
    # reported.
    with open(os.path.join(self._junk_directory, 'subdir', 'file2'), 'w'):
      pass
    self.assertEqual(
        set(),
        self._watcher._get_changed_paths())

  def test_many_directories(self):
    # Linux supports a limited number of watches per file descriptor. The
    # default is 8192 (i.e. 2^13).
    self._create_directory_tree('bigdir', num_directories=10000)
    self._watcher.start()
    path = self._create_file('bigdir/dir4/dir4/file')
    self.assertEqual(
        set([path]),
        self._watcher._get_changed_paths())

if __name__ == '__main__':
  unittest.main()

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""Tests for google.appengine.tools.devappserver2.shutdown."""

import os
import signal
import time
import unittest

import google

import mox

from google.appengine.tools.devappserver2 import shutdown


class ShutdownTest(unittest.TestCase):

  def setUp(self):
    self.mox = mox.Mox()
    self.mox.StubOutWithMock(os, 'abort')
    shutdown._shutting_down = False
    shutdown._num_terminate_requests = 0
    self._sigint_handler = signal.getsignal(signal.SIGINT)
    self._sigterm_handler = signal.getsignal(signal.SIGTERM)

  def tearDown(self):
    self.mox.UnsetStubs()
    signal.signal(signal.SIGINT, self._sigint_handler)
    signal.signal(signal.SIGTERM, self._sigterm_handler)

  def test_async_quit(self):
    self.mox.ReplayAll()
    shutdown.async_quit()
    self.assertTrue(shutdown._shutting_down)
    self.mox.VerifyAll()

  def test_async_terminate(self):
    self.mox.ReplayAll()
    shutdown._async_terminate()
    self.assertTrue(shutdown._shutting_down)
    shutdown._async_terminate()
    self.mox.VerifyAll()

  def test_async_terminate_abort(self):
    os.abort()
    self.mox.ReplayAll()
    shutdown._async_terminate()
    self.assertTrue(shutdown._shutting_down)
    shutdown._async_terminate()
    shutdown._async_terminate()
    self.mox.VerifyAll()

  def test_install_signal_handlers(self):
    shutdown.install_signal_handlers()
    self.assertEqual(shutdown._async_terminate, signal.getsignal(signal.SIGINT))
    self.assertEqual(shutdown._async_terminate,
                     signal.getsignal(signal.SIGTERM))

  def test_wait_until_shutdown(self):
    self.mox.StubOutWithMock(time, 'sleep')
    time.sleep(1).WithSideEffects(lambda _: shutdown.async_quit())
    self.mox.ReplayAll()
    shutdown.wait_until_shutdown()
    self.mox.VerifyAll()

  def test_wait_until_shutdown_raise_interrupted_io(self):

    def quit_and_raise(*_):
      shutdown.async_quit()
      raise IOError

    self.mox.StubOutWithMock(time, 'sleep')
    time.sleep(1).WithSideEffects(quit_and_raise)
    self.mox.ReplayAll()
    shutdown.wait_until_shutdown()
    self.mox.VerifyAll()


if __name__ == '__main__':
  unittest.main()

#!/usr/bin/env python
#
# Copyright 2007 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#




"""MySQL FLAG Constants.

These flags are used along with the FIELD_TYPE to indicate various
properties of columns in a result set.
"""
























# Copyright (c) The PyAMF Project.
# See LICENSE for details.

"""
SQLAlchemy adapter module.

@see: U{SQLAlchemy homepage<http://www.sqlalchemy.org>}

@since: 0.4
"""

from sqlalchemy.orm import collections

import pyamf
from pyamf.adapters import util


pyamf.add_type(collections.InstrumentedList, util.to_list)
pyamf.add_type(collections.InstrumentedDict, util.to_dict)
pyamf.add_type(collections.InstrumentedSet, util.to_set)

# Copyright (c) The PyAMF Project.
# See LICENSE.txt for details.

"""
PyAMF Django adapter tests.

@since: 0.3.1
"""

import unittest
import sys
import os
import datetime

import pyamf
from pyamf.tests import util

try:
    import django
except ImportError:
    django = None

if django and django.VERSION < (1, 0):
    django = None

try:
    reload(settings)
except NameError:
    from pyamf.tests.adapters.django_app import settings


context = None

#: django modules/functions used once bootstrapped
create_test_db = None
destroy_test_db = None
management = None
setup_test_environment = None
teardown_test_environment = None

# test app data
models = None
adapter = None

def init_django():
    """
    Bootstrap Django and initialise this module
    """
    global django, management, create_test_db, destroy_test_db
    global setup_test_environment, teardown_test_environment

    if not django:
        return

    from django.core import management

    project_dir = management.setup_environ(settings)
    sys.path.insert(0, project_dir)

    try:
        from django.test.utils import create_test_db, destroy_test_db
    except ImportError:
        from django.db import connection

        create_test_db = connection.creation.create_test_db
        destroy_test_db = connection.creation.destroy_test_db

    from django.test.utils import setup_test_environment, teardown_test_environment

    return True


def setUpModule():
    """
    Called to set up the module by the test runner
    """
    global context, models, adapter

    context = {
        'sys.path': sys.path[:],
        'sys.modules': sys.modules.copy(),
        'os.environ': os.environ.copy(),
    }

    if init_django():
        from pyamf.tests.adapters.django_app.adapters import models
        from pyamf.adapters import _django_db_models_base as adapter

        setup_test_environment()

        settings.DATABASE_NAME = create_test_db(0, True)


def teadDownModule():
    # remove all the stuff that django installed
    teardown_test_environment()

    sys.path = context['sys.path']
    util.replace_dict(context['sys.modules'], sys.modules)
    util.replace_dict(context['os.environ'], os.environ)

    destroy_test_db(settings.DATABASE_NAME, 2)


class BaseTestCase(unittest.TestCase):
    """
    """

    def setUp(self):
        if not django:
            self.skipTest("'django' is not available")


class TypeMapTestCase(BaseTestCase):
    """
    Tests for basic encoding functionality
    """

    def test_objects_all(self):
        encoder = pyamf.get_encoder(pyamf.AMF0)

        encoder.writeElement(models.SimplestModel.objects.all())
        self.assertEqual(encoder.stream.getvalue(), '\n\x00\x00\x00\x00')

        encoder = pyamf.get_encoder(pyamf.AMF3)
        encoder.writeElement(models.SimplestModel.objects.all())
        self.assertEqual(encoder.stream.getvalue(), '\t\x01\x01')

    def test_NOT_PROVIDED(self):
        from django.db.models import fields

        self.assertEqual(pyamf.encode(fields.NOT_PROVIDED, encoding=pyamf.AMF0).getvalue(),
            '\x06')

        encoder = pyamf.get_encoder(pyamf.AMF3)
        encoder.writeElement(fields.NOT_PROVIDED)
        self.assertEqual(encoder.stream.getvalue(), '\x00')


class ClassAliasTestCase(BaseTestCase):
    def test_time(self):
        x = models.TimeClass()

        x.t = datetime.time(12, 12, 12)
        x.d = datetime.date(2008, 3, 12)
        x.dt = datetime.datetime(2008, 3, 12, 12, 12, 12)

        alias = adapter.DjangoClassAlias(models.TimeClass, None)
        attrs = alias.getEncodableAttributes(x)

        self.assertEqual(attrs, {
            'id': None,
            'd': datetime.datetime(2008, 3, 12, 0, 0),
            'dt': datetime.datetime(2008, 3, 12, 12, 12, 12),
            't': datetime.datetime(1970, 1, 1, 12, 12, 12)
        })

        y = models.TimeClass()

        alias.applyAttributes(y, {
            'id': None,
            'd': datetime.datetime(2008, 3, 12, 0, 0),
            'dt': datetime.datetime(2008, 3, 12, 12, 12, 12),
            't': datetime.datetime(1970, 1, 1, 12, 12, 12)
        })

        self.assertEqual(y.id, None)
        self.assertEqual(y.d, datetime.date(2008, 3, 12))
        self.assertEqual(y.dt, datetime.datetime(2008, 3, 12, 12, 12, 12))
        self.assertEqual(y.t, datetime.time(12, 12, 12))

        y = models.TimeClass()

        alias.applyAttributes(y, {
            'id': None,
            'd': None,
            'dt': None,
            't': None
        })

        self.assertEqual(y.id, None)
        self.assertEqual(y.d, None)
        self.assertEqual(y.dt, None)
        self.assertEqual(y.t, None)

    def test_undefined(self):
        from django.db import models
        from django.db.models import fields

        class UndefinedClass(models.Model):
            pass

        alias = adapter.DjangoClassAlias(UndefinedClass, None)

        x = UndefinedClass()

        alias.applyAttributes(x, {
            'id': pyamf.Undefined
        })

        self.assertEqual(x.id, fields.NOT_PROVIDED)

        x.id = fields.NOT_PROVIDED

        attrs = alias.getEncodableAttributes(x)
        self.assertEqual(attrs, {'id': pyamf.Undefined})

    def test_non_field_prop(self):
        from django.db import models

        class Book(models.Model):
            def _get_number_of_odd_pages(self):
                return 234

            # note the lack of a setter callable ..
            numberOfOddPages = property(_get_number_of_odd_pages)

        alias = adapter.DjangoClassAlias(Book, 'Book')

        x = Book()

        self.assertEqual(alias.getEncodableAttributes(x),
            {'numberOfOddPages': 234, 'id': None})

        # now we test sending the numberOfOddPages attribute
        alias.applyAttributes(x, {'numberOfOddPages': 24, 'id': None})

        # test it hasn't been set
        self.assertEqual(x.numberOfOddPages, 234)

    def test_dynamic(self):
        """
        Test for dynamic property encoding.
        """
        alias = adapter.DjangoClassAlias(models.SimplestModel, 'Book')

        x = models.SimplestModel()
        x.spam = 'eggs'

        self.assertEqual(alias.getEncodableAttributes(x),
            {'spam': 'eggs', 'id': None})

        # now we test sending the numberOfOddPages attribute
        alias.applyAttributes(x, {'spam': 'foo', 'id': None})

        # test it has been set
        self.assertEqual(x.spam, 'foo')

    def test_properties(self):
        """
        See #764
        """
        from django.db import models

        class Foob(models.Model):
            def _get_days(self):
                return 1

            def _set_days(self, val):
                assert 1 == val

            days = property(_get_days, _set_days)

        alias = adapter.DjangoClassAlias(Foob, 'Bar')

        x = Foob()

        self.assertEqual(x.days, 1)

        self.assertEqual(alias.getEncodableAttributes(x),
            {'days': 1, 'id': None})

        # now we test sending the numberOfOddPages attribute
        alias.applyAttributes(x, {'id': None})


class ForeignKeyTestCase(BaseTestCase):
    def test_one_to_many(self):
        # initialise the db ..
        r = models.Reporter(first_name='John', last_name='Smith', email='john@example.com')
        r.save()
        self.addCleanup(r.delete)

        r2 = models.Reporter(first_name='Paul', last_name='Jones', email='paul@example.com')
        r2.save()
        self.addCleanup(r2.delete)

        a = models.Article(headline="This is a test", reporter=r)
        a.save()
        self.addCleanup(a.delete)

        self.assertEqual(a.id, 1)

        del a

        a = models.Article.objects.filter(pk=1)[0]

        self.assertFalse('_reporter_cache' in a.__dict__)
        a.reporter
        self.assertTrue('_reporter_cache' in a.__dict__)

        del a

        a = models.Article.objects.filter(pk=1)[0]
        alias = adapter.DjangoClassAlias(models.Article, defer=True)

        self.assertFalse(hasattr(alias, 'fields'))
        attrs = alias.getEncodableAttributes(a)

        # note that the reporter attribute does not exist.
        self.assertEqual(attrs, {
            'headline': u'This is a test',
            'id': 1,
            'publications': []
        })

        self.assertFalse('_reporter_cache' in a.__dict__)
        self.assertEqual(pyamf.encode(a, encoding=pyamf.AMF3).getvalue(),
            '\n\x0b\x01\x11headline\x06\x1dThis is a test\x05id\x04\x01'
            '\x19publications\t\x01\x01\x01')

        del a

        # now with select_related to pull in the reporter object
        a = models.Article.objects.select_related('reporter').filter(pk=1)[0]

        alias = adapter.DjangoClassAlias(models.Article, defer=True)

        self.assertFalse(hasattr(alias, 'fields'))
        self.assertEqual(alias.getEncodableAttributes(a), {
            'headline': u'This is a test',
            'id': 1,
            'reporter': r,
            'publications': []
        })

        self.assertTrue('_reporter_cache' in a.__dict__)
        self.assertEqual(pyamf.encode(a, encoding=pyamf.AMF3).getvalue(),
            '\n\x0b\x01\x11reporter\n\x0b\x01\x15first_name\x06\tJohn\x13'
            'last_name\x06\x0bSmith\x05id\x04\x01\x0bemail\x06!john'
            '@example.com\x01\x11headline\x06\x1dThis is a test\x19'
            'publications\t\x01\x01\n\x04\x01\x01')

    def test_many_to_many(self):
        # install some test data - taken from
        # http://www.djangoproject.com/documentation/models/many_to_many/
        p1 = models.Publication(id=None, title='The Python Journal')
        p1.save()
        p2 = models.Publication(id=None, title='Science News')
        p2.save()
        p3 = models.Publication(id=None, title='Science Weekly')
        p3.save()

        self.addCleanup(p1.delete)
        self.addCleanup(p2.delete)
        self.addCleanup(p3.delete)

        # Create an Article.
        a1 = models.Article(id=None, headline='Django lets you build Web apps easily')
        a1.save()
        self.addCleanup(a1.delete)
        self.assertEqual(a1.id, 1)

        # Associate the Article with a Publication.
        a1.publications.add(p1)

        pub_alias = adapter.DjangoClassAlias(models.Publication, None)
        art_alias = adapter.DjangoClassAlias(models.Article, None)

        test_publication = models.Publication.objects.filter(pk=1)[0]
        test_article = models.Article.objects.filter(pk=1)[0]

        attrs = pub_alias.getEncodableAttributes(test_publication)
        self.assertEqual(attrs, {'id': 1, 'title': u'The Python Journal'})

        attrs = art_alias.getEncodableAttributes(test_article)
        self.assertEqual(attrs, {
            'headline': u'Django lets you build Web apps easily',
            'id': 1,
            'publications': [p1]
        })

        x = models.Article()

        art_alias.applyAttributes(x, {
            'headline': u'Test',
            'id': 1,
            'publications': [p1]
        })

        self.assertEqual(x.headline, u'Test')
        self.assertEqual(x.id, 1)
        self.assertEqual(list(x.publications.all()), [p1])

        y = models.Article()
        attrs = art_alias.getDecodableAttributes(y, {
            'headline': u'Django lets you build Web apps easily',
            'id': 0,
            'publications': []
        })

        self.assertEqual(attrs, {'headline': u'Django lets you build Web apps easily'})

    def test_nullable_foreign_keys(self):
        x = models.SimplestModel()
        x.save()
        self.addCleanup(x.delete)

        nfk_alias = adapter.DjangoClassAlias(models.NullForeignKey, None)
        bfk_alias = adapter.DjangoClassAlias(models.BlankForeignKey, None)

        nfk = models.NullForeignKey()
        attrs = nfk_alias.getEncodableAttributes(nfk)

        self.assertEqual(attrs, {'id': None})

        bfk = models.BlankForeignKey()
        attrs = bfk_alias.getEncodableAttributes(bfk)

        self.assertEqual(attrs, {'id': None})

    def test_static_relation(self):
        """
        @see: #693
        """
        from pyamf import util

        pyamf.register_class(models.StaticRelation)
        alias = adapter.DjangoClassAlias(models.StaticRelation,
            static_attrs=('gak',))

        alias.compile()

        self.assertTrue('gak' in alias.relations)
        self.assertTrue('gak' in alias.decodable_properties)
        self.assertTrue('gak' in alias.static_attrs)

        x = models.StaticRelation()

        # just run this to ensure that it doesn't blow up
        alias.getDecodableAttributes(x, {'id': None, 'gak': 'foo'})


class I18NTestCase(BaseTestCase):
    def test_encode(self):
        from django.utils.translation import ugettext_lazy

        self.assertEqual(pyamf.encode(ugettext_lazy('Hello')).getvalue(),
            '\x06\x0bHello')


class PKTestCase(BaseTestCase):
    """
    See ticket #599 for this. Check to make sure that django pk fields
    are set first
    """

    def test_behaviour(self):
        p = models.Publication(id=None, title='The Python Journal')
        a = models.Article(id=None, headline='Django lets you build Web apps easily')

        # Associate the Article with a Publication.
        self.assertRaises(ValueError, lambda a, p: a.publications.add(p), a, p)

        p.save()
        a.save()

        self.addCleanup(p.delete)
        self.addCleanup(a.delete)

        self.assertEqual(a.id, 1)

        article_alias = adapter.DjangoClassAlias(models.Article, None)
        x = models.Article()

        article_alias.applyAttributes(x, {
            'headline': 'Foo bar!',
            'id': 1,
            'publications': [p]
        })

        self.assertEqual(x.headline, 'Foo bar!')
        self.assertEqual(x.id, 1)
        self.assertEqual(list(x.publications.all()), [p])

    def test_none(self):
        """
        See #556. Make sure that PK fields with a value of 0 are actually set
        to C{None}.
        """
        alias = adapter.DjangoClassAlias(models.SimplestModel, None)

        x = models.SimplestModel()

        self.assertEqual(x.id, None)

        alias.applyAttributes(x, {
            'id': 0
        })

        self.assertEqual(x.id, None)

    def test_no_pk(self):
        """
        Ensure that Models without a primary key are correctly serialized.
        See #691.
        """
        instances = [models.NotSaved(name="a"), models.NotSaved(name="b")]
        encoded = pyamf.encode(instances, encoding=pyamf.AMF3).getvalue()

        decoded = pyamf.decode(encoded, encoding=pyamf.AMF3).next()
        self.assertEqual(decoded[0]['name'], 'a')
        self.assertEqual(decoded[1]['name'], 'b')


class ModelInheritanceTestCase(BaseTestCase):
    """
    Tests for L{Django model inheritance<http://docs.djangoproject.com/en/dev/topics/db/models/#model-inheritance>}
    """

    def test_abstract(self):
        alias = adapter.DjangoClassAlias(models.Student)

        x = models.Student()

        attrs = alias.getEncodableAttributes(x)

        self.assertEqual(attrs, {
            'age': None,
            'home_group': '',
            'id': None,
            'name': ''
        })

    def test_concrete(self):
        alias = adapter.DjangoClassAlias(models.Place)
        x = models.Place()

        attrs = alias.getEncodableAttributes(x)

        self.assertEqual(attrs, {
            'id': None,
            'name': '',
            'address': ''
        })

        alias = adapter.DjangoClassAlias(models.Restaurant)
        x = models.Restaurant()

        attrs = alias.getEncodableAttributes(x)

        self.assertEqual(attrs, {
            'id': None,
            'name': '',
            'address': '',
            'serves_hot_dogs': False,
            'serves_pizza': False
        })


class MockFile(object):
    """
    mock for L{django.core.files.base.File}
    """

    def chunks(self):
        return []

    def __len__(self):
        return 0

    def read(self, n):
        return ''


class FieldsTestCase(BaseTestCase):
    """
    Tests for L{fields}
    """

    def test_file(self):
        alias = adapter.DjangoClassAlias(models.FileModel)

        i = models.FileModel()
        i.file.save('bar', MockFile())
        self.addCleanup(i.file.delete)

        i.save()

        attrs = alias.getEncodableAttributes(i)

        self.assertEqual(attrs, {'text': '', 'id': 1, 'file': u'file_model/bar'})

        attrs = alias.getDecodableAttributes(i, attrs)

        self.assertEqual(attrs, {'text': ''})


class ImageTestCase(BaseTestCase):
    """
    Tests for L{fields}
    """

    def setUp(self):
        try:
            import PIL
        except ImportError:
            self.skipTest("'PIL' is not available")

        BaseTestCase.setUp(self)

    def test_image(self):
        alias = adapter.DjangoClassAlias(models.Profile)

        i = models.Profile()
        i.file.save('bar', MockFile())
        self.addCleanup(i.file.delete)

        i.save()
        self.addCleanup(i.delete)

        attrs = alias.getEncodableAttributes(i)

        self.assertEqual(attrs, {'text': '', 'id': 1, 'file': u'profile/bar'})

        attrs = alias.getDecodableAttributes(i, attrs)

        self.assertEqual(attrs, {'text': ''})


class ReferenceTestCase(BaseTestCase, util.EncoderMixIn):
    """
    Test case to make sure that the same object from the database is encoded
    by reference.
    """

    amf_type = pyamf.AMF3

    def setUp(self):
        BaseTestCase.setUp(self)
        util.EncoderMixIn.setUp(self)

    def test_not_referenced(self):
        """
        Test to ensure that we observe the correct behaviour in the Django
        ORM.
        """
        f = models.ParentReference()
        f.name = 'foo'

        b = models.ChildReference()
        b.name = 'bar'

        f.save()
        b.foo = f
        b.save()
        f.bar = b
        f.save()

        self.addCleanup(f.delete)
        self.addCleanup(b.delete)

        self.assertEqual(f.id, 1)
        foo = models.ParentReference.objects.select_related().get(id=1)

        self.assertFalse(foo.bar.foo is foo)

    def test_referenced_encode(self):
        f = models.ParentReference()
        f.name = 'foo'

        b = models.ChildReference()
        b.name = 'bar'

        f.save()
        b.foo = f
        b.save()
        f.bar = b
        f.save()

        self.addCleanup(f.delete)
        self.addCleanup(b.delete)

        self.assertEqual(f.id, 1)
        foo = models.ParentReference.objects.select_related().get(id=1)

        # ensure the referenced attribute resolves
        foo.bar.foo

        self.assertEncoded(foo, '\n\x0b\x01\x07bar\n\x0b\x01\x07foo\n\x00\x05'
            'id\x04\x01\tname\x06\x00\x01\x04\x04\x01\x06\x06\x02\x01')


class AuthTestCase(BaseTestCase):
    """
    Tests for L{django.contrib.auth.models}
    """

    def test_user(self):
        from django.contrib.auth import models

        alias = pyamf.get_class_alias(models.User)

        self.assertEqual(alias, 'django.contrib.auth.models.User')
        self.assertEqual(alias.exclude_attrs, ('message_set', 'password'))
        self.assertEqual(alias.readonly_attrs, ('username',))


class DBColumnTestCase(BaseTestCase):
    """
    Tests for #807
    """

    def setUp(self):
        BaseTestCase.setUp(self)

        self.alias = adapter.DjangoClassAlias(models.DBColumnModel, None)
        self.model = models.DBColumnModel()

    def test_encodable_attrs(self):
        def attrs():
            return self.alias.getEncodableAttributes(self.model)

        self.assertEqual(attrs(), {'id': None})

        x = models.SimplestModel()

        x.save()
        self.addCleanup(x.delete)

        self.model.bar = x

        self.assertEqual(attrs(), {'id': None, 'bar': x})

import os
import warnings

import cherrypy
from cherrypy._cpcompat import iteritems, copykeys, builtins


class Checker(object):
    """A checker for CherryPy sites and their mounted applications.
    
    When this object is called at engine startup, it executes each
    of its own methods whose names start with ``check_``. If you wish
    to disable selected checks, simply add a line in your global
    config which sets the appropriate method to False::
    
        [global]
        checker.check_skipped_app_config = False
    
    You may also dynamically add or replace ``check_*`` methods in this way.
    """
    
    on = True
    """If True (the default), run all checks; if False, turn off all checks."""
    
    
    def __init__(self):
        self._populate_known_types()
    
    def __call__(self):
        """Run all check_* methods."""
        if self.on:
            oldformatwarning = warnings.formatwarning
            warnings.formatwarning = self.formatwarning
            try:
                for name in dir(self):
                    if name.startswith("check_"):
                        method = getattr(self, name)
                        if method and hasattr(method, '__call__'):
                            method()
            finally:
                warnings.formatwarning = oldformatwarning
    
    def formatwarning(self, message, category, filename, lineno, line=None):
        """Function to format a warning."""
        return "CherryPy Checker:\n%s\n\n" % message
    
    # This value should be set inside _cpconfig.
    global_config_contained_paths = False
    
    def check_app_config_entries_dont_start_with_script_name(self):
        """Check for Application config with sections that repeat script_name."""
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                continue
            if sn == '':
                continue
            sn_atoms = sn.strip("/").split("/")
            for key in app.config.keys():
                key_atoms = key.strip("/").split("/")
                if key_atoms[:len(sn_atoms)] == sn_atoms:
                    warnings.warn(
                        "The application mounted at %r has config " \
                        "entries that start with its script name: %r" % (sn, key))
    
    def check_site_config_entries_in_app_config(self):
        """Check for mounted Applications that have site-scoped config."""
        for sn, app in iteritems(cherrypy.tree.apps):
            if not isinstance(app, cherrypy.Application):
                continue
            
            msg = []
            for section, entries in iteritems(app.config):
                if section.startswith('/'):
                    for key, value in iteritems(entries):
                        for n in ("engine.", "server.", "tree.", "checker."):
                            if key.startswith(n):
                                msg.append("[%s] %s = %s" % (section, key, value))
            if msg:
                msg.insert(0,
                    "The application mounted at %r contains the following "
                    "config entries, which are only allowed in site-wide "
                    "config. Move them to a [global] section and pass them "
                    "to cherrypy.config.update() instead of tree.mount()." % sn)
                warnings.warn(os.linesep.join(msg))
    
    def check_skipped_app_config(self):
        """Check for mounted Applications that have no config."""
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                msg = "The Application mounted at %r has an empty config." % sn
                if self.global_config_contained_paths:
                    msg += (" It looks like the config you passed to "
                            "cherrypy.config.update() contains application-"
                            "specific sections. You must explicitly pass "
                            "application config via "
                            "cherrypy.tree.mount(..., config=app_config)")
                warnings.warn(msg)
                return
    
    def check_app_config_brackets(self):
        """Check for Application config with extraneous brackets in section names."""
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            if not app.config:
                continue
            for key in app.config.keys():
                if key.startswith("[") or key.endswith("]"):
                    warnings.warn(
                        "The application mounted at %r has config " \
                        "section names with extraneous brackets: %r. "
                        "Config *files* need brackets; config *dicts* "
                        "(e.g. passed to tree.mount) do not." % (sn, key))
    
    def check_static_paths(self):
        """Check Application config for incorrect static paths."""
        # Use the dummy Request object in the main thread.
        request = cherrypy.request
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            request.app = app
            for section in app.config:
                # get_resource will populate request.config
                request.get_resource(section + "/dummy.html")
                conf = request.config.get
                
                if conf("tools.staticdir.on", False):
                    msg = ""
                    root = conf("tools.staticdir.root")
                    dir = conf("tools.staticdir.dir")
                    if dir is None:
                        msg = "tools.staticdir.dir is not set."
                    else:
                        fulldir = ""
                        if os.path.isabs(dir):
                            fulldir = dir
                            if root:
                                msg = ("dir is an absolute path, even "
                                       "though a root is provided.")
                                testdir = os.path.join(root, dir[1:])
                                if os.path.exists(testdir):
                                    msg += ("\nIf you meant to serve the "
                                            "filesystem folder at %r, remove "
                                            "the leading slash from dir." % testdir)
                        else:
                            if not root:
                                msg = "dir is a relative path and no root provided."
                            else:
                                fulldir = os.path.join(root, dir)
                                if not os.path.isabs(fulldir):
                                    msg = "%r is not an absolute path." % fulldir
                        
                        if fulldir and not os.path.exists(fulldir):
                            if msg:
                                msg += "\n"
                            msg += ("%r (root + dir) is not an existing "
                                    "filesystem path." % fulldir)
                    
                    if msg:
                        warnings.warn("%s\nsection: [%s]\nroot: %r\ndir: %r"
                                      % (msg, section, root, dir))
    
    
    # -------------------------- Compatibility -------------------------- #
    
    obsolete = {
        'server.default_content_type': 'tools.response_headers.headers',
        'log_access_file': 'log.access_file',
        'log_config_options': None,
        'log_file': 'log.error_file',
        'log_file_not_found': None,
        'log_request_headers': 'tools.log_headers.on',
        'log_to_screen': 'log.screen',
        'show_tracebacks': 'request.show_tracebacks',
        'throw_errors': 'request.throw_errors',
        'profiler.on': ('cherrypy.tree.mount(profiler.make_app('
                        'cherrypy.Application(Root())))'),
        }
    
    deprecated = {}
    
    def _compat(self, config):
        """Process config and warn on each obsolete or deprecated entry."""
        for section, conf in config.items():
            if isinstance(conf, dict):
                for k, v in conf.items():
                    if k in self.obsolete:
                        warnings.warn("%r is obsolete. Use %r instead.\n"
                                      "section: [%s]" %
                                      (k, self.obsolete[k], section))
                    elif k in self.deprecated:
                        warnings.warn("%r is deprecated. Use %r instead.\n"
                                      "section: [%s]" %
                                      (k, self.deprecated[k], section))
            else:
                if section in self.obsolete:
                    warnings.warn("%r is obsolete. Use %r instead."
                                  % (section, self.obsolete[section]))
                elif section in self.deprecated:
                    warnings.warn("%r is deprecated. Use %r instead."
                                  % (section, self.deprecated[section]))
    
    def check_compatibility(self):
        """Process config and warn on each obsolete or deprecated entry."""
        self._compat(cherrypy.config)
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._compat(app.config)
    
    
    # ------------------------ Known Namespaces ------------------------ #
    
    extra_config_namespaces = []
    
    def _known_ns(self, app):
        ns = ["wsgi"]
        ns.extend(copykeys(app.toolboxes))
        ns.extend(copykeys(app.namespaces))
        ns.extend(copykeys(app.request_class.namespaces))
        ns.extend(copykeys(cherrypy.config.namespaces))
        ns += self.extra_config_namespaces
        
        for section, conf in app.config.items():
            is_path_section = section.startswith("/")
            if is_path_section and isinstance(conf, dict):
                for k, v in conf.items():
                    atoms = k.split(".")
                    if len(atoms) > 1:
                        if atoms[0] not in ns:
                            # Spit out a special warning if a known
                            # namespace is preceded by "cherrypy."
                            if (atoms[0] == "cherrypy" and atoms[1] in ns):
                                msg = ("The config entry %r is invalid; "
                                       "try %r instead.\nsection: [%s]"
                                       % (k, ".".join(atoms[1:]), section))
                            else:
                                msg = ("The config entry %r is invalid, because "
                                       "the %r config namespace is unknown.\n"
                                       "section: [%s]" % (k, atoms[0], section))
                            warnings.warn(msg)
                        elif atoms[0] == "tools":
                            if atoms[1] not in dir(cherrypy.tools):
                                msg = ("The config entry %r may be invalid, "
                                       "because the %r tool was not found.\n"
                                       "section: [%s]" % (k, atoms[1], section))
                                warnings.warn(msg)
    
    def check_config_namespaces(self):
        """Process config and warn on each unknown config namespace."""
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._known_ns(app)


    
    
    # -------------------------- Config Types -------------------------- #
    
    known_config_types = {}
    
    def _populate_known_types(self):
        b = [x for x in vars(builtins).values()
             if type(x) is type(str)]
        
        def traverse(obj, namespace):
            for name in dir(obj):
                # Hack for 3.2's warning about body_params
                if name == 'body_params':
                    continue
                vtype = type(getattr(obj, name, None))
                if vtype in b:
                    self.known_config_types[namespace + "." + name] = vtype
        
        traverse(cherrypy.request, "request")
        traverse(cherrypy.response, "response")
        traverse(cherrypy.server, "server")
        traverse(cherrypy.engine, "engine")
        traverse(cherrypy.log, "log")
    
    def _known_types(self, config):
        msg = ("The config entry %r in section %r is of type %r, "
               "which does not match the expected type %r.")
        
        for section, conf in config.items():
            if isinstance(conf, dict):
                for k, v in conf.items():
                    if v is not None:
                        expected_type = self.known_config_types.get(k, None)
                        vtype = type(v)
                        if expected_type and vtype != expected_type:
                            warnings.warn(msg % (k, section, vtype.__name__,
                                                 expected_type.__name__))
            else:
                k, v = section, conf
                if v is not None:
                    expected_type = self.known_config_types.get(k, None)
                    vtype = type(v)
                    if expected_type and vtype != expected_type:
                        warnings.warn(msg % (k, section, vtype.__name__,
                                             expected_type.__name__))
    
    def check_config_types(self):
        """Assert that config values are of the same type as default values."""
        self._known_types(cherrypy.config)
        for sn, app in cherrypy.tree.apps.items():
            if not isinstance(app, cherrypy.Application):
                continue
            self._known_types(app.config)
    
    
    # -------------------- Specific config warnings -------------------- #
    
    def check_localhost(self):
        """Warn if any socket_host is 'localhost'. See #711."""
        for k, v in cherrypy.config.items():
            if k == 'server.socket_host' and v == 'localhost':
                warnings.warn("The use of 'localhost' as a socket host can "
                    "cause problems on newer systems, since 'localhost' can "
                    "map to either an IPv4 or an IPv6 address. You should "
                    "use '127.0.0.1' or '[::1]' instead.")

import warnings
warnings.warn('cherrypy.lib.http has been deprecated and will be removed '
              'in CherryPy 3.3 use cherrypy.lib.httputil instead.',
              DeprecationWarning)

from cherrypy.lib.httputil import *


# This file is part of CherryPy <http://www.cherrypy.org/>
# -*- coding: utf-8 -*-
# vim:ts=4:sw=4:expandtab:fileencoding=utf-8


import cherrypy
from cherrypy.lib import auth_digest

from cherrypy.test import helper

class DigestAuthTest(helper.CPWebCase):

    def setup_server():
        class Root:
            def index(self):
                return "This is public."
            index.exposed = True

        class DigestProtected:
            def index(self):
                return "Hello %s, you've been authorized." % cherrypy.request.login
            index.exposed = True

        def fetch_users():
            return {'test': 'test'}


        get_ha1 = cherrypy.lib.auth_digest.get_ha1_dict_plain(fetch_users())
        conf = {'/digest': {'tools.auth_digest.on': True,
                            'tools.auth_digest.realm': 'localhost',
                            'tools.auth_digest.get_ha1': get_ha1,
                            'tools.auth_digest.key': 'a565c27146791cfb',
                            'tools.auth_digest.debug': 'True'}}

        root = Root()
        root.digest = DigestProtected()
        cherrypy.tree.mount(root, config=conf)
    setup_server = staticmethod(setup_server)
    
    def testPublic(self):
        self.getPage("/")
        self.assertStatus('200 OK')
        self.assertHeader('Content-Type', 'text/html;charset=utf-8')
        self.assertBody('This is public.')

    def testDigest(self):
        self.getPage("/digest/")
        self.assertStatus(401)

        value = None
        for k, v in self.headers:
            if k.lower() == "www-authenticate":
                if v.startswith("Digest"):
                    value = v
                    break

        if value is None:
            self._handlewebError("Digest authentification scheme was not found")

        value = value[7:]
        items = value.split(', ')
        tokens = {}
        for item in items:
            key, value = item.split('=')
            tokens[key.lower()] = value

        missing_msg = "%s is missing"
        bad_value_msg = "'%s' was expecting '%s' but found '%s'"
        nonce = None
        if 'realm' not in tokens:
            self._handlewebError(missing_msg % 'realm')
        elif tokens['realm'] != '"localhost"':
            self._handlewebError(bad_value_msg % ('realm', '"localhost"', tokens['realm']))
        if 'nonce' not in tokens:
            self._handlewebError(missing_msg % 'nonce')
        else:
            nonce = tokens['nonce'].strip('"')
        if 'algorithm' not in tokens:
            self._handlewebError(missing_msg % 'algorithm')
        elif tokens['algorithm'] != '"MD5"':
            self._handlewebError(bad_value_msg % ('algorithm', '"MD5"', tokens['algorithm']))
        if 'qop' not in tokens:
            self._handlewebError(missing_msg % 'qop')
        elif tokens['qop'] != '"auth"':
            self._handlewebError(bad_value_msg % ('qop', '"auth"', tokens['qop']))

        get_ha1 = auth_digest.get_ha1_dict_plain({'test' : 'test'})

        # Test user agent response with a wrong value for 'realm'
        base_auth = 'Digest username="test", realm="wrong realm", nonce="%s", uri="/digest/", algorithm=MD5, response="%s", qop=auth, nc=%s, cnonce="1522e61005789929"'

        auth_header = base_auth % (nonce, '11111111111111111111111111111111', '00000001')
        auth = auth_digest.HttpDigestAuthorization(auth_header, 'GET')
        # calculate the response digest
        ha1 = get_ha1(auth.realm, 'test')
        response = auth.request_digest(ha1)
        # send response with correct response digest, but wrong realm
        auth_header = base_auth % (nonce, response, '00000001')
        self.getPage('/digest/', [('Authorization', auth_header)])
        self.assertStatus(401)

        # Test that must pass
        base_auth = 'Digest username="test", realm="localhost", nonce="%s", uri="/digest/", algorithm=MD5, response="%s", qop=auth, nc=%s, cnonce="1522e61005789929"'

        auth_header = base_auth % (nonce, '11111111111111111111111111111111', '00000001')
        auth = auth_digest.HttpDigestAuthorization(auth_header, 'GET')
        # calculate the response digest
        ha1 = get_ha1('localhost', 'test')
        response = auth.request_digest(ha1)
        # send response with correct response digest
        auth_header = base_auth % (nonce, response, '00000001')
        self.getPage('/digest/', [('Authorization', auth_header)])
        self.assertStatus('200 OK')
        self.assertBody("Hello test, you've been authorized.")


import os
curdir = os.path.join(os.getcwd(), os.path.dirname(__file__))

import cherrypy
from cherrypy.test import helper


class VirtualHostTest(helper.CPWebCase):

    def setup_server():
        class Root:
            def index(self):
                return "Hello, world"
            index.exposed = True
            
            def dom4(self):
                return "Under construction"
            dom4.exposed = True
            
            def method(self, value):
                return "You sent %s" % value
            method.exposed = True
        
        class VHost:
            def __init__(self, sitename):
                self.sitename = sitename
            
            def index(self):
                return "Welcome to %s" % self.sitename
            index.exposed = True
            
            def vmethod(self, value):
                return "You sent %s" % value
            vmethod.exposed = True
            
            def url(self):
                return cherrypy.url("nextpage")
            url.exposed = True
            
            # Test static as a handler (section must NOT include vhost prefix)
            static = cherrypy.tools.staticdir.handler(section='/static', dir=curdir)
        
        root = Root()
        root.mydom2 = VHost("Domain 2")
        root.mydom3 = VHost("Domain 3")
        hostmap = {'www.mydom2.com': '/mydom2',
                   'www.mydom3.com': '/mydom3',
                   'www.mydom4.com': '/dom4',
                   }
        cherrypy.tree.mount(root, config={
            '/': {'request.dispatch': cherrypy.dispatch.VirtualHost(**hostmap)},
            # Test static in config (section must include vhost prefix)
            '/mydom2/static2': {'tools.staticdir.on': True,
                                'tools.staticdir.root': curdir,
                                'tools.staticdir.dir': 'static',
                                'tools.staticdir.index': 'index.html',
                                },
            })
    setup_server = staticmethod(setup_server)
    
    def testVirtualHost(self):
        self.getPage("/", [('Host', 'www.mydom1.com')])
        self.assertBody('Hello, world')
        self.getPage("/mydom2/", [('Host', 'www.mydom1.com')])
        self.assertBody('Welcome to Domain 2')
        
        self.getPage("/", [('Host', 'www.mydom2.com')])
        self.assertBody('Welcome to Domain 2')
        self.getPage("/", [('Host', 'www.mydom3.com')])
        self.assertBody('Welcome to Domain 3')
        self.getPage("/", [('Host', 'www.mydom4.com')])
        self.assertBody('Under construction')
        
        # Test GET, POST, and positional params
        self.getPage("/method?value=root")
        self.assertBody("You sent root")
        self.getPage("/vmethod?value=dom2+GET", [('Host', 'www.mydom2.com')])
        self.assertBody("You sent dom2 GET")
        self.getPage("/vmethod", [('Host', 'www.mydom3.com')], method="POST",
                     body="value=dom3+POST")
        self.assertBody("You sent dom3 POST")
        self.getPage("/vmethod/pos", [('Host', 'www.mydom3.com')])
        self.assertBody("You sent pos")
        
        # Test that cherrypy.url uses the browser url, not the virtual url
        self.getPage("/url", [('Host', 'www.mydom2.com')])
        self.assertBody("%s://www.mydom2.com/nextpage" % self.scheme)
    
    def test_VHost_plus_Static(self):
        # Test static as a handler
        self.getPage("/static/style.css", [('Host', 'www.mydom2.com')])
        self.assertStatus('200 OK')
        self.assertHeader('Content-Type', 'text/css;charset=utf-8')
        
        # Test static in config
        self.getPage("/static2/dirback.jpg", [('Host', 'www.mydom2.com')])
        self.assertStatus('200 OK')
        self.assertHeader('Content-Type', 'image/jpeg')
        
        # Test static config with "index" arg
        self.getPage("/static2/", [('Host', 'www.mydom2.com')])
        self.assertStatus('200 OK')
        self.assertBody('Hello, world\r\n')
        # Since tools.trailing_slash is on by default, this should redirect
        self.getPage("/static2", [('Host', 'www.mydom2.com')])
        self.assertStatus(301)


# Copyright 2009 Brian Quinlan. All Rights Reserved.
# Licensed to PSF under a Contributor Agreement.

"""Implements ThreadPoolExecutor."""

from __future__ import with_statement
import atexit
import threading
import weakref

from concurrent.futures import _base

try:
    import queue
except ImportError:
    import Queue as queue

__author__ = 'Brian Quinlan (brian@sweetapp.com)'

# Workers are created as daemon threads. This is done to allow the interpreter
# to exit when there are still idle threads in a ThreadPoolExecutor's thread
# pool (i.e. shutdown() was not called). However, allowing workers to die with
# the interpreter has two undesirable properties:
#   - The workers would still be running during interpretor shutdown,
#     meaning that they would fail in unpredictable ways.
#   - The workers could be killed while evaluating a work item, which could
#     be bad if the callable being evaluated has external side-effects e.g.
#     writing to a file.
#
# To work around this problem, an exit handler is installed which tells the
# workers to exit when their work queues are empty and then waits until the
# threads finish.

_threads_queues = weakref.WeakKeyDictionary()
_shutdown = False

def _python_exit():
    global _shutdown
    _shutdown = True
    items = _threads_queues.items()
    for t, q in items:
        q.put(None)
    for t, q in items:
        t.join()

atexit.register(_python_exit)

class _WorkItem(object):
    def __init__(self, future, fn, args, kwargs):
        self.future = future
        self.fn = fn
        self.args = args
        self.kwargs = kwargs

    def run(self):
        if not self.future.set_running_or_notify_cancel():
            return

        try:
            result = self.fn(*self.args, **self.kwargs)
        except BaseException as e:
            self.future.set_exception(e)
        else:
            self.future.set_result(result)

def _worker(executor_reference, work_queue):
    try:
        while True:
            work_item = work_queue.get(block=True)
            if work_item is not None:
                work_item.run()
                continue
            executor = executor_reference()
            # Exit if:
            #   - The interpreter is shutting down OR
            #   - The executor that owns the worker has been collected OR
            #   - The executor that owns the worker has been shutdown.
            if _shutdown or executor is None or executor._shutdown:
                # Notice other workers
                work_queue.put(None)
                return
            del executor
    except BaseException:
        _base.LOGGER.critical('Exception in worker', exc_info=True)

class ThreadPoolExecutor(_base.Executor):
    def __init__(self, max_workers):
        """Initializes a new ThreadPoolExecutor instance.

        Args:
            max_workers: The maximum number of threads that can be used to
                execute the given calls.
        """
        self._max_workers = max_workers
        self._work_queue = queue.Queue()
        self._threads = set()
        self._shutdown = False
        self._shutdown_lock = threading.Lock()

    def submit(self, fn, *args, **kwargs):
        with self._shutdown_lock:
            if self._shutdown:
                raise RuntimeError('cannot schedule new futures after shutdown')

            f = _base.Future()
            w = _WorkItem(f, fn, args, kwargs)

            self._work_queue.put(w)
            self._adjust_thread_count()
            return f
    submit.__doc__ = _base.Executor.submit.__doc__

    def _adjust_thread_count(self):
        # When the executor gets lost, the weakref callback will wake up
        # the worker threads.
        def weakref_cb(_, q=self._work_queue):
            q.put(None)
        # TODO(bquinlan): Should avoid creating new threads if there are more
        # idle threads than items in the work queue.
        if len(self._threads) < self._max_workers:
            t = threading.Thread(target=_worker,
                                 args=(weakref.ref(self, weakref_cb),
                                       self._work_queue))
            t.daemon = True
            t.start()
            self._threads.add(t)
            _threads_queues[t] = self._work_queue

    def shutdown(self, wait=True):
        with self._shutdown_lock:
            self._shutdown = True
            self._work_queue.put(None)
        if wait:
            for t in self._threads:
                t.join()
    shutdown.__doc__ = _base.Executor.shutdown.__doc__

from django.core.handlers.base import BaseHandler
from django.core import signals
from django.dispatch import dispatcher
from django.utils import datastructures
from django import http
from pprint import pformat
import os

# NOTE: do *not* import settings (or any module which eventually imports
# settings) until after ModPythonHandler has been called; otherwise os.environ
# won't be set up correctly (with respect to settings).

class ModPythonRequest(http.HttpRequest):
    def __init__(self, req):
        self._req = req
        self.path = req.uri

    def __repr__(self):
        # Since this is called as part of error handling, we need to be very
        # robust against potentially malformed input.
        try:
            get = pformat(self.GET)
        except:
            get = '<could not parse>'
        try:
            post = pformat(self.POST)
        except:
            post = '<could not parse>'
        try:
            cookies = pformat(self.COOKIES)
        except:
            cookies = '<could not parse>'
        try:
            meta = pformat(self.META)
        except:
            meta = '<could not parse>'
        return '<ModPythonRequest\npath:%s,\nGET:%s,\nPOST:%s,\nCOOKIES:%s,\nMETA:%s>' % \
            (self.path, get, post, cookies, meta)

    def get_full_path(self):
        return '%s%s' % (self.path, self._req.args and ('?' + self._req.args) or '')

    def is_secure(self):
        # Note: modpython 3.2.10+ has req.is_https(), but we need to support previous versions
        return self._req.subprocess_env.has_key('HTTPS') and self._req.subprocess_env['HTTPS'] == 'on'

    def _load_post_and_files(self):
        "Populates self._post and self._files"
        if self._req.headers_in.has_key('content-type') and self._req.headers_in['content-type'].startswith('multipart'):
            self._post, self._files = http.parse_file_upload(self._req.headers_in, self.raw_post_data)
        else:
            self._post, self._files = http.QueryDict(self.raw_post_data), datastructures.MultiValueDict()

    def _get_request(self):
        if not hasattr(self, '_request'):
            self._request = datastructures.MergeDict(self.POST, self.GET)
        return self._request

    def _get_get(self):
        if not hasattr(self, '_get'):
            self._get = http.QueryDict(self._req.args)
        return self._get

    def _set_get(self, get):
        self._get = get

    def _get_post(self):
        if not hasattr(self, '_post'):
            self._load_post_and_files()
        return self._post

    def _set_post(self, post):
        self._post = post

    def _get_cookies(self):
        if not hasattr(self, '_cookies'):
            self._cookies = http.parse_cookie(self._req.headers_in.get('cookie', ''))
        return self._cookies

    def _set_cookies(self, cookies):
        self._cookies = cookies

    def _get_files(self):
        if not hasattr(self, '_files'):
            self._load_post_and_files()
        return self._files

    def _get_meta(self):
        "Lazy loader that returns self.META dictionary"
        if not hasattr(self, '_meta'):
            self._meta = {
                'AUTH_TYPE':         self._req.ap_auth_type,
                'CONTENT_LENGTH':    self._req.clength, # This may be wrong
                'CONTENT_TYPE':      self._req.content_type, # This may be wrong
                'GATEWAY_INTERFACE': 'CGI/1.1',
                'PATH_INFO':         self._req.path_info,
                'PATH_TRANSLATED':   None, # Not supported
                'QUERY_STRING':      self._req.args,
                'REMOTE_ADDR':       self._req.connection.remote_ip,
                'REMOTE_HOST':       None, # DNS lookups not supported
                'REMOTE_IDENT':      self._req.connection.remote_logname,
                'REMOTE_USER':       self._req.user,
                'REQUEST_METHOD':    self._req.method,
                'SCRIPT_NAME':       None, # Not supported
                'SERVER_NAME':       self._req.server.server_hostname,
                'SERVER_PORT':       self._req.server.port,
                'SERVER_PROTOCOL':   self._req.protocol,
                'SERVER_SOFTWARE':   'mod_python'
            }
            for key, value in self._req.headers_in.items():
                key = 'HTTP_' + key.upper().replace('-', '_')
                self._meta[key] = value
        return self._meta

    def _get_raw_post_data(self):
        try:
            return self._raw_post_data
        except AttributeError:
            self._raw_post_data = self._req.read()
            return self._raw_post_data

    def _get_method(self):
        return self.META['REQUEST_METHOD'].upper()

    GET = property(_get_get, _set_get)
    POST = property(_get_post, _set_post)
    COOKIES = property(_get_cookies, _set_cookies)
    FILES = property(_get_files)
    META = property(_get_meta)
    REQUEST = property(_get_request)
    raw_post_data = property(_get_raw_post_data)
    method = property(_get_method)

class ModPythonHandler(BaseHandler):
    def __call__(self, req):
        # mod_python fakes the environ, and thus doesn't process SetEnv.  This fixes that
        os.environ.update(req.subprocess_env)

        # now that the environ works we can see the correct settings, so imports
        # that use settings now can work
        from django.conf import settings

        # if we need to set up middleware, now that settings works we can do it now.
        if self._request_middleware is None:
            self.load_middleware()

        dispatcher.send(signal=signals.request_started)
        try:
            request = ModPythonRequest(req)
            response = self.get_response(request)

            # Apply response middleware
            for middleware_method in self._response_middleware:
                response = middleware_method(request, response)

        finally:
            dispatcher.send(signal=signals.request_finished)

        # Convert our custom HttpResponse object back into the mod_python req.
        req.content_type = response['Content-Type']
        for key, value in response.headers.items():
            if key != 'Content-Type':
                req.headers_out[key] = value
        for c in response.cookies.values():
            req.headers_out.add('Set-Cookie', c.output(header=''))
        req.status = response.status_code
        try:
            for chunk in response:
                req.write(chunk)
        finally:
            response.close()

        return 0 # mod_python.apache.OK

def handler(req):
    # mod_python hooks into this function.
    return ModPythonHandler()(req)

from django.db.backends.dummy.base import complain

get_table_list = complain
get_table_description = complain
get_relations = complain
get_indexes = complain

DATA_TYPES_REVERSE = {}

"""
Extra HTML Widget classes
"""

from django.newforms.widgets import Widget, Select
from django.utils.dates import MONTHS
import datetime

__all__ = ('SelectDateWidget',)

class SelectDateWidget(Widget):
    """
    A Widget that splits date input into three <select> boxes.

    This also serves as an example of a Widget that has more than one HTML
    element and hence implements value_from_datadict.
    """
    month_field = '%s_month'
    day_field = '%s_day'
    year_field = '%s_year'

    def __init__(self, attrs=None, years=None):
        # years is an optional list/tuple of years to use in the "year" select box.
        self.attrs = attrs or {}
        if years:
            self.years = years
        else:
            this_year = datetime.date.today().year
            self.years = range(this_year, this_year+10)

    def render(self, name, value, attrs=None):
        try:
            value = datetime.date(*map(int, value.split('-')))
            year_val, month_val, day_val = value.year, value.month, value.day
        except (AttributeError, TypeError, ValueError):
            year_val = month_val = day_val = None

        output = []

        month_choices = MONTHS.items()
        month_choices.sort()
        select_html = Select(choices=month_choices).render(self.month_field % name, month_val)
        output.append(select_html)

        day_choices = [(i, i) for i in range(1, 32)]
        select_html = Select(choices=day_choices).render(self.day_field % name, day_val)
        output.append(select_html)

        year_choices = [(i, i) for i in self.years]
        select_html = Select(choices=year_choices).render(self.year_field % name, year_val)
        output.append(select_html)

        return u'\n'.join(output)

    def value_from_datadict(self, data, name):
        y, m, d = data.get(self.year_field % name), data.get(self.month_field % name), data.get(self.day_field % name)
        if y and m and d:
            return '%s-%s-%s' % (y, m, d)
        return None

"""
This module contains helper functions for controlling caching. It does so by
managing the "Vary" header of responses. It includes functions to patch the
header of response objects directly and decorators that change functions to do
that header-patching themselves.

For information on the Vary header, see:

    http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.44

Essentially, the "Vary" HTTP header defines which headers a cache should take
into account when building its cache key. Requests with the same path but
different header content for headers named in "Vary" need to get different
cache keys to prevent delivery of wrong content.

A example: i18n middleware would need to distinguish caches by the
"Accept-language" header.
"""

import datetime, md5, re
from django.conf import settings
from django.core.cache import cache

cc_delim_re = re.compile(r'\s*,\s*')

def patch_cache_control(response, **kwargs):
    """
    This function patches the Cache-Control header by adding all
    keyword arguments to it. The transformation is as follows:

    * All keyword parameter names are turned to lowercase, and underscores
      are converted to hyphens.
    * If the value of a parameter is True (exactly True, not just a
      true value), only the parameter name is added to the header.
    * All other parameters are added with their value, after applying
      str() to it.
    """
    def dictitem(s):
        t = s.split('=',1)
        if len(t) > 1:
            return (t[0].lower().replace('-', '_'), t[1])
        else:
            return (t[0].lower().replace('-', '_'), True)

    def dictvalue(t):
        if t[1] == True:
            return t[0]
        else:
            return t[0] + '=' + str(t[1])

    if response.has_header('Cache-Control'):
        cc = cc_delim_re.split(response['Cache-Control'])
        cc = dict([dictitem(el) for el in cc])
    else:
        cc = {}
    for (k,v) in kwargs.items():
        cc[k.replace('_', '-')] = v
    cc = ', '.join([dictvalue(el) for el in cc.items()])
    response['Cache-Control'] = cc

vary_delim_re = re.compile(r',\s*')

def patch_response_headers(response, cache_timeout=None):
    """
    Adds some useful headers to the given HttpResponse object:
        ETag, Last-Modified, Expires and Cache-Control

    Each header is only added if it isn't already set.

    cache_timeout is in seconds. The CACHE_MIDDLEWARE_SECONDS setting is used
    by default.
    """
    if cache_timeout is None:
        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
    now = datetime.datetime.utcnow()
    if not response.has_header('ETag'):
        response['ETag'] = md5.new(response.content).hexdigest()
    if not response.has_header('Last-Modified'):
        response['Last-Modified'] = now.strftime('%a, %d %b %Y %H:%M:%S GMT')
    if not response.has_header('Expires'):
        expires = now + datetime.timedelta(0, cache_timeout)
        response['Expires'] = expires.strftime('%a, %d %b %Y %H:%M:%S GMT')
    if cache_timeout < 0:
        cache_timeout = 0 # Can't have max-age negative
    patch_cache_control(response, max_age=cache_timeout)

def add_never_cache_headers(response):
    """
    Add headers to a response to indicate that 
    a page should never be cached.
    """
    patch_response_headers(response, cache_timeout=-1)

def patch_vary_headers(response, newheaders):
    """
    Adds (or updates) the "Vary" header in the given HttpResponse object.
    newheaders is a list of header names that should be in "Vary". Existing
    headers in "Vary" aren't removed.
    """
    # Note that we need to keep the original order intact, because cache
    # implementations may rely on the order of the Vary contents in, say,
    # computing an MD5 hash.
    vary = []
    if response.has_header('Vary'):
        vary = vary_delim_re.split(response['Vary'])
    oldheaders = dict([(el.lower(), 1) for el in vary])
    for newheader in newheaders:
        if not newheader.lower() in oldheaders:
            vary.append(newheader)
    response['Vary'] = ', '.join(vary)

def _generate_cache_key(request, headerlist, key_prefix):
    "Returns a cache key from the headers given in the header list."
    ctx = md5.new()
    for header in headerlist:
        value = request.META.get(header, None)
        if value is not None:
            ctx.update(value)
    return 'views.decorators.cache.cache_page.%s.%s.%s' % (key_prefix, request.path, ctx.hexdigest())

def get_cache_key(request, key_prefix=None):
    """
    Returns a cache key based on the request path. It can be used in the
    request phase because it pulls the list of headers to take into account
    from the global path registry and uses those to build a cache key to check
    against.

    If there is no headerlist stored, the page needs to be rebuilt, so this
    function returns None.
    """
    if key_prefix is None:
        key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
    cache_key = 'views.decorators.cache.cache_header.%s.%s' % (key_prefix, request.path)
    headerlist = cache.get(cache_key, None)
    if headerlist is not None:
        return _generate_cache_key(request, headerlist, key_prefix)
    else:
        return None

def learn_cache_key(request, response, cache_timeout=None, key_prefix=None):
    """
    Learns what headers to take into account for some request path from the
    response object. It stores those headers in a global path registry so that
    later access to that path will know what headers to take into account
    without building the response object itself. The headers are named in the
    Vary header of the response, but we want to prevent response generation.

    The list of headers to use for cache key generation is stored in the same
    cache as the pages themselves. If the cache ages some data out of the
    cache, this just means that we have to build the response once to get at
    the Vary header and so at the list of headers to use for the cache key.
    """
    if key_prefix is None:
        key_prefix = settings.CACHE_MIDDLEWARE_KEY_PREFIX
    if cache_timeout is None:
        cache_timeout = settings.CACHE_MIDDLEWARE_SECONDS
    cache_key = 'views.decorators.cache.cache_header.%s.%s' % (key_prefix, request.path)
    if response.has_header('Vary'):
        headerlist = ['HTTP_'+header.upper().replace('-', '_') for header in vary_delim_re.split(response['Vary'])]
        cache.set(cache_key, headerlist, cache_timeout)
        return _generate_cache_key(request, headerlist, key_prefix)
    else:
        # if there is no Vary header, we still need a cache key
        # for the request.path
        cache.set(cache_key, [], cache_timeout)
        return _generate_cache_key(request, [], key_prefix)

"""
Decorators for views based on HTTP headers.
"""

from django.utils.decorators import decorator_from_middleware
from django.middleware.http import ConditionalGetMiddleware
from django.http import HttpResponseNotAllowed

conditional_page = decorator_from_middleware(ConditionalGetMiddleware)

def require_http_methods(request_method_list):
    """
    Decorator to make a view only accept particular request methods.  Usage::

        @require_http_methods(["GET", "POST"])
        def my_view(request):
            # I can assume now that only GET or POST requests make it this far
            # ...

    Note that request methods should be in uppercase.
    """
    def decorator(func):
        def inner(request, *args, **kwargs):
            if request.method not in request_method_list:
                return HttpResponseNotAllowed(request_method_list)
            return func(request, *args, **kwargs)
        return inner
    return decorator

require_GET = require_http_methods(["GET"])
require_GET.__doc__ = "Decorator to require that a view only accept the GET method."

require_POST = require_http_methods(["POST"])
require_POST.__doc__ = "Decorator to require that a view only accept the POST method."
# -*- encoding: utf-8 -*-
# This file is distributed under the same license as the Django package.
#

DATE_FORMAT = r'j \de F \de Y'
TIME_FORMAT = 'H:i:s'
DATETIME_FORMAT = r'j \de F \de Y \a \l\a\s H:i'
YEAR_MONTH_FORMAT = r'F \de Y'
MONTH_DAY_FORMAT = r'j \de F'
SHORT_DATE_FORMAT = 'd/m/Y'
SHORT_DATETIME_FORMAT = 'd/m/Y H:i'
FIRST_DAY_OF_WEEK = 1 # Monday
DATE_INPUT_FORMATS = (
    # '31/12/2009', '31/12/09'
    '%d/%m/%Y', '%d/%m/%y'
)
TIME_INPUT_FORMATS = (
    # '14:30:59', '14:30'
    '%H:%M:%S', '%H:%M'
)
DATETIME_INPUT_FORMATS = (
    '%d/%m/%Y %H:%M:%S',
    '%d/%m/%Y %H:%M',
    '%d/%m/%y %H:%M:%S',
    '%d/%m/%y %H:%M',
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3


# -*- encoding: utf-8 -*-
# This file is distributed under the same license as the Django package.
#

DATE_FORMAT = 'j F Y'                   # '20 januari 2009'
TIME_FORMAT = 'H:i'                     # '15:23'
DATETIME_FORMAT = 'j F Y H:i'           # '20 januari 2009 15:23'
YEAR_MONTH_FORMAT = 'F Y'               # 'januari 2009'
MONTH_DAY_FORMAT = 'j F'                # '20 januari'
SHORT_DATE_FORMAT = 'j-n-Y'             # '20-1-2009'
SHORT_DATETIME_FORMAT = 'j-n-Y H:i'     # '20-1-2009 15:23'
FIRST_DAY_OF_WEEK = 1                   # Monday (in Dutch 'maandag')
DATE_INPUT_FORMATS = (
    '%d-%m-%Y', '%d-%m-%y', '%Y-%m-%d', # '20-01-2009', '20-01-09', '2009-01-20'
    # '%d %b %Y', '%d %b %y',             # '20 jan 2009', '20 jan 09'
    # '%d %B %Y', '%d %B %y',             # '20 januari 2009', '20 januari 09'
)
TIME_INPUT_FORMATS = (
    '%H:%M:%S',                         # '15:23:35'
    '%H.%M:%S',                         # '15.23:35'
    '%H.%M',                            # '15.23'
    '%H:%M',                            # '15:23'
)
DATETIME_INPUT_FORMATS = (
    # With time in %H:%M:%S :
    '%d-%m-%Y %H:%M:%S', '%d-%m-%y %H:%M:%S', '%Y-%m-%d %H:%M:%S',  # '20-01-2009 15:23:35', '20-01-09 15:23:35', '2009-01-20 15:23:35'
    # '%d %b %Y %H:%M:%S', '%d %b %y %H:%M:%S',   # '20 jan 2009 15:23:35', '20 jan 09 15:23:35'
    # '%d %B %Y %H:%M:%S', '%d %B %y %H:%M:%S',   # '20 januari 2009 15:23:35', '20 januari 2009 15:23:35'
    # With time in %H.%M:%S :
    '%d-%m-%Y %H.%M:%S', '%d-%m-%y %H.%M:%S',   # '20-01-2009 15.23:35', '20-01-09 15.23:35'
    # '%d %b %Y %H.%M:%S', '%d %b %y %H.%M:%S',   # '20 jan 2009 15.23:35', '20 jan 09 15.23:35'
    # '%d %B %Y %H.%M:%S', '%d %B %y %H.%M:%S',   # '20 januari 2009 15.23:35', '20 januari 2009 15.23:35'
    # With time in %H:%M :
    '%d-%m-%Y %H:%M', '%d-%m-%y %H:%M', '%Y-%m-%d %H:%M',   # '20-01-2009 15:23', '20-01-09 15:23', '2009-01-20 15:23'
    # '%d %b %Y %H:%M', '%d %b %y %H:%M',         # '20 jan 2009 15:23', '20 jan 09 15:23'
    # '%d %B %Y %H:%M', '%d %B %y %H:%M',         # '20 januari 2009 15:23', '20 januari 2009 15:23'
    # With time in %H.%M :
    '%d-%m-%Y %H.%M', '%d-%m-%y %H.%M',         # '20-01-2009 15.23', '20-01-09 15.23'
    # '%d %b %Y %H.%M', '%d %b %y %H.%M',         # '20 jan 2009 15.23', '20 jan 09 15.23'
    # '%d %B %Y %H.%M', '%d %B %y %H.%M',         # '20 januari 2009 15.23', '20 januari 2009 15.23'
    # Without time :
    '%d-%m-%Y', '%d-%m-%y', '%Y-%m-%d',         # '20-01-2009', '20-01-09', '2009-01-20'
    # '%d %b %Y', '%d %b %y',                     # '20 jan 2009', '20 jan 09'
    # '%d %B %Y', '%d %B %y',                     # '20 januari 2009', '20 januari 2009'
)
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
NUMBER_GROUPING = 3

from django import template

register = template.Library()

def prepopulated_fields_js(context):
    """
    Creates a list of prepopulated_fields that should render Javascript for
    the prepopulated fields for both the admin form and inlines.
    """
    prepopulated_fields = []
    if context['add'] and 'adminform' in context:
        prepopulated_fields.extend(context['adminform'].prepopulated_fields)
    if 'inline_admin_formsets' in context:
        for inline_admin_formset in context['inline_admin_formsets']:
            for inline_admin_form in inline_admin_formset:
                if inline_admin_form.original is None:
                    prepopulated_fields.extend(inline_admin_form.prepopulated_fields)
    context.update({'prepopulated_fields': prepopulated_fields})
    return context
prepopulated_fields_js = register.inclusion_tag('admin/prepopulated_fields_js.html', takes_context=True)(prepopulated_fields_js)

def submit_row(context):
    """
    Displays the row of buttons for delete and save. 
    """
    opts = context['opts']
    change = context['change']
    is_popup = context['is_popup']
    save_as = context['save_as']
    return {
        'onclick_attrib': (opts.get_ordered_objects() and change
                            and 'onclick="submitOrderForm();"' or ''),
        'show_delete_link': (not is_popup and context['has_delete_permission']
                              and (change or context['show_delete'])),
        'show_save_as_new': not is_popup and change and save_as,
        'show_save_and_add_another': context['has_add_permission'] and 
                            not is_popup and (not save_as or context['add']),
        'show_save_and_continue': not is_popup and context['has_change_permission'],
        'is_popup': is_popup,
        'show_save': True
    }
submit_row = register.inclusion_tag('admin/submit_line.html', takes_context=True)(submit_row)

from django.conf import settings
from django.contrib.auth.models import User, Group, Permission, AnonymousUser
from django.contrib.contenttypes.models import ContentType
from django.core.exceptions import ImproperlyConfigured
from django.test import TestCase


class BackendTest(TestCase):

    backend = 'django.contrib.auth.backends.ModelBackend'

    def setUp(self):
        self.curr_auth = settings.AUTHENTICATION_BACKENDS
        settings.AUTHENTICATION_BACKENDS = (self.backend,)
        User.objects.create_user('test', 'test@example.com', 'test')
        User.objects.create_superuser('test2', 'test2@example.com', 'test')

    def tearDown(self):
        settings.AUTHENTICATION_BACKENDS = self.curr_auth
        # The custom_perms test messes with ContentTypes, which will
        # be cached; flush the cache to ensure there are no side effects
        # Refs #14975, #14925
        ContentType.objects.clear_cache()

    def test_has_perm(self):
        user = User.objects.get(username='test')
        self.assertEqual(user.has_perm('auth.test'), False)
        user.is_staff = True
        user.save()
        self.assertEqual(user.has_perm('auth.test'), False)
        user.is_superuser = True
        user.save()
        self.assertEqual(user.has_perm('auth.test'), True)
        user.is_staff = False
        user.is_superuser = False
        user.save()
        self.assertEqual(user.has_perm('auth.test'), False)
        user.is_staff = True
        user.is_superuser = True
        user.is_active = False
        user.save()
        self.assertEqual(user.has_perm('auth.test'), False)

    def test_custom_perms(self):
        user = User.objects.get(username='test')
        content_type=ContentType.objects.get_for_model(Group)
        perm = Permission.objects.create(name='test', content_type=content_type, codename='test')
        user.user_permissions.add(perm)
        user.save()

        # reloading user to purge the _perm_cache
        user = User.objects.get(username='test')
        self.assertEqual(user.get_all_permissions() == set([u'auth.test']), True)
        self.assertEqual(user.get_group_permissions(), set([]))
        self.assertEqual(user.has_module_perms('Group'), False)
        self.assertEqual(user.has_module_perms('auth'), True)
        perm = Permission.objects.create(name='test2', content_type=content_type, codename='test2')
        user.user_permissions.add(perm)
        user.save()
        perm = Permission.objects.create(name='test3', content_type=content_type, codename='test3')
        user.user_permissions.add(perm)
        user.save()
        user = User.objects.get(username='test')
        self.assertEqual(user.get_all_permissions(), set([u'auth.test2', u'auth.test', u'auth.test3']))
        self.assertEqual(user.has_perm('test'), False)
        self.assertEqual(user.has_perm('auth.test'), True)
        self.assertEqual(user.has_perms(['auth.test2', 'auth.test3']), True)
        perm = Permission.objects.create(name='test_group', content_type=content_type, codename='test_group')
        group = Group.objects.create(name='test_group')
        group.permissions.add(perm)
        group.save()
        user.groups.add(group)
        user = User.objects.get(username='test')
        exp = set([u'auth.test2', u'auth.test', u'auth.test3', u'auth.test_group'])
        self.assertEqual(user.get_all_permissions(), exp)
        self.assertEqual(user.get_group_permissions(), set([u'auth.test_group']))
        self.assertEqual(user.has_perms(['auth.test3', 'auth.test_group']), True)

        user = AnonymousUser()
        self.assertEqual(user.has_perm('test'), False)
        self.assertEqual(user.has_perms(['auth.test2', 'auth.test3']), False)

    def test_has_no_object_perm(self):
        """Regressiontest for #12462"""
        user = User.objects.get(username='test')
        content_type=ContentType.objects.get_for_model(Group)
        perm = Permission.objects.create(name='test', content_type=content_type, codename='test')
        user.user_permissions.add(perm)
        user.save()

        self.assertEqual(user.has_perm('auth.test', 'object'), False)
        self.assertEqual(user.get_all_permissions('object'), set([]))
        self.assertEqual(user.has_perm('auth.test'), True)
        self.assertEqual(user.get_all_permissions(), set(['auth.test']))

    def test_get_all_superuser_permissions(self):
        "A superuser has all permissions. Refs #14795"
        user = User.objects.get(username='test2')
        self.assertEqual(len(user.get_all_permissions()), len(Permission.objects.all()))

class TestObj(object):
    pass


class SimpleRowlevelBackend(object):
    supports_object_permissions = True

    # This class also supports tests for anonymous user permissions,
    # via subclasses which just set the 'supports_anonymous_user' attribute.

    def has_perm(self, user, perm, obj=None):
        if not obj:
            return # We only support row level perms

        if isinstance(obj, TestObj):
            if user.username == 'test2':
                return True
            elif user.is_anonymous() and perm == 'anon':
                # not reached due to supports_anonymous_user = False
                return True
        return False

    def has_module_perms(self, user, app_label):
        return app_label == "app1"

    def get_all_permissions(self, user, obj=None):
        if not obj:
            return [] # We only support row level perms

        if not isinstance(obj, TestObj):
            return ['none']

        if user.is_anonymous():
            return ['anon']
        if user.username == 'test2':
            return ['simple', 'advanced']
        else:
            return ['simple']

    def get_group_permissions(self, user, obj=None):
        if not obj:
            return # We only support row level perms

        if not isinstance(obj, TestObj):
            return ['none']

        if 'test_group' in [group.name for group in user.groups.all()]:
            return ['group_perm']
        else:
            return ['none']


class RowlevelBackendTest(TestCase):
    """
    Tests for auth backend that supports object level permissions
    """
    backend = 'django.contrib.auth.tests.auth_backends.SimpleRowlevelBackend'

    def setUp(self):
        self.curr_auth = settings.AUTHENTICATION_BACKENDS
        settings.AUTHENTICATION_BACKENDS = tuple(self.curr_auth) + (self.backend,)
        self.user1 = User.objects.create_user('test', 'test@example.com', 'test')
        self.user2 = User.objects.create_user('test2', 'test2@example.com', 'test')
        self.user3 = User.objects.create_user('test3', 'test3@example.com', 'test')

    def tearDown(self):
        settings.AUTHENTICATION_BACKENDS = self.curr_auth
        # The get_group_permissions test messes with ContentTypes, which will
        # be cached; flush the cache to ensure there are no side effects
        # Refs #14975, #14925
        ContentType.objects.clear_cache()

    def test_has_perm(self):
        self.assertEqual(self.user1.has_perm('perm', TestObj()), False)
        self.assertEqual(self.user2.has_perm('perm', TestObj()), True)
        self.assertEqual(self.user2.has_perm('perm'), False)
        self.assertEqual(self.user2.has_perms(['simple', 'advanced'], TestObj()), True)
        self.assertEqual(self.user3.has_perm('perm', TestObj()), False)
        self.assertEqual(self.user3.has_perm('anon', TestObj()), False)
        self.assertEqual(self.user3.has_perms(['simple', 'advanced'], TestObj()), False)

    def test_get_all_permissions(self):
        self.assertEqual(self.user1.get_all_permissions(TestObj()), set(['simple']))
        self.assertEqual(self.user2.get_all_permissions(TestObj()), set(['simple', 'advanced']))
        self.assertEqual(self.user2.get_all_permissions(), set([]))

    def test_get_group_permissions(self):
        content_type=ContentType.objects.get_for_model(Group)
        group = Group.objects.create(name='test_group')
        self.user3.groups.add(group)
        self.assertEqual(self.user3.get_group_permissions(TestObj()), set(['group_perm']))


class AnonymousUserBackend(SimpleRowlevelBackend):

    supports_anonymous_user = True


class NoAnonymousUserBackend(SimpleRowlevelBackend):

    supports_anonymous_user = False


class AnonymousUserBackendTest(TestCase):
    """
    Tests for AnonymousUser delegating to backend if it has 'supports_anonymous_user' = True
    """

    backend = 'django.contrib.auth.tests.auth_backends.AnonymousUserBackend'

    def setUp(self):
        self.curr_auth = settings.AUTHENTICATION_BACKENDS
        settings.AUTHENTICATION_BACKENDS = (self.backend,)
        self.user1 = AnonymousUser()

    def tearDown(self):
        settings.AUTHENTICATION_BACKENDS = self.curr_auth

    def test_has_perm(self):
        self.assertEqual(self.user1.has_perm('perm', TestObj()), False)
        self.assertEqual(self.user1.has_perm('anon', TestObj()), True)

    def test_has_perms(self):
        self.assertEqual(self.user1.has_perms(['anon'], TestObj()), True)
        self.assertEqual(self.user1.has_perms(['anon', 'perm'], TestObj()), False)

    def test_has_module_perms(self):
        self.assertEqual(self.user1.has_module_perms("app1"), True)
        self.assertEqual(self.user1.has_module_perms("app2"), False)

    def test_get_all_permissions(self):
        self.assertEqual(self.user1.get_all_permissions(TestObj()), set(['anon']))


class NoAnonymousUserBackendTest(TestCase):
    """
    Tests that AnonymousUser does not delegate to backend if it has 'supports_anonymous_user' = False
    """
    backend = 'django.contrib.auth.tests.auth_backends.NoAnonymousUserBackend'

    def setUp(self):
        self.curr_auth = settings.AUTHENTICATION_BACKENDS
        settings.AUTHENTICATION_BACKENDS = tuple(self.curr_auth) + (self.backend,)
        self.user1 = AnonymousUser()

    def tearDown(self):
        settings.AUTHENTICATION_BACKENDS = self.curr_auth

    def test_has_perm(self):
        self.assertEqual(self.user1.has_perm('perm', TestObj()), False)
        self.assertEqual(self.user1.has_perm('anon', TestObj()), False)

    def test_has_perms(self):
        self.assertEqual(self.user1.has_perms(['anon'], TestObj()), False)

    def test_has_module_perms(self):
        self.assertEqual(self.user1.has_module_perms("app1"), False)
        self.assertEqual(self.user1.has_module_perms("app2"), False)

    def test_get_all_permissions(self):
        self.assertEqual(self.user1.get_all_permissions(TestObj()), set())

class NoBackendsTest(TestCase):
    """
    Tests that an appropriate error is raised if no auth backends are provided.
    """
    def setUp(self):
        self.old_AUTHENTICATION_BACKENDS = settings.AUTHENTICATION_BACKENDS
        settings.AUTHENTICATION_BACKENDS = []
        self.user = User.objects.create_user('test', 'test@example.com', 'test')

    def tearDown(self):
        settings.AUTHENTICATION_BACKENDS = self.old_AUTHENTICATION_BACKENDS

    def test_raises_exception(self):
        self.assertRaises(ImproperlyConfigured, self.user.has_perm, ('perm', TestObj(),))

from django import db
from django.conf import settings
from django.contrib.contenttypes.models import ContentType
from django.contrib.sites.models import Site
from django.contrib.contenttypes.views import shortcut
from django.core.exceptions import ObjectDoesNotExist
from django.http import HttpRequest
from django.test import TestCase


class ContentTypesTests(TestCase):

    def setUp(self):
        # First, let's make sure we're dealing with a blank slate (and that
        # DEBUG is on so that queries get logged)
        self.old_DEBUG = settings.DEBUG
        self.old_Site_meta_installed = Site._meta.installed
        settings.DEBUG = True
        ContentType.objects.clear_cache()
        db.reset_queries()

    def tearDown(self):
        settings.DEBUG = self.old_DEBUG
        Site._meta.installed = self.old_Site_meta_installed

    def test_lookup_cache(self):
        """
        Make sure that the content type cache (see ContentTypeManager)
        works correctly. Lookups for a particular content type -- by model or
        by ID -- should hit the database only on the first lookup.
        """

        # At this point, a lookup for a ContentType should hit the DB
        ContentType.objects.get_for_model(ContentType)
        self.assertEqual(1, len(db.connection.queries))

        # A second hit, though, won't hit the DB, nor will a lookup by ID
        ct = ContentType.objects.get_for_model(ContentType)
        self.assertEqual(1, len(db.connection.queries))
        ContentType.objects.get_for_id(ct.id)
        self.assertEqual(1, len(db.connection.queries))

        # Once we clear the cache, another lookup will again hit the DB
        ContentType.objects.clear_cache()
        ContentType.objects.get_for_model(ContentType)
        len(db.connection.queries)
        self.assertEqual(2, len(db.connection.queries))

    def test_shortcut_view(self):
        """
        Check that the shortcut view (used for the admin "view on site"
        functionality) returns a complete URL regardless of whether the sites
        framework is installed
        """

        request = HttpRequest()
        request.META = {
            "SERVER_NAME": "Example.com",
            "SERVER_PORT": "80",
        }
        from django.contrib.auth.models import User
        user_ct = ContentType.objects.get_for_model(User)
        obj = User.objects.create(username="john")

        if Site._meta.installed:
            response = shortcut(request, user_ct.id, obj.id)
            self.assertEqual("http://example.com/users/john/", response._headers.get("location")[1])

        Site._meta.installed = False
        response = shortcut(request, user_ct.id, obj.id)
        self.assertEqual("http://Example.com/users/john/", response._headers.get("location")[1])

from django.conf import settings
from django.contrib.gis.gdal import OGRException
from django.contrib.gis.geos import GEOSGeometry, GEOSException
from django.forms.widgets import Textarea
from django.template import loader, Context
from django.utils import translation

# Creating a template context that contains Django settings
# values needed by admin map templates.
geo_context = Context({'ADMIN_MEDIA_PREFIX' : settings.ADMIN_MEDIA_PREFIX,
                       'LANGUAGE_BIDI' : translation.get_language_bidi(),
                       })

class OpenLayersWidget(Textarea):
    """
    Renders an OpenLayers map using the WKT of the geometry.
    """
    def render(self, name, value, attrs=None):
        # Update the template parameters with any attributes passed in.
        if attrs: self.params.update(attrs)

        # Defaulting the WKT value to a blank string -- this
        # will be tested in the JavaScript and the appropriate
        # interface will be constructed.
        self.params['wkt'] = ''

        # If a string reaches here (via a validation error on another
        # field) then just reconstruct the Geometry.
        if isinstance(value, basestring):
            try:
                value = GEOSGeometry(value)
            except (GEOSException, ValueError):
                value = None

        if value and value.geom_type.upper() != self.geom_type:
            value = None

        # Constructing the dictionary of the map options.
        self.params['map_options'] = self.map_options()

        # Constructing the JavaScript module name using the name of
        # the GeometryField (passed in via the `attrs` keyword).
        # Use the 'name' attr for the field name (rather than 'field')
        self.params['name'] = name
        # note: we must switch out dashes for underscores since js
        # functions are created using the module variable
        js_safe_name = self.params['name'].replace('-','_')
        self.params['module'] = 'geodjango_%s' % js_safe_name

        if value:
            # Transforming the geometry to the projection used on the
            # OpenLayers map.
            srid = self.params['srid']
            if value.srid != srid:
                try:
                    ogr = value.ogr
                    ogr.transform(srid)
                    wkt = ogr.wkt
                except OGRException:
                    wkt = ''
            else:
                wkt = value.wkt

            # Setting the parameter WKT with that of the transformed
            # geometry.
            self.params['wkt'] = wkt

        return loader.render_to_string(self.template, self.params,
                                       context_instance=geo_context)

    def map_options(self):
        "Builds the map options hash for the OpenLayers template."

        # JavaScript construction utilities for the Bounds and Projection.
        def ol_bounds(extent):
            return 'new OpenLayers.Bounds(%s)' % str(extent)
        def ol_projection(srid):
            return 'new OpenLayers.Projection("EPSG:%s")' % srid

        # An array of the parameter name, the name of their OpenLayers
        # counterpart, and the type of variable they are.
        map_types = [('srid', 'projection', 'srid'),
                     ('display_srid', 'displayProjection', 'srid'),
                     ('units', 'units', str),
                     ('max_resolution', 'maxResolution', float),
                     ('max_extent', 'maxExtent', 'bounds'),
                     ('num_zoom', 'numZoomLevels', int),
                     ('max_zoom', 'maxZoomLevels', int),
                     ('min_zoom', 'minZoomLevel', int),
                     ]

        # Building the map options hash.
        map_options = {}
        for param_name, js_name, option_type in map_types:
            if self.params.get(param_name, False):
                if option_type == 'srid':
                    value = ol_projection(self.params[param_name])
                elif option_type == 'bounds':
                    value = ol_bounds(self.params[param_name])
                elif option_type in (float, int):
                    value = self.params[param_name]
                elif option_type in (str,):
                    value = '"%s"' % self.params[param_name]
                else:
                    raise TypeError
                map_options[js_name] = value
        return map_options

"""
A collection of utility routines and classes used by the spatial
backends.
"""

def gqn(val):
    """
    The geographic quote name function; used for quoting tables and
    geometries (they use single rather than the double quotes of the
    backend quotename function).
    """
    if isinstance(val, basestring):
        if isinstance(val, unicode): val = val.encode('ascii')
        return "'%s'" % val
    else:
        return str(val)

class SpatialOperation(object):
    """
    Base class for generating spatial SQL.
    """
    sql_template = '%(geo_col)s %(operator)s %(geometry)s'

    def __init__(self, function='', operator='', result='', **kwargs):
        self.function = function
        self.operator = operator
        self.result = result
        self.extra = kwargs

    def as_sql(self, geo_col, geometry='%s'):
        return self.sql_template % self.params(geo_col, geometry)

    def params(self, geo_col, geometry):
        params = {'function' : self.function,
                  'geo_col' : geo_col,
                  'geometry' : geometry,
                  'operator' : self.operator,
                  'result' : self.result,
                  }
        params.update(self.extra)
        return params

class SpatialFunction(SpatialOperation):
    """
    Base class for generating spatial SQL related to a function.
    """
    sql_template = '%(function)s(%(geo_col)s, %(geometry)s)'

    def __init__(self, func, result='', operator='', **kwargs):
        # Getting the function prefix.
        default = {'function' : func,
                   'operator' : operator,
                   'result' : result
                   }
        kwargs.update(default)
        super(SpatialFunction, self).__init__(**kwargs)

"""
 This module houses the ctypes function prototypes for OGR DataSource
 related data structures. OGR_Dr_*, OGR_DS_*, OGR_L_*, OGR_F_*, 
 OGR_Fld_* routines are relevant here.
"""
from ctypes import c_char_p, c_double, c_int, c_long, c_void_p, POINTER
from django.contrib.gis.gdal.envelope import OGREnvelope
from django.contrib.gis.gdal.libgdal import lgdal
from django.contrib.gis.gdal.prototypes.generation import \
    const_string_output, double_output, geom_output, int_output, \
    srs_output, void_output, voidptr_output

c_int_p = POINTER(c_int) # shortcut type

### Driver Routines ###
register_all = void_output(lgdal.OGRRegisterAll, [], errcheck=False)
cleanup_all = void_output(lgdal.OGRCleanupAll, [], errcheck=False)
get_driver = voidptr_output(lgdal.OGRGetDriver, [c_int])
get_driver_by_name = voidptr_output(lgdal.OGRGetDriverByName, [c_char_p])
get_driver_count = int_output(lgdal.OGRGetDriverCount, [])
get_driver_name = const_string_output(lgdal.OGR_Dr_GetName, [c_void_p])

### DataSource ###
open_ds = voidptr_output(lgdal.OGROpen, [c_char_p, c_int, POINTER(c_void_p)])
destroy_ds = void_output(lgdal.OGR_DS_Destroy, [c_void_p], errcheck=False)
release_ds = void_output(lgdal.OGRReleaseDataSource, [c_void_p])
get_ds_name = const_string_output(lgdal.OGR_DS_GetName, [c_void_p])
get_layer = voidptr_output(lgdal.OGR_DS_GetLayer, [c_void_p, c_int])
get_layer_by_name = voidptr_output(lgdal.OGR_DS_GetLayerByName, [c_void_p, c_char_p])
get_layer_count = int_output(lgdal.OGR_DS_GetLayerCount, [c_void_p])

### Layer Routines ###
get_extent = void_output(lgdal.OGR_L_GetExtent, [c_void_p, POINTER(OGREnvelope), c_int])
get_feature = voidptr_output(lgdal.OGR_L_GetFeature, [c_void_p, c_long])
get_feature_count = int_output(lgdal.OGR_L_GetFeatureCount, [c_void_p, c_int])
get_layer_defn = voidptr_output(lgdal.OGR_L_GetLayerDefn, [c_void_p])
get_layer_srs = srs_output(lgdal.OGR_L_GetSpatialRef, [c_void_p])
get_next_feature = voidptr_output(lgdal.OGR_L_GetNextFeature, [c_void_p])
reset_reading = void_output(lgdal.OGR_L_ResetReading, [c_void_p], errcheck=False)
test_capability = int_output(lgdal.OGR_L_TestCapability, [c_void_p, c_char_p])
get_spatial_filter = geom_output(lgdal.OGR_L_GetSpatialFilter, [c_void_p])
set_spatial_filter = void_output(lgdal.OGR_L_SetSpatialFilter, [c_void_p, c_void_p], errcheck=False)
set_spatial_filter_rect = void_output(lgdal.OGR_L_SetSpatialFilterRect, [c_void_p, c_double, c_double, c_double, c_double], errcheck=False)

### Feature Definition Routines ###
get_fd_geom_type = int_output(lgdal.OGR_FD_GetGeomType, [c_void_p])
get_fd_name = const_string_output(lgdal.OGR_FD_GetName, [c_void_p])
get_feat_name = const_string_output(lgdal.OGR_FD_GetName, [c_void_p])
get_field_count = int_output(lgdal.OGR_FD_GetFieldCount, [c_void_p])
get_field_defn = voidptr_output(lgdal.OGR_FD_GetFieldDefn, [c_void_p, c_int])

### Feature Routines ###
clone_feature = voidptr_output(lgdal.OGR_F_Clone, [c_void_p])
destroy_feature = void_output(lgdal.OGR_F_Destroy, [c_void_p], errcheck=False)
feature_equal = int_output(lgdal.OGR_F_Equal, [c_void_p, c_void_p])
get_feat_geom_ref = geom_output(lgdal.OGR_F_GetGeometryRef, [c_void_p])
get_feat_field_count = int_output(lgdal.OGR_F_GetFieldCount, [c_void_p])
get_feat_field_defn = voidptr_output(lgdal.OGR_F_GetFieldDefnRef, [c_void_p, c_int])
get_fid = int_output(lgdal.OGR_F_GetFID, [c_void_p])
get_field_as_datetime = int_output(lgdal.OGR_F_GetFieldAsDateTime, [c_void_p, c_int, c_int_p, c_int_p, c_int_p, c_int_p, c_int_p, c_int_p])
get_field_as_double = double_output(lgdal.OGR_F_GetFieldAsDouble, [c_void_p, c_int])
get_field_as_integer = int_output(lgdal.OGR_F_GetFieldAsInteger, [c_void_p, c_int])
get_field_as_string = const_string_output(lgdal.OGR_F_GetFieldAsString, [c_void_p, c_int])
get_field_index = int_output(lgdal.OGR_F_GetFieldIndex, [c_void_p, c_char_p])

### Field Routines ###
get_field_name = const_string_output(lgdal.OGR_Fld_GetNameRef, [c_void_p])
get_field_precision = int_output(lgdal.OGR_Fld_GetPrecision, [c_void_p])
get_field_type = int_output(lgdal.OGR_Fld_GetType, [c_void_p])
get_field_type_name = const_string_output(lgdal.OGR_GetFieldTypeName, [c_int])
get_field_width = int_output(lgdal.OGR_Fld_GetWidth, [c_void_p])

from ctypes import c_uint, byref
from django.contrib.gis.geos.error import GEOSIndexError
from django.contrib.gis.geos.geometry import GEOSGeometry
from django.contrib.gis.geos.libgeos import get_pointer_arr, GEOM_PTR
from django.contrib.gis.geos.linestring import LinearRing
from django.contrib.gis.geos import prototypes as capi

class Polygon(GEOSGeometry):
    _minlength = 1

    def __init__(self, *args, **kwargs):
        """
        Initializes on an exterior ring and a sequence of holes (both
        instances may be either LinearRing instances, or a tuple/list
        that may be constructed into a LinearRing).

        Examples of initialization, where shell, hole1, and hole2 are
        valid LinearRing geometries:
        >>> poly = Polygon(shell, hole1, hole2)
        >>> poly = Polygon(shell, (hole1, hole2))

        Example where a tuple parameters are used:
        >>> poly = Polygon(((0, 0), (0, 10), (10, 10), (0, 10), (0, 0)),
                           ((4, 4), (4, 6), (6, 6), (6, 4), (4, 4)))
        """
        if not args:
            raise TypeError('Must provide at least one LinearRing, or a tuple, to initialize a Polygon.')

        # Getting the ext_ring and init_holes parameters from the argument list
        ext_ring = args[0]
        init_holes = args[1:]
        n_holes = len(init_holes)

        # If initialized as Polygon(shell, (LinearRing, LinearRing)) [for backward-compatibility]
        if n_holes == 1 and isinstance(init_holes[0], (tuple, list)):
            if len(init_holes[0]) == 0:
                init_holes  = ()
                n_holes     = 0
            elif isinstance(init_holes[0][0], LinearRing):
                init_holes  = init_holes[0]
                n_holes     = len(init_holes)

        polygon = self._create_polygon(n_holes + 1, (ext_ring,) + init_holes)
        super(Polygon, self).__init__(polygon, **kwargs)

    def __iter__(self):
        "Iterates over each ring in the polygon."
        for i in xrange(len(self)):
            yield self[i]

    def __len__(self):
        "Returns the number of rings in this Polygon."
        return self.num_interior_rings + 1

    @classmethod
    def from_bbox(cls, bbox):
        "Constructs a Polygon from a bounding box (4-tuple)."
        x0, y0, x1, y1 = bbox
        return GEOSGeometry( 'POLYGON((%s %s, %s %s, %s %s, %s %s, %s %s))' %  (
                x0, y0, x0, y1, x1, y1, x1, y0, x0, y0) )

    ### These routines are needed for list-like operation w/ListMixin ###
    def _create_polygon(self, length, items):
        # Instantiate LinearRing objects if necessary, but don't clone them yet
        # _construct_ring will throw a TypeError if a parameter isn't a valid ring
        # If we cloned the pointers here, we wouldn't be able to clean up
        # in case of error.
        rings = []
        for r in items:
            if isinstance(r, GEOM_PTR):
                rings.append(r)
            else:
                rings.append(self._construct_ring(r))

        shell = self._clone(rings.pop(0))

        n_holes = length - 1
        if n_holes:
            holes = get_pointer_arr(n_holes)
            for i, r in enumerate(rings):
                holes[i] = self._clone(r)
                holes_param = byref(holes)
        else:
            holes_param = None

        return capi.create_polygon(shell, holes_param, c_uint(n_holes))

    def _clone(self, g):
        if isinstance(g, GEOM_PTR):
            return capi.geom_clone(g)
        else:
            return capi.geom_clone(g.ptr)

    def _construct_ring(self, param, msg='Parameter must be a sequence of LinearRings or objects that can initialize to LinearRings'):
        "Helper routine for trying to construct a ring from the given parameter."
        if isinstance(param, LinearRing): return param
        try:
            ring = LinearRing(param)
            return ring
        except TypeError:
            raise TypeError(msg)

    def _set_list(self, length, items):
        # Getting the current pointer, replacing with the newly constructed
        # geometry, and destroying the old geometry.
        prev_ptr = self.ptr
        srid = self.srid
        self.ptr = self._create_polygon(length, items)
        if srid: self.srid = srid
        capi.destroy_geom(prev_ptr)

    def _get_single_internal(self, index):
        """
        Returns the ring at the specified index.  The first index, 0, will
        always return the exterior ring.  Indices > 0 will return the
        interior ring at the given index (e.g., poly[1] and poly[2] would
        return the first and second interior ring, respectively).

        CAREFUL: Internal/External are not the same as Interior/Exterior!
        _get_single_internal returns a pointer from the existing geometries for use
        internally by the object's methods.  _get_single_external returns a clone
        of the same geometry for use by external code.
        """
        if index == 0:
            return capi.get_extring(self.ptr)
        else:
            # Getting the interior ring, have to subtract 1 from the index.
            return capi.get_intring(self.ptr, index-1)

    def _get_single_external(self, index):
        return GEOSGeometry(capi.geom_clone(self._get_single_internal(index)), srid=self.srid)

    _set_single = GEOSGeometry._set_single_rebuild
    _assign_extended_slice = GEOSGeometry._assign_extended_slice_rebuild

    #### Polygon Properties ####
    @property
    def num_interior_rings(self):
        "Returns the number of interior rings."
        # Getting the number of rings
        return capi.get_nrings(self.ptr)

    def _get_ext_ring(self):
        "Gets the exterior ring of the Polygon."
        return self[0]

    def _set_ext_ring(self, ring):
        "Sets the exterior ring of the Polygon."
        self[0] = ring

    # Properties for the exterior ring/shell.
    exterior_ring = property(_get_ext_ring, _set_ext_ring)
    shell = exterior_ring

    @property
    def tuple(self):
        "Gets the tuple for each ring in this Polygon."
        return tuple([self[i].tuple for i in xrange(len(self))])
    coords = tuple

    @property
    def kml(self):
        "Returns the KML representation of this Polygon."
        inner_kml = ''.join(["<innerBoundaryIs>%s</innerBoundaryIs>" % self[i+1].kml
                             for i in xrange(self.num_interior_rings)])
        return "<Polygon><outerBoundaryIs>%s</outerBoundaryIs>%s</Polygon>" % (self[0].kml, inner_kml)

# Geo-enabled Sitemap classes.
from django.contrib.gis.sitemaps.georss import GeoRSSSitemap
from django.contrib.gis.sitemaps.kml import KMLSitemap, KMZSitemap


"""
 This module contains useful utilities for GeoDjango.
"""
# Importing the utilities that depend on GDAL, if available.
from django.contrib.gis.gdal import HAS_GDAL
if HAS_GDAL:
    from django.contrib.gis.utils.ogrinfo import ogrinfo, sample
    from django.contrib.gis.utils.ogrinspect import mapping, ogrinspect
    from django.contrib.gis.utils.srs import add_postgis_srs, add_srs_entry
    try:
        # LayerMapping requires DJANGO_SETTINGS_MODULE to be set, 
        # so this needs to be in try/except.
        from django.contrib.gis.utils.layermapping import LayerMapping, LayerMapError
    except:
        pass
    
# Attempting to import the GeoIP class.
try:
    from django.contrib.gis.utils.geoip import GeoIP, GeoIPException
    HAS_GEOIP = True
except:
    HAS_GEOIP = False

from django.contrib.gis.utils.wkt import precision_wkt


"""
Polish-specific form helpers
"""

import re

from django.forms import ValidationError
from django.forms.fields import Select, RegexField
from django.utils.translation import ugettext_lazy as _
from django.core.validators import EMPTY_VALUES

class PLProvinceSelect(Select):
    """
    A select widget with list of Polish administrative provinces as choices.
    """
    def __init__(self, attrs=None):
        from pl_voivodeships import VOIVODESHIP_CHOICES
        super(PLProvinceSelect, self).__init__(attrs, choices=VOIVODESHIP_CHOICES)

class PLCountySelect(Select):
    """
    A select widget with list of Polish administrative units as choices.
    """
    def __init__(self, attrs=None):
        from pl_administrativeunits import ADMINISTRATIVE_UNIT_CHOICES
        super(PLCountySelect, self).__init__(attrs, choices=ADMINISTRATIVE_UNIT_CHOICES)

class PLPESELField(RegexField):
    """
    A form field that validates as Polish Identification Number (PESEL).

    Checks the following rules:
        * the length consist of 11 digits
        * has a valid checksum

    The algorithm is documented at http://en.wikipedia.org/wiki/PESEL.
    """
    default_error_messages = {
        'invalid': _(u'National Identification Number consists of 11 digits.'),
        'checksum': _(u'Wrong checksum for the National Identification Number.'),
    }

    def __init__(self, *args, **kwargs):
        super(PLPESELField, self).__init__(r'^\d{11}$',
            max_length=None, min_length=None, *args, **kwargs)

    def clean(self,value):
        super(PLPESELField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        if not self.has_valid_checksum(value):
            raise ValidationError(self.error_messages['checksum'])
        return u'%s' % value

    def has_valid_checksum(self, number):
        """
        Calculates a checksum with the provided algorithm.
        """
        multiple_table = (1, 3, 7, 9, 1, 3, 7, 9, 1, 3, 1)
        result = 0
        for i in range(len(number)):
            result += int(number[i]) * multiple_table[i]
        return result % 10 == 0

class PLNIPField(RegexField):
    """
    A form field that validates as Polish Tax Number (NIP).
    Valid forms are: XXX-XXX-YY-YY or XX-XX-YYY-YYY.

    Checksum algorithm based on documentation at
    http://wipos.p.lodz.pl/zylla/ut/nip-rego.html
    """
    default_error_messages = {
        'invalid': _(u'Enter a tax number field (NIP) in the format XXX-XXX-XX-XX or XX-XX-XXX-XXX.'),
        'checksum': _(u'Wrong checksum for the Tax Number (NIP).'),
    }

    def __init__(self, *args, **kwargs):
        super(PLNIPField, self).__init__(r'^\d{3}-\d{3}-\d{2}-\d{2}$|^\d{2}-\d{2}-\d{3}-\d{3}$',
            max_length=None, min_length=None, *args, **kwargs)

    def clean(self,value):
        super(PLNIPField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        value = re.sub("[-]", "", value)
        if not self.has_valid_checksum(value):
            raise ValidationError(self.error_messages['checksum'])
        return u'%s' % value

    def has_valid_checksum(self, number):
        """
        Calculates a checksum with the provided algorithm.
        """
        multiple_table = (6, 5, 7, 2, 3, 4, 5, 6, 7)
        result = 0
        for i in range(len(number)-1):
            result += int(number[i]) * multiple_table[i]

        result %= 11
        if result == int(number[-1]):
            return True
        else:
            return False

class PLREGONField(RegexField):
    """
    A form field that validates its input is a REGON number.

    Valid regon number consists of 9 or 14 digits.
    See http://www.stat.gov.pl/bip/regon_ENG_HTML.htm for more information.
    """
    default_error_messages = {
        'invalid': _(u'National Business Register Number (REGON) consists of 9 or 14 digits.'),
        'checksum': _(u'Wrong checksum for the National Business Register Number (REGON).'),
    }

    def __init__(self, *args, **kwargs):
        super(PLREGONField, self).__init__(r'^\d{9,14}$',
            max_length=None, min_length=None, *args, **kwargs)

    def clean(self,value):
        super(PLREGONField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        if not self.has_valid_checksum(value):
            raise ValidationError(self.error_messages['checksum'])
        return u'%s' % value

    def has_valid_checksum(self, number):
        """
        Calculates a checksum with the provided algorithm.
        """
        weights = (
            (8, 9, 2, 3, 4, 5, 6, 7, -1),
            (2, 4, 8, 5, 0, 9, 7, 3, 6, 1, 2, 4, 8, -1),
            (8, 9, 2, 3, 4, 5, 6, 7, -1, 0, 0, 0, 0, 0),
        )

        weights = [table for table in weights if len(table) == len(number)]

        for table in weights:
            checksum = sum([int(n) * w for n, w in zip(number, table)])
            if checksum % 11 % 10:
                return False

        return bool(weights)

class PLPostalCodeField(RegexField):
    """
    A form field that validates as Polish postal code.
    Valid code is XX-XXX where X is digit.
    """
    default_error_messages = {
        'invalid': _(u'Enter a postal code in the format XX-XXX.'),
    }

    def __init__(self, *args, **kwargs):
        super(PLPostalCodeField, self).__init__(r'^\d{2}-\d{3}$',
            max_length=None, min_length=None, *args, **kwargs)

from django.conf import settings
from django.contrib.messages.storage import default_storage


class MessageMiddleware(object):
    """
    Middleware that handles temporary messages.
    """

    def process_request(self, request):
        request._messages = default_storage(request)

    def process_response(self, request, response):
        """
        Updates the storage backend (i.e., saves the messages).

        If not all messages could not be stored and ``DEBUG`` is ``True``, a
        ``ValueError`` is raised.
        """
        # A higher middleware layer may return a request which does not contain
        # messages storage, so make no assumption that it will be there.
        if hasattr(request, '_messages'):
            unstored_messages = request._messages.update(response)
            if unstored_messages and settings.DEBUG:
                raise ValueError('Not all temporary messages could be stored.')
        return response

r"""

>>> from django.conf import settings
>>> from django.contrib.sessions.backends.db import SessionStore as DatabaseSession
>>> from django.contrib.sessions.backends.cache import SessionStore as CacheSession
>>> from django.contrib.sessions.backends.cached_db import SessionStore as CacheDBSession
>>> from django.contrib.sessions.backends.file import SessionStore as FileSession
>>> from django.contrib.sessions.backends.base import SessionBase
>>> from django.contrib.sessions.models import Session

>>> db_session = DatabaseSession()
>>> db_session.modified
False
>>> db_session.get('cat')
>>> db_session['cat'] = "dog"
>>> db_session.modified
True
>>> db_session.pop('cat')
'dog'
>>> db_session.pop('some key', 'does not exist')
'does not exist'
>>> db_session.save()
>>> db_session.exists(db_session.session_key)
True
>>> db_session.delete(db_session.session_key)
>>> db_session.exists(db_session.session_key)
False

>>> db_session['foo'] = 'bar'
>>> db_session.save()
>>> db_session.exists(db_session.session_key)
True
>>> prev_key = db_session.session_key
>>> db_session.flush()
>>> db_session.exists(prev_key)
False
>>> db_session.session_key == prev_key
False
>>> db_session.modified, db_session.accessed
(True, True)
>>> db_session['a'], db_session['b'] = 'c', 'd'
>>> db_session.save()
>>> prev_key = db_session.session_key
>>> prev_data = db_session.items()
>>> db_session.cycle_key()
>>> db_session.session_key == prev_key
False
>>> db_session.items() == prev_data
True

# Submitting an invalid session key (either by guessing, or if the db has
# removed the key) results in a new key being generated.
>>> Session.objects.filter(pk=db_session.session_key).delete()
>>> db_session = DatabaseSession(db_session.session_key)
>>> db_session.save()
>>> DatabaseSession('1').get('cat')

#
# Cached DB session tests
#

>>> cdb_session = CacheDBSession()
>>> cdb_session.modified
False
>>> cdb_session['cat'] = "dog"
>>> cdb_session.modified
True
>>> cdb_session.pop('cat')
'dog'
>>> cdb_session.pop('some key', 'does not exist')
'does not exist'
>>> cdb_session.save()
>>> cdb_session.exists(cdb_session.session_key)
True
>>> cdb_session.delete(cdb_session.session_key)
>>> cdb_session.exists(cdb_session.session_key)
False

#
# File session tests.
#

# Do file session tests in an isolated directory, and kill it after we're done.
>>> original_session_file_path = settings.SESSION_FILE_PATH
>>> import tempfile
>>> temp_session_store = settings.SESSION_FILE_PATH = tempfile.mkdtemp()

>>> file_session = FileSession()
>>> file_session.modified
False
>>> file_session['cat'] = "dog"
>>> file_session.modified
True
>>> file_session.pop('cat')
'dog'
>>> file_session.pop('some key', 'does not exist')
'does not exist'
>>> file_session.save()
>>> file_session.exists(file_session.session_key)
True
>>> file_session.delete(file_session.session_key)
>>> file_session.exists(file_session.session_key)
False
>>> FileSession('1').get('cat')

>>> file_session['foo'] = 'bar'
>>> file_session.save()
>>> file_session.exists(file_session.session_key)
True
>>> prev_key = file_session.session_key
>>> file_session.flush()
>>> file_session.exists(prev_key)
False
>>> file_session.session_key == prev_key
False
>>> file_session.modified, file_session.accessed
(True, True)
>>> file_session['a'], file_session['b'] = 'c', 'd'
>>> file_session.save()
>>> prev_key = file_session.session_key
>>> prev_data = file_session.items()
>>> file_session.cycle_key()
>>> file_session.session_key == prev_key
False
>>> file_session.items() == prev_data
True

>>> Session.objects.filter(pk=file_session.session_key).delete()
>>> file_session = FileSession(file_session.session_key)
>>> file_session.save()

# Ensure we don't allow directory traversal
>>> FileSession("a/b/c").load()
Traceback (innermost last):
    ...
SuspiciousOperation: Invalid characters in session key

>>> FileSession("a\\b\\c").load()
Traceback (innermost last):
    ...
SuspiciousOperation: Invalid characters in session key

# Make sure the file backend checks for a good storage dir
>>> settings.SESSION_FILE_PATH = "/if/this/directory/exists/you/have/a/weird/computer"
>>> FileSession()
Traceback (innermost last):
    ...
ImproperlyConfigured: The session storage path '/if/this/directory/exists/you/have/a/weird/computer' doesn't exist. Please set your SESSION_FILE_PATH setting to an existing directory in which Django can store session data.

# Clean up after the file tests
>>> settings.SESSION_FILE_PATH = original_session_file_path
>>> import shutil
>>> shutil.rmtree(temp_session_store)

#
# Cache-based tests
# NB: be careful to delete any sessions created; stale sessions fill up the
# /tmp and eventually overwhelm it after lots of runs (think buildbots)
#

>>> cache_session = CacheSession()
>>> cache_session.modified
False
>>> cache_session['cat'] = "dog"
>>> cache_session.modified
True
>>> cache_session.pop('cat')
'dog'
>>> cache_session.pop('some key', 'does not exist')
'does not exist'
>>> cache_session.save()
>>> cache_session.delete(cache_session.session_key)
>>> cache_session.exists(cache_session.session_key)
False
>>> cache_session['foo'] = 'bar'
>>> cache_session.save()
>>> cache_session.exists(cache_session.session_key)
True
>>> prev_key = cache_session.session_key
>>> cache_session.flush()
>>> cache_session.exists(prev_key)
False
>>> cache_session.session_key == prev_key
False
>>> cache_session.modified, cache_session.accessed
(True, True)
>>> cache_session['a'], cache_session['b'] = 'c', 'd'
>>> cache_session.save()
>>> prev_key = cache_session.session_key
>>> prev_data = cache_session.items()
>>> cache_session.cycle_key()
>>> cache_session.session_key == prev_key
False
>>> cache_session.items() == prev_data
True
>>> cache_session = CacheSession()
>>> cache_session.save()
>>> key = cache_session.session_key
>>> cache_session.exists(key)
True

>>> Session.objects.filter(pk=cache_session.session_key).delete()
>>> cache_session = CacheSession(cache_session.session_key)
>>> cache_session.save()
>>> cache_session.delete(cache_session.session_key)

>>> s = SessionBase()
>>> s._session['some key'] = 'exists' # Pre-populate the session with some data
>>> s.accessed = False   # Reset to pretend this wasn't accessed previously

>>> s.accessed, s.modified
(False, False)

>>> s.pop('non existant key', 'does not exist')
'does not exist'
>>> s.accessed, s.modified
(True, False)

>>> s.setdefault('foo', 'bar')
'bar'
>>> s.setdefault('foo', 'baz')
'bar'

>>> s.accessed = False  # Reset the accessed flag

>>> s.pop('some key')
'exists'
>>> s.accessed, s.modified
(True, True)

>>> s.pop('some key', 'does not exist')
'does not exist'


>>> s.get('update key', None)

# test .update()
>>> s.modified = s.accessed = False   # Reset to pretend this wasn't accessed previously
>>> s.update({'update key':1})
>>> s.accessed, s.modified
(True, True)
>>> s.get('update key', None)
1

# test .has_key()
>>> s.modified = s.accessed = False   # Reset to pretend this wasn't accessed previously
>>> s.has_key('update key')
True
>>> s.accessed, s.modified
(True, False)

# test .values()
>>> s = SessionBase()
>>> s.values()
[]
>>> s.accessed
True
>>> s['x'] = 1
>>> s.values()
[1]

# test .iterkeys()
>>> s.accessed = False
>>> i = s.iterkeys()
>>> hasattr(i,'__iter__')
True
>>> s.accessed
True
>>> list(i)
['x']

# test .itervalues()
>>> s.accessed = False
>>> i = s.itervalues()
>>> hasattr(i,'__iter__')
True
>>> s.accessed
True
>>> list(i)
[1]

# test .iteritems()
>>> s.accessed = False
>>> i = s.iteritems()
>>> hasattr(i,'__iter__')
True
>>> s.accessed
True
>>> list(i)
[('x', 1)]

# test .clear()
>>> s.modified = s.accessed = False
>>> s.items()
[('x', 1)]
>>> s.clear()
>>> s.items()
[]
>>> s.accessed, s.modified
(True, True)

#########################
# Custom session expiry #
#########################

>>> from django.conf import settings
>>> from datetime import datetime, timedelta

>>> td10 = timedelta(seconds=10)

# A normal session has a max age equal to settings
>>> s.get_expiry_age() == settings.SESSION_COOKIE_AGE
True

# So does a custom session with an idle expiration time of 0 (but it'll expire
# at browser close)
>>> s.set_expiry(0)
>>> s.get_expiry_age() == settings.SESSION_COOKIE_AGE
True

# Custom session idle expiration time
>>> s.set_expiry(10)
>>> delta = s.get_expiry_date() - datetime.now()
>>> delta.seconds in (9, 10)
True
>>> age = s.get_expiry_age()
>>> age in (9, 10)
True

# Custom session fixed expiry date (timedelta)
>>> s.set_expiry(td10)
>>> delta = s.get_expiry_date() - datetime.now()
>>> delta.seconds in (9, 10)
True
>>> age = s.get_expiry_age()
>>> age in (9, 10)
True

# Custom session fixed expiry date (fixed datetime)
>>> s.set_expiry(datetime.now() + td10)
>>> delta = s.get_expiry_date() - datetime.now()
>>> delta.seconds in (9, 10)
True
>>> age = s.get_expiry_age()
>>> age in (9, 10)
True

# Set back to default session age
>>> s.set_expiry(None)
>>> s.get_expiry_age() == settings.SESSION_COOKIE_AGE
True

# Allow to set back to default session age even if no alternate has been set
>>> s.set_expiry(None)


# We're changing the setting then reverting back to the original setting at the
# end of these tests.
>>> original_expire_at_browser_close = settings.SESSION_EXPIRE_AT_BROWSER_CLOSE
>>> settings.SESSION_EXPIRE_AT_BROWSER_CLOSE = False

# Custom session age
>>> s.set_expiry(10)
>>> s.get_expire_at_browser_close()
False

# Custom expire-at-browser-close
>>> s.set_expiry(0)
>>> s.get_expire_at_browser_close()
True

# Default session age
>>> s.set_expiry(None)
>>> s.get_expire_at_browser_close()
False

>>> settings.SESSION_EXPIRE_AT_BROWSER_CLOSE = True

# Custom session age
>>> s.set_expiry(10)
>>> s.get_expire_at_browser_close()
False

# Custom expire-at-browser-close
>>> s.set_expiry(0)
>>> s.get_expire_at_browser_close()
True

# Default session age
>>> s.set_expiry(None)
>>> s.get_expire_at_browser_close()
True

>>> settings.SESSION_EXPIRE_AT_BROWSER_CLOSE = original_expire_at_browser_close
"""

if __name__ == '__main__':
    import doctest
    doctest.testmod()

import os
try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

from django.utils.encoding import smart_str, smart_unicode
from django.core.files.utils import FileProxyMixin

class File(FileProxyMixin):
    DEFAULT_CHUNK_SIZE = 64 * 2**10

    def __init__(self, file, name=None):
        self.file = file
        if name is None:
            name = getattr(file, 'name', None)
        self.name = name
        self.mode = getattr(file, 'mode', None)

    def __str__(self):
        return smart_str(self.name or '')

    def __unicode__(self):
        return smart_unicode(self.name or u'')

    def __repr__(self):
        return "<%s: %s>" % (self.__class__.__name__, self or "None")

    def __nonzero__(self):
        return bool(self.name)

    def __len__(self):
        return self.size

    def _get_size(self):
        if not hasattr(self, '_size'):
            if hasattr(self.file, 'size'):
                self._size = self.file.size
            elif os.path.exists(self.file.name):
                self._size = os.path.getsize(self.file.name)
            else:
                raise AttributeError("Unable to determine the file's size.")
        return self._size

    def _set_size(self, size):
        self._size = size

    size = property(_get_size, _set_size)

    def _get_closed(self):
        return not self.file or self.file.closed
    closed = property(_get_closed)

    def chunks(self, chunk_size=None):
        """
        Read the file and yield chucks of ``chunk_size`` bytes (defaults to
        ``UploadedFile.DEFAULT_CHUNK_SIZE``).
        """
        if not chunk_size:
            chunk_size = self.DEFAULT_CHUNK_SIZE

        if hasattr(self, 'seek'):
            self.seek(0)
        # Assume the pointer is at zero...
        counter = self.size

        while counter > 0:
            yield self.read(chunk_size)
            counter -= chunk_size

    def multiple_chunks(self, chunk_size=None):
        """
        Returns ``True`` if you can expect multiple chunks.

        NB: If a particular file representation is in memory, subclasses should
        always return ``False`` -- there's no good reason to read from memory in
        chunks.
        """
        if not chunk_size:
            chunk_size = self.DEFAULT_CHUNK_SIZE
        return self.size > chunk_size

    def __iter__(self):
        # Iterate over this file-like object by newlines
        buffer_ = None
        for chunk in self.chunks():
            chunk_buffer = StringIO(chunk)

            for line in chunk_buffer:
                if buffer_:
                    line = buffer_ + line
                    buffer_ = None

                # If this is the end of a line, yield
                # otherwise, wait for the next round
                if line[-1] in ('\n', '\r'):
                    yield line
                else:
                    buffer_ = line

        if buffer_ is not None:
            yield buffer_

    def open(self, mode=None):
        if not self.closed:
            self.seek(0)
        elif self.name and os.path.exists(self.name):
            self.file = open(self.name, mode or self.mode)
        else:
            raise ValueError("The file cannot be reopened.")

    def close(self):
        self.file.close()

class ContentFile(File):
    """
    A File-like object that takes just raw content, rather than an actual file.
    """
    def __init__(self, content):
        content = content or ''
        super(ContentFile, self).__init__(StringIO(content))
        self.size = len(content)

    def __str__(self):
        return 'Raw content'

    def __nonzero__(self):
        return True

    def open(self, mode=None):
        self.seek(0)

    def close(self):
        pass

import keyword
from optparse import make_option

from django.core.management.base import NoArgsCommand, CommandError
from django.db import connections, DEFAULT_DB_ALIAS

class Command(NoArgsCommand):
    help = "Introspects the database tables in the given database and outputs a Django model module."

    option_list = NoArgsCommand.option_list + (
        make_option('--database', action='store', dest='database',
            default=DEFAULT_DB_ALIAS, help='Nominates a database to '
                'introspect.  Defaults to using the "default" database.'),
    )

    requires_model_validation = False

    db_module = 'django.db'

    def handle_noargs(self, **options):
        try:
            for line in self.handle_inspection(options):
                self.stdout.write("%s\n" % line)
        except NotImplementedError:
            raise CommandError("Database inspection isn't supported for the currently selected database backend.")

    def handle_inspection(self, options):
        connection = connections[options.get('database', DEFAULT_DB_ALIAS)]

        table2model = lambda table_name: table_name.title().replace('_', '').replace(' ', '').replace('-', '')

        cursor = connection.cursor()
        yield "# This is an auto-generated Django model module."
        yield "# You'll have to do the following manually to clean this up:"
        yield "#     * Rearrange models' order"
        yield "#     * Make sure each model has one field with primary_key=True"
        yield "# Feel free to rename the models, but don't rename db_table values or field names."
        yield "#"
        yield "# Also note: You'll have to insert the output of 'django-admin.py sqlcustom [appname]'"
        yield "# into your database."
        yield ''
        yield 'from %s import models' % self.db_module
        yield ''
        for table_name in connection.introspection.get_table_list(cursor):
            yield 'class %s(models.Model):' % table2model(table_name)
            try:
                relations = connection.introspection.get_relations(cursor, table_name)
            except NotImplementedError:
                relations = {}
            try:
                indexes = connection.introspection.get_indexes(cursor, table_name)
            except NotImplementedError:
                indexes = {}
            for i, row in enumerate(connection.introspection.get_table_description(cursor, table_name)):
                column_name = row[0]
                att_name = column_name.lower()
                comment_notes = [] # Holds Field notes, to be displayed in a Python comment.
                extra_params = {}  # Holds Field parameters such as 'db_column'.

                # If the column name can't be used verbatim as a Python
                # attribute, set the "db_column" for this Field.
                if ' ' in att_name or '-' in att_name or keyword.iskeyword(att_name) or column_name != att_name:
                    extra_params['db_column'] = column_name

                # Modify the field name to make it Python-compatible.
                if ' ' in att_name:
                    att_name = att_name.replace(' ', '_')
                    comment_notes.append('Field renamed to remove spaces.')
                    
                if '-' in att_name:
                    att_name = att_name.replace('-', '_')
                    comment_notes.append('Field renamed to remove dashes.')
                    
                if column_name != att_name:
                    comment_notes.append('Field name made lowercase.')

                if i in relations:
                    rel_to = relations[i][1] == table_name and "'self'" or table2model(relations[i][1])
                    field_type = 'ForeignKey(%s' % rel_to
                    if att_name.endswith('_id'):
                        att_name = att_name[:-3]
                    else:
                        extra_params['db_column'] = column_name
                else:
                    # Calling `get_field_type` to get the field type string and any
                    # additional paramters and notes.
                    field_type, field_params, field_notes = self.get_field_type(connection, table_name, row)
                    extra_params.update(field_params)
                    comment_notes.extend(field_notes)

                    # Add primary_key and unique, if necessary.
                    if column_name in indexes:
                        if indexes[column_name]['primary_key']:
                            extra_params['primary_key'] = True
                        elif indexes[column_name]['unique']:
                            extra_params['unique'] = True

                    field_type += '('
                    
                if keyword.iskeyword(att_name):
                    att_name += '_field'
                    comment_notes.append('Field renamed because it was a Python reserved word.')

                # Don't output 'id = meta.AutoField(primary_key=True)', because
                # that's assumed if it doesn't exist.
                if att_name == 'id' and field_type == 'AutoField(' and extra_params == {'primary_key': True}:
                    continue

                # Add 'null' and 'blank', if the 'null_ok' flag was present in the
                # table description.
                if row[6]: # If it's NULL...
                    extra_params['blank'] = True
                    if not field_type in ('TextField(', 'CharField('):
                        extra_params['null'] = True

                field_desc = '%s = models.%s' % (att_name, field_type)
                if extra_params:
                    if not field_desc.endswith('('):
                        field_desc += ', '
                    field_desc += ', '.join(['%s=%r' % (k, v) for k, v in extra_params.items()])
                field_desc += ')'
                if comment_notes:
                    field_desc += ' # ' + ' '.join(comment_notes)
                yield '    %s' % field_desc
            for meta_line in self.get_meta(table_name):
                yield meta_line

    def get_field_type(self, connection, table_name, row):
        """
        Given the database connection, the table name, and the cursor row
        description, this routine will return the given field type name, as
        well as any additional keyword parameters and notes for the field.
        """
        field_params = {}
        field_notes = []

        try:
            field_type = connection.introspection.get_field_type(row[1], row)
        except KeyError:
            field_type = 'TextField'
            field_notes.append('This field type is a guess.')

        # This is a hook for DATA_TYPES_REVERSE to return a tuple of
        # (field_type, field_params_dict).
        if type(field_type) is tuple:
            field_type, new_params = field_type
            field_params.update(new_params)

        # Add max_length for all CharFields.
        if field_type == 'CharField' and row[3]:
            field_params['max_length'] = row[3]

        if field_type == 'DecimalField':
            field_params['max_digits'] = row[4]
            field_params['decimal_places'] = row[5]

        return field_type, field_params, field_notes

    def get_meta(self, table_name):
        """
        Return a sequence comprising the lines of code necessary
        to construct the inner Meta class for the model corresponding
        to the given database table name.
        """
        return ['    class Meta:',
                '        db_table = %r' % table_name,
                '']

"""
YAML serializer.

Requires PyYaml (http://pyyaml.org/), but that's checked for in __init__.
"""

from StringIO import StringIO
import decimal
import yaml

from django.db import models
from django.core.serializers.python import Serializer as PythonSerializer
from django.core.serializers.python import Deserializer as PythonDeserializer

class DjangoSafeDumper(yaml.SafeDumper):
    def represent_decimal(self, data):
        return self.represent_scalar('tag:yaml.org,2002:str', str(data))

DjangoSafeDumper.add_representer(decimal.Decimal, DjangoSafeDumper.represent_decimal)

class Serializer(PythonSerializer):
    """
    Convert a queryset to YAML.
    """

    internal_use_only = False

    def handle_field(self, obj, field):
        # A nasty special case: base YAML doesn't support serialization of time
        # types (as opposed to dates or datetimes, which it does support). Since
        # we want to use the "safe" serializer for better interoperability, we
        # need to do something with those pesky times. Converting 'em to strings
        # isn't perfect, but it's better than a "!!python/time" type which would
        # halt deserialization under any other language.
        if isinstance(field, models.TimeField) and getattr(obj, field.name) is not None:
            self._current[field.name] = str(getattr(obj, field.name))
        else:
            super(Serializer, self).handle_field(obj, field)

    def end_serialization(self):
        yaml.dump(self.objects, self.stream, Dumper=DjangoSafeDumper, **self.options)

    def getvalue(self):
        return self.stream.getvalue()

def Deserializer(stream_or_string, **options):
    """
    Deserialize a stream or string of YAML data.
    """
    if isinstance(stream_or_string, basestring):
        stream = StringIO(stream_or_string)
    else:
        stream = stream_or_string
    for obj in PythonDeserializer(yaml.load(stream), **options):
        yield obj


"""
Extracts the version of the PostgreSQL server.
"""

import re

# This reg-exp is intentionally fairly flexible here.
# Needs to be able to handle stuff like:
#   PostgreSQL 8.3.6
#   EnterpriseDB 8.3
#   PostgreSQL 8.3 beta4
#   PostgreSQL 8.4beta1
VERSION_RE = re.compile(r'\S+ (\d+)\.(\d+)\.?(\d+)?')

def _parse_version(text):
    "Internal parsing method. Factored out for testing purposes."
    major, major2, minor = VERSION_RE.search(text).groups()
    try:
        return int(major), int(major2), int(minor)
    except (ValueError, TypeError):
        return int(major), int(major2), None

def get_version(cursor):
    """
    Returns a tuple representing the major, minor and revision number of the
    server. For example, (7, 4, 1) or (8, 3, 4). The revision number will be
    None in the case of initial releases (e.g., 'PostgreSQL 8.3') or in the
    case of beta and prereleases ('PostgreSQL 8.4beta1').
    """
    cursor.execute("SELECT version()")
    return _parse_version(cursor.fetchone()[0])

import re

# Valid query types (a dictionary is used for speedy lookups).
QUERY_TERMS = dict([(x, None) for x in (
    'exact', 'iexact', 'contains', 'icontains', 'gt', 'gte', 'lt', 'lte', 'in',
    'startswith', 'istartswith', 'endswith', 'iendswith', 'range', 'year',
    'month', 'day', 'week_day', 'isnull', 'search', 'regex', 'iregex',
    )])

# Size of each "chunk" for get_iterator calls.
# Larger values are slightly faster at the expense of more storage space.
GET_ITERATOR_CHUNK_SIZE = 100

# Separator used to split filter strings apart.
LOOKUP_SEP = '__'

# Constants to make looking up tuple values clearer.
# Join lists (indexes into the tuples that are values in the alias_map
# dictionary in the Query class).
TABLE_NAME = 0
RHS_ALIAS = 1
JOIN_TYPE = 2
LHS_ALIAS = 3
LHS_JOIN_COL = 4
RHS_JOIN_COL = 5
NULLABLE = 6

# How many results to expect from a cursor.execute call
MULTI = 'multi'
SINGLE = 'single'

ORDER_PATTERN = re.compile(r'\?|[-+]?[.\w]+$')
ORDER_DIR = {
    'ASC': ('ASC', 'DESC'),
    'DESC': ('DESC', 'ASC')}



"this is the locale selecting middleware that will look at accept headers"

from django.utils.cache import patch_vary_headers
from django.utils import translation

class LocaleMiddleware(object):
    """
    This is a very simple middleware that parses a request
    and decides what translation object to install in the current
    thread context. This allows pages to be dynamically
    translated to the language the user desires (if the language
    is available, of course).
    """

    def process_request(self, request):
        language = translation.get_language_from_request(request)
        translation.activate(language)
        request.LANGUAGE_CODE = translation.get_language()

    def process_response(self, request, response):
        patch_vary_headers(response, ('Accept-Language',))
        if 'Content-Language' not in response:
            response['Content-Language'] = translation.get_language()
        translation.deactivate()
        return response

from django.template import loader, RequestContext
from django.http import Http404, HttpResponse
from django.core.xheaders import populate_xheaders
from django.core.paginator import Paginator, InvalidPage
from django.core.exceptions import ObjectDoesNotExist

def object_list(request, queryset, paginate_by=None, page=None,
        allow_empty=True, template_name=None, template_loader=loader,
        extra_context=None, context_processors=None, template_object_name='object',
        mimetype=None):
    """
    Generic list of objects.

    Templates: ``<app_label>/<model_name>_list.html``
    Context:
        object_list
            list of objects
        is_paginated
            are the results paginated?
        results_per_page
            number of objects per page (if paginated)
        has_next
            is there a next page?
        has_previous
            is there a prev page?
        page
            the current page
        next
            the next page
        previous
            the previous page
        pages
            number of pages, total
        hits
            number of objects, total
        last_on_page
            the result number of the last of object in the
            object_list (1-indexed)
        first_on_page
            the result number of the first object in the
            object_list (1-indexed)
        page_range:
            A list of the page numbers (1-indexed).
    """
    if extra_context is None: extra_context = {}
    queryset = queryset._clone()
    if paginate_by:
        paginator = Paginator(queryset, paginate_by, allow_empty_first_page=allow_empty)
        if not page:
            page = request.GET.get('page', 1)
        try:
            page_number = int(page)
        except ValueError:
            if page == 'last':
                page_number = paginator.num_pages
            else:
                # Page is not 'last', nor can it be converted to an int.
                raise Http404
        try:
            page_obj = paginator.page(page_number)
        except InvalidPage:
            raise Http404
        c = RequestContext(request, {
            '%s_list' % template_object_name: page_obj.object_list,
            'paginator': paginator,
            'page_obj': page_obj,

            # Legacy template context stuff. New templates should use page_obj
            # to access this instead.
            'is_paginated': page_obj.has_other_pages(),
            'results_per_page': paginator.per_page,
            'has_next': page_obj.has_next(),
            'has_previous': page_obj.has_previous(),
            'page': page_obj.number,
            'next': page_obj.next_page_number(),
            'previous': page_obj.previous_page_number(),
            'first_on_page': page_obj.start_index(),
            'last_on_page': page_obj.end_index(),
            'pages': paginator.num_pages,
            'hits': paginator.count,
            'page_range': paginator.page_range,
        }, context_processors)
    else:
        c = RequestContext(request, {
            '%s_list' % template_object_name: queryset,
            'paginator': None,
            'page_obj': None,
            'is_paginated': False,
        }, context_processors)
        if not allow_empty and len(queryset) == 0:
            raise Http404
    for key, value in extra_context.items():
        if callable(value):
            c[key] = value()
        else:
            c[key] = value
    if not template_name:
        model = queryset.model
        template_name = "%s/%s_list.html" % (model._meta.app_label, model._meta.object_name.lower())
    t = template_loader.get_template(template_name)
    return HttpResponse(t.render(c), mimetype=mimetype)

def object_detail(request, queryset, object_id=None, slug=None,
        slug_field='slug', template_name=None, template_name_field=None,
        template_loader=loader, extra_context=None,
        context_processors=None, template_object_name='object',
        mimetype=None):
    """
    Generic detail of an object.

    Templates: ``<app_label>/<model_name>_detail.html``
    Context:
        object
            the object
    """
    if extra_context is None: extra_context = {}
    model = queryset.model
    if object_id:
        queryset = queryset.filter(pk=object_id)
    elif slug and slug_field:
        queryset = queryset.filter(**{slug_field: slug})
    else:
        raise AttributeError("Generic detail view must be called with either an object_id or a slug/slug_field.")
    try:
        obj = queryset.get()
    except ObjectDoesNotExist:
        raise Http404("No %s found matching the query" % (model._meta.verbose_name))
    if not template_name:
        template_name = "%s/%s_detail.html" % (model._meta.app_label, model._meta.object_name.lower())
    if template_name_field:
        template_name_list = [getattr(obj, template_name_field), template_name]
        t = template_loader.select_template(template_name_list)
    else:
        t = template_loader.get_template(template_name)
    c = RequestContext(request, {
        template_object_name: obj,
    }, context_processors)
    for key, value in extra_context.items():
        if callable(value):
            c[key] = value()
        else:
            c[key] = value
    response = HttpResponse(t.render(c), mimetype=mimetype)
    populate_xheaders(request, response, model, getattr(obj, obj._meta.pk.name))
    return response

from django.db.models import sql
from django.db.models.loading import cache
from django.db.models.query import CollectedObjects
from django.db.models.query_utils import CyclicDependency
from django.test import TestCase

from models import A, B, C, D, E, F


class DeleteTests(TestCase):
    def clear_rel_obj_caches(self, *models):
        for m in models:
            if hasattr(m._meta, '_related_objects_cache'):
                del m._meta._related_objects_cache

    def order_models(self, *models):
        cache.app_models["delete"].keyOrder = models

    def setUp(self):
        self.order_models("a", "b", "c", "d", "e", "f")
        self.clear_rel_obj_caches(A, B, C, D, E, F)

    def tearDown(self):
        self.order_models("a", "b", "c", "d", "e", "f")
        self.clear_rel_obj_caches(A, B, C, D, E, F)

    def test_collected_objects(self):
        g = CollectedObjects()
        self.assertFalse(g.add("key1", 1, "item1", None))
        self.assertEqual(g["key1"], {1: "item1"})

        self.assertFalse(g.add("key2", 1, "item1", "key1"))
        self.assertFalse(g.add("key2", 2, "item2", "key1"))

        self.assertEqual(g["key2"], {1: "item1", 2: "item2"})

        self.assertFalse(g.add("key3", 1, "item1", "key1"))
        self.assertTrue(g.add("key3", 1, "item1", "key2"))
        self.assertEqual(g.ordered_keys(), ["key3", "key2", "key1"])

        self.assertTrue(g.add("key2", 1, "item1", "key3"))
        self.assertRaises(CyclicDependency, g.ordered_keys)

    def test_delete(self):
        ## Second, test the usage of CollectedObjects by Model.delete()

        # Due to the way that transactions work in the test harness, doing
        # m.delete() here can work but fail in a real situation, since it may
        # delete all objects, but not in the right order. So we manually check
        # that the order of deletion is correct.

        # Also, it is possible that the order is correct 'accidentally', due
        # solely to order of imports etc.  To check this, we set the order that
        # 'get_models()' will retrieve to a known 'nice' order, and then try
        # again with a known 'tricky' order.  Slightly naughty access to
        # internals here :-)

        # If implementation changes, then the tests may need to be simplified:
        #  - remove the lines that set the .keyOrder and clear the related
        #    object caches
        #  - remove the second set of tests (with a2, b2 etc)

        a1 = A.objects.create()
        b1 = B.objects.create(a=a1)
        c1 = C.objects.create(b=b1)
        d1 = D.objects.create(c=c1, a=a1)

        o = CollectedObjects()
        a1._collect_sub_objects(o)
        self.assertEqual(o.keys(), [D, C, B, A])
        a1.delete()

        # Same again with a known bad order
        self.order_models("d", "c", "b", "a")
        self.clear_rel_obj_caches(A, B, C, D)

        a2 = A.objects.create()
        b2 = B.objects.create(a=a2)
        c2 = C.objects.create(b=b2)
        d2 = D.objects.create(c=c2, a=a2)

        o = CollectedObjects()
        a2._collect_sub_objects(o)
        self.assertEqual(o.keys(), [D, C, B, A])
        a2.delete()

    def test_collected_objects_null(self):
        g = CollectedObjects()
        self.assertFalse(g.add("key1", 1, "item1", None))
        self.assertFalse(g.add("key2", 1, "item1", "key1", nullable=True))
        self.assertTrue(g.add("key1", 1, "item1", "key2"))
        self.assertEqual(g.ordered_keys(), ["key1", "key2"])

    def test_delete_nullable(self):
        e1 = E.objects.create()
        f1 = F.objects.create(e=e1)
        e1.f = f1
        e1.save()

        # Since E.f is nullable, we should delete F first (after nulling out
        # the E.f field), then E.

        o = CollectedObjects()
        e1._collect_sub_objects(o)
        self.assertEqual(o.keys(), [F, E])

        # temporarily replace the UpdateQuery class to verify that E.f is
        # actually nulled out first

        logged = []
        class LoggingUpdateQuery(sql.UpdateQuery):
            def clear_related(self, related_field, pk_list, using):
                logged.append(related_field.name)
                return super(LoggingUpdateQuery, self).clear_related(related_field, pk_list, using)
        original = sql.UpdateQuery
        sql.UpdateQuery = LoggingUpdateQuery

        e1.delete()
        self.assertEqual(logged, ["f"])
        logged = []

        e2 = E.objects.create()
        f2 = F.objects.create(e=e2)
        e2.f = f2
        e2.save()

        # Same deal as before, though we are starting from the other object.
        o = CollectedObjects()
        f2._collect_sub_objects(o)
        self.assertEqual(o.keys(), [F, E])
        f2.delete()
        self.assertEqual(logged, ["f"])
        logged = []

        sql.UpdateQuery = original

"""
7. The lookup API

This demonstrates features of the database API.
"""

from django.db import models

class Article(models.Model):
    headline = models.CharField(max_length=100)
    pub_date = models.DateTimeField()
    class Meta:
        ordering = ('-pub_date', 'headline')

    def __unicode__(self):
        return self.headline

from operator import attrgetter

from django.conf import settings
from django.core.exceptions import FieldError
from django.db import connection
from django.test import TestCase

from models import (Chef, CommonInfo, ItalianRestaurant, ParkingLot, Place,
    Post, Restaurant, Student, StudentWorker, Supplier, Worker, MixinModel)


class ModelInheritanceTests(TestCase):
    def test_abstract(self):
        # The Student and Worker models both have 'name' and 'age' fields on
        # them and inherit the __unicode__() method, just as with normal Python
        # subclassing. This is useful if you want to factor out common
        # information for programming purposes, but still completely
        # independent separate models at the database level.
        w1 = Worker.objects.create(name="Fred", age=35, job="Quarry worker")
        w2 = Worker.objects.create(name="Barney", age=34, job="Quarry worker")

        s = Student.objects.create(name="Pebbles", age=5, school_class="1B")

        self.assertEqual(unicode(w1), "Worker Fred")
        self.assertEqual(unicode(s), "Student Pebbles")

        # The children inherit the Meta class of their parents (if they don't
        # specify their own).
        self.assertQuerysetEqual(
            Worker.objects.values("name"), [
                {"name": "Barney"},
                {"name": "Fred"},
            ],
            lambda o: o
        )

        # Since Student does not subclass CommonInfo's Meta, it has the effect
        # of completely overriding it. So ordering by name doesn't take place
        # for Students.
        self.assertEqual(Student._meta.ordering, [])

        # However, the CommonInfo class cannot be used as a normal model (it
        # doesn't exist as a model).
        self.assertRaises(AttributeError, lambda: CommonInfo.objects.all())

        # A StudentWorker which does not exist is both a Student and Worker
        # which does not exist.
        self.assertRaises(Student.DoesNotExist,
            StudentWorker.objects.get, pk=12321321
        )
        self.assertRaises(Worker.DoesNotExist,
            StudentWorker.objects.get, pk=12321321
        )

        # MultipleObjectsReturned is also inherited.
        # This is written out "long form", rather than using __init__/create()
        # because of a bug with diamond inheritance (#10808)
        sw1 = StudentWorker()
        sw1.name = "Wilma"
        sw1.age = 35
        sw1.save()
        sw2 = StudentWorker()
        sw2.name = "Betty"
        sw2.age = 24
        sw2.save()

        self.assertRaises(Student.MultipleObjectsReturned,
            StudentWorker.objects.get, pk__lt=sw2.pk + 100
        )
        self.assertRaises(Worker.MultipleObjectsReturned,
            StudentWorker.objects.get, pk__lt=sw2.pk + 100
        )

    def test_multiple_table(self):
        post = Post.objects.create(title="Lorem Ipsum")
        # The Post model has distinct accessors for the Comment and Link models.
        post.attached_comment_set.create(content="Save $ on V1agr@", is_spam=True)
        post.attached_link_set.create(
            content="The Web framework for perfections with deadlines.",
            url="http://www.djangoproject.com/"
        )

        # The Post model doesn't have an attribute called
        # 'attached_%(class)s_set'.
        self.assertRaises(AttributeError,
            getattr, post, "attached_%(class)s_set"
        )

        # The Place/Restaurant/ItalianRestaurant models all exist as
        # independent models. However, the subclasses also have transparent
        # access to the fields of their ancestors.
        # Create a couple of Places.
        p1 = Place.objects.create(name="Master Shakes", address="666 W. Jersey")
        p2 = Place.objects.create(name="Ace Harware", address="1013 N. Ashland")

        # Test constructor for Restaurant.
        r = Restaurant.objects.create(
            name="Demon Dogs",
            address="944 W. Fullerton",
            serves_hot_dogs=True,
            serves_pizza=False,
            rating=2
        )
        # Test the constructor for ItalianRestaurant.
        c = Chef.objects.create(name="Albert")
        ir = ItalianRestaurant.objects.create(
            name="Ristorante Miron",
            address="1234 W. Ash",
            serves_hot_dogs=False,
            serves_pizza=False,
            serves_gnocchi=True,
            rating=4,
            chef=c
        )
        self.assertQuerysetEqual(
            ItalianRestaurant.objects.filter(address="1234 W. Ash"), [
                "Ristorante Miron",
            ],
            attrgetter("name")
        )
        ir.address = "1234 W. Elm"
        ir.save()
        self.assertQuerysetEqual(
            ItalianRestaurant.objects.filter(address="1234 W. Elm"), [
                "Ristorante Miron",
            ],
            attrgetter("name")
        )

        # Make sure Restaurant and ItalianRestaurant have the right fields in
        # the right order.
        self.assertEqual(
            [f.name for f in Restaurant._meta.fields],
            ["id", "name", "address", "place_ptr", "rating", "serves_hot_dogs", "serves_pizza", "chef"]
        )
        self.assertEqual(
            [f.name for f in ItalianRestaurant._meta.fields],
            ["id", "name", "address", "place_ptr", "rating", "serves_hot_dogs", "serves_pizza", "chef", "restaurant_ptr", "serves_gnocchi"],
        )
        self.assertEqual(Restaurant._meta.ordering, ["-rating"])

        # Even though p.supplier for a Place 'p' (a parent of a Supplier), a
        # Restaurant object cannot access that reverse relation, since it's not
        # part of the Place-Supplier Hierarchy.
        self.assertQuerysetEqual(Place.objects.filter(supplier__name="foo"), [])
        self.assertRaises(FieldError,
            Restaurant.objects.filter, supplier__name="foo"
        )

        # Parent fields can be used directly in filters on the child model.
        self.assertQuerysetEqual(
            Restaurant.objects.filter(name="Demon Dogs"), [
                "Demon Dogs",
            ],
            attrgetter("name")
        )
        self.assertQuerysetEqual(
            ItalianRestaurant.objects.filter(address="1234 W. Elm"), [
                "Ristorante Miron",
            ],
            attrgetter("name")
        )

        # Filters against the parent model return objects of the parent's type.
        p = Place.objects.get(name="Demon Dogs")
        self.assertTrue(type(p) is Place)

        # Since the parent and child are linked by an automatically created
        # OneToOneField, you can get from the parent to the child by using the
        # child's name.
        self.assertEqual(
            p.restaurant, Restaurant.objects.get(name="Demon Dogs")
        )
        self.assertEqual(
            Place.objects.get(name="Ristorante Miron").restaurant.italianrestaurant,
            ItalianRestaurant.objects.get(name="Ristorante Miron")
        )
        self.assertEqual(
            Restaurant.objects.get(name="Ristorante Miron").italianrestaurant,
            ItalianRestaurant.objects.get(name="Ristorante Miron")
        )

        # This won't work because the Demon Dogs restaurant is not an Italian
        # restaurant.
        self.assertRaises(ItalianRestaurant.DoesNotExist,
            lambda: p.restaurant.italianrestaurant
        )
        # An ItalianRestaurant which does not exist is also a Place which does
        # not exist.
        self.assertRaises(Place.DoesNotExist,
            ItalianRestaurant.objects.get, name="The Noodle Void"
        )
        # MultipleObjectsReturned is also inherited.
        self.assertRaises(Place.MultipleObjectsReturned,
            Restaurant.objects.get, id__lt=12321
        )

        # Related objects work just as they normally do.
        s1 = Supplier.objects.create(name="Joe's Chickens", address="123 Sesame St")
        s1.customers = [r, ir]
        s2 = Supplier.objects.create(name="Luigi's Pasta", address="456 Sesame St")
        s2.customers = [ir]

        # This won't work because the Place we select is not a Restaurant (it's
        # a Supplier).
        p = Place.objects.get(name="Joe's Chickens")
        self.assertRaises(Restaurant.DoesNotExist,
            lambda: p.restaurant
        )

        self.assertEqual(p.supplier, s1)
        self.assertQuerysetEqual(
            ir.provider.order_by("-name"), [
                "Luigi's Pasta",
                "Joe's Chickens"
            ],
            attrgetter("name")
        )
        self.assertQuerysetEqual(
            Restaurant.objects.filter(provider__name__contains="Chickens"), [
                "Ristorante Miron",
                "Demon Dogs",
            ],
            attrgetter("name")
        )
        self.assertQuerysetEqual(
            ItalianRestaurant.objects.filter(provider__name__contains="Chickens"), [
                "Ristorante Miron",
            ],
            attrgetter("name"),
        )

        park1 = ParkingLot.objects.create(
            name="Main St", address="111 Main St", main_site=s1
        )
        park2 = ParkingLot.objects.create(
            name="Well Lit", address="124 Sesame St", main_site=ir
        )

        self.assertEqual(
            Restaurant.objects.get(lot__name="Well Lit").name,
            "Ristorante Miron"
        )

        # The update() command can update fields in parent and child classes at
        # once (although it executed multiple SQL queries to do so).
        rows = Restaurant.objects.filter(
            serves_hot_dogs=True, name__contains="D"
        ).update(
            name="Demon Puppies", serves_hot_dogs=False
        )
        self.assertEqual(rows, 1)

        r1 = Restaurant.objects.get(pk=r.pk)
        self.assertFalse(r1.serves_hot_dogs)
        self.assertEqual(r1.name, "Demon Puppies")

        # The values() command also works on fields from parent models.
        self.assertQuerysetEqual(
            ItalianRestaurant.objects.values("name", "rating"), [
                {"rating": 4, "name": "Ristorante Miron"}
            ],
            lambda o: o
        )

        # select_related works with fields from the parent object as if they
        # were a normal part of the model.
        old_DEBUG = settings.DEBUG
        try:
            settings.DEBUG = True
            starting_queries = len(connection.queries)
            ItalianRestaurant.objects.all()[0].chef
            self.assertEqual(len(connection.queries) - starting_queries, 2)

            starting_queries = len(connection.queries)
            ItalianRestaurant.objects.select_related("chef")[0].chef
            self.assertEqual(len(connection.queries) - starting_queries, 1)
        finally:
            settings.DEBUG = old_DEBUG

    def test_mixin_init(self):
        m = MixinModel()
        self.assertEqual(m.other_attr, 1)

"""
18. Using SQL reserved names

Need to use a reserved SQL name as a column name or table name? Need to include
a hyphen in a column or table name? No problem. Django quotes names
appropriately behind the scenes, so your database won't complain about
reserved-name usage.
"""

from django.db import models

class Thing(models.Model):
    when = models.CharField(max_length=1, primary_key=True)
    join = models.CharField(max_length=1)
    like = models.CharField(max_length=1)
    drop = models.CharField(max_length=1)
    alter = models.CharField(max_length=1)
    having = models.CharField(max_length=1)
    where = models.DateField(max_length=1)
    has_hyphen = models.CharField(max_length=1, db_column='has-hyphen')
    class Meta:
       db_table = 'select'

    def __unicode__(self):
        return self.when
from datetime import datetime
from django.core.exceptions import ValidationError
from django.db import models
from django.test import TestCase


def validate_answer_to_universe(value):
    if value != 42:
        raise ValidationError('This is not the answer to life, universe and everything!', code='not42')

class ModelToValidate(models.Model):
    name = models.CharField(max_length=100)
    created = models.DateTimeField(default=datetime.now)
    number = models.IntegerField(db_column='number_val')
    parent = models.ForeignKey('self', blank=True, null=True, limit_choices_to={'number': 10})
    email = models.EmailField(blank=True)
    url = models.URLField(blank=True)
    f_with_custom_validator = models.IntegerField(blank=True, null=True, validators=[validate_answer_to_universe])

    def clean(self):
        super(ModelToValidate, self).clean()
        if self.number == 11:
            raise ValidationError('Invalid number supplied!')

class UniqueFieldsModel(models.Model):
    unique_charfield = models.CharField(max_length=100, unique=True)
    unique_integerfield = models.IntegerField(unique=True)
    non_unique_field = models.IntegerField()

class CustomPKModel(models.Model):
    my_pk_field = models.CharField(max_length=100, primary_key=True)

class UniqueTogetherModel(models.Model):
    cfield = models.CharField(max_length=100)
    ifield = models.IntegerField()
    efield = models.EmailField()

    class Meta:
        unique_together = (('ifield', 'cfield',), ['ifield', 'efield'])

class UniqueForDateModel(models.Model):
    start_date = models.DateField()
    end_date = models.DateTimeField()
    count = models.IntegerField(unique_for_date="start_date", unique_for_year="end_date")
    order = models.IntegerField(unique_for_month="end_date")
    name = models.CharField(max_length=100)

class CustomMessagesModel(models.Model):
    other  = models.IntegerField(blank=True, null=True)
    number = models.IntegerField(db_column='number_val',
        error_messages={'null': 'NULL', 'not42': 'AAARGH', 'not_equal': '%s != me'},
        validators=[validate_answer_to_universe]
    )

class Author(models.Model):
    name = models.CharField(max_length=100)

class Article(models.Model):
    title = models.CharField(max_length=100)
    author = models.ForeignKey(Author)
    pub_date = models.DateTimeField(blank=True)

    def clean(self):
        if self.pub_date is None:
            self.pub_date = datetime.now()

class Post(models.Model):
    title = models.CharField(max_length=50, unique_for_date='posted', blank=True)
    slug = models.CharField(max_length=50, unique_for_year='posted', blank=True)
    subtitle = models.CharField(max_length=50, unique_for_month='posted', blank=True)
    posted = models.DateField()

    def __unicode__(self):
        return self.name

class FlexibleDatePost(models.Model):
    title = models.CharField(max_length=50, unique_for_date='posted', blank=True)
    slug = models.CharField(max_length=50, unique_for_year='posted', blank=True)
    subtitle = models.CharField(max_length=50, unique_for_month='posted', blank=True)
    posted = models.DateField(blank=True, null=True)

from datetime import datetime
import unittest

from django.conf import settings
from django.db import models
from django.utils.formats import localize
from django.test import TestCase

from django.contrib import admin
from django.contrib.admin.util import display_for_field, label_for_field, lookup_field
from django.contrib.admin.views.main import EMPTY_CHANGELIST_VALUE
from django.contrib.sites.models import Site
from django.contrib.admin.util import NestedObjects

from models import Article, Count, Event, Location


class NestedObjectsTests(TestCase):
    """
    Tests for ``NestedObject`` utility collection.

    """
    def setUp(self):
        self.n = NestedObjects()
        self.objs = [Count.objects.create(num=i) for i in range(5)]

    def _check(self, target):
        self.assertEquals(self.n.nested(lambda obj: obj.num), target)

    def _add(self, obj, parent=None):
        # don't bother providing the extra args that NestedObjects ignores
        self.n.add(None, None, obj, None, parent)

    def test_unrelated_roots(self):
        self._add(self.objs[0])
        self._add(self.objs[1])
        self._add(self.objs[2], self.objs[1])

        self._check([0, 1, [2]])

    def test_siblings(self):
        self._add(self.objs[0])
        self._add(self.objs[1], self.objs[0])
        self._add(self.objs[2], self.objs[0])

        self._check([0, [1, 2]])

    def test_duplicate_instances(self):
        self._add(self.objs[0])
        self._add(self.objs[1])
        dupe = Count.objects.get(num=1)
        self._add(dupe, self.objs[0])

        self._check([0, 1])

    def test_non_added_parent(self):
        self._add(self.objs[0], self.objs[1])

        self._check([0])

    def test_cyclic(self):
        self._add(self.objs[0], self.objs[2])
        self._add(self.objs[1], self.objs[0])
        self._add(self.objs[2], self.objs[1])
        self._add(self.objs[0], self.objs[2])

        self._check([0, [1, [2]]])


class UtilTests(unittest.TestCase):
    def test_values_from_lookup_field(self):
        """
        Regression test for #12654: lookup_field
        """
        SITE_NAME = 'example.com'
        TITLE_TEXT = 'Some title'
        CREATED_DATE = datetime.min
        ADMIN_METHOD = 'admin method'
        SIMPLE_FUNCTION = 'function'
        INSTANCE_ATTRIBUTE = 'attr'

        class MockModelAdmin(object):
            def get_admin_value(self, obj):
                return ADMIN_METHOD

        simple_function = lambda obj: SIMPLE_FUNCTION

        article = Article(
            site=Site(domain=SITE_NAME),
            title=TITLE_TEXT,
            created=CREATED_DATE,
        )
        article.non_field = INSTANCE_ATTRIBUTE

        verifications = (
            ('site', SITE_NAME),
            ('created', localize(CREATED_DATE)),
            ('title', TITLE_TEXT),
            ('get_admin_value', ADMIN_METHOD),
            (simple_function, SIMPLE_FUNCTION),
            ('test_from_model', article.test_from_model()),
            ('non_field', INSTANCE_ATTRIBUTE)
        )

        mock_admin = MockModelAdmin()
        for name, value in verifications:
            field, attr, resolved_value = lookup_field(name, article, mock_admin)

            if field is not None:
                resolved_value = display_for_field(resolved_value, field)

            self.assertEqual(value, resolved_value)

    def test_null_display_for_field(self):
        """
        Regression test for #12550: display_for_field should handle None
        value.
        """
        display_value = display_for_field(None, models.CharField())
        self.assertEqual(display_value, EMPTY_CHANGELIST_VALUE)

        display_value = display_for_field(None, models.CharField(
            choices=(
                (None, "test_none"),
            )
        ))
        self.assertEqual(display_value, "test_none")

        display_value = display_for_field(None, models.DateField())
        self.assertEqual(display_value, EMPTY_CHANGELIST_VALUE)

        display_value = display_for_field(None, models.TimeField())
        self.assertEqual(display_value, EMPTY_CHANGELIST_VALUE)

        # Regression test for #13071: NullBooleanField has special
        # handling.
        display_value = display_for_field(None, models.NullBooleanField())
        expected = u'<img src="%simg/admin/icon-unknown.gif" alt="None" />' % settings.ADMIN_MEDIA_PREFIX
        self.assertEqual(display_value, expected)

        display_value = display_for_field(None, models.DecimalField())
        self.assertEqual(display_value, EMPTY_CHANGELIST_VALUE)

        display_value = display_for_field(None, models.FloatField())
        self.assertEqual(display_value, EMPTY_CHANGELIST_VALUE)

    def test_label_for_field(self):
        """
        Tests for label_for_field
        """
        self.assertEquals(
            label_for_field("title", Article),
            "title"
        )
        self.assertEquals(
            label_for_field("title2", Article),
            "another name"
        )
        self.assertEquals(
            label_for_field("title2", Article, return_attr=True),
            ("another name", None)
        )

        self.assertEquals(
            label_for_field("__unicode__", Article),
            "article"
        )
        self.assertEquals(
            label_for_field("__str__", Article),
            "article"
        )

        self.assertRaises(
            AttributeError,
            lambda: label_for_field("unknown", Article)
        )

        def test_callable(obj):
            return "nothing"
        self.assertEquals(
            label_for_field(test_callable, Article),
            "Test callable"
        )
        self.assertEquals(
            label_for_field(test_callable, Article, return_attr=True),
            ("Test callable", test_callable)
        )

        self.assertEquals(
            label_for_field("test_from_model", Article),
            "Test from model"
        )
        self.assertEquals(
            label_for_field("test_from_model", Article, return_attr=True),
            ("Test from model", Article.test_from_model)
        )
        self.assertEquals(
            label_for_field("test_from_model_with_override", Article),
            "not What you Expect"
        )

        self.assertEquals(
            label_for_field(lambda x: "nothing", Article),
            "--"
        )

        class MockModelAdmin(object):
            def test_from_model(self, obj):
                return "nothing"
            test_from_model.short_description = "not Really the Model"

        self.assertEquals(
            label_for_field("test_from_model", Article, model_admin=MockModelAdmin),
            "not Really the Model"
        )
        self.assertEquals(
            label_for_field("test_from_model", Article,
                model_admin = MockModelAdmin,
                return_attr = True
            ),
            ("not Really the Model", MockModelAdmin.test_from_model)
        )

    def test_related_name(self):
        """
        Regression test for #13963
        """
        self.assertEquals(
            label_for_field('location', Event, return_attr=True),
            ('location', None),
        )
        self.assertEquals(
            label_for_field('event', Location, return_attr=True),
            ('awesome event', None),
        )
        self.assertEquals(
            label_for_field('guest', Event, return_attr=True),
            ('awesome guest', None),
        )

from django.conf.urls.defaults import *
import views

urlpatterns = patterns('',
    (r'^upload/$',          views.file_upload_view),
    (r'^verify/$',          views.file_upload_view_verify),
    (r'^unicode_name/$',    views.file_upload_unicode_name),
    (r'^echo/$',            views.file_upload_echo),
    (r'^quota/$',           views.file_upload_quota),
    (r'^quota/broken/$',    views.file_upload_quota_broken),
    (r'^getlist_count/$',   views.file_upload_getlist_count),
    (r'^upload_errors/$',   views.file_upload_errors),
)

from django.contrib.localflavor.sk.forms import (SKRegionSelect,
    SKPostalCodeField, SKDistrictSelect)

from utils import LocalFlavorTestCase


class SKLocalFlavorTests(LocalFlavorTestCase):
    def test_SKRegionSelect(self):
        f = SKRegionSelect()
        out = u'''<select name="regions">
<option value="BB">Banska Bystrica region</option>
<option value="BA">Bratislava region</option>
<option value="KE">Kosice region</option>
<option value="NR">Nitra region</option>
<option value="PO">Presov region</option>
<option value="TN">Trencin region</option>
<option value="TT" selected="selected">Trnava region</option>
<option value="ZA">Zilina region</option>
</select>'''
        self.assertEqual(f.render('regions', 'TT'), out)

    def test_SKDistrictSelect(self):
        f = SKDistrictSelect()
        out = u'''<select name="Districts">
<option value="BB">Banska Bystrica</option>
<option value="BS">Banska Stiavnica</option>
<option value="BJ">Bardejov</option>
<option value="BN">Banovce nad Bebravou</option>
<option value="BR">Brezno</option>
<option value="BA1">Bratislava I</option>
<option value="BA2">Bratislava II</option>
<option value="BA3">Bratislava III</option>
<option value="BA4">Bratislava IV</option>
<option value="BA5">Bratislava V</option>
<option value="BY">Bytca</option>
<option value="CA">Cadca</option>
<option value="DT">Detva</option>
<option value="DK">Dolny Kubin</option>
<option value="DS">Dunajska Streda</option>
<option value="GA">Galanta</option>
<option value="GL">Gelnica</option>
<option value="HC">Hlohovec</option>
<option value="HE">Humenne</option>
<option value="IL">Ilava</option>
<option value="KK">Kezmarok</option>
<option value="KN">Komarno</option>
<option value="KE1">Kosice I</option>
<option value="KE2">Kosice II</option>
<option value="KE3">Kosice III</option>
<option value="KE4">Kosice IV</option>
<option value="KEO">Kosice - okolie</option>
<option value="KA">Krupina</option>
<option value="KM">Kysucke Nove Mesto</option>
<option value="LV">Levice</option>
<option value="LE">Levoca</option>
<option value="LM">Liptovsky Mikulas</option>
<option value="LC">Lucenec</option>
<option value="MA">Malacky</option>
<option value="MT">Martin</option>
<option value="ML">Medzilaborce</option>
<option value="MI">Michalovce</option>
<option value="MY">Myjava</option>
<option value="NO">Namestovo</option>
<option value="NR">Nitra</option>
<option value="NM">Nove Mesto nad Vahom</option>
<option value="NZ">Nove Zamky</option>
<option value="PE">Partizanske</option>
<option value="PK">Pezinok</option>
<option value="PN">Piestany</option>
<option value="PT">Poltar</option>
<option value="PP">Poprad</option>
<option value="PB">Povazska Bystrica</option>
<option value="PO">Presov</option>
<option value="PD">Prievidza</option>
<option value="PU">Puchov</option>
<option value="RA">Revuca</option>
<option value="RS">Rimavska Sobota</option>
<option value="RV">Roznava</option>
<option value="RK" selected="selected">Ruzomberok</option>
<option value="SB">Sabinov</option>
<option value="SC">Senec</option>
<option value="SE">Senica</option>
<option value="SI">Skalica</option>
<option value="SV">Snina</option>
<option value="SO">Sobrance</option>
<option value="SN">Spisska Nova Ves</option>
<option value="SL">Stara Lubovna</option>
<option value="SP">Stropkov</option>
<option value="SK">Svidnik</option>
<option value="SA">Sala</option>
<option value="TO">Topolcany</option>
<option value="TV">Trebisov</option>
<option value="TN">Trencin</option>
<option value="TT">Trnava</option>
<option value="TR">Turcianske Teplice</option>
<option value="TS">Tvrdosin</option>
<option value="VK">Velky Krtis</option>
<option value="VT">Vranov nad Toplou</option>
<option value="ZM">Zlate Moravce</option>
<option value="ZV">Zvolen</option>
<option value="ZC">Zarnovica</option>
<option value="ZH">Ziar nad Hronom</option>
<option value="ZA">Zilina</option>
</select>'''
        self.assertEqual(f.render('Districts', 'RK'), out)

    def test_SKPostalCodeField(self):
        error_format = [u'Enter a postal code in the format XXXXX or XXX XX.']
        valid = {
            '91909': '91909',
            '917 01': '91701',
        }
        invalid = {
            '84545x': error_format,
        }
        self.assertFieldOutput(SKPostalCodeField, valid, invalid)

from django.test import TestCase
from django.contrib.contenttypes.models import ContentType
from django.db.models import Q
from models import *

class GenericRelationTests(TestCase):

    def test_inherited_models_content_type(self):
        """
        Test that GenericRelations on inherited classes use the correct content
        type.
        """

        p = Place.objects.create(name="South Park")
        r = Restaurant.objects.create(name="Chubby's")
        l1 = Link.objects.create(content_object=p)
        l2 = Link.objects.create(content_object=r)
        self.assertEqual(list(p.links.all()), [l1])
        self.assertEqual(list(r.links.all()), [l2])

    def test_reverse_relation_pk(self):
        """
        Test that the correct column name is used for the primary key on the
        originating model of a query.  See #12664.
        """
        p = Person.objects.create(account=23, name='Chef')
        a = Address.objects.create(street='123 Anywhere Place',
                                   city='Conifer', state='CO',
                                   zipcode='80433', content_object=p)

        qs = Person.objects.filter(addresses__zipcode='80433')
        self.assertEqual(1, qs.count())
        self.assertEqual('Chef', qs[0].name)

    def test_charlink_delete(self):
        oddrel = OddRelation1.objects.create(name='clink')
        cl = CharLink.objects.create(content_object=oddrel)
        oddrel.delete()

    def test_textlink_delete(self):
        oddrel = OddRelation2.objects.create(name='tlink')
        tl = TextLink.objects.create(content_object=oddrel)
        oddrel.delete()

    def test_q_object_or(self):
        """
        Tests that SQL query parameters for generic relations are properly
        grouped when OR is used.

        Test for bug http://code.djangoproject.com/ticket/11535

        In this bug the first query (below) works while the second, with the
        query parameters the same but in reverse order, does not.

        The issue is that the generic relation conditions do not get properly
        grouped in parentheses.
        """
        note_contact = Contact.objects.create()
        org_contact = Contact.objects.create()
        note = Note.objects.create(note='note', content_object=note_contact)
        org = Organization.objects.create(name='org name')
        org.contacts.add(org_contact)
        # search with a non-matching note and a matching org name
        qs = Contact.objects.filter(Q(notes__note__icontains=r'other note') |
                            Q(organizations__name__icontains=r'org name'))
        self.assertTrue(org_contact in qs)
        # search again, with the same query parameters, in reverse order
        qs = Contact.objects.filter(
            Q(organizations__name__icontains=r'org name') |
            Q(notes__note__icontains=r'other note'))
        self.assertTrue(org_contact in qs)




import os
import re
import shutil
from django.test import TestCase
from django.core import management

LOCALE='de'

class ExtractorTests(TestCase):

    PO_FILE='locale/%s/LC_MESSAGES/django.po' % LOCALE

    def setUp(self):
        self._cwd = os.getcwd()
        self.test_dir = os.path.abspath(os.path.dirname(__file__))

    def _rmrf(self, dname):
        if os.path.commonprefix([self.test_dir, os.path.abspath(dname)]) != self.test_dir:
            return
        shutil.rmtree(dname)

    def tearDown(self):
        os.chdir(self.test_dir)
        try:
            self._rmrf('locale/%s' % LOCALE)
        except OSError:
            pass
        os.chdir(self._cwd)

    def assertMsgId(self, msgid, s):
        return self.assert_(re.search('^msgid "%s"' % msgid, s, re.MULTILINE))

    def assertNotMsgId(self, msgid, s):
        return self.assert_(not re.search('^msgid "%s"' % msgid, s, re.MULTILINE))


class TemplateExtractorTests(ExtractorTests):

    def test_templatize(self):
        os.chdir(self.test_dir)
        management.call_command('makemessages', locale=LOCALE, verbosity=0)
        self.assert_(os.path.exists(self.PO_FILE))
        po_contents = open(self.PO_FILE, 'r').read()
        self.assertMsgId('I think that 100%% is more that 50%% of anything.', po_contents)
        self.assertMsgId('I think that 100%% is more that 50%% of %\(obj\)s.', po_contents)


class JavascriptExtractorTests(ExtractorTests):

    PO_FILE='locale/%s/LC_MESSAGES/djangojs.po' % LOCALE

    def test_javascript_literals(self):
        os.chdir(self.test_dir)
        management.call_command('makemessages', domain='djangojs', locale=LOCALE, verbosity=0)
        self.assert_(os.path.exists(self.PO_FILE))
        po_contents = open(self.PO_FILE, 'r').read()
        self.assertMsgId('This literal should be included.', po_contents)
        self.assertMsgId('This one as well.', po_contents)


class IgnoredExtractorTests(ExtractorTests):

    def test_ignore_option(self):
        os.chdir(self.test_dir)
        management.call_command('makemessages', locale=LOCALE, verbosity=0, ignore_patterns=['ignore_dir/*'])
        self.assert_(os.path.exists(self.PO_FILE))
        po_contents = open(self.PO_FILE, 'r').read()
        self.assertMsgId('This literal should be included.', po_contents)
        self.assertNotMsgId('This should be ignored.', po_contents)


class SymlinkExtractorTests(ExtractorTests):

    def setUp(self):
        self._cwd = os.getcwd()
        self.test_dir = os.path.abspath(os.path.dirname(__file__))
        self.symlinked_dir = os.path.join(self.test_dir, 'templates_symlinked')

    def tearDown(self):
        super(SymlinkExtractorTests, self).tearDown()
        os.chdir(self.test_dir)
        try:
            os.remove(self.symlinked_dir)
        except OSError:
            pass
        os.chdir(self._cwd)

    def test_symlink(self):
        if hasattr(os, 'symlink'):
            if os.path.exists(self.symlinked_dir):
                self.assert_(os.path.islink(self.symlinked_dir))
            else:
                os.symlink(os.path.join(self.test_dir, 'templates'), self.symlinked_dir)
            os.chdir(self.test_dir)
            management.call_command('makemessages', locale=LOCALE, verbosity=0, symlinks=True)
            self.assert_(os.path.exists(self.PO_FILE))
            po_contents = open(self.PO_FILE, 'r').read()
            self.assertMsgId('This literal should be included.', po_contents)
            self.assert_('templates_symlinked/test.html' in po_contents)


class CopyPluralFormsExtractorTests(ExtractorTests):

    def test_copy_plural_forms(self):
        os.chdir(self.test_dir)
        management.call_command('makemessages', locale=LOCALE, verbosity=0)
        self.assert_(os.path.exists(self.PO_FILE))
        po_contents = open(self.PO_FILE, 'r').read()
        self.assert_('Plural-Forms: nplurals=2; plural=(n != 1)' in po_contents)

# coding: utf-8
from datetime import date

from django.db import models
from django.contrib.auth.models import User

class Band(models.Model):
    name = models.CharField(max_length=100)
    bio = models.TextField()
    sign_date = models.DateField()
    
    def __unicode__(self):
        return self.name

class Concert(models.Model):
    main_band = models.ForeignKey(Band, related_name='main_concerts')
    opening_band = models.ForeignKey(Band, related_name='opening_concerts',
        blank=True)
    day = models.CharField(max_length=3, choices=((1, 'Fri'), (2, 'Sat')))
    transport = models.CharField(max_length=100, choices=(
        (1, 'Plane'),
        (2, 'Train'),
        (3, 'Bus')
    ), blank=True)

class ValidationTestModel(models.Model):
    name = models.CharField(max_length=100)
    slug = models.SlugField()
    users = models.ManyToManyField(User)
    state = models.CharField(max_length=2, choices=(("CO", "Colorado"), ("WA", "Washington")))
    is_active = models.BooleanField()
    pub_date = models.DateTimeField()
    band = models.ForeignKey(Band)

class ValidationTestInlineModel(models.Model):
    parent = models.ForeignKey(ValidationTestModel)

"""
Tests for django.core.servers.
"""

import os

import django
from django.test import TestCase
from django.core.handlers.wsgi import WSGIHandler
from django.core.servers.basehttp import AdminMediaHandler


class AdminMediaHandlerTests(TestCase):

    def setUp(self):
        self.admin_media_file_path = os.path.abspath(
            os.path.join(django.__path__[0], 'contrib', 'admin', 'media')
        )
        self.handler = AdminMediaHandler(WSGIHandler())

    def test_media_urls(self):
        """
        Tests that URLs that look like absolute file paths after the
        settings.ADMIN_MEDIA_PREFIX don't turn into absolute file paths.
        """
        # Cases that should work on all platforms.
        data = (
            ('/media/css/base.css', ('css', 'base.css')),
        )
        # Cases that should raise an exception.
        bad_data = ()

        # Add platform-specific cases.
        if os.sep == '/':
            data += (
                # URL, tuple of relative path parts.
                ('/media/\\css/base.css', ('\\css', 'base.css')),
            )
            bad_data += (
                '/media//css/base.css',
                '/media////css/base.css',
                '/media/../css/base.css',
            )
        elif os.sep == '\\':
            bad_data += (
                '/media/C:\css/base.css',
                '/media//\\css/base.css',
                '/media/\\css/base.css',
                '/media/\\\\css/base.css'
            )
        for url, path_tuple in data:
            try:
                output = self.handler.file_path(url)
            except ValueError:
                self.fail("Got a ValueError exception, but wasn't expecting"
                          " one. URL was: %s" % url)
            rel_path = os.path.join(*path_tuple)
            desired = os.path.normcase(
                os.path.join(self.admin_media_file_path, rel_path))
            self.assertEqual(output, desired,
                "Got: %s, Expected: %s, URL was: %s" % (output, desired, url))
        for url in bad_data:
            try:
                output = self.handler.file_path(url)
            except ValueError:
                continue
            self.fail('URL: %s should have caused a ValueError exception.'
                      % url)

from django.test import TestCase

class DecoratorFromMiddlewareTests(TestCase):
    """
    Tests for view decorators created using
    ``django.utils.decorators.decorator_from_middleware``.
    """

    def test_process_view_middleware(self):
        """
        Test a middleware that implements process_view.
        """
        self.client.get('/utils/xview/')

    def test_callable_process_view_middleware(self):
        """
        Test a middleware that implements process_view, operating on a callable class.
        """
        self.client.get('/utils/class_xview/')

# -*- encoding: utf-8 -*-
# This file is distributed under the same license as the Django package.
#

DATE_FORMAT = 'j F Y'
TIME_FORMAT = 'H:i:s'
DATETIME_FORMAT = 'j F Y, H:i:s'
YEAR_MONTH_FORMAT = 'F Y'
MONTH_DAY_FORMAT = 'j F'
SHORT_DATE_FORMAT = 'd.m.Y'
SHORT_DATETIME_FORMAT = 'd.m.Y, H:i:s'
# FIRST_DAY_OF_WEEK = 
# DATE_INPUT_FORMATS = 
# TIME_INPUT_FORMATS = 
# DATETIME_INPUT_FORMATS = 
DECIMAL_SEPARATOR = ','
THOUSAND_SEPARATOR = '.'
# NUMBER_GROUPING = 

"""
A few bits of helper functions for comment views.
"""

import urllib
import textwrap
from django.http import HttpResponseRedirect
from django.core import urlresolvers
from django.shortcuts import render_to_response
from django.template import RequestContext
from django.core.exceptions import ObjectDoesNotExist
from django.contrib import comments

def next_redirect(data, default, default_view, **get_kwargs):
    """
    Handle the "where should I go next?" part of comment views.

    The next value could be a kwarg to the function (``default``), or a
    ``?next=...`` GET arg, or the URL of a given view (``default_view``). See
    the view modules for examples.

    Returns an ``HttpResponseRedirect``.
    """
    next = data.get("next", default)
    if next is None:
        next = urlresolvers.reverse(default_view)
    if get_kwargs:
        if '#' in next:
            tmp = next.rsplit('#', 1)
            next = tmp[0]
            anchor = '#' + tmp[1]
        else:
            anchor = ''

        joiner = ('?' in next) and '&' or '?'
        next += joiner + urllib.urlencode(get_kwargs) + anchor
    return HttpResponseRedirect(next)

def confirmation_view(template, doc="Display a confirmation view."):
    """
    Confirmation view generator for the "comment was
    posted/flagged/deleted/approved" views.
    """
    def confirmed(request):
        comment = None
        if 'c' in request.GET:
            try:
                comment = comments.get_model().objects.get(pk=request.GET['c'])
            except (ObjectDoesNotExist, ValueError):
                pass
        return render_to_response(template,
            {'comment': comment},
            context_instance=RequestContext(request)
        )

    confirmed.__doc__ = textwrap.dedent("""\
        %s

        Templates: `%s``
        Context:
            comment
                The posted comment
        """ % (doc, template)
    )
    return confirmed

"""
Module for executing all of the GDAL tests.  None
of these tests require the use of the database.
"""
from django.utils.unittest import TestSuite, TextTestRunner

# Importing the GDAL test modules.
import test_driver, test_ds, test_envelope, test_geom, test_srs

test_suites = [test_driver.suite(),
               test_ds.suite(),
               test_envelope.suite(),
               test_geom.suite(),
               test_srs.suite(),
               ]

def suite():
    "Builds a test suite for the GDAL tests."
    s = TestSuite()
    map(s.addTest, test_suites)
    return s

def run(verbosity=1):
    "Runs the GDAL tests."
    TextTestRunner(verbosity=verbosity).run(suite())

"""
Belgium-specific Form helpers
"""
import re

from django.core.validators import EMPTY_VALUES
from django.forms import ValidationError
from django.forms.fields import RegexField, Select
from django.utils.translation import ugettext_lazy as _

class BEPostalCodeField(RegexField):
    """
    A form field that validates its input as a belgium postal code.
    
    Belgium postal code is a 4 digits string. The first digit indicates
    the province (except for the 3ddd numbers that are shared by the
    eastern part of Flemish Brabant and Limburg and the and 1ddd that
    are shared by the Brussels Capital Region, the western part of
    Flemish Brabant and Walloon Brabant)
    """
    default_error_messages = {
        'invalid': _(
            'Enter a valid postal code in the range and format 1XXX - 9XXX.'),
    }

    def __init__(self, *args, **kwargs):
        super(BEPostalCodeField, self).__init__(r'^[1-9]\d{3}$',
                max_length=None, min_length=None, *args, **kwargs)

class BEPhoneNumberField(RegexField):
    """
    A form field that validates its input as a belgium phone number.

    Landlines have a seven-digit subscriber number and a one-digit area code,
    while smaller cities have a six-digit subscriber number and a two-digit 
    area code. Cell phones have a six-digit subscriber number and a two-digit 
    area code preceeded by the number 4.
    0d ddd dd dd, 0d/ddd.dd.dd, 0d.ddd.dd.dd, 
    0dddddddd - dialling a bigger city
    0dd dd dd dd, 0dd/dd.dd.dd, 0dd.dd.dd.dd, 
    0dddddddd - dialling a smaller city
    04dd ddd dd dd, 04dd/ddd.dd.dd, 
    04dd.ddd.dd.dd, 04ddddddddd - dialling a mobile number
    """
    default_error_messages = {
        'invalid': _('Enter a valid phone number in one of the formats '
                     '0x xxx xx xx, 0xx xx xx xx, 04xx xx xx xx, '
                     '0x/xxx.xx.xx, 0xx/xx.xx.xx, 04xx/xx.xx.xx, '
                     '0x.xxx.xx.xx, 0xx.xx.xx.xx, 04xx.xx.xx.xx, '
                     '0xxxxxxxx or 04xxxxxxxx.'),
    }

    def __init__(self, *args, **kwargs):
        super(BEPhoneNumberField, self).__init__(r'^[0]\d{1}[/. ]?\d{3}[. ]\d{2}[. ]?\d{2}$|^[0]\d{2}[/. ]?\d{2}[. ]?\d{2}[. ]?\d{2}$|^[0][4]\d{2}[/. ]?\d{2}[. ]?\d{2}[. ]?\d{2}$',
            max_length=None, min_length=None, *args, **kwargs)

class BERegionSelect(Select):
    """
    A Select widget that uses a list of belgium regions as its choices.
    """
    def __init__(self, attrs=None):
        from be_regions import REGION_CHOICES
        super(BERegionSelect, self).__init__(attrs, choices=REGION_CHOICES)

class BEProvinceSelect(Select):
    """
    A Select widget that uses a list of belgium provinces as its choices.
    """
    def __init__(self, attrs=None):
        from be_provinces import PROVINCE_CHOICES
        super(BEProvinceSelect, self).__init__(attrs, choices=PROVINCE_CHOICES)

from django.conf import settings
from django.contrib.sites.models import Site, RequestSite, get_current_site
from django.core.exceptions import ObjectDoesNotExist
from django.http import HttpRequest
from django.test import TestCase


class SitesFrameworkTests(TestCase):

    def setUp(self):
        Site(id=settings.SITE_ID, domain="example.com", name="example.com").save()
        self.old_Site_meta_installed = Site._meta.installed
        Site._meta.installed = True

    def tearDown(self):
        Site._meta.installed = self.old_Site_meta_installed

    def test_site_manager(self):
        # Make sure that get_current() does not return a deleted Site object.
        s = Site.objects.get_current()
        self.assertTrue(isinstance(s, Site))
        s.delete()
        self.assertRaises(ObjectDoesNotExist, Site.objects.get_current)

    def test_site_cache(self):
        # After updating a Site object (e.g. via the admin), we shouldn't return a
        # bogus value from the SITE_CACHE.
        site = Site.objects.get_current()
        self.assertEqual(u"example.com", site.name)
        s2 = Site.objects.get(id=settings.SITE_ID)
        s2.name = "Example site"
        s2.save()
        site = Site.objects.get_current()
        self.assertEqual(u"Example site", site.name)

    def test_get_current_site(self):
        # Test that the correct Site object is returned
        request = HttpRequest()
        request.META = {
            "SERVER_NAME": "example.com",
            "SERVER_PORT": "80",
        }
        site = get_current_site(request)
        self.assertTrue(isinstance(site, Site))
        self.assertEqual(site.id, settings.SITE_ID)

        # Test that an exception is raised if the sites framework is installed
        # but there is no matching Site
        site.delete()
        self.assertRaises(ObjectDoesNotExist, get_current_site, request)

        # A RequestSite is returned if the sites framework is not installed
        Site._meta.installed = False
        site = get_current_site(request)
        self.assertTrue(isinstance(site, RequestSite))
        self.assertEqual(site.name, u"example.com")

from django.conf import settings
from django.core.exceptions import ObjectDoesNotExist, ImproperlyConfigured
from django.db import connection
from django.db.models.loading import get_apps, get_app, get_models, get_model, register_models
from django.db.models.query import Q
from django.db.models.expressions import F
from django.db.models.manager import Manager
from django.db.models.base import Model
from django.db.models.aggregates import *
from django.db.models.fields import *
from django.db.models.fields.subclassing import SubfieldBase
from django.db.models.fields.files import FileField, ImageField
from django.db.models.fields.related import ForeignKey, OneToOneField, ManyToManyField, ManyToOneRel, ManyToManyRel, OneToOneRel
from django.db.models.deletion import CASCADE, PROTECT, SET, SET_NULL, SET_DEFAULT, DO_NOTHING, ProtectedError
from django.db.models import signals
from django.utils.decorators import wraps

# Admin stages.
ADD, CHANGE, BOTH = 1, 2, 3

def permalink(func):
    """
    Decorator that calls urlresolvers.reverse() to return a URL using
    parameters returned by the decorated function "func".

    "func" should be a function that returns a tuple in one of the
    following formats:
        (viewname, viewargs)
        (viewname, viewargs, viewkwargs)
    """
    from django.core.urlresolvers import reverse
    @wraps(func)
    def inner(*args, **kwargs):
        bits = func(*args, **kwargs)
        return reverse(bits[0], None, *bits[1:3])
    return inner

"""
Django's standard crypto functions and utilities.
"""
import hmac

from django.conf import settings
from django.utils.hashcompat import sha_constructor, sha_hmac


def salted_hmac(key_salt, value, secret=None):
    """
    Returns the HMAC-SHA1 of 'value', using a key generated from key_salt and a
    secret (which defaults to settings.SECRET_KEY).

    A different key_salt should be passed in for every application of HMAC.
    """
    if secret is None:
        secret = settings.SECRET_KEY

    # We need to generate a derived key from our base key.  We can do this by
    # passing the key_salt and our base key through a pseudo-random function and
    # SHA1 works nicely.

    key = sha_constructor(key_salt + secret).digest()

    # If len(key_salt + secret) > sha_constructor().block_size, the above
    # line is redundant and could be replaced by key = key_salt + secret, since
    # the hmac module does the same thing for keys longer than the block size.
    # However, we need to ensure that we *always* do this.

    return hmac.new(key, msg=value, digestmod=sha_hmac)


def constant_time_compare(val1, val2):
    """
    Returns True if the two strings are equal, False otherwise.

    The time taken is independent of the number of characters that match.
    """
    if len(val1) != len(val2):
        return False
    result = 0
    for x, y in zip(val1, val2):
        result |= ord(x) ^ ord(y)
    return result == 0

try:
    from functools import wraps
except ImportError:
    from django.utils.functional import wraps  # Python 2.4 fallback.

from django.utils.decorators import decorator_from_middleware_with_args, available_attrs
from django.utils.cache import patch_cache_control, add_never_cache_headers
from django.middleware.cache import CacheMiddleware


def cache_page(*args, **kwargs):
    """
    Decorator for views that tries getting the page from the cache and
    populates the cache if the page isn't in the cache yet.

    The cache is keyed by the URL and some data from the headers.
    Additionally there is the key prefix that is used to distinguish different
    cache areas in a multi-site setup. You could use the
    sites.get_current().domain, for example, as that is unique across a Django
    project.

    Additionally, all headers from the response's Vary header will be taken
    into account on caching -- just like the middleware does.
    """
    # We need backwards compatibility with code which spells it this way:
    #   def my_view(): pass
    #   my_view = cache_page(my_view, 123)
    # and this way:
    #   my_view = cache_page(123)(my_view)
    # and this:
    #   my_view = cache_page(my_view, 123, key_prefix="foo")
    # and this:
    #   my_view = cache_page(123, key_prefix="foo")(my_view)
    # and possibly this way (?):
    #   my_view = cache_page(123, my_view)
    # and also this way:
    #   my_view = cache_page(my_view)
    # and also this way:
    #   my_view = cache_page()(my_view)

    # We also add some asserts to give better error messages in case people are
    # using other ways to call cache_page that no longer work.
    cache_alias = kwargs.pop('cache', None)
    key_prefix = kwargs.pop('key_prefix', None)
    assert not kwargs, "The only keyword arguments are cache and key_prefix"
    if len(args) > 1:
        assert len(args) == 2, "cache_page accepts at most 2 arguments"
        if callable(args[0]):
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[1], cache_alias=cache_alias, key_prefix=key_prefix)(args[0])
        elif callable(args[1]):
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)(args[1])
        else:
            assert False, "cache_page must be passed a view function if called with two arguments"
    elif len(args) == 1:
        if callable(args[0]):
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)(args[0])
        else:
            return decorator_from_middleware_with_args(CacheMiddleware)(cache_timeout=args[0], cache_alias=cache_alias, key_prefix=key_prefix)
    else:
        return decorator_from_middleware_with_args(CacheMiddleware)(cache_alias=cache_alias, key_prefix=key_prefix)


def cache_control(**kwargs):
    def _cache_controller(viewfunc):
        def _cache_controlled(request, *args, **kw):
            response = viewfunc(request, *args, **kw)
            patch_cache_control(response, **kwargs)
            return response
        return wraps(viewfunc, assigned=available_attrs(viewfunc))(_cache_controlled)
    return _cache_controller


def never_cache(view_func):
    """
    Decorator that adds headers to a response so that it will
    never be cached.
    """
    def _wrapped_view_func(request, *args, **kwargs):
        response = view_func(request, *args, **kwargs)
        add_never_cache_headers(response)
        return response
    return wraps(view_func, assigned=available_attrs(view_func))(_wrapped_view_func)

import shutil
import sys

from django.core.cache import cache
from django.core.files.base import ContentFile
from django.core.files.uploadedfile import SimpleUploadedFile
from django.test import TestCase

from models import Storage, temp_storage, temp_storage_location
if sys.version_info >= (2, 5):
    from tests_25 import FileObjTests


class FileTests(TestCase):
    def tearDown(self):
        shutil.rmtree(temp_storage_location)

    def test_files(self):
        # Attempting to access a FileField from the class raises a descriptive
        # error
        self.assertRaises(AttributeError, lambda: Storage.normal)

        # An object without a file has limited functionality.
        obj1 = Storage()
        self.assertEqual(obj1.normal.name, "")
        self.assertRaises(ValueError, lambda: obj1.normal.size)

        # Saving a file enables full functionality.
        obj1.normal.save("django_test.txt", ContentFile("content"))
        self.assertEqual(obj1.normal.name, "tests/django_test.txt")
        self.assertEqual(obj1.normal.size, 7)
        self.assertEqual(obj1.normal.read(), "content")
        obj1.normal.close()

        # File objects can be assigned to FileField attributes, but shouldn't
        # get committed until the model it's attached to is saved.
        obj1.normal = SimpleUploadedFile("assignment.txt", "content")
        dirs, files = temp_storage.listdir("tests")
        self.assertEqual(dirs, [])
        self.assertEqual(sorted(files), ["default.txt", "django_test.txt"])

        obj1.save()
        dirs, files = temp_storage.listdir("tests")
        self.assertEqual(
            sorted(files), ["assignment.txt", "default.txt", "django_test.txt"]
        )

        # Files can be read in a little at a time, if necessary.
        obj1.normal.open()
        self.assertEqual(obj1.normal.read(3), "con")
        self.assertEqual(obj1.normal.read(), "tent")
        self.assertEqual(list(obj1.normal.chunks(chunk_size=2)), ["co", "nt", "en", "t"])
        obj1.normal.close()

        # Save another file with the same name.
        obj2 = Storage()
        obj2.normal.save("django_test.txt", ContentFile("more content"))
        self.assertEqual(obj2.normal.name, "tests/django_test_1.txt")
        self.assertEqual(obj2.normal.size, 12)

        # Push the objects into the cache to make sure they pickle properly
        cache.set("obj1", obj1)
        cache.set("obj2", obj2)
        self.assertEqual(cache.get("obj2").normal.name, "tests/django_test_1.txt")

        # Deleting an object does not delete the file it uses.
        obj2.delete()
        obj2.normal.save("django_test.txt", ContentFile("more content"))
        self.assertEqual(obj2.normal.name, "tests/django_test_2.txt")

        # Multiple files with the same name get _N appended to them.
        objs = [Storage() for i in range(3)]
        for o in objs:
            o.normal.save("multiple_files.txt", ContentFile("Same Content"))
        self.assertEqual(
            [o.normal.name for o in objs],
            ["tests/multiple_files.txt", "tests/multiple_files_1.txt", "tests/multiple_files_2.txt"]
        )
        for o in objs:
            o.delete()

        # Default values allow an object to access a single file.
        obj3 = Storage.objects.create()
        self.assertEqual(obj3.default.name, "tests/default.txt")
        self.assertEqual(obj3.default.read(), "default content")
        obj3.default.close()

        # But it shouldn't be deleted, even if there are no more objects using
        # it.
        obj3.delete()
        obj3 = Storage()
        self.assertEqual(obj3.default.read(), "default content")
        obj3.default.close()

        # Verify the fix for #5655, making sure the directory is only
        # determined once.
        obj4 = Storage()
        obj4.random.save("random_file", ContentFile("random content"))
        self.assertTrue(obj4.random.name.endswith("/random_file"))

        # Clean up the temporary files and dir.
        obj1.normal.delete()
        obj2.normal.delete()
        obj3.default.delete()
        obj4.random.delete()

from datetime import datetime
from django.core.exceptions import ValidationError
from django.db import models
from django.test import TestCase


def validate_answer_to_universe(value):
    if value != 42:
        raise ValidationError('This is not the answer to life, universe and everything!', code='not42')

class ModelToValidate(models.Model):
    name = models.CharField(max_length=100)
    created = models.DateTimeField(default=datetime.now)
    number = models.IntegerField(db_column='number_val')
    parent = models.ForeignKey('self', blank=True, null=True, limit_choices_to={'number': 10})
    email = models.EmailField(blank=True)
    url = models.URLField(blank=True)
    url_verify = models.URLField(blank=True, verify_exists=True)
    f_with_custom_validator = models.IntegerField(blank=True, null=True, validators=[validate_answer_to_universe])

    def clean(self):
        super(ModelToValidate, self).clean()
        if self.number == 11:
            raise ValidationError('Invalid number supplied!')

class UniqueFieldsModel(models.Model):
    unique_charfield = models.CharField(max_length=100, unique=True)
    unique_integerfield = models.IntegerField(unique=True)
    non_unique_field = models.IntegerField()

class CustomPKModel(models.Model):
    my_pk_field = models.CharField(max_length=100, primary_key=True)

class UniqueTogetherModel(models.Model):
    cfield = models.CharField(max_length=100)
    ifield = models.IntegerField()
    efield = models.EmailField()

    class Meta:
        unique_together = (('ifield', 'cfield',), ['ifield', 'efield'])

class UniqueForDateModel(models.Model):
    start_date = models.DateField()
    end_date = models.DateTimeField()
    count = models.IntegerField(unique_for_date="start_date", unique_for_year="end_date")
    order = models.IntegerField(unique_for_month="end_date")
    name = models.CharField(max_length=100)

class CustomMessagesModel(models.Model):
    other  = models.IntegerField(blank=True, null=True)
    number = models.IntegerField(db_column='number_val',
        error_messages={'null': 'NULL', 'not42': 'AAARGH', 'not_equal': '%s != me'},
        validators=[validate_answer_to_universe]
    )

class Author(models.Model):
    name = models.CharField(max_length=100)

class Article(models.Model):
    title = models.CharField(max_length=100)
    author = models.ForeignKey(Author)
    pub_date = models.DateTimeField(blank=True)

    def clean(self):
        if self.pub_date is None:
            self.pub_date = datetime.now()

class Post(models.Model):
    title = models.CharField(max_length=50, unique_for_date='posted', blank=True)
    slug = models.CharField(max_length=50, unique_for_year='posted', blank=True)
    subtitle = models.CharField(max_length=50, unique_for_month='posted', blank=True)
    posted = models.DateField()

    def __unicode__(self):
        return self.name

class FlexibleDatePost(models.Model):
    title = models.CharField(max_length=50, unique_for_date='posted', blank=True)
    slug = models.CharField(max_length=50, unique_for_year='posted', blank=True)
    subtitle = models.CharField(max_length=50, unique_for_month='posted', blank=True)
    posted = models.DateField(blank=True, null=True)

from django.contrib.contenttypes import generic
from django.contrib.contenttypes.models import ContentType
from django.db import models
from django.db import connection


class Square(models.Model):
    root = models.IntegerField()
    square = models.PositiveIntegerField()

    def __unicode__(self):
        return "%s ** 2 == %s" % (self.root, self.square)


class Person(models.Model):
    first_name = models.CharField(max_length=20)
    last_name = models.CharField(max_length=20)

    def __unicode__(self):
        return u'%s %s' % (self.first_name, self.last_name)


class SchoolClass(models.Model):
    year = models.PositiveIntegerField()
    day = models.CharField(max_length=9, blank=True)
    last_updated = models.DateTimeField()

# Unfortunately, the following model breaks MySQL hard.
# Until #13711 is fixed, this test can't be run under MySQL.
if connection.features.supports_long_model_names:
    class VeryLongModelNameZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ(models.Model):
        class Meta:
            # We need to use a short actual table name or
            # we hit issue #8548 which we're not testing!
            verbose_name = 'model_with_long_table_name'
        primary_key_is_quite_long_zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz = models.AutoField(primary_key=True)
        charfield_is_quite_long_zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz = models.CharField(max_length=100)
        m2m_also_quite_long_zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz = models.ManyToManyField(Person,blank=True)


class Tag(models.Model):
    name = models.CharField(max_length=30)
    content_type = models.ForeignKey(ContentType, related_name='backend_tags')
    object_id = models.PositiveIntegerField()
    content_object = generic.GenericForeignKey('content_type', 'object_id')


class Post(models.Model):
    name = models.CharField(max_length=30)
    text = models.TextField()
    tags = generic.GenericRelation('Tag')

    class Meta:
        db_table = 'CaseSensitive_Post'


class Reporter(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=30)

    def __unicode__(self):
        return u"%s %s" % (self.first_name, self.last_name)


class Article(models.Model):
    headline = models.CharField(max_length=100)
    pub_date = models.DateField()
    reporter = models.ForeignKey(Reporter)

    def __unicode__(self):
        return self.headline

from django.core.exceptions import FieldError
from django.test import TestCase

from models import (SelfRefer, Tag, TagCollection, Entry, SelfReferChild,
    SelfReferChildSibling, Worksheet)


class M2MRegressionTests(TestCase):
    def test_multiple_m2m(self):
        # Multiple m2m references to model must be distinguished when
        # accessing the relations through an instance attribute.

        s1 = SelfRefer.objects.create(name='s1')
        s2 = SelfRefer.objects.create(name='s2')
        s3 = SelfRefer.objects.create(name='s3')
        s1.references.add(s2)
        s1.related.add(s3)

        e1 = Entry.objects.create(name='e1')
        t1 = Tag.objects.create(name='t1')
        t2 = Tag.objects.create(name='t2')

        e1.topics.add(t1)
        e1.related.add(t2)

        self.assertQuerysetEqual(s1.references.all(), ["<SelfRefer: s2>"])
        self.assertQuerysetEqual(s1.related.all(), ["<SelfRefer: s3>"])

        self.assertQuerysetEqual(e1.topics.all(), ["<Tag: t1>"])
        self.assertQuerysetEqual(e1.related.all(), ["<Tag: t2>"])

    def test_internal_related_name_not_in_error_msg(self):
        # The secret internal related names for self-referential many-to-many
        # fields shouldn't appear in the list when an error is made.

        self.assertRaisesRegexp(FieldError,
            "Choices are: id, name, references, related, selfreferchild, selfreferchildsibling$",
            lambda: SelfRefer.objects.filter(porcupine='fred')
        )

    def test_m2m_inheritance_symmetry(self):
        # Test to ensure that the relationship between two inherited models
        # with a self-referential m2m field maintains symmetry

        sr_child = SelfReferChild(name="Hanna")
        sr_child.save()

        sr_sibling = SelfReferChildSibling(name="Beth")
        sr_sibling.save()
        sr_child.related.add(sr_sibling)

        self.assertQuerysetEqual(sr_child.related.all(), ["<SelfRefer: Beth>"])
        self.assertQuerysetEqual(sr_sibling.related.all(), ["<SelfRefer: Hanna>"])

    def test_m2m_pk_field_type(self):
        # Regression for #11311 - The primary key for models in a m2m relation
        # doesn't have to be an AutoField

        w = Worksheet(id='abc')
        w.save()
        w.delete()

    def test_add_m2m_with_base_class(self):
        # Regression for #11956 -- You can add an object to a m2m with the
        # base class without causing integrity errors

        t1 = Tag.objects.create(name='t1')
        t2 = Tag.objects.create(name='t2')

        c1 = TagCollection.objects.create(name='c1')
        c1.tags = [t1,t2]
        c1 = TagCollection.objects.get(name='c1')

        self.assertQuerysetEqual(c1.tags.all(), ["<Tag: t1>", "<Tag: t2>"])
        self.assertQuerysetEqual(t1.tag_collections.all(), ["<TagCollection: c1>"])

import time
from datetime import datetime, timedelta
from StringIO import StringIO

from django.conf import settings
from django.core.handlers.modpython import ModPythonRequest
from django.core.handlers.wsgi import WSGIRequest, LimitedStream
from django.http import HttpRequest, HttpResponse, parse_cookie
from django.utils import unittest
from django.utils.http import cookie_date


class RequestsTests(unittest.TestCase):
    def test_httprequest(self):
        request = HttpRequest()
        self.assertEqual(request.GET.keys(), [])
        self.assertEqual(request.POST.keys(), [])
        self.assertEqual(request.COOKIES.keys(), [])
        self.assertEqual(request.META.keys(), [])

    def test_wsgirequest(self):
        request = WSGIRequest({'PATH_INFO': 'bogus', 'REQUEST_METHOD': 'bogus', 'wsgi.input': StringIO('')})
        self.assertEqual(request.GET.keys(), [])
        self.assertEqual(request.POST.keys(), [])
        self.assertEqual(request.COOKIES.keys(), [])
        self.assertEqual(set(request.META.keys()), set(['PATH_INFO', 'REQUEST_METHOD', 'SCRIPT_NAME', 'wsgi.input']))
        self.assertEqual(request.META['PATH_INFO'], 'bogus')
        self.assertEqual(request.META['REQUEST_METHOD'], 'bogus')
        self.assertEqual(request.META['SCRIPT_NAME'], '')

    def test_modpythonrequest(self):
        class FakeModPythonRequest(ModPythonRequest):
           def __init__(self, *args, **kwargs):
               super(FakeModPythonRequest, self).__init__(*args, **kwargs)
               self._get = self._post = self._meta = self._cookies = {}

        class Dummy:
            def get_options(self):
                return {}

        req = Dummy()
        req.uri = 'bogus'
        request = FakeModPythonRequest(req)
        self.assertEqual(request.path, 'bogus')
        self.assertEqual(request.GET.keys(), [])
        self.assertEqual(request.POST.keys(), [])
        self.assertEqual(request.COOKIES.keys(), [])
        self.assertEqual(request.META.keys(), [])

    def test_parse_cookie(self):
        self.assertEqual(parse_cookie('invalid:key=true'), {})

    def test_httprequest_location(self):
        request = HttpRequest()
        self.assertEqual(request.build_absolute_uri(location="https://www.example.com/asdf"),
            'https://www.example.com/asdf')

        request.get_host = lambda: 'www.example.com'
        request.path = ''
        self.assertEqual(request.build_absolute_uri(location="/path/with:colons"),
            'http://www.example.com/path/with:colons')

    def test_http_get_host(self):
        old_USE_X_FORWARDED_HOST = settings.USE_X_FORWARDED_HOST
        try:
            settings.USE_X_FORWARDED_HOST = False

            # Check if X_FORWARDED_HOST is provided.
            request = HttpRequest()
            request.META = {
                u'HTTP_X_FORWARDED_HOST': u'forward.com',
                u'HTTP_HOST': u'example.com',
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            # X_FORWARDED_HOST is ignored.
            self.assertEqual(request.get_host(), 'example.com')

            # Check if X_FORWARDED_HOST isn't provided.
            request = HttpRequest()
            request.META = {
                u'HTTP_HOST': u'example.com',
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            self.assertEqual(request.get_host(), 'example.com')

            # Check if HTTP_HOST isn't provided.
            request = HttpRequest()
            request.META = {
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            self.assertEqual(request.get_host(), 'internal.com')

            # Check if HTTP_HOST isn't provided, and we're on a nonstandard port
            request = HttpRequest()
            request.META = {
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 8042,
            }
            self.assertEqual(request.get_host(), 'internal.com:8042')

        finally:
            settings.USE_X_FORWARDED_HOST = old_USE_X_FORWARDED_HOST

    def test_http_get_host_with_x_forwarded_host(self):
        old_USE_X_FORWARDED_HOST = settings.USE_X_FORWARDED_HOST
        try:
            settings.USE_X_FORWARDED_HOST = True

            # Check if X_FORWARDED_HOST is provided.
            request = HttpRequest()
            request.META = {
                u'HTTP_X_FORWARDED_HOST': u'forward.com',
                u'HTTP_HOST': u'example.com',
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            # X_FORWARDED_HOST is obeyed.
            self.assertEqual(request.get_host(), 'forward.com')

            # Check if X_FORWARDED_HOST isn't provided.
            request = HttpRequest()
            request.META = {
                u'HTTP_HOST': u'example.com',
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            self.assertEqual(request.get_host(), 'example.com')

            # Check if HTTP_HOST isn't provided.
            request = HttpRequest()
            request.META = {
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 80,
            }
            self.assertEqual(request.get_host(), 'internal.com')

            # Check if HTTP_HOST isn't provided, and we're on a nonstandard port
            request = HttpRequest()
            request.META = {
                u'SERVER_NAME': u'internal.com',
                u'SERVER_PORT': 8042,
            }
            self.assertEqual(request.get_host(), 'internal.com:8042')

        finally:
            settings.USE_X_FORWARDED_HOST = old_USE_X_FORWARDED_HOST

    def test_near_expiration(self):
        "Cookie will expire when an near expiration time is provided"
        response = HttpResponse()
        # There is a timing weakness in this test; The
        # expected result for max-age requires that there be
        # a very slight difference between the evaluated expiration
        # time, and the time evaluated in set_cookie(). If this
        # difference doesn't exist, the cookie time will be
        # 1 second larger. To avoid the problem, put in a quick sleep,
        # which guarantees that there will be a time difference.
        expires = datetime.utcnow() + timedelta(seconds=10)
        time.sleep(0.001)
        response.set_cookie('datetime', expires=expires)
        datetime_cookie = response.cookies['datetime']
        self.assertEqual(datetime_cookie['max-age'], 10)

    def test_far_expiration(self):
        "Cookie will expire when an distant expiration time is provided"
        response = HttpResponse()
        response.set_cookie('datetime', expires=datetime(2028, 1, 1, 4, 5, 6))
        datetime_cookie = response.cookies['datetime']
        self.assertEqual(datetime_cookie['expires'], 'Sat, 01-Jan-2028 04:05:06 GMT')

    def test_max_age_expiration(self):
        "Cookie will expire if max_age is provided"
        response = HttpResponse()
        response.set_cookie('max_age', max_age=10)
        max_age_cookie = response.cookies['max_age']
        self.assertEqual(max_age_cookie['max-age'], 10)
        self.assertEqual(max_age_cookie['expires'], cookie_date(time.time()+10))

    def test_httponly_cookie(self):
        response = HttpResponse()
        response.set_cookie('example', httponly=True)
        example_cookie = response.cookies['example']
        # A compat cookie may be in use -- check that it has worked
        # both as an output string, and using the cookie attributes
        self.assertTrue('; httponly' in str(example_cookie))
        self.assertTrue(example_cookie['httponly'])

    def test_limited_stream(self):
        # Read all of a limited stream
        stream = LimitedStream(StringIO('test'), 2)
        self.assertEqual(stream.read(), 'te')
        # Reading again returns nothing.
        self.assertEqual(stream.read(), '')

        # Read a number of characters greater than the stream has to offer
        stream = LimitedStream(StringIO('test'), 2)
        self.assertEqual(stream.read(5), 'te')
        # Reading again returns nothing.
        self.assertEqual(stream.readline(5), '')

        # Read sequentially from a stream
        stream = LimitedStream(StringIO('12345678'), 8)
        self.assertEqual(stream.read(5), '12345')
        self.assertEqual(stream.read(5), '678')
        # Reading again returns nothing.
        self.assertEqual(stream.readline(5), '')

        # Read lines from a stream
        stream = LimitedStream(StringIO('1234\n5678\nabcd\nefgh\nijkl'), 24)
        # Read a full line, unconditionally
        self.assertEqual(stream.readline(), '1234\n')
        # Read a number of characters less than a line
        self.assertEqual(stream.readline(2), '56')
        # Read the rest of the partial line
        self.assertEqual(stream.readline(), '78\n')
        # Read a full line, with a character limit greater than the line length
        self.assertEqual(stream.readline(6), 'abcd\n')
        # Read the next line, deliberately terminated at the line end
        self.assertEqual(stream.readline(4), 'efgh')
        # Read the next line... just the line end
        self.assertEqual(stream.readline(), '\n')
        # Read everything else.
        self.assertEqual(stream.readline(), 'ijkl')

        # Regression for #15018
        # If a stream contains a newline, but the provided length
        # is less than the number of provided characters, the newline
        # doesn't reset the available character count
        stream = LimitedStream(StringIO('1234\nabcdef'), 9)
        self.assertEqual(stream.readline(10), '1234\n')
        self.assertEqual(stream.readline(3), 'abc')
        # Now expire the available characters
        self.assertEqual(stream.readline(3), 'd')
        # Reading again returns nothing.
        self.assertEqual(stream.readline(2), '')

        # Same test, but with read, not readline.
        stream = LimitedStream(StringIO('1234\nabcdef'), 9)
        self.assertEqual(stream.read(6), '1234\na')
        self.assertEqual(stream.read(2), 'bc')
        self.assertEqual(stream.read(2), 'd')
        self.assertEqual(stream.read(2), '')
        self.assertEqual(stream.read(), '')

    def test_stream(self):
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        self.assertEqual(request.read(), 'name=value')

    def test_read_after_value(self):
        """
        Reading from request is allowed after accessing request contents as
        POST or raw_post_data.
        """
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        self.assertEqual(request.POST, {u'name': [u'value']})
        self.assertEqual(request.raw_post_data, 'name=value')
        self.assertEqual(request.read(), 'name=value')

    def test_value_after_read(self):
        """
        Construction of POST or raw_post_data is not allowed after reading
        from request.
        """
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        self.assertEqual(request.read(2), 'na')
        self.assertRaises(Exception, lambda: request.raw_post_data)
        self.assertEqual(request.POST, {})

    def test_raw_post_data_after_POST_multipart(self):
        """
        Reading raw_post_data after parsing multipart is not allowed
        """
        # Because multipart is used for large amounts fo data i.e. file uploads,
        # we don't want the data held in memory twice, and we don't want to
        # silence the error by setting raw_post_data = '' either.
        payload = "\r\n".join([
                '--boundary',
                'Content-Disposition: form-data; name="name"',
                '',
                'value',
                '--boundary--'
                ''])
        request = WSGIRequest({'REQUEST_METHOD': 'POST',
                               'CONTENT_TYPE': 'multipart/form-data; boundary=boundary',
                               'CONTENT_LENGTH': len(payload),
                               'wsgi.input': StringIO(payload)})
        self.assertEqual(request.POST, {u'name': [u'value']})
        self.assertRaises(Exception, lambda: request.raw_post_data)

    def test_POST_multipart_with_content_length_zero(self):
        """
        Multipart POST requests with Content-Length >= 0 are valid and need to be handled.
        """
        # According to:
        # http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.13
        # Every request.POST with Content-Length >= 0 is a valid request,
        # this test ensures that we handle Content-Length == 0.
        payload = "\r\n".join([
                '--boundary',
                'Content-Disposition: form-data; name="name"',
                '',
                'value',
                '--boundary--'
                ''])
        request = WSGIRequest({'REQUEST_METHOD': 'POST',
                               'CONTENT_TYPE': 'multipart/form-data; boundary=boundary',
                               'CONTENT_LENGTH': 0,
                               'wsgi.input': StringIO(payload)})
        self.assertEqual(request.POST, {})

    def test_read_by_lines(self):
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        self.assertEqual(list(request), ['name=value'])

    def test_POST_after_raw_post_data_read(self):
        """
        POST should be populated even if raw_post_data is read first
        """
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        raw_data = request.raw_post_data
        self.assertEqual(request.POST, {u'name': [u'value']})

    def test_POST_after_raw_post_data_read_and_stream_read(self):
        """
        POST should be populated even if raw_post_data is read first, and then
        the stream is read second.
        """
        request = WSGIRequest({'REQUEST_METHOD': 'POST', 'wsgi.input': StringIO('name=value')})
        raw_data = request.raw_post_data
        self.assertEqual(request.read(1), u'n')
        self.assertEqual(request.POST, {u'name': [u'value']})

    def test_POST_after_raw_post_data_read_and_stream_read_multipart(self):
        """
        POST should be populated even if raw_post_data is read first, and then
        the stream is read second. Using multipart/form-data instead of urlencoded.
        """
        payload = "\r\n".join([
                '--boundary',
                'Content-Disposition: form-data; name="name"',
                '',
                'value',
                '--boundary--'
                ''])
        request = WSGIRequest({'REQUEST_METHOD': 'POST',
                               'CONTENT_TYPE': 'multipart/form-data; boundary=boundary',
                               'CONTENT_LENGTH': len(payload),
                               'wsgi.input': StringIO(payload)})
        raw_data = request.raw_post_data
        # Consume enough data to mess up the parsing:
        self.assertEqual(request.read(13), u'--boundary\r\nC')
        self.assertEqual(request.POST, {u'name': [u'value']})

from django import template
from django.template.defaultfilters import stringfilter

register = template.Library()

def trim(value, num):
    return value[:num]
trim = stringfilter(trim)

register.filter(trim)

@register.simple_tag
def no_params():
    """Expected no_params __doc__"""
    return "no_params - Expected result"
no_params.anything = "Expected no_params __dict__"

@register.simple_tag
def one_param(arg):
    """Expected one_param __doc__"""
    return "one_param - Expected result: %s" % arg
one_param.anything = "Expected one_param __dict__"

@register.simple_tag(takes_context=False)
def explicit_no_context(arg):
    """Expected explicit_no_context __doc__"""
    return "explicit_no_context - Expected result: %s" % arg
explicit_no_context.anything = "Expected explicit_no_context __dict__"

@register.simple_tag(takes_context=True)
def no_params_with_context(context):
    """Expected no_params_with_context __doc__"""
    return "no_params_with_context - Expected result (context value: %s)" % context['value']
no_params_with_context.anything = "Expected no_params_with_context __dict__"

@register.simple_tag(takes_context=True)
def params_and_context(context, arg):
    """Expected params_and_context __doc__"""
    return "params_and_context - Expected result (context value: %s): %s" % (context['value'], arg)
params_and_context.anything = "Expected params_and_context __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_no_params():
    """Expected inclusion_no_params __doc__"""
    return {"result" : "inclusion_no_params - Expected result"}
inclusion_no_params.anything = "Expected inclusion_no_params __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_one_param(arg):
    """Expected inclusion_one_param __doc__"""
    return {"result" : "inclusion_one_param - Expected result: %s" % arg}
inclusion_one_param.anything = "Expected inclusion_one_param __dict__"

@register.inclusion_tag('inclusion.html', takes_context=False)
def inclusion_explicit_no_context(arg):
    """Expected inclusion_explicit_no_context __doc__"""
    return {"result" : "inclusion_explicit_no_context - Expected result: %s" % arg}
inclusion_explicit_no_context.anything = "Expected inclusion_explicit_no_context __dict__"

@register.inclusion_tag('inclusion.html', takes_context=True)
def inclusion_no_params_with_context(context):
    """Expected inclusion_no_params_with_context __doc__"""
    return {"result" : "inclusion_no_params_with_context - Expected result (context value: %s)" % context['value']}
inclusion_no_params_with_context.anything = "Expected inclusion_no_params_with_context __dict__"

@register.inclusion_tag('inclusion.html', takes_context=True)
def inclusion_params_and_context(context, arg):
    """Expected inclusion_params_and_context __doc__"""
    return {"result" : "inclusion_params_and_context - Expected result (context value: %s): %s" % (context['value'], arg)}
inclusion_params_and_context.anything = "Expected inclusion_params_and_context __dict__"


from django.http import HttpResponse
from django.middleware.doc import XViewMiddleware
from django.template import Template, Context
from django.template.response import TemplateResponse
from django.test import TestCase, RequestFactory
from django.utils.decorators import decorator_from_middleware


xview_dec = decorator_from_middleware(XViewMiddleware)


@xview_dec
def xview(request):
    return HttpResponse()


class ClassXView(object):
    def __call__(self, request):
        return HttpResponse()

class_xview = xview_dec(ClassXView())


class FullMiddleware(object):
    def process_request(self, request):
        request.process_request_reached = True

    def process_view(sef, request, view_func, view_args, view_kwargs):
        request.process_view_reached = True

    def process_template_response(self, request, response):
        request.process_template_response_reached = True
        return response

    def process_response(self, request, response):
        # This should never receive unrendered content.
        request.process_response_content = response.content
        request.process_response_reached = True
        return response

full_dec = decorator_from_middleware(FullMiddleware)


class DecoratorFromMiddlewareTests(TestCase):
    """
    Tests for view decorators created using
    ``django.utils.decorators.decorator_from_middleware``.
    """
    rf = RequestFactory()

    def test_process_view_middleware(self):
        """
        Test a middleware that implements process_view.
        """
        xview(self.rf.get('/'))

    def test_callable_process_view_middleware(self):
        """
        Test a middleware that implements process_view, operating on a callable class.
        """
        class_xview(self.rf.get('/'))

    def test_full_dec_normal(self):
        """
        Test that all methods of middleware are called for normal HttpResponses
        """

        @full_dec
        def normal_view(request):
            t = Template("Hello world")
            return HttpResponse(t.render(Context({})))

        request = self.rf.get('/')
        response = normal_view(request)
        self.assertTrue(getattr(request, 'process_request_reached', False))
        self.assertTrue(getattr(request, 'process_view_reached', False))
        # process_template_response must not be called for HttpResponse
        self.assertFalse(getattr(request, 'process_template_response_reached', False))
        self.assertTrue(getattr(request, 'process_response_reached', False))

    def test_full_dec_templateresponse(self):
        """
        Test that all methods of middleware are called for TemplateResponses in
        the right sequence.
        """

        @full_dec
        def template_response_view(request):
            t = Template("Hello world")
            return TemplateResponse(request, t, {})

        request = self.rf.get('/')
        response = template_response_view(request)
        self.assertTrue(getattr(request, 'process_request_reached', False))
        self.assertTrue(getattr(request, 'process_view_reached', False))
        self.assertTrue(getattr(request, 'process_template_response_reached', False))
        # response must not be rendered yet.
        self.assertFalse(response._is_rendered)
        # process_response must not be called until after response is rendered,
        # otherwise some decorators like csrf_protect and gzip_page will not
        # work correctly. See #16004
        self.assertFalse(getattr(request, 'process_response_reached', False))
        response.render()
        self.assertTrue(getattr(request, 'process_response_reached', False))
        # Check that process_response saw the rendered content
        self.assertEqual(request.process_response_content, "Hello world")

"""
Built-in, globally-available admin actions.
"""

from django.core.exceptions import PermissionDenied
from django.contrib.admin import helpers
from django.contrib.admin.util import get_deleted_objects, model_ngettext
from django.db import router
from django.template.response import TemplateResponse
from django.utils.encoding import force_unicode
from django.utils.translation import ugettext_lazy, ugettext as _

def delete_selected(modeladmin, request, queryset):
    """
    Default action which deletes the selected objects.

    This action first displays a confirmation page whichs shows all the
    deleteable objects, or, if the user has no permission one of the related
    childs (foreignkeys), a "permission denied" message.

    Next, it delets all selected objects and redirects back to the change list.
    """
    opts = modeladmin.model._meta
    app_label = opts.app_label

    # Check that the user has delete permission for the actual model
    if not modeladmin.has_delete_permission(request):
        raise PermissionDenied

    using = router.db_for_write(modeladmin.model)

    # Populate deletable_objects, a data structure of all related objects that
    # will also be deleted.
    deletable_objects, perms_needed, protected = get_deleted_objects(
        queryset, opts, request.user, modeladmin.admin_site, using)

    # The user has already confirmed the deletion.
    # Do the deletion and return a None to display the change list view again.
    if request.POST.get('post'):
        if perms_needed:
            raise PermissionDenied
        n = queryset.count()
        if n:
            for obj in queryset:
                obj_display = force_unicode(obj)
                modeladmin.log_deletion(request, obj, obj_display)
            queryset.delete()
            modeladmin.message_user(request, _("Successfully deleted %(count)d %(items)s.") % {
                "count": n, "items": model_ngettext(modeladmin.opts, n)
            })
        # Return None to display the change list page again.
        return None

    if len(queryset) == 1:
        objects_name = force_unicode(opts.verbose_name)
    else:
        objects_name = force_unicode(opts.verbose_name_plural)

    if perms_needed or protected:
        title = _("Cannot delete %(name)s") % {"name": objects_name}
    else:
        title = _("Are you sure?")

    context = {
        "title": title,
        "objects_name": objects_name,
        "deletable_objects": [deletable_objects],
        'queryset': queryset,
        "perms_lacking": perms_needed,
        "protected": protected,
        "opts": opts,
        "app_label": app_label,
        'action_checkbox_name': helpers.ACTION_CHECKBOX_NAME,
    }

    # Display the confirmation page
    return TemplateResponse(request, modeladmin.delete_selected_confirmation_template or [
        "admin/%s/%s/delete_selected_confirmation.html" % (app_label, opts.object_name.lower()),
        "admin/%s/delete_selected_confirmation.html" % app_label,
        "admin/delete_selected_confirmation.html"
    ], context, current_app=modeladmin.admin_site.name)

delete_selected.short_description = ugettext_lazy("Delete selected %(verbose_name_plural)s")

import hashlib

from django.conf import settings
from django.utils import importlib
from django.utils.datastructures import SortedDict
from django.utils.encoding import smart_str
from django.core.exceptions import ImproperlyConfigured
from django.utils.crypto import (
    pbkdf2, constant_time_compare, get_random_string)
from django.utils.translation import ugettext_noop as _


UNUSABLE_PASSWORD = '!'  # This will never be a valid encoded hash
HASHERS = None  # lazily loaded from PASSWORD_HASHERS
PREFERRED_HASHER = None  # defaults to first item in PASSWORD_HASHERS


def is_password_usable(encoded):
    return (encoded is not None and encoded != UNUSABLE_PASSWORD)


def check_password(password, encoded, setter=None, preferred='default'):
    """
    Returns a boolean of whether the raw password matches the three
    part encoded digest.

    If setter is specified, it'll be called when you need to
    regenerate the password.
    """
    if not password or not is_password_usable(encoded):
        return False

    preferred = get_hasher(preferred)
    raw_password = password
    password = smart_str(password)
    encoded = smart_str(encoded)

    if len(encoded) == 32 and '$' not in encoded:
        hasher = get_hasher('unsalted_md5')
    else:
        algorithm = encoded.split('$', 1)[0]
        hasher = get_hasher(algorithm)

    must_update = hasher.algorithm != preferred.algorithm
    is_correct = hasher.verify(password, encoded)
    if setter and is_correct and must_update:
        setter(raw_password)
    return is_correct


def make_password(password, salt=None, hasher='default'):
    """
    Turn a plain-text password into a hash for database storage

    Same as encode() but generates a new random salt.  If
    password is None or blank then UNUSABLE_PASSWORD will be
    returned which disallows logins.
    """
    if not password:
        return UNUSABLE_PASSWORD

    hasher = get_hasher(hasher)
    password = smart_str(password)

    if not salt:
        salt = hasher.salt()
    salt = smart_str(salt)

    return hasher.encode(password, salt)


def load_hashers(password_hashers=None):
    global HASHERS
    global PREFERRED_HASHER
    hashers = []
    if not password_hashers:
        password_hashers = settings.PASSWORD_HASHERS
    for backend in password_hashers:
        try:
            mod_path, cls_name = backend.rsplit('.', 1)
            mod = importlib.import_module(mod_path)
            hasher_cls = getattr(mod, cls_name)
        except (AttributeError, ImportError, ValueError):
            raise ImproperlyConfigured("hasher not found: %s" % backend)
        hasher = hasher_cls()
        if not getattr(hasher, 'algorithm'):
            raise ImproperlyConfigured("hasher doesn't specify an "
                                       "algorithm name: %s" % backend)
        hashers.append(hasher)
    HASHERS = dict([(hasher.algorithm, hasher) for hasher in hashers])
    PREFERRED_HASHER = hashers[0]


def get_hasher(algorithm='default'):
    """
    Returns an instance of a loaded password hasher.

    If algorithm is 'default', the default hasher will be returned.
    This function will also lazy import hashers specified in your
    settings file if needed.
    """
    if hasattr(algorithm, 'algorithm'):
        return algorithm

    elif algorithm == 'default':
        if PREFERRED_HASHER is None:
            load_hashers()
        return PREFERRED_HASHER
    else:
        if HASHERS is None:
            load_hashers()
        if algorithm not in HASHERS:
            raise ValueError("Unknown password hashing algorithm '%s'. "
                             "Did you specify it in the PASSWORD_HASHERS "
                             "setting?" % algorithm)
        return HASHERS[algorithm]


def mask_hash(hash, show=6, char="*"):
    """
    Returns the given hash, with only the first ``show`` number shown. The
    rest are masked with ``char`` for security reasons.
    """
    masked = hash[:show]
    masked += char * len(hash[show:])
    return masked


class BasePasswordHasher(object):
    """
    Abstract base class for password hashers

    When creating your own hasher, you need to override algorithm,
    verify(), encode() and safe_summary().

    PasswordHasher objects are immutable.
    """
    algorithm = None
    library = None

    def _load_library(self):
        if self.library is not None:
            if isinstance(self.library, (tuple, list)):
                name, mod_path = self.library
            else:
                name = mod_path = self.library
            try:
                module = importlib.import_module(mod_path)
            except ImportError:
                raise ValueError("Couldn't load %s password algorithm "
                                 "library" % name)
            return module
        raise ValueError("Hasher '%s' doesn't specify a library attribute" %
                         self.__class__)

    def salt(self):
        """
        Generates a cryptographically secure nonce salt in ascii
        """
        return get_random_string()

    def verify(self, password, encoded):
        """
        Checks if the given password is correct
        """
        raise NotImplementedError()

    def encode(self, password, salt):
        """
        Creates an encoded database value

        The result is normally formatted as "algorithm$salt$hash" and
        must be fewer than 128 characters.
        """
        raise NotImplementedError()

    def safe_summary(self, encoded):
        """
        Returns a summary of safe values

        The result is a dictionary and will be used where the password field
        must be displayed to construct a safe representation of the password.
        """
        raise NotImplementedError()


class PBKDF2PasswordHasher(BasePasswordHasher):
    """
    Secure password hashing using the PBKDF2 algorithm (recommended)

    Configured to use PBKDF2 + HMAC + SHA256 with 10000 iterations.
    The result is a 64 byte binary string.  Iterations may be changed
    safely but you must rename the algorithm if you change SHA256.
    """
    algorithm = "pbkdf2_sha256"
    iterations = 10000
    digest = hashlib.sha256

    def encode(self, password, salt, iterations=None):
        assert password
        assert salt and '$' not in salt
        if not iterations:
            iterations = self.iterations
        hash = pbkdf2(password, salt, iterations, digest=self.digest)
        hash = hash.encode('base64').strip()
        return "%s$%d$%s$%s" % (self.algorithm, iterations, salt, hash)

    def verify(self, password, encoded):
        algorithm, iterations, salt, hash = encoded.split('$', 3)
        assert algorithm == self.algorithm
        encoded_2 = self.encode(password, salt, int(iterations))
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        algorithm, iterations, salt, hash = encoded.split('$', 3)
        assert algorithm == self.algorithm
        return SortedDict([
            (_('algorithm'), algorithm),
            (_('iterations'), iterations),
            (_('salt'), mask_hash(salt)),
            (_('hash'), mask_hash(hash)),
        ])


class PBKDF2SHA1PasswordHasher(PBKDF2PasswordHasher):
    """
    Alternate PBKDF2 hasher which uses SHA1, the default PRF
    recommended by PKCS #5. This is compatible with other
    implementations of PBKDF2, such as openssl's
    PKCS5_PBKDF2_HMAC_SHA1().
    """
    algorithm = "pbkdf2_sha1"
    digest = hashlib.sha1


class BCryptPasswordHasher(BasePasswordHasher):
    """
    Secure password hashing using the bcrypt algorithm (recommended)

    This is considered by many to be the most secure algorithm but you
    must first install the py-bcrypt library.  Please be warned that
    this library depends on native C code and might cause portability
    issues.
    """
    algorithm = "bcrypt"
    library = ("py-bcrypt", "bcrypt")
    rounds = 12

    def salt(self):
        bcrypt = self._load_library()
        return bcrypt.gensalt(self.rounds)

    def encode(self, password, salt):
        bcrypt = self._load_library()
        data = bcrypt.hashpw(password, salt)
        return "%s$%s" % (self.algorithm, data)

    def verify(self, password, encoded):
        algorithm, data = encoded.split('$', 1)
        assert algorithm == self.algorithm
        bcrypt = self._load_library()
        return constant_time_compare(data, bcrypt.hashpw(password, data))

    def safe_summary(self, encoded):
        algorithm, empty, algostr, work_factor, data = encoded.split('$', 4)
        assert algorithm == self.algorithm
        salt, checksum = data[:22], data[22:]
        return SortedDict([
            (_('algorithm'), algorithm),
            (_('work factor'), work_factor),
            (_('salt'), mask_hash(salt)),
            (_('checksum'), mask_hash(checksum)),
        ])


class SHA1PasswordHasher(BasePasswordHasher):
    """
    The SHA1 password hashing algorithm (not recommended)
    """
    algorithm = "sha1"

    def encode(self, password, salt):
        assert password
        assert salt and '$' not in salt
        hash = hashlib.sha1(salt + password).hexdigest()
        return "%s$%s$%s" % (self.algorithm, salt, hash)

    def verify(self, password, encoded):
        algorithm, salt, hash = encoded.split('$', 2)
        assert algorithm == self.algorithm
        encoded_2 = self.encode(password, salt)
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        algorithm, salt, hash = encoded.split('$', 2)
        assert algorithm == self.algorithm
        return SortedDict([
            (_('algorithm'), algorithm),
            (_('salt'), mask_hash(salt, show=2)),
            (_('hash'), mask_hash(hash)),
        ])


class MD5PasswordHasher(BasePasswordHasher):
    """
    The Salted MD5 password hashing algorithm (not recommended)
    """
    algorithm = "md5"

    def encode(self, password, salt):
        assert password
        assert salt and '$' not in salt
        hash = hashlib.md5(salt + password).hexdigest()
        return "%s$%s$%s" % (self.algorithm, salt, hash)

    def verify(self, password, encoded):
        algorithm, salt, hash = encoded.split('$', 2)
        assert algorithm == self.algorithm
        encoded_2 = self.encode(password, salt)
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        algorithm, salt, hash = encoded.split('$', 2)
        assert algorithm == self.algorithm
        return SortedDict([
            (_('algorithm'), algorithm),
            (_('salt'), mask_hash(salt, show=2)),
            (_('hash'), mask_hash(hash)),
        ])


class UnsaltedMD5PasswordHasher(BasePasswordHasher):
    """
    I am an incredibly insecure algorithm you should *never* use;
    stores unsalted MD5 hashes without the algorithm prefix.

    This class is implemented because Django used to store passwords
    this way. Some older Django installs still have these values
    lingering around so we need to handle and upgrade them properly.
    """
    algorithm = "unsalted_md5"

    def salt(self):
        return ''

    def encode(self, password, salt):
        return hashlib.md5(password).hexdigest()

    def verify(self, password, encoded):
        encoded_2 = self.encode(password, '')
        return constant_time_compare(encoded, encoded_2)

    def safe_summary(self, encoded):
        return SortedDict([
            (_('algorithm'), self.algorithm),
            (_('hash'), mask_hash(encoded, show=3)),
        ])


class CryptPasswordHasher(BasePasswordHasher):
    """
    Password hashing using UNIX crypt (not recommended)

    The crypt module is not supported on all platforms.
    """
    algorithm = "crypt"
    library = "crypt"

    def salt(self):
        return get_random_string(2)

    def encode(self, password, salt):
        crypt = self._load_library()
        assert len(salt) == 2
        data = crypt.crypt(password, salt)
        # we don't need to store the salt, but Django used to do this
        return "%s$%s$%s" % (self.algorithm, '', data)

    def verify(self, password, encoded):
        crypt = self._load_library()
        algorithm, salt, data = encoded.split('$', 2)
        assert algorithm == self.algorithm
        return constant_time_compare(data, crypt.crypt(password, data))

    def safe_summary(self, encoded):
        algorithm, salt, data = encoded.split('$', 2)
        assert algorithm == self.algorithm
        return SortedDict([
            (_('algorithm'), algorithm),
            (_('salt'), salt),
            (_('hash'), mask_hash(data, show=3)),
        ])


from django.test import TestCase
from django.core import signing
from django.core.exceptions import SuspiciousOperation
from django.http import HttpResponse

from django.contrib.formtools.wizard.storage.cookie import CookieStorage
from django.contrib.formtools.tests.wizard.storage import get_request, TestStorage


class TestCookieStorage(TestStorage, TestCase):
    def get_storage(self):
        return CookieStorage

    def test_manipulated_cookie(self):
        request = get_request()
        storage = self.get_storage()('wizard1', request, None)

        cookie_signer = signing.get_cookie_signer(storage.prefix)

        storage.request.COOKIES[storage.prefix] = cookie_signer.sign(
            storage.encoder.encode({'key1': 'value1'}))

        self.assertEqual(storage.load_data(), {'key1': 'value1'})

        storage.request.COOKIES[storage.prefix] = 'i_am_manipulated'
        self.assertRaises(SuspiciousOperation, storage.load_data)

    def test_reset_cookie(self):
        request = get_request()
        storage = self.get_storage()('wizard1', request, None)

        storage.data = {'key1': 'value1'}

        response = HttpResponse()
        storage.update_response(response)

        cookie_signer = signing.get_cookie_signer(storage.prefix)
        signed_cookie_data = cookie_signer.sign(storage.encoder.encode(storage.data))
        self.assertEqual(response.cookies[storage.prefix].value, signed_cookie_data)

        storage.init_data()
        storage.update_response(response)
        unsigned_cookie_data = cookie_signer.unsign(response.cookies[storage.prefix].value)
        self.assertEqual(unsigned_cookie_data, '{"step_files":{},"step":null,"extra_data":{},"step_data":{}}')

from django.contrib.gis.db.models.sql.compiler import GeoSQLCompiler as BaseGeoSQLCompiler
from django.db.backends.oracle import compiler

SQLCompiler = compiler.SQLCompiler

class GeoSQLCompiler(BaseGeoSQLCompiler, SQLCompiler):
    pass

class SQLInsertCompiler(compiler.SQLInsertCompiler, GeoSQLCompiler):
    pass

class SQLDeleteCompiler(compiler.SQLDeleteCompiler, GeoSQLCompiler):
    pass

class SQLUpdateCompiler(compiler.SQLUpdateCompiler, GeoSQLCompiler):
    pass

class SQLAggregateCompiler(compiler.SQLAggregateCompiler, GeoSQLCompiler):
    pass

class SQLDateCompiler(compiler.SQLDateCompiler, GeoSQLCompiler):
    pass

from ctypes import c_char_p, c_float, c_int, string_at, Structure, POINTER
from django.contrib.gis.geoip.libgeoip import lgeoip, free

#### GeoIP C Structure definitions ####

class GeoIPRecord(Structure):
    _fields_ = [('country_code', c_char_p),
                ('country_code3', c_char_p),
                ('country_name', c_char_p),
                ('region', c_char_p),
                ('city', c_char_p),
                ('postal_code', c_char_p),
                ('latitude', c_float),
                ('longitude', c_float),
                # TODO: In 1.4.6 this changed from `int dma_code;` to
                # `union {int metro_code; int dma_code;};`.  Change
                # to a `ctypes.Union` in to accomodate in future when
                # pre-1.4.6 versions are no longer distributed.
                ('dma_code', c_int),
                ('area_code', c_int),
                ('charset', c_int),
                ('continent_code', c_char_p),
                ]
geoip_char_fields = [name for name, ctype in GeoIPRecord._fields_ if ctype is c_char_p]
geoip_encodings = { 0: 'iso-8859-1',
                    1: 'utf8',
                    }

class GeoIPTag(Structure): pass

RECTYPE = POINTER(GeoIPRecord)
DBTYPE = POINTER(GeoIPTag)

#### ctypes function prototypes ####

# GeoIP_lib_version appeared in version 1.4.7.
if hasattr(lgeoip, 'GeoIP_lib_version'):
    GeoIP_lib_version = lgeoip.GeoIP_lib_version
    GeoIP_lib_version.argtypes = None
    GeoIP_lib_version.restype = c_char_p
else:
    GeoIP_lib_version = None

# For freeing memory allocated within a record
GeoIPRecord_delete = lgeoip.GeoIPRecord_delete
GeoIPRecord_delete.argtypes = [RECTYPE]
GeoIPRecord_delete.restype = None

# For retrieving records by name or address.
def check_record(result, func, cargs):
    if bool(result):
        # Checking the pointer to the C structure, if valid pull out elements
        # into a dicionary.
        rec = result.contents
        record = dict((fld, getattr(rec, fld)) for fld, ctype in rec._fields_)

        # Now converting the strings to unicode using the proper encoding.
        encoding = geoip_encodings[record['charset']]
        for char_field in geoip_char_fields:
            if record[char_field]:
                record[char_field] = record[char_field].decode(encoding)

        # Free the memory allocated for the struct & return.
        GeoIPRecord_delete(result)
        return record
    else:
        return None

def record_output(func):
    func.argtypes = [DBTYPE, c_char_p]
    func.restype = RECTYPE
    func.errcheck = check_record
    return func
GeoIP_record_by_addr = record_output(lgeoip.GeoIP_record_by_addr)
GeoIP_record_by_name = record_output(lgeoip.GeoIP_record_by_name)


# For opening & closing GeoIP database files.
GeoIP_open = lgeoip.GeoIP_open
GeoIP_open.restype = DBTYPE
GeoIP_delete = lgeoip.GeoIP_delete
GeoIP_delete.argtypes = [DBTYPE]
GeoIP_delete.restype = None

# This is so the string pointer can be freed within Python.
class geoip_char_p(c_char_p):
    pass

def check_string(result, func, cargs):
    if result:
        s = string_at(result)
        free(result)
    else:
        s = ''
    return s

GeoIP_database_info = lgeoip.GeoIP_database_info
GeoIP_database_info.restype = geoip_char_p
GeoIP_database_info.errcheck = check_string

# String output routines.
def string_output(func):
    func.restype = c_char_p
    return func

GeoIP_country_code_by_addr = string_output(lgeoip.GeoIP_country_code_by_addr)
GeoIP_country_code_by_name = string_output(lgeoip.GeoIP_country_code_by_name)
GeoIP_country_name_by_addr = string_output(lgeoip.GeoIP_country_name_by_addr)
GeoIP_country_name_by_name = string_output(lgeoip.GeoIP_country_name_by_name)

from __future__ import absolute_import

from django.contrib.gis import feeds

from .models import City


class TestGeoRSS1(feeds.Feed):
    link = '/city/'
    title = 'Test GeoDjango Cities'

    def items(self):
        return City.objects.all()

    def item_link(self, item):
        return '/city/%s/' % item.pk

    def item_geometry(self, item):
        return item.point

class TestGeoRSS2(TestGeoRSS1):
    def geometry(self, obj):
        # This should attach a <georss:box> element for the extent of
        # of the cities in the database.  This tuple came from
        # calling `City.objects.extent()` -- we can't do that call here
        # because `extent` is not implemented for MySQL/Oracle.
        return (-123.30, -41.32, 174.78, 48.46)

    def item_geometry(self, item):
        # Returning a simple tuple for the geometry.
        return item.point.x, item.point.y

class TestGeoAtom1(TestGeoRSS1):
    feed_type = feeds.GeoAtom1Feed

class TestGeoAtom2(TestGeoRSS2):
    feed_type = feeds.GeoAtom1Feed

    def geometry(self, obj):
        # This time we'll use a 2-tuple of coordinates for the box.
        return ((-123.30, -41.32), (174.78, 48.46))

class TestW3CGeo1(TestGeoRSS1):
    feed_type = feeds.W3CGeoFeed

# The following feeds are invalid, and will raise exceptions.
class TestW3CGeo2(TestGeoRSS2):
    feed_type = feeds.W3CGeoFeed

class TestW3CGeo3(TestGeoRSS1):
    feed_type = feeds.W3CGeoFeed

    def item_geometry(self, item):
        from django.contrib.gis.geos import Polygon
        return Polygon(((0, 0), (0, 1), (1, 1), (1, 0), (0, 0)))

# The feed dictionary to use for URLs.
feed_dict = {
    'rss1' : TestGeoRSS1,
    'rss2' : TestGeoRSS2,
    'atom1' : TestGeoAtom1,
    'atom2' : TestGeoAtom2,
    'w3cgeo1' : TestW3CGeo1,
    'w3cgeo2' : TestW3CGeo2,
    'w3cgeo3' : TestW3CGeo3,
}

"""
Canada-specific Form helpers
"""

from __future__ import absolute_import

import re

from django.core.validators import EMPTY_VALUES
from django.forms import ValidationError
from django.forms.fields import Field, CharField, Select
from django.utils.encoding import smart_unicode
from django.utils.translation import ugettext_lazy as _


phone_digits_re = re.compile(r'^(?:1-?)?(\d{3})[-\.]?(\d{3})[-\.]?(\d{4})$')
sin_re = re.compile(r"^(\d{3})-(\d{3})-(\d{3})$")

class CAPostalCodeField(CharField):
    """
    Canadian postal code field.

    Validates against known invalid characters: D, F, I, O, Q, U
    Additionally the first character cannot be Z or W.
    For more info see:
    http://www.canadapost.ca/tools/pg/manual/PGaddress-e.asp#1402170
    """
    default_error_messages = {
        'invalid': _(u'Enter a postal code in the format XXX XXX.'),
    }

    postcode_regex = re.compile(r'^([ABCEGHJKLMNPRSTVXY]\d[ABCEGHJKLMNPRSTVWXYZ]) *(\d[ABCEGHJKLMNPRSTVWXYZ]\d)$')

    def clean(self, value):
        value = super(CAPostalCodeField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        postcode = value.upper().strip()
        m = self.postcode_regex.match(postcode)
        if not m:
            raise ValidationError(self.default_error_messages['invalid'])
        return "%s %s" % (m.group(1), m.group(2))

class CAPhoneNumberField(Field):
    """Canadian phone number field."""
    default_error_messages = {
        'invalid': u'Phone numbers must be in XXX-XXX-XXXX format.',
    }

    def clean(self, value):
        """Validate a phone number.
        """
        super(CAPhoneNumberField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        value = re.sub('(\(|\)|\s+)', '', smart_unicode(value))
        m = phone_digits_re.search(value)
        if m:
            return u'%s-%s-%s' % (m.group(1), m.group(2), m.group(3))
        raise ValidationError(self.error_messages['invalid'])

class CAProvinceField(Field):
    """
    A form field that validates its input is a Canadian province name or abbreviation.
    It normalizes the input to the standard two-leter postal service
    abbreviation for the given province.
    """
    default_error_messages = {
        'invalid': u'Enter a Canadian province or territory.',
    }

    def clean(self, value):
        super(CAProvinceField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''
        try:
            value = value.strip().lower()
        except AttributeError:
            pass
        else:
            # Load data in memory only when it is required, see also #17275
            from django.contrib.localflavor.ca.ca_provinces import PROVINCES_NORMALIZED
            try:
                return PROVINCES_NORMALIZED[value.strip().lower()].decode('ascii')
            except KeyError:
                pass
        raise ValidationError(self.error_messages['invalid'])

class CAProvinceSelect(Select):
    """
    A Select widget that uses a list of Canadian provinces and
    territories as its choices.
    """
    def __init__(self, attrs=None):
        # Load data in memory only when it is required, see also #17275
        from django.contrib.localflavor.ca.ca_provinces import PROVINCE_CHOICES
        super(CAProvinceSelect, self).__init__(attrs, choices=PROVINCE_CHOICES)

class CASocialInsuranceNumberField(Field):
    """
    A Canadian Social Insurance Number (SIN).

    Checks the following rules to determine whether the number is valid:

        * Conforms to the XXX-XXX-XXX format.
        * Passes the check digit process "Luhn Algorithm"
             See: http://en.wikipedia.org/wiki/Social_Insurance_Number
    """
    default_error_messages = {
        'invalid': _('Enter a valid Canadian Social Insurance number in XXX-XXX-XXX format.'),
    }

    def clean(self, value):
        super(CASocialInsuranceNumberField, self).clean(value)
        if value in EMPTY_VALUES:
            return u''

        match = re.match(sin_re, value)
        if not match:
            raise ValidationError(self.error_messages['invalid'])

        number = u'%s-%s-%s' % (match.group(1), match.group(2), match.group(3))
        check_number = u'%s%s%s' % (match.group(1), match.group(2), match.group(3))
        if not self.luhn_checksum_is_valid(check_number):
            raise ValidationError(self.error_messages['invalid'])
        return number

    def luhn_checksum_is_valid(self, number):
        """
        Checks to make sure that the SIN passes a luhn mod-10 checksum
        See: http://en.wikipedia.org/wiki/Luhn_algorithm
        """

        sum = 0
        num_digits = len(number)
        oddeven = num_digits & 1

        for count in range(0, num_digits):
            digit = int(number[count])

            if not (( count & 1 ) ^ oddeven ):
                digit = digit * 2
            if digit > 9:
                digit = digit - 9

            sum = sum + digit

        return ( (sum % 10) == 0 )

"""
Set of "markup" template filters for Django.  These filters transform plain text
markup syntaxes to HTML; currently there is support for:

    * Textile, which requires the PyTextile library available at
      http://loopcore.com/python-textile/

    * Markdown, which requires the Python-markdown library from
      http://www.freewisdom.org/projects/python-markdown

    * reStructuredText, which requires docutils from http://docutils.sf.net/
"""

import warnings

from django import template
from django.conf import settings
from django.utils.encoding import smart_str, force_unicode
from django.utils.safestring import mark_safe

register = template.Library()

@register.filter(is_safe=True)
def textile(value):
    try:
        import textile
    except ImportError:
        if settings.DEBUG:
            raise template.TemplateSyntaxError("Error in 'textile' filter: The Python textile library isn't installed.")
        return force_unicode(value)
    else:
        return mark_safe(force_unicode(textile.textile(smart_str(value), encoding='utf-8', output='utf-8')))

@register.filter(is_safe=True)
def markdown(value, arg=''):
    """
    Runs Markdown over a given value, optionally using various
    extensions python-markdown supports.

    Syntax::

        {{ value|markdown:"extension1_name,extension2_name..." }}

    To enable safe mode, which strips raw HTML and only returns HTML
    generated by actual Markdown syntax, pass "safe" as the first
    extension in the list.

    If the version of Markdown in use does not support extensions,
    they will be silently ignored.

    """
    try:
        import markdown
    except ImportError:
        if settings.DEBUG:
            raise template.TemplateSyntaxError("Error in 'markdown' filter: The Python markdown library isn't installed.")
        return force_unicode(value)
    else:
        # markdown.version was first added in 1.6b. The only version of markdown
        # to fully support extensions before 1.6b was the shortlived 1.6a.
        if hasattr(markdown, 'version'):
            extensions = [e for e in arg.split(",") if e]
            if len(extensions) > 0 and extensions[0] == "safe":
                extensions = extensions[1:]
                safe_mode = True
            else:
                safe_mode = False
            python_markdown_deprecation = ("The use of Python-Markdown "
            "< 2.1 in Django is deprecated; please update to the current version")
            # Unicode support only in markdown v1.7 or above. Version_info
            # exist only in markdown v1.6.2rc-2 or above.
            markdown_vers = getattr(markdown, "version_info", None)
            if markdown_vers < (1,7):
                warnings.warn(python_markdown_deprecation, DeprecationWarning)
                return mark_safe(force_unicode(markdown.markdown(smart_str(value), extensions, safe_mode=safe_mode)))
            else:
                if markdown_vers >= (2,1):
                    if safe_mode:
                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode, enable_attributes=False))
                    else:
                        return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))
                else:
                    warnings.warn(python_markdown_deprecation, DeprecationWarning)
                    return mark_safe(markdown.markdown(force_unicode(value), extensions, safe_mode=safe_mode))
        else:
            warnings.warn(python_markdown_deprecation, DeprecationWarning)
            return mark_safe(force_unicode(markdown.markdown(smart_str(value))))

@register.filter(is_safe=True)
def restructuredtext(value):
    try:
        from docutils.core import publish_parts
    except ImportError:
        if settings.DEBUG:
            raise template.TemplateSyntaxError("Error in 'restructuredtext' filter: The Python docutils library isn't installed.")
        return force_unicode(value)
    else:
        docutils_settings = getattr(settings, "RESTRUCTUREDTEXT_FILTER_SETTINGS", {})
        parts = publish_parts(source=smart_str(value), writer_name="html4css1", settings_overrides=docutils_settings)
        return mark_safe(force_unicode(parts["fragment"]))

import sys
import os
from optparse import make_option, OptionParser

from django.conf import settings
from django.core.management.base import BaseCommand
from django.test.utils import get_runner

class Command(BaseCommand):
    option_list = BaseCommand.option_list + (
        make_option('--noinput',
            action='store_false', dest='interactive', default=True,
            help='Tells Django to NOT prompt the user for input of any kind.'),
        make_option('--failfast',
            action='store_true', dest='failfast', default=False,
            help='Tells Django to stop running the test suite after first '
                 'failed test.'),
        make_option('--testrunner',
            action='store', dest='testrunner',
            help='Tells Django to use specified test runner class instead of '
                 'the one specified by the TEST_RUNNER setting.'),
        make_option('--liveserver',
            action='store', dest='liveserver', default=None,
            help='Overrides the default address where the live server (used '
                 'with LiveServerTestCase) is expected to run from. The '
                 'default value is localhost:8081.'),
    )
    help = ('Runs the test suite for the specified applications, or the '
            'entire site if no apps are specified.')
    args = '[appname ...]'

    requires_model_validation = False

    def __init__(self):
        self.test_runner = None
        super(Command, self).__init__()

    def run_from_argv(self, argv):
        """
        Pre-parse the command line to extract the value of the --testrunner
        option. This allows a test runner to define additional command line
        arguments.
        """
        option = '--testrunner='
        for arg in argv[2:]:
            if arg.startswith(option):
                self.test_runner = arg[len(option):]
                break
        super(Command, self).run_from_argv(argv)

    def create_parser(self, prog_name, subcommand):
        test_runner_class = get_runner(settings, self.test_runner)
        options = self.option_list + getattr(
            test_runner_class, 'option_list', ())
        return OptionParser(prog=prog_name,
                            usage=self.usage(subcommand),
                            version=self.get_version(),
                            option_list=options)

    def handle(self, *test_labels, **options):
        from django.conf import settings
        from django.test.utils import get_runner

        TestRunner = get_runner(settings, options.get('testrunner'))
        options['verbosity'] = int(options.get('verbosity'))

        if options.get('liveserver') is not None:
            os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = options['liveserver']
            del options['liveserver']

        test_runner = TestRunner(**options)
        failures = test_runner.run_tests(test_labels)

        if failures:
            sys.exit(bool(failures))

import psycopg2.extensions

from django.db.backends.creation import BaseDatabaseCreation
from django.db.backends.util import truncate_name


class DatabaseCreation(BaseDatabaseCreation):
    # This dictionary maps Field objects to their associated PostgreSQL column
    # types, as strings. Column-type strings can contain format strings; they'll
    # be interpolated against the values of Field.__dict__ before being output.
    # If a column type is set to None, it won't be included in the output.
    data_types = {
        'AutoField':         'serial',
        'BooleanField':      'boolean',
        'CharField':         'varchar(%(max_length)s)',
        'CommaSeparatedIntegerField': 'varchar(%(max_length)s)',
        'DateField':         'date',
        'DateTimeField':     'timestamp with time zone',
        'DecimalField':      'numeric(%(max_digits)s, %(decimal_places)s)',
        'FileField':         'varchar(%(max_length)s)',
        'FilePathField':     'varchar(%(max_length)s)',
        'FloatField':        'double precision',
        'IntegerField':      'integer',
        'BigIntegerField':   'bigint',
        'IPAddressField':    'inet',
        'GenericIPAddressField': 'inet',
        'NullBooleanField':  'boolean',
        'OneToOneField':     'integer',
        'PositiveIntegerField': 'integer CHECK ("%(column)s" >= 0)',
        'PositiveSmallIntegerField': 'smallint CHECK ("%(column)s" >= 0)',
        'SlugField':         'varchar(%(max_length)s)',
        'SmallIntegerField': 'smallint',
        'TextField':         'text',
        'TimeField':         'time',
    }

    def sql_table_creation_suffix(self):
        assert self.connection.settings_dict['TEST_COLLATION'] is None, "PostgreSQL does not support collation setting at database creation time."
        if self.connection.settings_dict['TEST_CHARSET']:
            return "WITH ENCODING '%s'" % self.connection.settings_dict['TEST_CHARSET']
        return ''

    def sql_indexes_for_field(self, model, f, style):
        if f.db_index and not f.unique:
            qn = self.connection.ops.quote_name
            db_table = model._meta.db_table
            tablespace = f.db_tablespace or model._meta.db_tablespace
            if tablespace:
                tablespace_sql = self.connection.ops.tablespace_sql(tablespace)
                if tablespace_sql:
                    tablespace_sql = ' ' + tablespace_sql
            else:
                tablespace_sql = ''

            def get_index_sql(index_name, opclass=''):
                return (style.SQL_KEYWORD('CREATE INDEX') + ' ' +
                        style.SQL_TABLE(qn(truncate_name(index_name,self.connection.ops.max_name_length()))) + ' ' +
                        style.SQL_KEYWORD('ON') + ' ' +
                        style.SQL_TABLE(qn(db_table)) + ' ' +
                        "(%s%s)" % (style.SQL_FIELD(qn(f.column)), opclass) +
                        "%s;" % tablespace_sql)

            output = [get_index_sql('%s_%s' % (db_table, f.column))]

            # Fields with database column types of `varchar` and `text` need
            # a second index that specifies their operator class, which is
            # needed when performing correct LIKE queries outside the
            # C locale. See #12234.
            db_type = f.db_type(connection=self.connection)
            if db_type.startswith('varchar'):
                output.append(get_index_sql('%s_%s_like' % (db_table, f.column),
                                            ' varchar_pattern_ops'))
            elif db_type.startswith('text'):
                output.append(get_index_sql('%s_%s_like' % (db_table, f.column),
                                            ' text_pattern_ops'))
        else:
            output = []
        return output

    def set_autocommit(self):
        self._prepare_for_test_db_ddl()

    def _prepare_for_test_db_ddl(self):
        """Rollback and close the active transaction."""
        self.connection.connection.rollback()
        self.connection.connection.set_isolation_level(
                psycopg2.extensions.ISOLATION_LEVEL_AUTOCOMMIT)

# Wrapper for loading templates from eggs via pkg_resources.resource_string.

try:
    from pkg_resources import resource_string
except ImportError:
    resource_string = None

from django.template.base import TemplateDoesNotExist
from django.template.loader import BaseLoader
from django.conf import settings

class Loader(BaseLoader):
    is_usable = resource_string is not None

    def load_template_source(self, template_name, template_dirs=None):
        """
        Loads templates from Python eggs via pkg_resource.resource_string.

        For every installed app, it tries to get the resource (app, template_name).
        """
        if resource_string is not None:
            pkg_name = 'templates/' + template_name
            for app in settings.INSTALLED_APPS:
                try:
                    return (resource_string(app, pkg_name).decode(settings.FILE_CHARSET), 'egg:%s:%s' % (app, pkg_name))
                except:
                    pass
        raise TemplateDoesNotExist(template_name)

_loader = Loader()

import django
import re

def get_svn_revision(path=None):
    """
    Returns the SVN revision in the form SVN-XXXX,
    where XXXX is the revision number.

    Returns SVN-unknown if anything goes wrong, such as an unexpected
    format of internal SVN files.

    If path is provided, it should be a directory whose SVN info you want to
    inspect. If it's not provided, this will use the root django/ package
    directory.
    """
    rev = None
    if path is None:
        path = django.__path__[0]
    entries_path = '%s/.svn/entries' % path

    try:
        entries = open(entries_path, 'r').read()
    except IOError:
        pass
    else:
        # Versions >= 7 of the entries file are flat text.  The first line is
        # the version number. The next set of digits after 'dir' is the revision.
        if re.match('(\d+)', entries):
            rev_match = re.search('\d+\s+dir\s+(\d+)', entries)
            if rev_match:
                rev = rev_match.groups()[0]
        # Older XML versions of the file specify revision as an attribute of
        # the first entries node.
        else:
            from xml.dom import minidom
            dom = minidom.parse(entries_path)
            rev = dom.getElementsByTagName('entry')[0].getAttribute('revision')

    if rev:
        return u'SVN-%s' % rev
    return u'SVN-unknown'

"""
21. Specifying 'choices' for a field

Most fields take a ``choices`` parameter, which should be a tuple of tuples
specifying which are the valid values for that field.

For each field that has ``choices``, a model instance gets a
``get_fieldname_display()`` method, where ``fieldname`` is the name of the
field. This method returns the "human-readable" value of the field.
"""

from django.db import models

GENDER_CHOICES = (
    ('M', 'Male'),
    ('F', 'Female'),
)

class Person(models.Model):
    name = models.CharField(max_length=20)
    gender = models.CharField(max_length=1, choices=GENDER_CHOICES)

    def __unicode__(self):
        return self.name

from django.db import models


class Article(models.Model):
    headline = models.CharField(max_length=100, default='Default headline')
    pub_date = models.DateTimeField()

    def __unicode__(self):
        return self.headline

    class Meta:
        app_label = 'fixtures_model_package'
        ordering = ('-pub_date', 'headline')

class Book(models.Model):
    name = models.CharField(max_length=100)

    class Meta:
        ordering = ('name',)

"""
5. Many-to-many relationships

To define a many-to-many relationship, use ``ManyToManyField()``.

In this example, an ``Article`` can be published in multiple ``Publication``
objects, and a ``Publication`` has multiple ``Article`` objects.
"""

from django.db import models

class Publication(models.Model):
    title = models.CharField(max_length=30)

    def __unicode__(self):
        return self.title

    class Meta:
        ordering = ('title',)

class Article(models.Model):
    headline = models.CharField(max_length=100)
    publications = models.ManyToManyField(Publication)

    def __unicode__(self):
        return self.headline

    class Meta:
        ordering = ('headline',)

from __future__ import with_statement, absolute_import

from django.contrib.contenttypes.models import ContentType
from django.db import connection
from django.test import TestCase
from django.test.utils import override_settings

from .models import (Author, Book, Reader, Qualification, Teacher, Department,
    TaggedItem, Bookmark, AuthorAddress, FavoriteAuthors, AuthorWithAge,
    BookWithYear, Person, House, Room, Employee, Comment)


class PrefetchRelatedTests(TestCase):

    def setUp(self):

        self.book1 = Book.objects.create(title="Poems")
        self.book2 = Book.objects.create(title="Jane Eyre")
        self.book3 = Book.objects.create(title="Wuthering Heights")
        self.book4 = Book.objects.create(title="Sense and Sensibility")

        self.author1 = Author.objects.create(name="Charlotte",
                                             first_book=self.book1)
        self.author2 = Author.objects.create(name="Anne",
                                             first_book=self.book1)
        self.author3 = Author.objects.create(name="Emily",
                                             first_book=self.book1)
        self.author4 = Author.objects.create(name="Jane",
                                             first_book=self.book4)

        self.book1.authors.add(self.author1, self.author2, self.author3)
        self.book2.authors.add(self.author1)
        self.book3.authors.add(self.author3)
        self.book4.authors.add(self.author4)

        self.reader1 = Reader.objects.create(name="Amy")
        self.reader2 = Reader.objects.create(name="Belinda")

        self.reader1.books_read.add(self.book1, self.book4)
        self.reader2.books_read.add(self.book2, self.book4)

    def test_m2m_forward(self):
        with self.assertNumQueries(2):
            lists = [list(b.authors.all()) for b in Book.objects.prefetch_related('authors')]

        normal_lists = [list(b.authors.all()) for b in Book.objects.all()]
        self.assertEqual(lists, normal_lists)


    def test_m2m_reverse(self):
        with self.assertNumQueries(2):
            lists = [list(a.books.all()) for a in Author.objects.prefetch_related('books')]

        normal_lists = [list(a.books.all()) for a in Author.objects.all()]
        self.assertEqual(lists, normal_lists)

    def test_foreignkey_forward(self):
        with self.assertNumQueries(2):
            books = [a.first_book for a in Author.objects.prefetch_related('first_book')]

        normal_books = [a.first_book for a in Author.objects.all()]
        self.assertEqual(books, normal_books)

    def test_foreignkey_reverse(self):
        with self.assertNumQueries(2):
            lists = [list(b.first_time_authors.all())
                     for b in Book.objects.prefetch_related('first_time_authors')]

        self.assertQuerysetEqual(self.book2.authors.all(), [u"<Author: Charlotte>"])

    def test_survives_clone(self):
        with self.assertNumQueries(2):
            lists = [list(b.first_time_authors.all())
                     for b in Book.objects.prefetch_related('first_time_authors').exclude(id=1000)]

    def test_len(self):
        with self.assertNumQueries(2):
            qs = Book.objects.prefetch_related('first_time_authors')
            length = len(qs)
            lists = [list(b.first_time_authors.all())
                     for b in qs]

    def test_bool(self):
        with self.assertNumQueries(2):
            qs = Book.objects.prefetch_related('first_time_authors')
            x = bool(qs)
            lists = [list(b.first_time_authors.all())
                     for b in qs]

    def test_count(self):
        with self.assertNumQueries(2):
            qs = Book.objects.prefetch_related('first_time_authors')
            [b.first_time_authors.count() for b in qs]

    def test_exists(self):
        with self.assertNumQueries(2):
            qs = Book.objects.prefetch_related('first_time_authors')
            [b.first_time_authors.exists() for b in qs]

    def test_clear(self):
        """
        Test that we can clear the behavior by calling prefetch_related()
        """
        with self.assertNumQueries(5):
            with_prefetch = Author.objects.prefetch_related('books')
            without_prefetch = with_prefetch.prefetch_related(None)
            lists = [list(a.books.all()) for a in without_prefetch]

    def test_m2m_then_m2m(self):
        """
        Test we can follow a m2m and another m2m
        """
        with self.assertNumQueries(3):
            qs = Author.objects.prefetch_related('books__read_by')
            lists = [[[unicode(r) for r in b.read_by.all()]
                      for b in a.books.all()]
                     for a in qs]
            self.assertEqual(lists,
            [
                [[u"Amy"], [u"Belinda"]],  # Charlotte - Poems, Jane Eyre
                [[u"Amy"]],                # Anne - Poems
                [[u"Amy"], []],            # Emily - Poems, Wuthering Heights
                [[u"Amy", u"Belinda"]],    # Jane - Sense and Sense
            ])

    def test_overriding_prefetch(self):
        with self.assertNumQueries(3):
            qs = Author.objects.prefetch_related('books', 'books__read_by')
            lists = [[[unicode(r) for r in b.read_by.all()]
                      for b in a.books.all()]
                     for a in qs]
            self.assertEqual(lists,
            [
                [[u"Amy"], [u"Belinda"]],  # Charlotte - Poems, Jane Eyre
                [[u"Amy"]],                # Anne - Poems
                [[u"Amy"], []],            # Emily - Poems, Wuthering Heights
                [[u"Amy", u"Belinda"]],    # Jane - Sense and Sense
            ])
        with self.assertNumQueries(3):
            qs = Author.objects.prefetch_related('books__read_by', 'books')
            lists = [[[unicode(r) for r in b.read_by.all()]
                      for b in a.books.all()]
                     for a in qs]
            self.assertEqual(lists,
            [
                [[u"Amy"], [u"Belinda"]],  # Charlotte - Poems, Jane Eyre
                [[u"Amy"]],                # Anne - Poems
                [[u"Amy"], []],            # Emily - Poems, Wuthering Heights
                [[u"Amy", u"Belinda"]],    # Jane - Sense and Sense
            ])

    def test_get(self):
        """
        Test that objects retrieved with .get() get the prefetch behavior.
        """
        # Need a double
        with self.assertNumQueries(3):
            author = Author.objects.prefetch_related('books__read_by').get(name="Charlotte")
            lists = [[unicode(r) for r in b.read_by.all()]
                      for b in author.books.all()]
            self.assertEqual(lists, [[u"Amy"], [u"Belinda"]])  # Poems, Jane Eyre

    def test_foreign_key_then_m2m(self):
        """
        Test we can follow an m2m relation after a relation like ForeignKey
        that doesn't have many objects
        """
        with self.assertNumQueries(2):
            qs = Author.objects.select_related('first_book').prefetch_related('first_book__read_by')
            lists = [[unicode(r) for r in a.first_book.read_by.all()]
                     for a in qs]
            self.assertEqual(lists, [[u"Amy"],
                                     [u"Amy"],
                                     [u"Amy"],
                                     [u"Amy", "Belinda"]])

    def test_attribute_error(self):
        qs = Reader.objects.all().prefetch_related('books_read__xyz')
        with self.assertRaises(AttributeError) as cm:
            list(qs)

        self.assertTrue('prefetch_related' in str(cm.exception))

    def test_invalid_final_lookup(self):
        qs = Book.objects.prefetch_related('authors__name')
        with self.assertRaises(ValueError) as cm:
            list(qs)

        self.assertTrue('prefetch_related' in str(cm.exception))
        self.assertTrue("name" in str(cm.exception))


class DefaultManagerTests(TestCase):

    def setUp(self):
        self.qual1 = Qualification.objects.create(name="BA")
        self.qual2 = Qualification.objects.create(name="BSci")
        self.qual3 = Qualification.objects.create(name="MA")
        self.qual4 = Qualification.objects.create(name="PhD")

        self.teacher1 = Teacher.objects.create(name="Mr Cleese")
        self.teacher2 = Teacher.objects.create(name="Mr Idle")
        self.teacher3 = Teacher.objects.create(name="Mr Chapman")

        self.teacher1.qualifications.add(self.qual1, self.qual2, self.qual3, self.qual4)
        self.teacher2.qualifications.add(self.qual1)
        self.teacher3.qualifications.add(self.qual2)

        self.dept1 = Department.objects.create(name="English")
        self.dept2 = Department.objects.create(name="Physics")

        self.dept1.teachers.add(self.teacher1, self.teacher2)
        self.dept2.teachers.add(self.teacher1, self.teacher3)

    def test_m2m_then_m2m(self):
        with self.assertNumQueries(3):
            # When we prefetch the teachers, and force the query, we don't want
            # the default manager on teachers to immediately get all the related
            # qualifications, since this will do one query per teacher.
            qs = Department.objects.prefetch_related('teachers')
            depts = "".join(["%s department: %s\n" %
                             (dept.name, ", ".join(unicode(t) for t in dept.teachers.all()))
                             for dept in qs])

            self.assertEqual(depts,
                             "English department: Mr Cleese (BA, BSci, MA, PhD), Mr Idle (BA)\n"
                             "Physics department: Mr Cleese (BA, BSci, MA, PhD), Mr Chapman (BSci)\n")


class GenericRelationTests(TestCase):

    def setUp(self):
        book1 = Book.objects.create(title="Winnie the Pooh")
        book2 = Book.objects.create(title="Do you like green eggs and spam?")
        book3 = Book.objects.create(title="Three Men In A Boat")

        reader1 = Reader.objects.create(name="me")
        reader2 = Reader.objects.create(name="you")
        reader3 = Reader.objects.create(name="someone")

        book1.read_by.add(reader1, reader2)
        book2.read_by.add(reader2)
        book3.read_by.add(reader3)

        self.book1, self.book2, self.book3 = book1, book2, book3
        self.reader1, self.reader2, self.reader3 = reader1, reader2, reader3

    def test_prefetch_GFK(self):
        TaggedItem.objects.create(tag="awesome", content_object=self.book1)
        TaggedItem.objects.create(tag="great", content_object=self.reader1)
        TaggedItem.objects.create(tag="stupid", content_object=self.book2)
        TaggedItem.objects.create(tag="amazing", content_object=self.reader3)

        # 1 for TaggedItem table, 1 for Book table, 1 for Reader table
        with self.assertNumQueries(3):
            qs = TaggedItem.objects.prefetch_related('content_object')
            list(qs)

    def test_prefetch_GFK_nonint_pk(self):
        Comment.objects.create(comment="awesome", content_object=self.book1)

        # 1 for Comment table, 1 for Book table
        with self.assertNumQueries(2):
            qs = Comment.objects.prefetch_related('content_object')
            [c.content_object for c in qs]

    def test_traverse_GFK(self):
        """
        Test that we can traverse a 'content_object' with prefetch_related() and
        get to related objects on the other side (assuming it is suitably
        filtered)
        """
        TaggedItem.objects.create(tag="awesome", content_object=self.book1)
        TaggedItem.objects.create(tag="awesome", content_object=self.book2)
        TaggedItem.objects.create(tag="awesome", content_object=self.book3)
        TaggedItem.objects.create(tag="awesome", content_object=self.reader1)
        TaggedItem.objects.create(tag="awesome", content_object=self.reader2)

        ct = ContentType.objects.get_for_model(Book)

        # We get 3 queries - 1 for main query, 1 for content_objects since they
        # all use the same table, and 1 for the 'read_by' relation.
        with self.assertNumQueries(3):
            # If we limit to books, we know that they will have 'read_by'
            # attributes, so the following makes sense:
            qs = TaggedItem.objects.filter(content_type=ct, tag='awesome').prefetch_related('content_object__read_by')
            readers_of_awesome_books = set([r.name for tag in qs
                                            for r in tag.content_object.read_by.all()])
            self.assertEqual(readers_of_awesome_books, set(["me", "you", "someone"]))

    def test_nullable_GFK(self):
        TaggedItem.objects.create(tag="awesome", content_object=self.book1,
                                  created_by=self.reader1)
        TaggedItem.objects.create(tag="great", content_object=self.book2)
        TaggedItem.objects.create(tag="rubbish", content_object=self.book3)

        with self.assertNumQueries(2):
            result = [t.created_by for t in TaggedItem.objects.prefetch_related('created_by')]

        self.assertEqual(result,
                         [t.created_by for t in TaggedItem.objects.all()])

    def test_generic_relation(self):
        b = Bookmark.objects.create(url='http://www.djangoproject.com/')
        t1 = TaggedItem.objects.create(content_object=b, tag='django')
        t2 = TaggedItem.objects.create(content_object=b, tag='python')

        with self.assertNumQueries(2):
            tags = [t.tag for b in Bookmark.objects.prefetch_related('tags')
                    for t in b.tags.all()]
            self.assertEqual(sorted(tags), ["django", "python"])


class MultiTableInheritanceTest(TestCase):

    def setUp(self):
        self.book1 = BookWithYear.objects.create(
            title="Poems", published_year=2010)
        self.book2 = BookWithYear.objects.create(
            title="More poems", published_year=2011)
        self.author1 = AuthorWithAge.objects.create(
            name='Jane', first_book=self.book1, age=50)
        self.author2 = AuthorWithAge.objects.create(
            name='Tom', first_book=self.book1, age=49)
        self.author3 = AuthorWithAge.objects.create(
            name='Robert', first_book=self.book2, age=48)
        self.authorAddress = AuthorAddress.objects.create(
            author=self.author1, address='SomeStreet 1')
        self.book2.aged_authors.add(self.author2, self.author3)

    def test_foreignkey(self):
        with self.assertNumQueries(2):
            qs = AuthorWithAge.objects.prefetch_related('addresses')
            addresses = [[unicode(address) for address in obj.addresses.all()]
                         for obj in qs]
        self.assertEquals(addresses, [[unicode(self.authorAddress)], [], []])

    def test_m2m_to_inheriting_model(self):
        qs = AuthorWithAge.objects.prefetch_related('books_with_year')
        with self.assertNumQueries(2):
            lst = [[unicode(book) for book in author.books_with_year.all()]
                   for author in qs]
        qs = AuthorWithAge.objects.all()
        lst2 = [[unicode(book) for book in author.books_with_year.all()]
                for author in qs]
        self.assertEquals(lst, lst2)

        qs = BookWithYear.objects.prefetch_related('aged_authors')
        with self.assertNumQueries(2):
            lst = [[unicode(author) for author in book.aged_authors.all()]
                   for book in qs]
        qs = BookWithYear.objects.all()
        lst2 = [[unicode(author) for author in book.aged_authors.all()]
               for book in qs]
        self.assertEquals(lst, lst2)

    def test_parent_link_prefetch(self):
        with self.assertNumQueries(2):
            [a.author for a in AuthorWithAge.objects.prefetch_related('author')]

    @override_settings(DEBUG=True)
    def test_child_link_prefetch(self):
        with self.assertNumQueries(2):
            l = [a.authorwithage for a in Author.objects.prefetch_related('authorwithage')]

        # Regression for #18090: the prefetching query must include an IN clause.
        self.assertIn('authorwithage', connection.queries[-1]['sql'])
        self.assertIn(' IN ', connection.queries[-1]['sql'])

        self.assertEqual(l, [a.authorwithage for a in Author.objects.all()])


class ForeignKeyToFieldTest(TestCase):

    def setUp(self):
        self.book = Book.objects.create(title="Poems")
        self.author1 = Author.objects.create(name='Jane', first_book=self.book)
        self.author2 = Author.objects.create(name='Tom', first_book=self.book)
        self.author3 = Author.objects.create(name='Robert', first_book=self.book)
        self.authorAddress = AuthorAddress.objects.create(
            author=self.author1, address='SomeStreet 1'
        )
        FavoriteAuthors.objects.create(author=self.author1,
                                       likes_author=self.author2)
        FavoriteAuthors.objects.create(author=self.author2,
                                       likes_author=self.author3)
        FavoriteAuthors.objects.create(author=self.author3,
                                       likes_author=self.author1)

    def test_foreignkey(self):
        with self.assertNumQueries(2):
            qs = Author.objects.prefetch_related('addresses')
            addresses = [[unicode(address) for address in obj.addresses.all()]
                         for obj in qs]
        self.assertEquals(addresses, [[unicode(self.authorAddress)], [], []])

    def test_m2m(self):
        with self.assertNumQueries(3):
            qs = Author.objects.all().prefetch_related('favorite_authors', 'favors_me')
            favorites = [(
                 [unicode(i_like) for i_like in author.favorite_authors.all()],
                 [unicode(likes_me) for likes_me in author.favors_me.all()]
                ) for author in qs]
            self.assertEquals(
                favorites,
                [
                    ([unicode(self.author2)],[unicode(self.author3)]),
                    ([unicode(self.author3)],[unicode(self.author1)]),
                    ([unicode(self.author1)],[unicode(self.author2)])
                ]
            )


class LookupOrderingTest(TestCase):
    """
    Test cases that demonstrate that ordering of lookups is important, and
    ensure it is preserved.
    """

    def setUp(self):
        self.person1 = Person.objects.create(name="Joe")
        self.person2 = Person.objects.create(name="Mary")

        self.house1 = House.objects.create(address="123 Main St")
        self.house2 = House.objects.create(address="45 Side St")
        self.house3 = House.objects.create(address="6 Downing St")
        self.house4 = House.objects.create(address="7 Regents St")

        self.room1_1 = Room.objects.create(name="Dining room", house=self.house1)
        self.room1_2 = Room.objects.create(name="Lounge", house=self.house1)
        self.room1_3 = Room.objects.create(name="Kitchen", house=self.house1)

        self.room2_1 = Room.objects.create(name="Dining room", house=self.house2)
        self.room2_2 = Room.objects.create(name="Lounge", house=self.house2)

        self.room3_1 = Room.objects.create(name="Dining room", house=self.house3)
        self.room3_2 = Room.objects.create(name="Lounge", house=self.house3)
        self.room3_3 = Room.objects.create(name="Kitchen", house=self.house3)

        self.room4_1 = Room.objects.create(name="Dining room", house=self.house4)
        self.room4_2 = Room.objects.create(name="Lounge", house=self.house4)

        self.person1.houses.add(self.house1, self.house2)
        self.person2.houses.add(self.house3, self.house4)

    def test_order(self):
        with self.assertNumQueries(4):
            # The following two queries must be done in the same order as written,
            # otherwise 'primary_house' will cause non-prefetched lookups
            qs = Person.objects.prefetch_related('houses__rooms',
                                                 'primary_house__occupants')
            [list(p.primary_house.occupants.all()) for p in qs]


class NullableTest(TestCase):

    def setUp(self):
        boss = Employee.objects.create(name="Peter")
        worker1 = Employee.objects.create(name="Joe", boss=boss)
        worker2 = Employee.objects.create(name="Angela", boss=boss)

    def test_traverse_nullable(self):
        # Because we use select_related() for 'boss', it doesn't need to be
        # prefetched, but we can still traverse it although it contains some nulls
        with self.assertNumQueries(2):
            qs = Employee.objects.select_related('boss').prefetch_related('boss__serfs')
            co_serfs = [list(e.boss.serfs.all()) if e.boss is not None else []
                        for e in qs]

        qs2 =  Employee.objects.select_related('boss')
        co_serfs2 =  [list(e.boss.serfs.all()) if e.boss is not None else []
                        for e in qs2]

        self.assertEqual(co_serfs, co_serfs2)

    def test_prefetch_nullable(self):
        # One for main employee, one for boss, one for serfs
        with self.assertNumQueries(3):
            qs = Employee.objects.prefetch_related('boss__serfs')
            co_serfs = [list(e.boss.serfs.all()) if e.boss is not None else []
                        for e in qs]

        qs2 =  Employee.objects.all()
        co_serfs2 =  [list(e.boss.serfs.all()) if e.boss is not None else []
                        for e in qs2]

        self.assertEqual(co_serfs, co_serfs2)

    def test_in_bulk(self):
        """
        In-bulk does correctly prefetch objects by not using .iterator()
        directly.
        """
        boss1 = Employee.objects.create(name="Peter")
        boss2 = Employee.objects.create(name="Jack")
        with self.assertNumQueries(2):
            # Check that prefetch is done and it does not cause any errors.
            bulk = Employee.objects.prefetch_related('serfs').in_bulk([boss1.pk, boss2.pk])
            for b in bulk.values():
                list(b.serfs.all())


class MultiDbTests(TestCase):
    multi_db = True

    def test_using_is_honored_m2m(self):
        B = Book.objects.using('other')
        A = Author.objects.using('other')
        book1 = B.create(title="Poems")
        book2 = B.create(title="Jane Eyre")
        book3 = B.create(title="Wuthering Heights")
        book4 = B.create(title="Sense and Sensibility")

        author1 = A.create(name="Charlotte", first_book=book1)
        author2 = A.create(name="Anne", first_book=book1)
        author3 = A.create(name="Emily", first_book=book1)
        author4 = A.create(name="Jane", first_book=book4)

        book1.authors.add(author1, author2, author3)
        book2.authors.add(author1)
        book3.authors.add(author3)
        book4.authors.add(author4)

        # Forward
        qs1 = B.prefetch_related('authors')
        with self.assertNumQueries(2, using='other'):
            books = "".join(["%s (%s)\n" %
                             (book.title, ", ".join(a.name for a in book.authors.all()))
                             for book in qs1])
        self.assertEqual(books,
                         "Poems (Charlotte, Anne, Emily)\n"
                         "Jane Eyre (Charlotte)\n"
                         "Wuthering Heights (Emily)\n"
                         "Sense and Sensibility (Jane)\n")

        # Reverse
        qs2 = A.prefetch_related('books')
        with self.assertNumQueries(2, using='other'):
            authors = "".join(["%s: %s\n" %
                               (author.name, ", ".join(b.title for b in author.books.all()))
                               for author in qs2])
        self.assertEquals(authors,
                          "Charlotte: Poems, Jane Eyre\n"
                          "Anne: Poems\n"
                          "Emily: Poems, Wuthering Heights\n"
                          "Jane: Sense and Sensibility\n")

    def test_using_is_honored_fkey(self):
        B = Book.objects.using('other')
        A = Author.objects.using('other')
        book1 = B.create(title="Poems")
        book2 = B.create(title="Sense and Sensibility")

        author1 = A.create(name="Charlotte Bronte", first_book=book1)
        author2 = A.create(name="Jane Austen", first_book=book2)

        # Forward
        with self.assertNumQueries(2, using='other'):
            books = ", ".join(a.first_book.title for a in A.prefetch_related('first_book'))
        self.assertEqual("Poems, Sense and Sensibility", books)

        # Reverse
        with self.assertNumQueries(2, using='other'):
            books = "".join("%s (%s)\n" %
                            (b.title, ", ".join(a.name for a in b.first_time_authors.all()))
                            for b in B.prefetch_related('first_time_authors'))
        self.assertEqual(books,
                         "Poems (Charlotte Bronte)\n"
                         "Sense and Sensibility (Jane Austen)\n")

    def test_using_is_honored_inheritance(self):
        B = BookWithYear.objects.using('other')
        A = AuthorWithAge.objects.using('other')
        book1 = B.create(title="Poems", published_year=2010)
        book2 = B.create(title="More poems", published_year=2011)
        author1 = A.create(name='Jane', first_book=book1, age=50)
        author2 = A.create(name='Tom', first_book=book1, age=49)

        # parent link
        with self.assertNumQueries(2, using='other'):
            authors = ", ".join(a.author.name for a in A.prefetch_related('author'))

        self.assertEqual(authors, "Jane, Tom")

        # child link
        with self.assertNumQueries(2, using='other'):
            ages = ", ".join(str(a.authorwithage.age) for a in A.prefetch_related('authorwithage'))

        self.assertEqual(ages, "50, 49")

from __future__ import absolute_import

from django.conf.urls import patterns
from django.views.generic import RedirectView

from . import views


urlpatterns = patterns('',
    (r'^get_view/$', views.get_view),
    (r'^post_view/$', views.post_view),
    (r'^header_view/$', views.view_with_header),
    (r'^raw_post_view/$', views.raw_post_view),
    (r'^redirect_view/$', views.redirect_view),
    (r'^secure_view/$', views.view_with_secure),
    (r'^permanent_redirect_view/$', RedirectView.as_view(url='/test_client/get_view/')),
    (r'^temporary_redirect_view/$', RedirectView.as_view(url='/test_client/get_view/', permanent=False)),
    (r'^http_redirect_view/$', RedirectView.as_view(url='/test_client/secure_view/')),
    (r'^https_redirect_view/$', RedirectView.as_view(url='https://testserver/test_client/secure_view/')),
    (r'^double_redirect_view/$', views.double_redirect_view),
    (r'^bad_view/$', views.bad_view),
    (r'^form_view/$', views.form_view),
    (r'^form_view_with_template/$', views.form_view_with_template),
    (r'^login_protected_view/$', views.login_protected_view),
    (r'^login_protected_method_view/$', views.login_protected_method_view),
    (r'^login_protected_view_custom_redirect/$', views.login_protected_view_changed_redirect),
    (r'^permission_protected_view/$', views.permission_protected_view),
    (r'^permission_protected_view_exception/$', views.permission_protected_view_exception),
    (r'^permission_protected_method_view/$', views.permission_protected_method_view),
    (r'^session_view/$', views.session_view),
    (r'^broken_view/$', views.broken_view),
    (r'^mail_sending_view/$', views.mail_sending_view),
    (r'^mass_mail_sending_view/$', views.mass_mail_sending_view)
)

from __future__ import absolute_import

from django.core.urlresolvers import reverse
from django.template.response import TemplateResponse
from django.test import TestCase

from .models import Action


class AdminCustomUrlsTest(TestCase):
    fixtures = ['users.json', 'actions.json']

    def setUp(self):
        self.client.login(username='super', password='secret')

    def tearDown(self):
        self.client.logout()

    def testBasicAddGet(self):
        """
        A smoke test to ensure GET on the add_view works.
        """
        response = self.client.get('/custom_urls/admin/admin_custom_urls/action/!add/')
        self.assertIsInstance(response, TemplateResponse)
        self.assertEqual(response.status_code, 200)

    def testAddWithGETArgs(self):
        response = self.client.get('/custom_urls/admin/admin_custom_urls/action/!add/', {'name': 'My Action'})
        self.assertEqual(response.status_code, 200)
        self.assertTrue(
            'value="My Action"' in response.content,
            "Couldn't find an input with the right value in the response."
        )

    def testBasicAddPost(self):
        """
        A smoke test to ensure POST on add_view works.
        """
        post_data = {
            '_popup': u'1',
            "name": u'Action added through a popup',
            "description": u"Description of added action",
        }
        response = self.client.post('/custom_urls/admin/admin_custom_urls/action/!add/', post_data)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'dismissAddAnotherPopup')
        self.assertContains(response, 'Action added through a popup')

    def testAdminUrlsNoClash(self):
        """
        Test that some admin URLs work correctly. The model has a CharField
        PK and the add_view URL has been customized.
        """
        # Should get the change_view for model instance with PK 'add', not show
        # the add_view
        response = self.client.get('/custom_urls/admin/admin_custom_urls/action/add/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Change action')

        # Ditto, but use reverse() to build the URL
        path = reverse('admin:%s_action_change' % Action._meta.app_label,
                args=('add',))
        response = self.client.get(path)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Change action')

        # Should correctly get the change_view for the model instance with the
        # funny-looking PK
        path = reverse('admin:%s_action_change' % Action._meta.app_label,
                args=("path/to/html/document.html",))
        response = self.client.get(path)
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, 'Change action')
        self.assertContains(response, 'value="path/to/html/document.html"')

"""
A series of tests to establish that the command-line managment tools work as
advertised - especially with regards to the handling of the DJANGO_SETTINGS_MODULE
and default settings.py files.
"""
from __future__ import with_statement

import os
import re
import shutil
import socket
import subprocess
import sys

from django import conf, bin, get_version
from django.conf import settings
from django.test.simple import DjangoTestSuiteRunner
from django.utils import unittest
from django.test import LiveServerTestCase

test_dir = os.path.dirname(os.path.dirname(__file__))
expected_query_re = re.compile(r'CREATE TABLE [`"]admin_scripts_article[`"]', re.IGNORECASE)


class AdminScriptTestCase(unittest.TestCase):
    def write_settings(self, filename, apps=None, is_dir=False, sdict=None):
        test_dir = os.path.dirname(os.path.dirname(__file__))
        if is_dir:
            settings_dir = os.path.join(test_dir, filename)
            os.mkdir(settings_dir)
            settings_file = open(os.path.join(settings_dir, '__init__.py'), 'w')
        else:
            settings_file = open(os.path.join(test_dir, filename), 'w')
        settings_file.write('# Settings file automatically generated by regressiontests.admin_scripts test case\n')
        exports = [
            'DATABASES',
            'ROOT_URLCONF',
            'SECRET_KEY',
        ]
        for s in exports:
            if hasattr(settings, s):
                o = getattr(settings, s)
                if not isinstance(o, dict):
                    o = "'%s'" % o
                settings_file.write("%s = %s\n" % (s, o))

        if apps is None:
            apps = ['django.contrib.auth', 'django.contrib.contenttypes', 'regressiontests.admin_scripts']

        settings_file.write("INSTALLED_APPS = %s\n" % apps)

        if sdict:
            for k, v in sdict.items():
                settings_file.write("%s = %s\n" % (k, v))

        settings_file.close()

    def remove_settings(self, filename, is_dir=False):
        full_name = os.path.join(test_dir, filename)
        if is_dir:
            shutil.rmtree(full_name)
        else:
            os.remove(full_name)

        # Also try to remove the compiled file; if it exists, it could
        # mess up later tests that depend upon the .py file not existing
        try:
            if sys.platform.startswith('java'):
                # Jython produces module$py.class files
                os.remove(re.sub(r'\.py$', '$py.class', full_name))
            else:
                # CPython produces module.pyc files
                os.remove(full_name + 'c')
        except OSError:
            pass

    def _ext_backend_paths(self):
        """
        Returns the paths for any external backend packages.
        """
        paths = []
        first_package_re = re.compile(r'(^[^\.]+)\.')
        for backend in settings.DATABASES.values():
            result = first_package_re.findall(backend['ENGINE'])
            if result and result != 'django':
                backend_pkg = __import__(result[0])
                backend_dir = os.path.dirname(backend_pkg.__file__)
                paths.append(os.path.dirname(backend_dir))
        return paths

    def run_test(self, script, args, settings_file=None, apps=None):
        project_dir = os.path.dirname(test_dir)
        base_dir = os.path.dirname(project_dir)
        ext_backend_base_dirs = self._ext_backend_paths()

        # Remember the old environment
        old_django_settings_module = os.environ.get('DJANGO_SETTINGS_MODULE', None)
        if sys.platform.startswith('java'):
            python_path_var_name = 'JYTHONPATH'
        else:
            python_path_var_name = 'PYTHONPATH'

        old_python_path = os.environ.get(python_path_var_name, None)
        old_cwd = os.getcwd()

        # Set the test environment
        if settings_file:
            os.environ['DJANGO_SETTINGS_MODULE'] = settings_file
        elif 'DJANGO_SETTINGS_MODULE' in os.environ:
            del os.environ['DJANGO_SETTINGS_MODULE']
        python_path = [project_dir, base_dir]
        python_path.extend(ext_backend_base_dirs)
        os.environ[python_path_var_name] = os.pathsep.join(python_path)

        # Silence the DeprecationWarning caused by having a locale directory
        # in the project directory.
        cmd = [sys.executable, '-Wignore:::django.utils.translation', script]

        # Move to the test directory and run
        os.chdir(test_dir)
        out, err = subprocess.Popen(cmd + args,
                stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()

        # Restore the old environment
        if old_django_settings_module:
            os.environ['DJANGO_SETTINGS_MODULE'] = old_django_settings_module
        if old_python_path:
            os.environ[python_path_var_name] = old_python_path
        # Move back to the old working directory
        os.chdir(old_cwd)

        return out, err

    def run_django_admin(self, args, settings_file=None):
        bin_dir = os.path.abspath(os.path.dirname(bin.__file__))
        return self.run_test(os.path.join(bin_dir, 'django-admin.py'), args, settings_file)

    def run_manage(self, args, settings_file=None):
        conf_dir = os.path.dirname(conf.__file__)
        template_manage_py = os.path.join(conf_dir, 'project_template', 'manage.py')

        test_manage_py = os.path.join(test_dir, 'manage.py')
        shutil.copyfile(template_manage_py, test_manage_py)

        with open(test_manage_py, 'r') as fp:
            manage_py_contents = fp.read()
        manage_py_contents = manage_py_contents.replace(
            "{{ project_name }}", "regressiontests")
        with open(test_manage_py, 'w') as fp:
            fp.write(manage_py_contents)

        stdout, stderr = self.run_test('./manage.py', args, settings_file)

        # Cleanup - remove the generated manage.py script
        os.remove(test_manage_py)

        return stdout, stderr

    def assertNoOutput(self, stream):
        "Utility assertion: assert that the given stream is empty"
        # HACK: Under Windows, ignore warnings of the form:
        # 'warning: Not loading directory '...\tests\regressiontests\locale': missing __init__.py'
        # It has been impossible to filter them out using other means like:
        # * Using warning.filterwarnings() (for the Python interpreter running the
        #   tests) and/or
        # * Using -Wignore:... (for the python interpreter spawned in self.run_test())
        # Instead use a strategy copied from Mercurial's setup.py
        if sys.platform == 'win32':
            stream = [e for e in stream.splitlines()
                if not e.startswith('warning: Not importing directory')]
        self.assertEqual(len(stream), 0, "Stream should be empty: actually contains '%s'" % stream)

    def assertOutput(self, stream, msg):
        "Utility assertion: assert that the given message exists in the output"
        self.assertTrue(msg in stream, "'%s' does not match actual output text '%s'" % (msg, stream))

    def assertNotInOutput(self, stream, msg):
        "Utility assertion: assert that the given message doesn't exist in the output"
        self.assertFalse(msg in stream, "'%s' matches actual output text '%s'" % (msg, stream))

##########################################################################
# DJANGO ADMIN TESTS
# This first series of test classes checks the environment processing
# of the django-admin.py script
##########################################################################


class DjangoAdminNoSettings(AdminScriptTestCase):
    "A series of tests for django-admin.py when there is no settings.py file."

    def test_builtin_command(self):
        "no settings: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_bad_settings(self):
        "no settings: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "no settings: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")


class DjangoAdminDefaultSettings(AdminScriptTestCase):
    """A series of tests for django-admin.py when using a settings.py file that
    contains the test application.
    """
    def setUp(self):
        self.write_settings('settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "default: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_settings(self):
        "default: django-admin builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "default: django-admin builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "default: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "default: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "default: django-admin can't execute user commands if it isn't provided settings"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "default: django-admin can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "default: django-admin can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

class DjangoAdminFullPathDefaultSettings(AdminScriptTestCase):
    """A series of tests for django-admin.py when using a settings.py file that
    contains the test application specified using a full path.
    """
    def setUp(self):
        self.write_settings('settings.py', ['django.contrib.auth', 'django.contrib.contenttypes', 'regressiontests.admin_scripts'])

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "fulldefault: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_settings(self):
        "fulldefault: django-admin builtin commands succeed if a settings file is provided"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "fulldefault: django-admin builtin commands succeed if the environment contains settings"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "fulldefault: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "fulldefault: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "fulldefault: django-admin can't execute user commands unless settings are provided"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "fulldefault: django-admin can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "fulldefault: django-admin can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

class DjangoAdminMinimalSettings(AdminScriptTestCase):
    """A series of tests for django-admin.py when using a settings.py file that
    doesn't contain the test application.
    """
    def setUp(self):
        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "minimal: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_settings(self):
        "minimal: django-admin builtin commands fail if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found')

    def test_builtin_with_environment(self):
        "minimal: django-admin builtin commands fail if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found')

    def test_builtin_with_bad_settings(self):
        "minimal: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "minimal: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "minimal: django-admin can't execute user commands unless settings are provided"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "minimal: django-admin can't execute user commands, even if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_environment(self):
        "minimal: django-admin can't execute user commands, even if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

class DjangoAdminAlternateSettings(AdminScriptTestCase):
    """A series of tests for django-admin.py when using a settings file
    with a name other than 'settings.py'.
    """
    def setUp(self):
        self.write_settings('alternate_settings.py')

    def tearDown(self):
        self.remove_settings('alternate_settings.py')

    def test_builtin_command(self):
        "alternate: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_settings(self):
        "alternate: django-admin builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.alternate_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "alternate: django-admin builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "alternate: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "alternate: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "alternate: django-admin can't execute user commands unless settings are provided"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "alternate: django-admin can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.alternate_settings']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "alternate: django-admin can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_django_admin(args, 'regressiontests.alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")


class DjangoAdminMultipleSettings(AdminScriptTestCase):
    """A series of tests for django-admin.py when multiple settings files
    (including the default 'settings.py') are available. The default settings
    file is insufficient for performing the operations described, so the
    alternate settings must be used by the running script.
    """
    def setUp(self):
        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])
        self.write_settings('alternate_settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')
        self.remove_settings('alternate_settings.py')

    def test_builtin_command(self):
        "alternate: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_settings(self):
        "alternate: django-admin builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.alternate_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "alternate: django-admin builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "alternate: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "alternate: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "alternate: django-admin can't execute user commands unless settings are provided"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "alternate: django-admin can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.alternate_settings']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "alternate: django-admin can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_django_admin(args, 'regressiontests.alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")


class DjangoAdminSettingsDirectory(AdminScriptTestCase):
    """
    A series of tests for django-admin.py when the settings file is in a
    directory. (see #9751).
    """

    def setUp(self):
        self.write_settings('settings', is_dir=True)

    def tearDown(self):
        self.remove_settings('settings', is_dir=True)

    def test_setup_environ(self):
        "directory: startapp creates the correct directory"
        args = ['startapp', 'settings_test']
        app_path = os.path.join(test_dir, 'settings_test')
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.addCleanup(shutil.rmtree, app_path)
        self.assertNoOutput(err)
        self.assertTrue(os.path.exists(app_path))

    def test_setup_environ_custom_template(self):
        "directory: startapp creates the correct directory with a custom template"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'app_template')
        args = ['startapp', '--template', template_path, 'custom_settings_test']
        app_path = os.path.join(test_dir, 'custom_settings_test')
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.addCleanup(shutil.rmtree, app_path)
        self.assertNoOutput(err)
        self.assertTrue(os.path.exists(app_path))
        self.assertTrue(os.path.exists(os.path.join(app_path, 'api.py')))

    def test_builtin_command(self):
        "directory: django-admin builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'environment variable DJANGO_SETTINGS_MODULE is undefined')

    def test_builtin_with_bad_settings(self):
        "directory: django-admin builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "directory: django-admin builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "directory: django-admin can't execute user commands unless settings are provided"
        args = ['noargs_command']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_builtin_with_settings(self):
        "directory: django-admin builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_django_admin(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "directory: django-admin builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_django_admin(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')


##########################################################################
# MANAGE.PY TESTS
# This next series of test classes checks the environment processing
# of the generated manage.py script
##########################################################################

class ManageNoSettings(AdminScriptTestCase):
    "A series of tests for manage.py when there is no settings.py file."

    def test_builtin_command(self):
        "no settings: manage.py builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'regressiontests.settings'")

    def test_builtin_with_bad_settings(self):
        "no settings: manage.py builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "no settings: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")


class ManageDefaultSettings(AdminScriptTestCase):
    """A series of tests for manage.py when using a settings.py file that
    contains the test application.
    """
    def setUp(self):
        self.write_settings('settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "default: manage.py builtin commands succeed when default settings are appropriate"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_settings(self):
        "default: manage.py builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "default: manage.py builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "default: manage.py builtin commands succeed if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "default: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "default: manage.py can execute user commands when default settings are appropriate"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_settings(self):
        "default: manage.py can execute user commands when settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "default: manage.py can execute user commands when settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")


class ManageFullPathDefaultSettings(AdminScriptTestCase):
    """A series of tests for manage.py when using a settings.py file that
    contains the test application specified using a full path.
    """
    def setUp(self):
        self.write_settings('settings.py', ['django.contrib.auth', 'django.contrib.contenttypes', 'regressiontests.admin_scripts'])

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "fulldefault: manage.py builtin commands succeed when default settings are appropriate"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_settings(self):
        "fulldefault: manage.py builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "fulldefault: manage.py builtin commands succeed if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "fulldefault: manage.py builtin commands succeed if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "fulldefault: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "fulldefault: manage.py can execute user commands when default settings are appropriate"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_settings(self):
        "fulldefault: manage.py can execute user commands when settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "fulldefault: manage.py can execute user commands when settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

class ManageMinimalSettings(AdminScriptTestCase):
    """A series of tests for manage.py when using a settings.py file that
    doesn't contain the test application.
    """
    def setUp(self):
        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_builtin_command(self):
        "minimal: manage.py builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found')

    def test_builtin_with_settings(self):
        "minimal: manage.py builtin commands fail if settings are provided as argument"
        args = ['sqlall', '--settings=regressiontests.settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found')

    def test_builtin_with_environment(self):
        "minimal: manage.py builtin commands fail if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found')

    def test_builtin_with_bad_settings(self):
        "minimal: manage.py builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "minimal: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "minimal: manage.py can't execute user commands without appropriate settings"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "minimal: manage.py can't execute user commands, even if settings are provided as argument"
        args = ['noargs_command', '--settings=regressiontests.settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_environment(self):
        "minimal: manage.py can't execute user commands, even if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_manage(args, 'regressiontests.settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

class ManageAlternateSettings(AdminScriptTestCase):
    """A series of tests for manage.py when using a settings file
    with a name other than 'settings.py'.
    """
    def setUp(self):
        self.write_settings('alternate_settings.py')

    def tearDown(self):
        self.remove_settings('alternate_settings.py')

    def test_builtin_command(self):
        "alternate: manage.py builtin commands fail with an import error when no default settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'regressiontests.settings'")

    def test_builtin_with_settings(self):
        "alternate: manage.py builtin commands work with settings provided as argument"
        args = ['sqlall', '--settings=alternate_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertRegexpMatches(out, expected_query_re)
        self.assertNoOutput(err)

    def test_builtin_with_environment(self):
        "alternate: manage.py builtin commands work if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'alternate_settings')
        self.assertRegexpMatches(out, expected_query_re)
        self.assertNoOutput(err)

    def test_builtin_with_bad_settings(self):
        "alternate: manage.py builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "alternate: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "alternate: manage.py can't execute user commands without settings"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "alternate: manage.py can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=alternate_settings']
        out, err = self.run_manage(args)
        self.assertOutput(out, "EXECUTE:NoArgsCommand options=[('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")
        self.assertNoOutput(err)

    def test_custom_command_with_environment(self):
        "alternate: manage.py can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_manage(args, 'alternate_settings')
        self.assertOutput(out, "EXECUTE:NoArgsCommand options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")
        self.assertNoOutput(err)


class ManageMultipleSettings(AdminScriptTestCase):
    """A series of tests for manage.py when multiple settings files
    (including the default 'settings.py') are available. The default settings
    file is insufficient for performing the operations described, so the
    alternate settings must be used by the running script.
    """
    def setUp(self):
        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])
        self.write_settings('alternate_settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')
        self.remove_settings('alternate_settings.py')

    def test_builtin_command(self):
        "multiple: manage.py builtin commands fail with an import error when no settings provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'App with label admin_scripts could not be found.')

    def test_builtin_with_settings(self):
        "multiple: manage.py builtin commands succeed if settings are provided as argument"
        args = ['sqlall', '--settings=alternate_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_environment(self):
        "multiple: manage.py can execute builtin commands if settings are provided in the environment"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, 'CREATE TABLE')

    def test_builtin_with_bad_settings(self):
        "multiple: manage.py builtin commands fail if settings file (from argument) doesn't exist"
        args = ['sqlall', '--settings=bad_settings', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_builtin_with_bad_environment(self):
        "multiple: manage.py builtin commands fail if settings file (from environment) doesn't exist"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args, 'bad_settings')
        self.assertNoOutput(out)
        self.assertOutput(err, "Could not import settings 'bad_settings'")

    def test_custom_command(self):
        "multiple: manage.py can't execute user commands using default settings"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Unknown command: 'noargs_command'")

    def test_custom_command_with_settings(self):
        "multiple: manage.py can execute user commands if settings are provided as argument"
        args = ['noargs_command', '--settings=alternate_settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

    def test_custom_command_with_environment(self):
        "multiple: manage.py can execute user commands if settings are provided in environment"
        args = ['noargs_command']
        out, err = self.run_manage(args, 'alternate_settings')
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand")

class ManageSettingsWithImportError(AdminScriptTestCase):
    """Tests for manage.py when using the default settings.py file
    with an import error. Ticket #14130.
    """
    def setUp(self):
        self.write_settings_with_import_error('settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')

    def write_settings_with_import_error(self, filename, apps=None, is_dir=False, sdict=None):
        if is_dir:
            settings_dir = os.path.join(test_dir, filename)
            os.mkdir(settings_dir)
            settings_file = open(os.path.join(settings_dir, '__init__.py'), 'w')
        else:
            settings_file = open(os.path.join(test_dir, filename), 'w')
        settings_file.write('# Settings file automatically generated by regressiontests.admin_scripts test case\n')
        settings_file.write('# The next line will cause an import error:\nimport foo42bar\n')

        settings_file.close()

    def test_builtin_command(self):
        "import error: manage.py builtin commands shows useful diagnostic info when settings with import errors is provided"
        args = ['sqlall', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "No module named foo42bar")

class ManageValidate(AdminScriptTestCase):
    def tearDown(self):
        self.remove_settings('settings.py')

    def test_nonexistent_app(self):
        "manage.py validate reports an error on a non-existent app in INSTALLED_APPS"
        self.write_settings('settings.py', apps=['admin_scriptz.broken_app'], sdict={'USE_I18N': False})
        args = ['validate']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'No module named admin_scriptz')

    def test_broken_app(self):
        "manage.py validate reports an ImportError if an app's models.py raises one on import"
        self.write_settings('settings.py', apps=['admin_scripts.broken_app'])
        args = ['validate']
        out, err = self.run_manage(args)
        self.assertNoOutput(out)
        self.assertOutput(err, 'ImportError')

    def test_complex_app(self):
        "manage.py validate does not raise an ImportError validating a complex app with nested calls to load_app"
        self.write_settings('settings.py',
            apps=['admin_scripts.complex_app', 'admin_scripts.simple_app'],
            sdict={'DEBUG': True})
        args = ['validate']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, '0 errors found')

    def test_app_with_import(self):
        "manage.py validate does not raise errors when an app imports a base class that itself has an abstract base"
        self.write_settings('settings.py',
            apps=['admin_scripts.app_with_import',
                  'django.contrib.comments',
                  'django.contrib.auth',
                  'django.contrib.contenttypes',
                  'django.contrib.sites'],
            sdict={'DEBUG': True})
        args = ['validate']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, '0 errors found')


class CustomTestRunner(DjangoTestSuiteRunner):

    def __init__(self, *args, **kwargs):
        assert 'liveserver' not in kwargs
        super(CustomTestRunner, self).__init__(*args, **kwargs)

    def run_tests(self, test_labels, extra_tests=None, **kwargs):
        pass

class ManageTestCommand(AdminScriptTestCase):
    def setUp(self):
        from django.core.management.commands.test import Command as TestCommand
        self.cmd = TestCommand()

    def test_liveserver(self):
        """
        Ensure that the --liveserver option sets the environment variable
        correctly.
        Refs #2879.
        """

        # Backup original state
        address_predefined = 'DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ
        old_address = os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS')

        self.cmd.handle(verbosity=0, testrunner='regressiontests.admin_scripts.tests.CustomTestRunner')

        # Original state hasn't changed
        self.assertEqual('DJANGO_LIVE_TEST_SERVER_ADDRESS' in os.environ, address_predefined)
        self.assertEqual(os.environ.get('DJANGO_LIVE_TEST_SERVER_ADDRESS'), old_address)

        self.cmd.handle(verbosity=0, testrunner='regressiontests.admin_scripts.tests.CustomTestRunner',
                        liveserver='blah')

        # Variable was correctly set
        self.assertEqual(os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'], 'blah')

        # Restore original state
        if address_predefined:
            os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS'] = old_address
        else:
            del os.environ['DJANGO_LIVE_TEST_SERVER_ADDRESS']


class ManageRunserver(AdminScriptTestCase):
    def setUp(self):
        from django.core.management.commands.runserver import BaseRunserverCommand

        def monkey_run(*args, **options):
            return

        self.cmd = BaseRunserverCommand()
        self.cmd.run = monkey_run

    def assertServerSettings(self, addr, port, ipv6=None, raw_ipv6=False):
        self.assertEqual(self.cmd.addr, addr)
        self.assertEqual(self.cmd.port, port)
        self.assertEqual(self.cmd.use_ipv6, ipv6)
        self.assertEqual(self.cmd._raw_ipv6, raw_ipv6)

    def test_runserver_addrport(self):
        self.cmd.handle()
        self.assertServerSettings('127.0.0.1', '8000')

        self.cmd.handle(addrport="1.2.3.4:8000")
        self.assertServerSettings('1.2.3.4', '8000')

        self.cmd.handle(addrport="7000")
        self.assertServerSettings('127.0.0.1', '7000')

    @unittest.skipUnless(socket.has_ipv6, "platform doesn't support IPv6")
    def test_runner_addrport_ipv6(self):
        self.cmd.handle(addrport="", use_ipv6=True)
        self.assertServerSettings('::1', '8000', ipv6=True, raw_ipv6=True)

        self.cmd.handle(addrport="7000", use_ipv6=True)
        self.assertServerSettings('::1', '7000', ipv6=True, raw_ipv6=True)

        self.cmd.handle(addrport="[2001:0db8:1234:5678::9]:7000")
        self.assertServerSettings('2001:0db8:1234:5678::9', '7000', ipv6=True, raw_ipv6=True)

    def test_runner_hostname(self):
        self.cmd.handle(addrport="localhost:8000")
        self.assertServerSettings('localhost', '8000')

        self.cmd.handle(addrport="test.domain.local:7000")
        self.assertServerSettings('test.domain.local', '7000')

    @unittest.skipUnless(socket.has_ipv6, "platform doesn't support IPv6")
    def test_runner_hostname_ipv6(self):
        self.cmd.handle(addrport="test.domain.local:7000", use_ipv6=True)
        self.assertServerSettings('test.domain.local', '7000', ipv6=True)

    def test_runner_ambiguous(self):
        # Only 4 characters, all of which could be in an ipv6 address
        self.cmd.handle(addrport="beef:7654")
        self.assertServerSettings('beef', '7654')

        # Uses only characters that could be in an ipv6 address
        self.cmd.handle(addrport="deadbeef:7654")
        self.assertServerSettings('deadbeef', '7654')


##########################################################################
# COMMAND PROCESSING TESTS
# Check that user-space commands are correctly handled - in particular,
# that arguments to the commands are correctly parsed and processed.
##########################################################################

class CommandTypes(AdminScriptTestCase):
    "Tests for the various types of base command types that can be defined."
    def setUp(self):
        self.write_settings('settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')

    def test_version(self):
        "version is handled as a special case"
        args = ['version']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, get_version())

    def test_version_alternative(self):
        "--version is equivalent to version"
        args1, args2 = ['version'], ['--version']
        self.assertEqual(self.run_manage(args1), self.run_manage(args2))

    def test_help(self):
        "help is handled as a special case"
        args = ['help']
        out, err = self.run_manage(args)
        self.assertOutput(out, "Usage: manage.py subcommand [options] [args]")
        self.assertOutput(out, "Type 'manage.py help <subcommand>' for help on a specific subcommand.")
        self.assertOutput(out, '[django]')
        self.assertOutput(out, 'startapp')
        self.assertOutput(out, 'startproject')

    def test_help_commands(self):
        "help --commands shows the list of all available commands"
        args = ['help', '--commands']
        out, err = self.run_manage(args)
        self.assertNotInOutput(out, 'Usage:')
        self.assertNotInOutput(out, 'Options:')
        self.assertNotInOutput(out, '[django]')
        self.assertOutput(out, 'startapp')
        self.assertOutput(out, 'startproject')
        self.assertNotInOutput(out, '\n\n')

    def test_help_alternative(self):
        "--help is equivalent to help"
        args1, args2 = ['help'], ['--help']
        self.assertEqual(self.run_manage(args1), self.run_manage(args2))

    def test_help_short_altert(self):
        "-h is handled as a short form of --help"
        args1, args2 = ['--help'], ['-h']
        self.assertEqual(self.run_manage(args1), self.run_manage(args2))

    def test_specific_help(self):
        "--help can be used on a specific command"
        args = ['sqlall', '--help']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "Prints the CREATE TABLE, custom SQL and CREATE INDEX SQL statements for the given model module name(s).")

    def test_base_command(self):
        "User BaseCommands can execute when a label is provided"
        args = ['base_command', 'testlabel']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_base_command_no_label(self):
        "User BaseCommands can execute when no labels are provided"
        args = ['base_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=(), options=[('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_base_command_multiple_label(self):
        "User BaseCommands can execute when no labels are provided"
        args = ['base_command', 'testlabel', 'anotherlabel']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel', 'anotherlabel'), options=[('option_a', '1'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_base_command_with_option(self):
        "User BaseCommands can execute with options when a label is provided"
        args = ['base_command', 'testlabel', '--option_a=x']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_base_command_with_options(self):
        "User BaseCommands can execute with multiple options when a label is provided"
        args = ['base_command', 'testlabel', '-a', 'x', '--option_b=y']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', 'y'), ('option_c', '3'), ('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_noargs(self):
        "NoArg Commands can be executed"
        args = ['noargs_command']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:NoArgsCommand options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_noargs_with_args(self):
        "NoArg Commands raise an error if an argument is provided"
        args = ['noargs_command', 'argument']
        out, err = self.run_manage(args)
        self.assertOutput(err, "Error: Command doesn't accept any arguments")

    def test_app_command(self):
        "User AppCommands can execute when a single app name is provided"
        args = ['app_command', 'auth']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:AppCommand app=<module 'django.contrib.auth.models'")
        self.assertOutput(out, os.sep.join(['django', 'contrib', 'auth', 'models.py']))
        self.assertOutput(out, "'>, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_app_command_no_apps(self):
        "User AppCommands raise an error when no app name is provided"
        args = ['app_command']
        out, err = self.run_manage(args)
        self.assertOutput(err, 'Error: Enter at least one appname.')

    def test_app_command_multiple_apps(self):
        "User AppCommands raise an error when multiple app names are provided"
        args = ['app_command', 'auth', 'contenttypes']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:AppCommand app=<module 'django.contrib.auth.models'")
        self.assertOutput(out, os.sep.join(['django', 'contrib', 'auth', 'models.py']))
        self.assertOutput(out, "'>, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")
        self.assertOutput(out, "EXECUTE:AppCommand app=<module 'django.contrib.contenttypes.models'")
        self.assertOutput(out, os.sep.join(['django', 'contrib', 'contenttypes', 'models.py']))
        self.assertOutput(out, "'>, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_app_command_invalid_appname(self):
        "User AppCommands can execute when a single app name is provided"
        args = ['app_command', 'NOT_AN_APP']
        out, err = self.run_manage(args)
        self.assertOutput(err, "App with label NOT_AN_APP could not be found")

    def test_app_command_some_invalid_appnames(self):
        "User AppCommands can execute when some of the provided app names are invalid"
        args = ['app_command', 'auth', 'NOT_AN_APP']
        out, err = self.run_manage(args)
        self.assertOutput(err, "App with label NOT_AN_APP could not be found")

    def test_label_command(self):
        "User LabelCommands can execute when a label is provided"
        args = ['label_command', 'testlabel']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:LabelCommand label=testlabel, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

    def test_label_command_no_label(self):
        "User LabelCommands raise an error if no label is provided"
        args = ['label_command']
        out, err = self.run_manage(args)
        self.assertOutput(err, 'Enter at least one label')

    def test_label_command_multiple_label(self):
        "User LabelCommands are executed multiple times if multiple labels are provided"
        args = ['label_command', 'testlabel', 'anotherlabel']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:LabelCommand label=testlabel, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")
        self.assertOutput(out, "EXECUTE:LabelCommand label=anotherlabel, options=[('pythonpath', None), ('settings', None), ('traceback', None), ('verbosity', '1')]")

class ArgumentOrder(AdminScriptTestCase):
    """Tests for 2-stage argument parsing scheme.

    django-admin command arguments are parsed in 2 parts; the core arguments
    (--settings, --traceback and --pythonpath) are parsed using a Lax parser.
    This Lax parser ignores any unknown options. Then the full settings are
    passed to the command parser, which extracts commands of interest to the
    individual command.
    """
    def setUp(self):
        self.write_settings('settings.py', apps=['django.contrib.auth', 'django.contrib.contenttypes'])
        self.write_settings('alternate_settings.py')

    def tearDown(self):
        self.remove_settings('settings.py')
        self.remove_settings('alternate_settings.py')

    def test_setting_then_option(self):
        "Options passed after settings are correctly handled"
        args = ['base_command', 'testlabel', '--settings=alternate_settings', '--option_a=x']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")

    def test_setting_then_short_option(self):
        "Short options passed after settings are correctly handled"
        args = ['base_command', 'testlabel', '--settings=alternate_settings', '--option_a=x']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")

    def test_option_then_setting(self):
        "Options passed before settings are correctly handled"
        args = ['base_command', 'testlabel', '--option_a=x', '--settings=alternate_settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")

    def test_short_option_then_setting(self):
        "Short options passed before settings are correctly handled"
        args = ['base_command', 'testlabel', '-a', 'x', '--settings=alternate_settings']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', '2'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")

    def test_option_then_setting_then_option(self):
        "Options are correctly handled when they are passed before and after a setting"
        args = ['base_command', 'testlabel', '--option_a=x', '--settings=alternate_settings', '--option_b=y']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, "EXECUTE:BaseCommand labels=('testlabel',), options=[('option_a', 'x'), ('option_b', 'y'), ('option_c', '3'), ('pythonpath', None), ('settings', 'alternate_settings'), ('traceback', None), ('verbosity', '1')]")


class StartProject(LiveServerTestCase, AdminScriptTestCase):

    def test_wrong_args(self):
        "Make sure passing the wrong kinds of arguments raises a CommandError"
        out, err = self.run_django_admin(['startproject'])
        self.assertNoOutput(out)
        self.assertOutput(err, "you must provide a project name")

    def test_simple_project(self):
        "Make sure the startproject management command creates a project"
        args = ['startproject', 'testproject']
        testproject_dir = os.path.join(test_dir, 'testproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))

        # running again..
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "already exists")

    def test_invalid_project_name(self):
        "Make sure the startproject management command validates a project name"

        def cleanup(p):
            if os.path.exists(p):
                shutil.rmtree(p)

        args = ['startproject', '7testproject']
        testproject_dir = os.path.join(test_dir, '7testproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(cleanup, testproject_dir)
        self.assertOutput(err, "Error: '7testproject' is not a valid project name. Please make sure the name begins with a letter or underscore.")
        self.assertFalse(os.path.exists(testproject_dir))

    def test_simple_project_different_directory(self):
        "Make sure the startproject management command creates a project in a specific directory"
        args = ['startproject', 'testproject', 'othertestproject']
        testproject_dir = os.path.join(test_dir, 'othertestproject')
        os.mkdir(testproject_dir)

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'manage.py')))

        # running again..
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "already exists")

    def test_custom_project_template(self):
        "Make sure the startproject management command is able to use a different project template"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template')
        args = ['startproject', '--template', template_path, 'customtestproject']
        testproject_dir = os.path.join(test_dir, 'customtestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'additional_dir')))

    def test_template_dir_with_trailing_slash(self):
        "Ticket 17475: Template dir passed has a trailing path separator"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template' + os.sep)
        args = ['startproject', '--template', template_path, 'customtestproject']
        testproject_dir = os.path.join(test_dir, 'customtestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'additional_dir')))

    def test_custom_project_template_from_tarball_by_path(self):
        "Make sure the startproject management command is able to use a different project template from a tarball"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template.tgz')
        args = ['startproject', '--template', template_path, 'tarballtestproject']
        testproject_dir = os.path.join(test_dir, 'tarballtestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'run.py')))

    def test_custom_project_template_from_tarball_to_alternative_location(self):
        "Startproject can use a project template from a tarball and create it in a specified location"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template.tgz')
        args = ['startproject', '--template', template_path, 'tarballtestproject', 'altlocation']
        testproject_dir = os.path.join(test_dir, 'altlocation')
        os.mkdir(testproject_dir)

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'run.py')))

    def test_custom_project_template_from_tarball_by_url(self):
        "Make sure the startproject management command is able to use a different project template from a tarball via a url"
        template_url = '%s/admin_scripts/custom_templates/project_template.tgz' % self.live_server_url

        args = ['startproject', '--template', template_url, 'urltestproject']
        testproject_dir = os.path.join(test_dir, 'urltestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'run.py')))

    def test_project_template_tarball_url(self):
        "Startproject management command handles project template tar/zip balls from non-canonical urls"
        template_url = '%s/admin_scripts/custom_templates/project_template.tgz/' % self.live_server_url

        args = ['startproject', '--template', template_url, 'urltestproject']
        testproject_dir = os.path.join(test_dir, 'urltestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'run.py')))

    def test_file_without_extension(self):
        "Make sure the startproject management command is able to render custom files"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template')
        args = ['startproject', '--template', template_path, 'customtestproject', '-e', 'txt', '-n', 'Procfile']
        testproject_dir = os.path.join(test_dir, 'customtestproject')

        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        self.assertTrue(os.path.isdir(testproject_dir))
        self.assertTrue(os.path.exists(os.path.join(testproject_dir, 'additional_dir')))
        base_path = os.path.join(testproject_dir, 'additional_dir')
        for f in ('Procfile', 'additional_file.py', 'requirements.txt'):
            self.assertTrue(os.path.exists(os.path.join(base_path, f)))
            with open(os.path.join(base_path, f)) as fh:
                self.assertEqual(fh.read(),
                    '# some file for customtestproject test project')

    def test_custom_project_template_context_variables(self):
        "Make sure template context variables are rendered with proper values"
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template')
        args = ['startproject', '--template', template_path, 'another_project', 'project_dir']
        testproject_dir = os.path.join(test_dir, 'project_dir')
        os.mkdir(testproject_dir)
        out, err = self.run_django_admin(args)
        self.addCleanup(shutil.rmtree, testproject_dir)
        self.assertNoOutput(err)
        test_manage_py = os.path.join(testproject_dir, 'manage.py')
        with open(test_manage_py, 'r') as fp:
            content = fp.read()
            self.assertIn("project_name = 'another_project'", content)
            self.assertIn("project_directory = '%s'" % testproject_dir, content)

    def test_custom_project_destination_missing(self):
        """
        Make sure an exception is raised when the provided
        destination directory doesn't exist
        """
        template_path = os.path.join(test_dir, 'admin_scripts', 'custom_templates', 'project_template')
        args = ['startproject', '--template', template_path, 'yet_another_project', 'project_dir2']
        testproject_dir = os.path.join(test_dir, 'project_dir2')
        out, err = self.run_django_admin(args)
        self.assertNoOutput(out)
        self.assertOutput(err, "Destination directory '%s' does not exist, please create it first." % testproject_dir)
        self.assertFalse(os.path.exists(testproject_dir))


from __future__ import absolute_import

from django.conf.urls import patterns, url

from . import views


urlpatterns = patterns('',
    (r'^request_attrs/$', views.request_processor),
)

"""
Upload handlers to test the upload API.
"""

from django.core.files.uploadhandler import FileUploadHandler, StopUpload

class QuotaUploadHandler(FileUploadHandler):
    """
    This test upload handler terminates the connection if more than a quota
    (5MB) is uploaded.
    """
    
    QUOTA = 5 * 2**20 # 5 MB
    
    def __init__(self, request=None):
        super(QuotaUploadHandler, self).__init__(request)
        self.total_upload = 0
        
    def receive_data_chunk(self, raw_data, start):
        self.total_upload += len(raw_data)
        if self.total_upload >= self.QUOTA:
            raise StopUpload(connection_reset=True)
        return raw_data
            
    def file_complete(self, file_size):
        return None

class CustomUploadError(Exception):
    pass

class ErroringUploadHandler(FileUploadHandler):
    """A handler that raises an exception."""
    def receive_data_chunk(self, raw_data, start):
        raise CustomUploadError("Oops!")

from __future__ import absolute_import

from django.core.exceptions import ImproperlyConfigured
from django.test import TestCase

from .models import Author, Artist


class ListViewTests(TestCase):
    fixtures = ['generic-views-test-data.json']
    urls = 'regressiontests.generic_views.urls'

    def test_items(self):
        res = self.client.get('/list/dict/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/list.html')
        self.assertEqual(res.context['object_list'][0]['first'], 'John')

    def test_queryset(self):
        res = self.client.get('/list/authors/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertIsNone(res.context['paginator'])
        self.assertIsNone(res.context['page_obj'])
        self.assertFalse(res.context['is_paginated'])

    def test_paginated_queryset(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')
        self.assertEqual(len(res.context['object_list']), 30)
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertTrue(res.context['is_paginated'])
        self.assertEqual(res.context['page_obj'].number, 1)
        self.assertEqual(res.context['paginator'].num_pages, 4)
        self.assertEqual(res.context['author_list'][0].name, 'Author 00')
        self.assertEqual(list(res.context['author_list'])[-1].name, 'Author 29')

    def test_paginated_queryset_shortdata(self):
        # Test that short datasets ALSO result in a paginated view.
        res = self.client.get('/list/authors/paginated/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertEqual(res.context['page_obj'].number, 1)
        self.assertEqual(res.context['paginator'].num_pages, 1)
        self.assertFalse(res.context['is_paginated'])

    def test_paginated_get_page_by_query_string(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/', {'page': '2'})
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')
        self.assertEqual(len(res.context['object_list']), 30)
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertEqual(res.context['author_list'][0].name, 'Author 30')
        self.assertEqual(res.context['page_obj'].number, 2)

    def test_paginated_get_last_page_by_query_string(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/', {'page': 'last'})
        self.assertEqual(res.status_code, 200)
        self.assertEqual(len(res.context['object_list']), 10)
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertEqual(res.context['author_list'][0].name, 'Author 90')
        self.assertEqual(res.context['page_obj'].number, 4)

    def test_paginated_get_page_by_urlvar(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/3/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')
        self.assertEqual(len(res.context['object_list']), 30)
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertEqual(res.context['author_list'][0].name, 'Author 60')
        self.assertEqual(res.context['page_obj'].number, 3)

    def test_paginated_page_out_of_range(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/42/')
        self.assertEqual(res.status_code, 404)

    def test_paginated_invalid_page(self):
        self._make_authors(100)
        res = self.client.get('/list/authors/paginated/?page=frog')
        self.assertEqual(res.status_code, 404)

    def test_paginated_custom_paginator_class(self):
        self._make_authors(7)
        res = self.client.get('/list/authors/paginated/custom_class/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(res.context['paginator'].num_pages, 1)
        # Custom pagination allows for 2 orphans on a page size of 5
        self.assertEqual(len(res.context['object_list']), 7)

    def test_paginated_custom_paginator_constructor(self):
        self._make_authors(7)
        res = self.client.get('/list/authors/paginated/custom_constructor/')
        self.assertEqual(res.status_code, 200)
        # Custom pagination allows for 2 orphans on a page size of 5
        self.assertEqual(len(res.context['object_list']), 7)

    def test_paginated_non_queryset(self):
        res = self.client.get('/list/dict/paginated/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(len(res.context['object_list']), 1)

    def test_verbose_name(self):
        res = self.client.get('/list/artists/')
        self.assertEqual(res.status_code, 200)
        self.assertTemplateUsed(res, 'generic_views/list.html')
        self.assertEqual(list(res.context['object_list']), list(Artist.objects.all()))
        self.assertIs(res.context['artist_list'], res.context['object_list'])
        self.assertIsNone(res.context['paginator'])
        self.assertIsNone(res.context['page_obj'])
        self.assertFalse(res.context['is_paginated'])

    def test_allow_empty_false(self):
        res = self.client.get('/list/authors/notempty/')
        self.assertEqual(res.status_code, 200)
        Author.objects.all().delete()
        res = self.client.get('/list/authors/notempty/')
        self.assertEqual(res.status_code, 404)

    def test_template_name(self):
        res = self.client.get('/list/authors/template_name/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertTemplateUsed(res, 'generic_views/list.html')

    def test_template_name_suffix(self):
        res = self.client.get('/list/authors/template_name_suffix/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertTemplateUsed(res, 'generic_views/author_objects.html')

    def test_context_object_name(self):
        res = self.client.get('/list/authors/context_object_name/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertNotIn('authors', res.context)
        self.assertIs(res.context['author_list'], res.context['object_list'])
        self.assertTemplateUsed(res, 'generic_views/author_list.html')

    def test_duplicate_context_object_name(self):
        res = self.client.get('/list/authors/dupe_context_object_name/')
        self.assertEqual(res.status_code, 200)
        self.assertEqual(list(res.context['object_list']), list(Author.objects.all()))
        self.assertNotIn('authors', res.context)
        self.assertNotIn('author_list', res.context)
        self.assertTemplateUsed(res, 'generic_views/author_list.html')

    def test_missing_items(self):
        self.assertRaises(ImproperlyConfigured, self.client.get, '/list/authors/invalid/')

    def _make_authors(self, n):
        Author.objects.all().delete()
        for i in range(n):
            Author.objects.create(name='Author %02i' % i, slug='a%s' % i)


from django.db import models


class Reporter(models.Model):
    first_name = models.CharField(max_length=30)
    last_name = models.CharField(max_length=30)
    email = models.EmailField()
    facebook_user_id = models.BigIntegerField(null=True)

    def __unicode__(self):
        return u"%s %s" % (self.first_name, self.last_name)

class Article(models.Model):
    headline = models.CharField(max_length=100)
    pub_date = models.DateField()
    reporter = models.ForeignKey(Reporter)

    def __unicode__(self):
        return self.headline

    class Meta:
        ordering = ('headline',)
from __future__ import absolute_import, with_statement

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

from django.core import management
from django.contrib.auth.models import User
from django.test import TestCase

from .models import (Person, Group, Membership, UserMembership, Car, Driver,
    CarDriver)


class M2MThroughTestCase(TestCase):
    def test_everything(self):
        bob = Person.objects.create(name="Bob")
        jim = Person.objects.create(name="Jim")

        rock = Group.objects.create(name="Rock")
        roll = Group.objects.create(name="Roll")

        frank = User.objects.create_user("frank", "frank@example.com", "password")
        jane = User.objects.create_user("jane", "jane@example.com", "password")

        Membership.objects.create(person=bob, group=rock)
        Membership.objects.create(person=bob, group=roll)
        Membership.objects.create(person=jim, group=rock)

        self.assertQuerysetEqual(
            bob.group_set.all(), [
                "<Group: Rock>",
                "<Group: Roll>",
            ]
        )

        self.assertQuerysetEqual(
            roll.members.all(), [
                "<Person: Bob>",
            ]
        )

        self.assertRaises(AttributeError, setattr, bob, "group_set", [])
        self.assertRaises(AttributeError, setattr, roll, "members", [])

        self.assertRaises(AttributeError, rock.members.create, name="Anne")
        self.assertRaises(AttributeError, bob.group_set.create, name="Funk")

        UserMembership.objects.create(user=frank, group=rock)
        UserMembership.objects.create(user=frank, group=roll)
        UserMembership.objects.create(user=jane, group=rock)

        self.assertQuerysetEqual(
            frank.group_set.all(), [
                "<Group: Rock>",
                "<Group: Roll>",
            ]
        )

        self.assertQuerysetEqual(
            roll.user_members.all(), [
                "<User: frank>",
            ]
        )

    def test_serialization(self):
        "m2m-through models aren't serialized as m2m fields. Refs #8134"

        p = Person.objects.create(name="Bob")
        g = Group.objects.create(name="Roll")
        m =Membership.objects.create(person=p, group=g)

        pks = {"p_pk": p.pk, "g_pk": g.pk, "m_pk": m.pk}

        out = StringIO()
        management.call_command("dumpdata", "m2m_through_regress", format="json", stdout=out)
        self.assertEqual(out.getvalue().strip(), """[{"pk": %(m_pk)s, "model": "m2m_through_regress.membership", "fields": {"person": %(p_pk)s, "price": 100, "group": %(g_pk)s}}, {"pk": %(p_pk)s, "model": "m2m_through_regress.person", "fields": {"name": "Bob"}}, {"pk": %(g_pk)s, "model": "m2m_through_regress.group", "fields": {"name": "Roll"}}]""" % pks)

        out = StringIO()
        management.call_command("dumpdata", "m2m_through_regress", format="xml",
            indent=2, stdout=out)
        self.assertEqual(out.getvalue().strip(), """
<?xml version="1.0" encoding="utf-8"?>
<django-objects version="1.0">
  <object pk="%(m_pk)s" model="m2m_through_regress.membership">
    <field to="m2m_through_regress.person" name="person" rel="ManyToOneRel">%(p_pk)s</field>
    <field to="m2m_through_regress.group" name="group" rel="ManyToOneRel">%(g_pk)s</field>
    <field type="IntegerField" name="price">100</field>
  </object>
  <object pk="%(p_pk)s" model="m2m_through_regress.person">
    <field type="CharField" name="name">Bob</field>
  </object>
  <object pk="%(g_pk)s" model="m2m_through_regress.group">
    <field type="CharField" name="name">Roll</field>
  </object>
</django-objects>
        """.strip() % pks)

    def test_join_trimming(self):
        "Check that we don't involve too many copies of the intermediate table when doing a join. Refs #8046, #8254"
        bob  = Person.objects.create(name="Bob")
        jim = Person.objects.create(name="Jim")

        rock = Group.objects.create(name="Rock")
        roll = Group.objects.create(name="Roll")

        Membership.objects.create(person=bob, group=rock)
        Membership.objects.create(person=jim, group=rock, price=50)
        Membership.objects.create(person=bob, group=roll, price=50)

        self.assertQuerysetEqual(
            rock.members.filter(membership__price=50), [
                "<Person: Jim>",
            ]
        )

        self.assertQuerysetEqual(
            bob.group_set.filter(membership__price=50), [
                "<Group: Roll>",
            ]
        )


class ToFieldThroughTests(TestCase):
    def setUp(self):
        self.car = Car.objects.create(make="Toyota")
        self.driver = Driver.objects.create(name="Ryan Briscoe")
        CarDriver.objects.create(car=self.car, driver=self.driver)
        # We are testing if wrong objects get deleted due to using wrong
        # field value in m2m queries. So, it is essential that the pk
        # numberings do not match.
        # Create one intentionally unused driver to mix up the autonumbering
        self.unused_driver = Driver.objects.create(name="Barney Gumble")
        # And two intentionally unused cars.
        self.unused_car1 = Car.objects.create(make="Trabant")
        self.unused_car2 = Car.objects.create(make="Wartburg")

    def test_to_field(self):
        self.assertQuerysetEqual(
            self.car.drivers.all(),
            ["<Driver: Ryan Briscoe>"]
        )

    def test_to_field_reverse(self):
        self.assertQuerysetEqual(
            self.driver.car_set.all(),
            ["<Car: Toyota>"]
        )

    def test_to_field_clear_reverse(self):
        self.driver.car_set.clear()
        self.assertQuerysetEqual(
            self.driver.car_set.all(),[])

    def test_to_field_clear(self):
        self.car.drivers.clear()
        self.assertQuerysetEqual(
            self.car.drivers.all(),[])

    # Low level tests for _add_items and _remove_items. We test these methods
    # because .add/.remove aren't available for m2m fields with through, but
    # through is the only way to set to_field currently. We do want to make
    # sure these methods are ready if the ability to use .add or .remove with
    # to_field relations is added some day.
    def test_add(self):
        self.assertQuerysetEqual(
            self.car.drivers.all(),
            ["<Driver: Ryan Briscoe>"]
        )
        # Yikes - barney is going to drive...
        self.car.drivers._add_items('car', 'driver', self.unused_driver)
        self.assertQuerysetEqual(
            self.car.drivers.all(),
            ["<Driver: Barney Gumble>", "<Driver: Ryan Briscoe>"]
        )

    def test_add_null(self):
        nullcar = Car.objects.create(make=None)
        with self.assertRaises(ValueError):
            nullcar.drivers._add_items('car', 'driver', self.unused_driver)

    def test_add_related_null(self):
        nulldriver = Driver.objects.create(name=None)
        with self.assertRaises(ValueError):
            self.car.drivers._add_items('car', 'driver', nulldriver)

    def test_add_reverse(self):
        car2 = Car.objects.create(make="Honda")
        self.assertQuerysetEqual(
            self.driver.car_set.all(),
            ["<Car: Toyota>"]
        )
        self.driver.car_set._add_items('driver', 'car', car2)
        self.assertQuerysetEqual(
            self.driver.car_set.all(),
            ["<Car: Toyota>", "<Car: Honda>"]
        )

    def test_add_null_reverse(self):
        nullcar = Car.objects.create(make=None)
        with self.assertRaises(ValueError):
            self.driver.car_set._add_items('driver', 'car', nullcar)

    def test_add_null_reverse_related(self):
        nulldriver = Driver.objects.create(name=None)
        with self.assertRaises(ValueError):
            nulldriver.car_set._add_items('driver', 'car', self.car)

    def test_remove(self):
        self.assertQuerysetEqual(
            self.car.drivers.all(),
            ["<Driver: Ryan Briscoe>"]
        )
        self.car.drivers._remove_items('car', 'driver', self.driver)
        self.assertQuerysetEqual(
            self.car.drivers.all(),[])

    def test_remove_reverse(self):
        self.assertQuerysetEqual(
            self.driver.car_set.all(),
            ["<Car: Toyota>"]
        )
        self.driver.car_set._remove_items('driver', 'car', self.car)
        self.assertQuerysetEqual(
            self.driver.car_set.all(),[])


class ThroughLoadDataTestCase(TestCase):
    fixtures = ["m2m_through"]

    def test_sequence_creation(self):
        "Check that sequences on an m2m_through are created for the through model, not a phantom auto-generated m2m table. Refs #11107"
        out = StringIO()
        management.call_command("dumpdata", "m2m_through_regress", format="json", stdout=out)
        self.assertEqual(out.getvalue().strip(), """[{"pk": 1, "model": "m2m_through_regress.usermembership", "fields": {"price": 100, "group": 1, "user": 1}}, {"pk": 1, "model": "m2m_through_regress.person", "fields": {"name": "Guido"}}, {"pk": 1, "model": "m2m_through_regress.group", "fields": {"name": "Python Core Group"}}]""")

from __future__ import absolute_import, with_statement

from datetime import date

from django import forms
from django.conf import settings
from django.contrib.admin.options import (ModelAdmin, TabularInline,
     InlineModelAdmin, HORIZONTAL, VERTICAL)
from django.contrib.admin.sites import AdminSite
from django.contrib.admin.validation import validate
from django.contrib.admin.widgets import AdminDateWidget, AdminRadioSelect
from django.contrib.admin import (SimpleListFilter,
     BooleanFieldListFilter)
from django.core.exceptions import ImproperlyConfigured
from django.forms.models import BaseModelFormSet
from django.forms.widgets import Select
from django.test import TestCase
from django.test.utils import override_settings
from django.utils import unittest

from .models import Band, Concert, ValidationTestModel, ValidationTestInlineModel


class MockRequest(object):
    pass

class MockSuperUser(object):
    def has_perm(self, perm):
        return True

request = MockRequest()
request.user = MockSuperUser()


class ModelAdminTests(TestCase):

    def setUp(self):
        self.band = Band.objects.create(
            name='The Doors',
            bio='',
            sign_date=date(1965, 1, 1),
        )
        self.site = AdminSite()

    # form/fields/fieldsets interaction ##############################

    def test_default_fields(self):
        ma = ModelAdmin(Band, self.site)

        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name', 'bio', 'sign_date'])

    def test_default_fieldsets(self):
        # fieldsets_add and fieldsets_change should return a special data structure that
        # is used in the templates. They should generate the "right thing" whether we
        # have specified a custom form, the fields argument, or nothing at all.
        #
        # Here's the default case. There are no custom form_add/form_change methods,
        # no fields argument, and no fieldsets argument.
        ma = ModelAdmin(Band, self.site)
        self.assertEqual(ma.get_fieldsets(request),
            [(None, {'fields': ['name', 'bio', 'sign_date']})])

        self.assertEqual(ma.get_fieldsets(request, self.band),
            [(None, {'fields': ['name', 'bio', 'sign_date']})])

    def test_field_arguments(self):
        # If we specify the fields argument, fieldsets_add and fielsets_change should
        # just stick the fields into a formsets structure and return it.
        class BandAdmin(ModelAdmin):
             fields = ['name']

        ma = BandAdmin(Band, self.site)

        self.assertEqual( ma.get_fieldsets(request),
            [(None, {'fields': ['name']})])

        self.assertEqual(ma.get_fieldsets(request, self.band),
            [(None, {'fields': ['name']})])

    def test_field_arguments_restricted_on_form(self):
        # If we specify fields or fieldsets, it should exclude fields on the Form class
        # to the fields specified. This may cause errors to be raised in the db layer if
        # required model fields arent in fields/fieldsets, but that's preferable to
        # ghost errors where you have a field in your Form class that isn't being
        # displayed because you forgot to add it to fields/fieldsets

        # Using `fields`.
        class BandAdmin(ModelAdmin):
            fields = ['name']

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(), ['name'])
        self.assertEqual(ma.get_form(request, self.band).base_fields.keys(),
            ['name'])

        # Using `fieldsets`.
        class BandAdmin(ModelAdmin):
            fieldsets = [(None, {'fields': ['name']})]

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(), ['name'])
        self.assertEqual(ma.get_form(request, self.band).base_fields.keys(),
            ['name'])

        # Using `exclude`.
        class BandAdmin(ModelAdmin):
            exclude = ['bio']

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name', 'sign_date'])

        # You can also pass a tuple to `exclude`.
        class BandAdmin(ModelAdmin):
            exclude = ('bio',)

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name', 'sign_date'])

        # Using `fields` and `exclude`.
        class BandAdmin(ModelAdmin):
            fields = ['name', 'bio']
            exclude = ['bio']

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name'])

    def test_custom_form_meta_exclude_with_readonly(self):
        """
        Ensure that the custom ModelForm's `Meta.exclude` is respected when
        used in conjunction with `ModelAdmin.readonly_fields` and when no
        `ModelAdmin.exclude` is defined.
        Refs #14496.
        """
        # First, with `ModelAdmin` -----------------------

        class AdminBandForm(forms.ModelForm):

            class Meta:
                model = Band
                exclude = ['bio']

        class BandAdmin(ModelAdmin):
            readonly_fields = ['name']
            form = AdminBandForm

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['sign_date',])

        # Then, with `InlineModelAdmin`  -----------------

        class AdminConcertForm(forms.ModelForm):

            class Meta:
                model = Concert
                exclude = ['day']

        class ConcertInline(TabularInline):
            readonly_fields = ['transport']
            form = AdminConcertForm
            fk_name = 'main_band'
            model = Concert

        class BandAdmin(ModelAdmin):
            inlines = [
                ConcertInline
            ]

        ma = BandAdmin(Band, self.site)
        self.assertEqual(
            list(ma.get_formsets(request))[0]().forms[0].fields.keys(),
            ['main_band', 'opening_band', 'id', 'DELETE',])

    def test_custom_form_meta_exclude(self):
        """
        Ensure that the custom ModelForm's `Meta.exclude` is overridden if
        `ModelAdmin.exclude` or `InlineModelAdmin.exclude` are defined.
        Refs #14496.
        """
        # First, with `ModelAdmin` -----------------------

        class AdminBandForm(forms.ModelForm):

            class Meta:
                model = Band
                exclude = ['bio']

        class BandAdmin(ModelAdmin):
            exclude = ['name']
            form = AdminBandForm

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['bio', 'sign_date',])

        # Then, with `InlineModelAdmin`  -----------------

        class AdminConcertForm(forms.ModelForm):

            class Meta:
                model = Concert
                exclude = ['day']

        class ConcertInline(TabularInline):
            exclude = ['transport']
            form = AdminConcertForm
            fk_name = 'main_band'
            model = Concert

        class BandAdmin(ModelAdmin):
            inlines = [
                ConcertInline
            ]

        ma = BandAdmin(Band, self.site)
        self.assertEqual(
            list(ma.get_formsets(request))[0]().forms[0].fields.keys(),
            ['main_band', 'opening_band', 'day', 'id', 'DELETE',])

    def test_custom_form_validation(self):
        # If we specify a form, it should use it allowing custom validation to work
        # properly. This won't, however, break any of the admin widgets or media.

        class AdminBandForm(forms.ModelForm):
            delete = forms.BooleanField()

            class Meta:
                model = Band

        class BandAdmin(ModelAdmin):
            form = AdminBandForm

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name', 'bio', 'sign_date', 'delete'])

        self.assertEqual(
            type(ma.get_form(request).base_fields['sign_date'].widget),
            AdminDateWidget)

    def test_form_exclude_kwarg_override(self):
        """
        Ensure that the `exclude` kwarg passed to `ModelAdmin.get_form()`
        overrides all other declarations. Refs #8999.
        """

        class AdminBandForm(forms.ModelForm):

            class Meta:
                model = Band
                exclude = ['name']

        class BandAdmin(ModelAdmin):
            exclude = ['sign_date',]
            form = AdminBandForm

            def get_form(self, request, obj=None, **kwargs):
                kwargs['exclude'] = ['bio']
                return super(BandAdmin, self).get_form(request, obj, **kwargs)

        ma = BandAdmin(Band, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['name', 'sign_date',])


    def test_formset_exclude_kwarg_override(self):
        """
        Ensure that the `exclude` kwarg passed to `InlineModelAdmin.get_formset()`
        overrides all other declarations. Refs #8999.
        """

        class AdminConcertForm(forms.ModelForm):

            class Meta:
                model = Concert
                exclude = ['day']

        class ConcertInline(TabularInline):
            exclude = ['transport']
            form = AdminConcertForm
            fk_name = 'main_band'
            model = Concert

            def get_formset(self, request, obj=None, **kwargs):
                kwargs['exclude'] = ['opening_band']
                return super(ConcertInline, self).get_formset(request, obj, **kwargs)

        class BandAdmin(ModelAdmin):
            inlines = [
                ConcertInline
            ]

        ma = BandAdmin(Band, self.site)
        self.assertEqual(
            list(ma.get_formsets(request))[0]().forms[0].fields.keys(),
            ['main_band', 'day', 'transport', 'id', 'DELETE',])

    def test_queryset_override(self):
        # If we need to override the queryset of a ModelChoiceField in our custom form
        # make sure that RelatedFieldWidgetWrapper doesn't mess that up.

        band2 = Band(name='The Beatles', bio='', sign_date=date(1962, 1, 1))
        band2.save()

        class ConcertAdmin(ModelAdmin):
            pass
        ma = ConcertAdmin(Concert, self.site)
        form = ma.get_form(request)()

        self.assertHTMLEqual(str(form["main_band"]),
            '<select name="main_band" id="id_main_band">\n'
            '<option value="" selected="selected">---------</option>\n'
            '<option value="%d">The Beatles</option>\n'
            '<option value="%d">The Doors</option>\n'
            '</select>' % (band2.id, self.band.id))

        class AdminConcertForm(forms.ModelForm):
            class Meta:
                model = Concert

            def __init__(self, *args, **kwargs):
                super(AdminConcertForm, self).__init__(*args, **kwargs)
                self.fields["main_band"].queryset = Band.objects.filter(name='The Doors')

        class ConcertAdmin(ModelAdmin):
            form = AdminConcertForm

        ma = ConcertAdmin(Concert, self.site)
        form = ma.get_form(request)()

        self.assertHTMLEqual(str(form["main_band"]),
            '<select name="main_band" id="id_main_band">\n'
            '<option value="" selected="selected">---------</option>\n'
            '<option value="%d">The Doors</option>\n'
            '</select>' % self.band.id)

    def test_regression_for_ticket_15820(self):
        """
        Ensure that `obj` is passed from `InlineModelAdmin.get_fieldsets()` to
        `InlineModelAdmin.get_formset()`.
        """
        class CustomConcertForm(forms.ModelForm):

            class Meta:
                model = Concert
                fields = ['day']

        class ConcertInline(TabularInline):
            model = Concert
            fk_name = 'main_band'

            def get_formset(self, request, obj=None, **kwargs):
                if obj:
                    kwargs['form'] = CustomConcertForm
                return super(ConcertInline, self).get_formset(request, obj, **kwargs)

        class BandAdmin(ModelAdmin):
            inlines = [
                ConcertInline
            ]

        concert = Concert.objects.create(main_band=self.band, opening_band=self.band, day=1)
        ma = BandAdmin(Band, self.site)
        inline_instances = ma.get_inline_instances(request)
        fieldsets = list(inline_instances[0].get_fieldsets(request))
        self.assertEqual(fieldsets[0][1]['fields'], ['main_band', 'opening_band', 'day', 'transport'])
        fieldsets = list(inline_instances[0].get_fieldsets(request, inline_instances[0].model))
        self.assertEqual(fieldsets[0][1]['fields'], ['day'])

    # radio_fields behavior ###########################################

    def test_default_foreign_key_widget(self):
        # First, without any radio_fields specified, the widgets for ForeignKey
        # and fields with choices specified ought to be a basic Select widget.
        # ForeignKey widgets in the admin are wrapped with RelatedFieldWidgetWrapper so
        # they need to be handled properly when type checking. For Select fields, all of
        # the choices lists have a first entry of dashes.

        cma = ModelAdmin(Concert, self.site)
        cmafa = cma.get_form(request)

        self.assertEqual(type(cmafa.base_fields['main_band'].widget.widget),
            Select)
        self.assertEqual(
            list(cmafa.base_fields['main_band'].widget.choices),
            [(u'', u'---------'), (self.band.id, u'The Doors')])

        self.assertEqual(
            type(cmafa.base_fields['opening_band'].widget.widget), Select)
        self.assertEqual(
            list(cmafa.base_fields['opening_band'].widget.choices),
            [(u'', u'---------'), (self.band.id, u'The Doors')])

        self.assertEqual(type(cmafa.base_fields['day'].widget), Select)
        self.assertEqual(list(cmafa.base_fields['day'].widget.choices),
            [('', '---------'), (1, 'Fri'), (2, 'Sat')])

        self.assertEqual(type(cmafa.base_fields['transport'].widget),
            Select)
        self.assertEqual(
            list(cmafa.base_fields['transport'].widget.choices),
            [('', '---------'), (1, 'Plane'), (2, 'Train'), (3, 'Bus')])

    def test_foreign_key_as_radio_field(self):
        # Now specify all the fields as radio_fields.  Widgets should now be
        # RadioSelect, and the choices list should have a first entry of 'None' if
        # blank=True for the model field.  Finally, the widget should have the
        # 'radiolist' attr, and 'inline' as well if the field is specified HORIZONTAL.

        class ConcertAdmin(ModelAdmin):
            radio_fields = {
                'main_band': HORIZONTAL,
                'opening_band': VERTICAL,
                'day': VERTICAL,
                'transport': HORIZONTAL,
            }

        cma = ConcertAdmin(Concert, self.site)
        cmafa = cma.get_form(request)

        self.assertEqual(type(cmafa.base_fields['main_band'].widget.widget),
            AdminRadioSelect)
        self.assertEqual(cmafa.base_fields['main_band'].widget.attrs,
            {'class': 'radiolist inline'})
        self.assertEqual(list(cmafa.base_fields['main_band'].widget.choices),
            [(self.band.id, u'The Doors')])

        self.assertEqual(
            type(cmafa.base_fields['opening_band'].widget.widget),
            AdminRadioSelect)
        self.assertEqual(cmafa.base_fields['opening_band'].widget.attrs,
            {'class': 'radiolist'})
        self.assertEqual(
            list(cmafa.base_fields['opening_band'].widget.choices),
            [(u'', u'None'), (self.band.id, u'The Doors')])

        self.assertEqual(type(cmafa.base_fields['day'].widget),
            AdminRadioSelect)
        self.assertEqual(cmafa.base_fields['day'].widget.attrs,
            {'class': 'radiolist'})
        self.assertEqual(list(cmafa.base_fields['day'].widget.choices),
            [(1, 'Fri'), (2, 'Sat')])

        self.assertEqual(type(cmafa.base_fields['transport'].widget),
            AdminRadioSelect)
        self.assertEqual(cmafa.base_fields['transport'].widget.attrs,
            {'class': 'radiolist inline'})
        self.assertEqual(list(cmafa.base_fields['transport'].widget.choices),
            [('', u'None'), (1, 'Plane'), (2, 'Train'), (3, 'Bus')])

        class AdminConcertForm(forms.ModelForm):
            class Meta:
                model = Concert
                exclude = ('transport',)

        class ConcertAdmin(ModelAdmin):
            form = AdminConcertForm

        ma = ConcertAdmin(Concert, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['main_band', 'opening_band', 'day'])

        class AdminConcertForm(forms.ModelForm):
            extra = forms.CharField()

            class Meta:
                model = Concert
                fields = ['extra', 'transport']

        class ConcertAdmin(ModelAdmin):
            form = AdminConcertForm

        ma = ConcertAdmin(Concert, self.site)
        self.assertEqual(ma.get_form(request).base_fields.keys(),
            ['extra', 'transport'])

        class ConcertInline(TabularInline):
            form = AdminConcertForm
            model = Concert
            fk_name = 'main_band'
            can_delete = True

        class BandAdmin(ModelAdmin):
            inlines = [
                ConcertInline
            ]

        ma = BandAdmin(Band, self.site)
        self.assertEqual(
            list(ma.get_formsets(request))[0]().forms[0].fields.keys(),
            ['extra', 'transport', 'id', 'DELETE', 'main_band'])


class ValidationTests(unittest.TestCase):
    def test_validation_only_runs_in_debug(self):
        # Ensure validation only runs when DEBUG = True
        try:
            settings.DEBUG = True

            class ValidationTestModelAdmin(ModelAdmin):
                raw_id_fields = 10

            site = AdminSite()

            self.assertRaisesRegexp(
                ImproperlyConfigured,
                "'ValidationTestModelAdmin.raw_id_fields' must be a list or tuple.",
                site.register,
                ValidationTestModel,
                ValidationTestModelAdmin,
            )
        finally:
            settings.DEBUG = False

        site = AdminSite()
        site.register(ValidationTestModel, ValidationTestModelAdmin)

    def test_raw_id_fields_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            raw_id_fields = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.raw_id_fields' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            raw_id_fields = ('non_existent_field',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.raw_id_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            raw_id_fields = ('name',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.raw_id_fields\[0\]', 'name' must be either a ForeignKey or ManyToManyField.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            raw_id_fields = ('users',)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_fieldsets_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.fieldsets' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = ({},)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.fieldsets\[0\]' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = ((),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.fieldsets\[0\]' does not have exactly two elements.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = (("General", ()),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.fieldsets\[0\]\[1\]' must be a dictionary.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = (("General", {}),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'fields' key is required in ValidationTestModelAdmin.fieldsets\[0\]\[1\] field options dict.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = (("General", {"fields": ("non_existent_field",)}),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.fieldsets\[0\]\[1\]\['fields'\]' refers to field 'non_existent_field' that is missing from the form.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = (("General", {"fields": ("name",)}),)

        validate(ValidationTestModelAdmin, ValidationTestModel)

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = (("General", {"fields": ("name",)}),)
            fields = ["name",]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "Both fieldsets and fields are specified in ValidationTestModelAdmin.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fieldsets = [(None, {'fields': ['name', 'name']})]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "There are duplicate field\(s\) in ValidationTestModelAdmin.fieldsets",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            fields = ["name", "name"]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "There are duplicate field\(s\) in ValidationTestModelAdmin.fields",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

    def test_form_validation(self):

        class FakeForm(object):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            form = FakeForm

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "ValidationTestModelAdmin.form does not inherit from BaseModelForm.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

    def test_fieldsets_with_custom_form_validation(self):

        class BandAdmin(ModelAdmin):

            fieldsets = (
                ('Band', {
                    'fields': ('non_existent_field',)
                }),
            )

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'BandAdmin.fieldsets\[0\]\[1\]\['fields'\]' refers to field 'non_existent_field' that is missing from the form.",
            validate,
            BandAdmin,
            Band,
        )

        class BandAdmin(ModelAdmin):
            fieldsets = (
                ('Band', {
                    'fields': ('name',)
                }),
            )

        validate(BandAdmin, Band)

        class AdminBandForm(forms.ModelForm):
            class Meta:
                model = Band

        class BandAdmin(ModelAdmin):
            form = AdminBandForm

            fieldsets = (
                ('Band', {
                    'fields': ('non_existent_field',)
                }),
            )

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'BandAdmin.fieldsets\[0]\[1\]\['fields'\]' refers to field 'non_existent_field' that is missing from the form.",
            validate,
            BandAdmin,
            Band,
        )

        class AdminBandForm(forms.ModelForm):
            delete = forms.BooleanField()

            class Meta:
                model = Band

        class BandAdmin(ModelAdmin):
            form = AdminBandForm

            fieldsets = (
                ('Band', {
                    'fields': ('name', 'bio', 'sign_date', 'delete')
                }),
            )

        validate(BandAdmin, Band)

    def test_filter_vertical_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            filter_vertical = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_vertical' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_vertical = ("non_existent_field",)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_vertical' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_vertical = ("name",)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_vertical\[0\]' must be a ManyToManyField.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_vertical = ("users",)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_filter_horizontal_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            filter_horizontal = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_horizontal' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_horizontal = ("non_existent_field",)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_horizontal' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_horizontal = ("name",)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.filter_horizontal\[0\]' must be a ManyToManyField.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            filter_horizontal = ("users",)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_radio_fields_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            radio_fields = ()

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.radio_fields' must be a dictionary.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            radio_fields = {"non_existent_field": None}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.radio_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            radio_fields = {"name": None}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.radio_fields\['name'\]' is neither an instance of ForeignKey nor does have choices set.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            radio_fields = {"state": None}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.radio_fields\['state'\]' is neither admin.HORIZONTAL nor admin.VERTICAL.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            radio_fields = {"state": VERTICAL}

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_prepopulated_fields_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            prepopulated_fields = ()

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.prepopulated_fields' must be a dictionary.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            prepopulated_fields = {"non_existent_field": None}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.prepopulated_fields' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            prepopulated_fields = {"slug": ("non_existent_field",)}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.prepopulated_fields\['slug'\]\[0\]' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            prepopulated_fields = {"users": ("name",)}

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.prepopulated_fields\['users'\]' is either a DateTimeField, ForeignKey or ManyToManyField. This isn't allowed.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            prepopulated_fields = {"slug": ("name",)}

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_list_display_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_display = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_display' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_display = ('non_existent_field',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "ValidationTestModelAdmin.list_display\[0\], 'non_existent_field' is not a callable or an attribute of 'ValidationTestModelAdmin' or found in the model 'ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_display = ('users',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_display\[0\]', 'users' is a ManyToManyField which is not supported.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        def a_callable(obj):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            def a_method(self, obj):
                pass
            list_display = ('name', 'decade_published_in', 'a_method', a_callable)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_list_display_links_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_display_links = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_display_links' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_display_links = ('non_existent_field',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_display_links\[0\]' refers to 'non_existent_field' which is not defined in 'list_display'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_display_links = ('name',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_display_links\[0\]' refers to 'name' which is not defined in 'list_display'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        def a_callable(obj):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            def a_method(self, obj):
                pass
            list_display = ('name', 'decade_published_in', 'a_method', a_callable)
            list_display_links = ('name', 'decade_published_in', 'a_method', a_callable)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_list_filter_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = ('non_existent_field',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter\[0\]' refers to 'non_existent_field' which does not refer to a Field.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class RandomClass(object):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = (RandomClass,)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter\[0\]' is 'RandomClass' which is not a descendant of ListFilter.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = (('is_active', RandomClass),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter\[0\]\[1\]' is 'RandomClass' which is not of type FieldListFilter.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class AwesomeFilter(SimpleListFilter):
            def get_title(self):
                return 'awesomeness'
            def get_choices(self, request):
                return (('bit', 'A bit awesome'), ('very', 'Very awesome'), )
            def get_query_set(self, cl, qs):
                return qs

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = (('is_active', AwesomeFilter),)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter\[0\]\[1\]' is 'AwesomeFilter' which is not of type FieldListFilter.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = (BooleanFieldListFilter,)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_filter\[0\]' is 'BooleanFieldListFilter' which is of type FieldListFilter but is not associated with a field name.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        # Valid declarations below -----------

        class ValidationTestModelAdmin(ModelAdmin):
            list_filter = ('is_active', AwesomeFilter, ('is_active', BooleanFieldListFilter), 'no')

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_list_per_page_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_per_page = 'hello'

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_per_page' should be a integer.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_per_page = 100

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_max_show_all_allowed_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_max_show_all = 'hello'

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_max_show_all' should be an integer.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_max_show_all = 200

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_search_fields_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            search_fields = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.search_fields' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

    def test_date_hierarchy_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            date_hierarchy = 'non_existent_field'

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.date_hierarchy' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            date_hierarchy = 'name'

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.date_hierarchy is neither an instance of DateField nor DateTimeField.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            date_hierarchy = 'pub_date'

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_ordering_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.ordering' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = ('non_existent_field',)

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.ordering\[0\]' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = ('?', 'name')

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.ordering' has the random ordering marker '\?', but contains other fields as well. Please either remove '\?' or the other fields.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = ('?',)

        validate(ValidationTestModelAdmin, ValidationTestModel)

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = ('band__name',)

        validate(ValidationTestModelAdmin, ValidationTestModel)

        class ValidationTestModelAdmin(ModelAdmin):
            ordering = ('name',)

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_list_select_related_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            list_select_related = 1

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.list_select_related' should be a boolean.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            list_select_related = False

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_save_as_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            save_as = 1

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.save_as' should be a boolean.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            save_as = True

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_save_on_top_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            save_on_top = 1

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.save_on_top' should be a boolean.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestModelAdmin(ModelAdmin):
            save_on_top = True

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_inlines_validation(self):

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = 10

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.inlines' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(object):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.inlines\[0\]' does not inherit from BaseModelAdmin.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            pass

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'model' is a required attribute of 'ValidationTestModelAdmin.inlines\[0\]'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class SomethingBad(object):
            pass

        class ValidationTestInline(TabularInline):
            model = SomethingBad

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestModelAdmin.inlines\[0\].model' does not inherit from models.Model.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_fields_validation(self):

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            fields = 10

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.fields' must be a list or tuple.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            fields = ("non_existent_field",)

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.fields' refers to field 'non_existent_field' that is missing from the form.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

    def test_fk_name_validation(self):

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            fk_name = "non_existent_field"

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.fk_name' refers to field 'non_existent_field' that is missing from model 'modeladmin.ValidationTestInlineModel'.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            fk_name = "parent"

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_extra_validation(self):

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            extra = "hello"

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.extra' should be a integer.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            extra = 2

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_max_num_validation(self):

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            max_num = "hello"

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.max_num' should be an integer or None \(default\).",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            max_num = 2

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        validate(ValidationTestModelAdmin, ValidationTestModel)

    def test_formset_validation(self):

        class FakeFormSet(object):
            pass

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            formset = FakeFormSet

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        self.assertRaisesRegexp(
            ImproperlyConfigured,
            "'ValidationTestInline.formset' does not inherit from BaseModelFormSet.",
            validate,
            ValidationTestModelAdmin,
            ValidationTestModel,
        )

        class RealModelFormSet(BaseModelFormSet):
            pass

        class ValidationTestInline(TabularInline):
            model = ValidationTestInlineModel
            formset = RealModelFormSet

        class ValidationTestModelAdmin(ModelAdmin):
            inlines = [ValidationTestInline]

        validate(ValidationTestModelAdmin, ValidationTestModel)

import operator

from django import template
from django.template.defaultfilters import stringfilter
from django.template.loader import get_template

register = template.Library()

@register.filter
@stringfilter
def trim(value, num):
    return value[:num]

@register.simple_tag
def no_params():
    """Expected no_params __doc__"""
    return "no_params - Expected result"
no_params.anything = "Expected no_params __dict__"

@register.simple_tag
def one_param(arg):
    """Expected one_param __doc__"""
    return "one_param - Expected result: %s" % arg
one_param.anything = "Expected one_param __dict__"

@register.simple_tag(takes_context=False)
def explicit_no_context(arg):
    """Expected explicit_no_context __doc__"""
    return "explicit_no_context - Expected result: %s" % arg
explicit_no_context.anything = "Expected explicit_no_context __dict__"

@register.simple_tag(takes_context=True)
def no_params_with_context(context):
    """Expected no_params_with_context __doc__"""
    return "no_params_with_context - Expected result (context value: %s)" % context['value']
no_params_with_context.anything = "Expected no_params_with_context __dict__"

@register.simple_tag(takes_context=True)
def params_and_context(context, arg):
    """Expected params_and_context __doc__"""
    return "params_and_context - Expected result (context value: %s): %s" % (context['value'], arg)
params_and_context.anything = "Expected params_and_context __dict__"

@register.simple_tag
def simple_two_params(one, two):
    """Expected simple_two_params __doc__"""
    return "simple_two_params - Expected result: %s, %s" % (one, two)
simple_two_params.anything = "Expected simple_two_params __dict__"

@register.simple_tag
def simple_one_default(one, two='hi'):
    """Expected simple_one_default __doc__"""
    return "simple_one_default - Expected result: %s, %s" % (one, two)
simple_one_default.anything = "Expected simple_one_default __dict__"

@register.simple_tag
def simple_unlimited_args(one, two='hi', *args):
    """Expected simple_unlimited_args __doc__"""
    return "simple_unlimited_args - Expected result: %s" % (', '.join([unicode(arg) for arg in [one, two] + list(args)]))
simple_unlimited_args.anything = "Expected simple_unlimited_args __dict__"

@register.simple_tag
def simple_only_unlimited_args(*args):
    """Expected simple_only_unlimited_args __doc__"""
    return "simple_only_unlimited_args - Expected result: %s" % ', '.join([unicode(arg) for arg in args])
simple_only_unlimited_args.anything = "Expected simple_only_unlimited_args __dict__"

@register.simple_tag
def simple_unlimited_args_kwargs(one, two='hi', *args, **kwargs):
    """Expected simple_unlimited_args_kwargs __doc__"""
    # Sort the dictionary by key to guarantee the order for testing.
    sorted_kwarg = sorted(kwargs.iteritems(), key=operator.itemgetter(0))
    return "simple_unlimited_args_kwargs - Expected result: %s / %s" % (
        ', '.join([unicode(arg) for arg in [one, two] + list(args)]),
        ', '.join(['%s=%s' % (k, v) for (k, v) in sorted_kwarg])
        )
simple_unlimited_args_kwargs.anything = "Expected simple_unlimited_args_kwargs __dict__"

@register.simple_tag(takes_context=True)
def simple_tag_without_context_parameter(arg):
    """Expected simple_tag_without_context_parameter __doc__"""
    return "Expected result"
simple_tag_without_context_parameter.anything = "Expected simple_tag_without_context_parameter __dict__"

@register.simple_tag(takes_context=True)
def current_app(context):
    return "%s" % context.current_app

@register.simple_tag(takes_context=True)
def use_l10n(context):
    return "%s" % context.use_l10n

@register.simple_tag(name='minustwo')
def minustwo_overridden_name(value):
    return value - 2

register.simple_tag(lambda x: x - 1, name='minusone')

@register.inclusion_tag('inclusion.html')
def inclusion_no_params():
    """Expected inclusion_no_params __doc__"""
    return {"result" : "inclusion_no_params - Expected result"}
inclusion_no_params.anything = "Expected inclusion_no_params __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_no_params_from_template():
    """Expected inclusion_no_params_from_template __doc__"""
    return {"result" : "inclusion_no_params_from_template - Expected result"}
inclusion_no_params_from_template.anything = "Expected inclusion_no_params_from_template __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_one_param(arg):
    """Expected inclusion_one_param __doc__"""
    return {"result" : "inclusion_one_param - Expected result: %s" % arg}
inclusion_one_param.anything = "Expected inclusion_one_param __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_one_param_from_template(arg):
    """Expected inclusion_one_param_from_template __doc__"""
    return {"result" : "inclusion_one_param_from_template - Expected result: %s" % arg}
inclusion_one_param_from_template.anything = "Expected inclusion_one_param_from_template __dict__"

@register.inclusion_tag('inclusion.html', takes_context=False)
def inclusion_explicit_no_context(arg):
    """Expected inclusion_explicit_no_context __doc__"""
    return {"result" : "inclusion_explicit_no_context - Expected result: %s" % arg}
inclusion_explicit_no_context.anything = "Expected inclusion_explicit_no_context __dict__"

@register.inclusion_tag(get_template('inclusion.html'), takes_context=False)
def inclusion_explicit_no_context_from_template(arg):
    """Expected inclusion_explicit_no_context_from_template __doc__"""
    return {"result" : "inclusion_explicit_no_context_from_template - Expected result: %s" % arg}
inclusion_explicit_no_context_from_template.anything = "Expected inclusion_explicit_no_context_from_template __dict__"

@register.inclusion_tag('inclusion.html', takes_context=True)
def inclusion_no_params_with_context(context):
    """Expected inclusion_no_params_with_context __doc__"""
    return {"result" : "inclusion_no_params_with_context - Expected result (context value: %s)" % context['value']}
inclusion_no_params_with_context.anything = "Expected inclusion_no_params_with_context __dict__"

@register.inclusion_tag(get_template('inclusion.html'), takes_context=True)
def inclusion_no_params_with_context_from_template(context):
    """Expected inclusion_no_params_with_context_from_template __doc__"""
    return {"result" : "inclusion_no_params_with_context_from_template - Expected result (context value: %s)" % context['value']}
inclusion_no_params_with_context_from_template.anything = "Expected inclusion_no_params_with_context_from_template __dict__"

@register.inclusion_tag('inclusion.html', takes_context=True)
def inclusion_params_and_context(context, arg):
    """Expected inclusion_params_and_context __doc__"""
    return {"result" : "inclusion_params_and_context - Expected result (context value: %s): %s" % (context['value'], arg)}
inclusion_params_and_context.anything = "Expected inclusion_params_and_context __dict__"

@register.inclusion_tag(get_template('inclusion.html'), takes_context=True)
def inclusion_params_and_context_from_template(context, arg):
    """Expected inclusion_params_and_context_from_template __doc__"""
    return {"result" : "inclusion_params_and_context_from_template - Expected result (context value: %s): %s" % (context['value'], arg)}
inclusion_params_and_context_from_template.anything = "Expected inclusion_params_and_context_from_template __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_two_params(one, two):
    """Expected inclusion_two_params __doc__"""
    return {"result": "inclusion_two_params - Expected result: %s, %s" % (one, two)}
inclusion_two_params.anything = "Expected inclusion_two_params __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_two_params_from_template(one, two):
    """Expected inclusion_two_params_from_template __doc__"""
    return {"result": "inclusion_two_params_from_template - Expected result: %s, %s" % (one, two)}
inclusion_two_params_from_template.anything = "Expected inclusion_two_params_from_template __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_one_default(one, two='hi'):
    """Expected inclusion_one_default __doc__"""
    return {"result": "inclusion_one_default - Expected result: %s, %s" % (one, two)}
inclusion_one_default.anything = "Expected inclusion_one_default __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_one_default_from_template(one, two='hi'):
    """Expected inclusion_one_default_from_template __doc__"""
    return {"result": "inclusion_one_default_from_template - Expected result: %s, %s" % (one, two)}
inclusion_one_default_from_template.anything = "Expected inclusion_one_default_from_template __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_unlimited_args(one, two='hi', *args):
    """Expected inclusion_unlimited_args __doc__"""
    return {"result": "inclusion_unlimited_args - Expected result: %s" % (', '.join([unicode(arg) for arg in [one, two] + list(args)]))}
inclusion_unlimited_args.anything = "Expected inclusion_unlimited_args __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_unlimited_args_from_template(one, two='hi', *args):
    """Expected inclusion_unlimited_args_from_template __doc__"""
    return {"result": "inclusion_unlimited_args_from_template - Expected result: %s" % (', '.join([unicode(arg) for arg in [one, two] + list(args)]))}
inclusion_unlimited_args_from_template.anything = "Expected inclusion_unlimited_args_from_template __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_only_unlimited_args(*args):
    """Expected inclusion_only_unlimited_args __doc__"""
    return {"result": "inclusion_only_unlimited_args - Expected result: %s" % (', '.join([unicode(arg) for arg in args]))}
inclusion_only_unlimited_args.anything = "Expected inclusion_only_unlimited_args __dict__"

@register.inclusion_tag(get_template('inclusion.html'))
def inclusion_only_unlimited_args_from_template(*args):
    """Expected inclusion_only_unlimited_args_from_template __doc__"""
    return {"result": "inclusion_only_unlimited_args_from_template - Expected result: %s" % (', '.join([unicode(arg) for arg in args]))}
inclusion_only_unlimited_args_from_template.anything = "Expected inclusion_only_unlimited_args_from_template __dict__"

@register.inclusion_tag('test_incl_tag_current_app.html', takes_context=True)
def inclusion_tag_current_app(context):
    """Expected inclusion_tag_current_app __doc__"""
    return {}
inclusion_tag_current_app.anything = "Expected inclusion_tag_current_app __dict__"

@register.inclusion_tag('test_incl_tag_use_l10n.html', takes_context=True)
def inclusion_tag_use_l10n(context):
    """Expected inclusion_tag_use_l10n __doc__"""
    return {}
inclusion_tag_use_l10n.anything = "Expected inclusion_tag_use_l10n __dict__"

@register.inclusion_tag('inclusion.html')
def inclusion_unlimited_args_kwargs(one, two='hi', *args, **kwargs):
    """Expected inclusion_unlimited_args_kwargs __doc__"""
    # Sort the dictionary by key to guarantee the order for testing.
    sorted_kwarg = sorted(kwargs.iteritems(), key=operator.itemgetter(0))
    return {"result": "inclusion_unlimited_args_kwargs - Expected result: %s / %s" % (
        ', '.join([unicode(arg) for arg in [one, two] + list(args)]),
        ', '.join(['%s=%s' % (k, v) for (k, v) in sorted_kwarg])
        )}
inclusion_unlimited_args_kwargs.anything = "Expected inclusion_unlimited_args_kwargs __dict__"

@register.inclusion_tag('inclusion.html', takes_context=True)
def inclusion_tag_without_context_parameter(arg):
    """Expected inclusion_tag_without_context_parameter __doc__"""
    return {}
inclusion_tag_without_context_parameter.anything = "Expected inclusion_tag_without_context_parameter __dict__"

@register.assignment_tag
def assignment_no_params():
    """Expected assignment_no_params __doc__"""
    return "assignment_no_params - Expected result"
assignment_no_params.anything = "Expected assignment_no_params __dict__"

@register.assignment_tag
def assignment_one_param(arg):
    """Expected assignment_one_param __doc__"""
    return "assignment_one_param - Expected result: %s" % arg
assignment_one_param.anything = "Expected assignment_one_param __dict__"

@register.assignment_tag(takes_context=False)
def assignment_explicit_no_context(arg):
    """Expected assignment_explicit_no_context __doc__"""
    return "assignment_explicit_no_context - Expected result: %s" % arg
assignment_explicit_no_context.anything = "Expected assignment_explicit_no_context __dict__"

@register.assignment_tag(takes_context=True)
def assignment_no_params_with_context(context):
    """Expected assignment_no_params_with_context __doc__"""
    return "assignment_no_params_with_context - Expected result (context value: %s)" % context['value']
assignment_no_params_with_context.anything = "Expected assignment_no_params_with_context __dict__"

@register.assignment_tag(takes_context=True)
def assignment_params_and_context(context, arg):
    """Expected assignment_params_and_context __doc__"""
    return "assignment_params_and_context - Expected result (context value: %s): %s" % (context['value'], arg)
assignment_params_and_context.anything = "Expected assignment_params_and_context __dict__"

@register.assignment_tag
def assignment_two_params(one, two):
    """Expected assignment_two_params __doc__"""
    return "assignment_two_params - Expected result: %s, %s" % (one, two)
assignment_two_params.anything = "Expected assignment_two_params __dict__"

@register.assignment_tag
def assignment_one_default(one, two='hi'):
    """Expected assignment_one_default __doc__"""
    return "assignment_one_default - Expected result: %s, %s" % (one, two)
assignment_one_default.anything = "Expected assignment_one_default __dict__"

@register.assignment_tag
def assignment_unlimited_args(one, two='hi', *args):
    """Expected assignment_unlimited_args __doc__"""
    return "assignment_unlimited_args - Expected result: %s" % (', '.join([unicode(arg) for arg in [one, two] + list(args)]))
assignment_unlimited_args.anything = "Expected assignment_unlimited_args __dict__"

@register.assignment_tag
def assignment_only_unlimited_args(*args):
    """Expected assignment_only_unlimited_args __doc__"""
    return "assignment_only_unlimited_args - Expected result: %s" % ', '.join([unicode(arg) for arg in args])
assignment_only_unlimited_args.anything = "Expected assignment_only_unlimited_args __dict__"

@register.assignment_tag
def assignment_unlimited_args_kwargs(one, two='hi', *args, **kwargs):
    """Expected assignment_unlimited_args_kwargs __doc__"""
    # Sort the dictionary by key to guarantee the order for testing.
    sorted_kwarg = sorted(kwargs.iteritems(), key=operator.itemgetter(0))
    return "assignment_unlimited_args_kwargs - Expected result: %s / %s" % (
        ', '.join([unicode(arg) for arg in [one, two] + list(args)]),
        ', '.join(['%s=%s' % (k, v) for (k, v) in sorted_kwarg])
        )
assignment_unlimited_args_kwargs.anything = "Expected assignment_unlimited_args_kwargs __dict__"

@register.assignment_tag(takes_context=True)
def assignment_tag_without_context_parameter(arg):
    """Expected assignment_tag_without_context_parameter __doc__"""
    return "Expected result"
assignment_tag_without_context_parameter.anything = "Expected assignment_tag_without_context_parameter __dict__"

from __future__ import absolute_import

from django.conf.urls import patterns, url, include

from .views import empty_view


urlpatterns = patterns('',
    url(r'^$', empty_view, name="named-url1"),
    url(r'^extra/(?P<extra>\w+)/$', empty_view, name="named-url2"),
    url(r'^(?P<one>\d+)|(?P<two>\d+)/$', empty_view),
    (r'^included/', include('regressiontests.urlpatterns_reverse.included_named_urls')),
)

#!/usr/bin/env python

# This works exactly like 2to3, except that it uses Django's fixers rather
# than 2to3's built-in fixers.

import sys
from lib2to3.main import main

sys.exit(main("django.utils.2to3_fixes"))


"""
This encapsulates the logic for displaying filters in the Django admin.
Filters are specified in models with the "list_filter" option.

Each filter subclass knows how to display a filter for a field that passes a
certain test -- e.g. being a DateField or ForeignKey.
"""
import datetime

from django.db import models
from django.core.exceptions import ImproperlyConfigured, ValidationError
from django.utils.encoding import smart_text, force_text
from django.utils.translation import ugettext_lazy as _
from django.utils import timezone
from django.contrib.admin.util import (get_model_from_relation,
    reverse_field_path, get_limit_choices_to_from_path, prepare_lookup_value)
from django.contrib.admin.options import IncorrectLookupParameters

class ListFilter(object):
    title = None  # Human-readable title to appear in the right sidebar.
    template = 'admin/filter.html'

    def __init__(self, request, params, model, model_admin):
        # This dictionary will eventually contain the request's query string
        # parameters actually used by this filter.
        self.used_parameters = {}
        if self.title is None:
            raise ImproperlyConfigured(
                "The list filter '%s' does not specify "
                "a 'title'." % self.__class__.__name__)

    def has_output(self):
        """
        Returns True if some choices would be output for this filter.
        """
        raise NotImplementedError

    def choices(self, cl):
        """
        Returns choices ready to be output in the template.
        """
        raise NotImplementedError

    def queryset(self, request, queryset):
        """
        Returns the filtered queryset.
        """
        raise NotImplementedError

    def expected_parameters(self):
        """
        Returns the list of parameter names that are expected from the
        request's query string and that will be used by this filter.
        """
        raise NotImplementedError


class SimpleListFilter(ListFilter):
    # The parameter that should be used in the query string for that filter.
    parameter_name = None

    def __init__(self, request, params, model, model_admin):
        super(SimpleListFilter, self).__init__(
            request, params, model, model_admin)
        if self.parameter_name is None:
            raise ImproperlyConfigured(
                "The list filter '%s' does not specify "
                "a 'parameter_name'." % self.__class__.__name__)
        lookup_choices = self.lookups(request, model_admin)
        if lookup_choices is None:
            lookup_choices = ()
        self.lookup_choices = list(lookup_choices)
        if self.parameter_name in params:
            value = params.pop(self.parameter_name)
            self.used_parameters[self.parameter_name] = value

    def has_output(self):
        return len(self.lookup_choices) > 0

    def value(self):
        """
        Returns the value (in string format) provided in the request's
        query string for this filter, if any. If the value wasn't provided then
        returns None.
        """
        return self.used_parameters.get(self.parameter_name, None)

    def lookups(self, request, model_admin):
        """
        Must be overriden to return a list of tuples (value, verbose value)
        """
        raise NotImplementedError

    def expected_parameters(self):
        return [self.parameter_name]

    def choices(self, cl):
        yield {
            'selected': self.value() is None,
            'query_string': cl.get_query_string({}, [self.parameter_name]),
            'display': _('All'),
        }
        for lookup, title in self.lookup_choices:
            yield {
                'selected': self.value() == force_text(lookup),
                'query_string': cl.get_query_string({
                    self.parameter_name: lookup,
                }, []),
                'display': title,
            }


class FieldListFilter(ListFilter):
    _field_list_filters = []
    _take_priority_index = 0

    def __init__(self, field, request, params, model, model_admin, field_path):
        self.field = field
        self.field_path = field_path
        self.title = getattr(field, 'verbose_name', field_path)
        super(FieldListFilter, self).__init__(
            request, params, model, model_admin)
        for p in self.expected_parameters():
            if p in params:
                value = params.pop(p)
                self.used_parameters[p] = prepare_lookup_value(p, value)

    def has_output(self):
        return True

    def queryset(self, request, queryset):
        try:
            return queryset.filter(**self.used_parameters)
        except ValidationError as e:
            raise IncorrectLookupParameters(e)

    @classmethod
    def register(cls, test, list_filter_class, take_priority=False):
        if take_priority:
            # This is to allow overriding the default filters for certain types
            # of fields with some custom filters. The first found in the list
            # is used in priority.
            cls._field_list_filters.insert(
                cls._take_priority_index, (test, list_filter_class))
            cls._take_priority_index += 1
        else:
            cls._field_list_filters.append((test, list_filter_class))

    @classmethod
    def create(cls, field, request, params, model, model_admin, field_path):
        for test, list_filter_class in cls._field_list_filters:
            if not test(field):
                continue
            return list_filter_class(field, request, params,
                model, model_admin, field_path=field_path)


class RelatedFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        other_model = get_model_from_relation(field)
        if hasattr(field, 'rel'):
            rel_name = field.rel.get_related_field().name
        else:
            rel_name = other_model._meta.pk.name
        self.lookup_kwarg = '%s__%s__exact' % (field_path, rel_name)
        self.lookup_kwarg_isnull = '%s__isnull' % field_path
        self.lookup_val = request.GET.get(self.lookup_kwarg, None)
        self.lookup_val_isnull = request.GET.get(
                                      self.lookup_kwarg_isnull, None)
        self.lookup_choices = field.get_choices(include_blank=False)
        super(RelatedFieldListFilter, self).__init__(
            field, request, params, model, model_admin, field_path)
        if hasattr(field, 'verbose_name'):
            self.lookup_title = field.verbose_name
        else:
            self.lookup_title = other_model._meta.verbose_name
        self.title = self.lookup_title

    def has_output(self):
        if (isinstance(self.field, models.related.RelatedObject)
                and self.field.field.null or hasattr(self.field, 'rel')
                    and self.field.null):
            extra = 1
        else:
            extra = 0
        return len(self.lookup_choices) + extra > 1

    def expected_parameters(self):
        return [self.lookup_kwarg, self.lookup_kwarg_isnull]

    def choices(self, cl):
        from django.contrib.admin.views.main import EMPTY_CHANGELIST_VALUE
        yield {
            'selected': self.lookup_val is None and not self.lookup_val_isnull,
            'query_string': cl.get_query_string({},
                [self.lookup_kwarg, self.lookup_kwarg_isnull]),
            'display': _('All'),
        }
        for pk_val, val in self.lookup_choices:
            yield {
                'selected': self.lookup_val == smart_text(pk_val),
                'query_string': cl.get_query_string({
                    self.lookup_kwarg: pk_val,
                }, [self.lookup_kwarg_isnull]),
                'display': val,
            }
        if (isinstance(self.field, models.related.RelatedObject)
                and self.field.field.null or hasattr(self.field, 'rel')
                    and self.field.null):
            yield {
                'selected': bool(self.lookup_val_isnull),
                'query_string': cl.get_query_string({
                    self.lookup_kwarg_isnull: 'True',
                }, [self.lookup_kwarg]),
                'display': EMPTY_CHANGELIST_VALUE,
            }

FieldListFilter.register(lambda f: (
        hasattr(f, 'rel') and bool(f.rel) or
        isinstance(f, models.related.RelatedObject)), RelatedFieldListFilter)


class BooleanFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.lookup_kwarg = '%s__exact' % field_path
        self.lookup_kwarg2 = '%s__isnull' % field_path
        self.lookup_val = request.GET.get(self.lookup_kwarg, None)
        self.lookup_val2 = request.GET.get(self.lookup_kwarg2, None)
        super(BooleanFieldListFilter, self).__init__(field,
            request, params, model, model_admin, field_path)

    def expected_parameters(self):
        return [self.lookup_kwarg, self.lookup_kwarg2]

    def choices(self, cl):
        for lookup, title in (
                (None, _('All')),
                ('1', _('Yes')),
                ('0', _('No'))):
            yield {
                'selected': self.lookup_val == lookup and not self.lookup_val2,
                'query_string': cl.get_query_string({
                        self.lookup_kwarg: lookup,
                    }, [self.lookup_kwarg2]),
                'display': title,
            }
        if isinstance(self.field, models.NullBooleanField):
            yield {
                'selected': self.lookup_val2 == 'True',
                'query_string': cl.get_query_string({
                        self.lookup_kwarg2: 'True',
                    }, [self.lookup_kwarg]),
                'display': _('Unknown'),
            }

FieldListFilter.register(lambda f: isinstance(f,
    (models.BooleanField, models.NullBooleanField)), BooleanFieldListFilter)


class ChoicesFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.lookup_kwarg = '%s__exact' % field_path
        self.lookup_val = request.GET.get(self.lookup_kwarg)
        super(ChoicesFieldListFilter, self).__init__(
            field, request, params, model, model_admin, field_path)

    def expected_parameters(self):
        return [self.lookup_kwarg]

    def choices(self, cl):
        yield {
            'selected': self.lookup_val is None,
            'query_string': cl.get_query_string({}, [self.lookup_kwarg]),
            'display': _('All')
        }
        for lookup, title in self.field.flatchoices:
            yield {
                'selected': smart_text(lookup) == self.lookup_val,
                'query_string': cl.get_query_string({
                                    self.lookup_kwarg: lookup}),
                'display': title,
            }

FieldListFilter.register(lambda f: bool(f.choices), ChoicesFieldListFilter)


class DateFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.field_generic = '%s__' % field_path
        self.date_params = dict([(k, v) for k, v in params.items()
                                 if k.startswith(self.field_generic)])

        now = timezone.now()
        # When time zone support is enabled, convert "now" to the user's time
        # zone so Django's definition of "Today" matches what the user expects.
        if timezone.is_aware(now):
            now = timezone.localtime(now)

        if isinstance(field, models.DateTimeField):
            today = now.replace(hour=0, minute=0, second=0, microsecond=0)
        else:       # field is a models.DateField
            today = now.date()
        tomorrow = today + datetime.timedelta(days=1)

        self.lookup_kwarg_since = '%s__gte' % field_path
        self.lookup_kwarg_until = '%s__lt' % field_path
        self.links = (
            (_('Any date'), {}),
            (_('Today'), {
                self.lookup_kwarg_since: str(today),
                self.lookup_kwarg_until: str(tomorrow),
            }),
            (_('Past 7 days'), {
                self.lookup_kwarg_since: str(today - datetime.timedelta(days=7)),
                self.lookup_kwarg_until: str(tomorrow),
            }),
            (_('This month'), {
                self.lookup_kwarg_since: str(today.replace(day=1)),
                self.lookup_kwarg_until: str(tomorrow),
            }),
            (_('This year'), {
                self.lookup_kwarg_since: str(today.replace(month=1, day=1)),
                self.lookup_kwarg_until: str(tomorrow),
            }),
        )
        super(DateFieldListFilter, self).__init__(
            field, request, params, model, model_admin, field_path)

    def expected_parameters(self):
        return [self.lookup_kwarg_since, self.lookup_kwarg_until]

    def choices(self, cl):
        for title, param_dict in self.links:
            yield {
                'selected': self.date_params == param_dict,
                'query_string': cl.get_query_string(
                                    param_dict, [self.field_generic]),
                'display': title,
            }

FieldListFilter.register(
    lambda f: isinstance(f, models.DateField), DateFieldListFilter)


# This should be registered last, because it's a last resort. For example,
# if a field is eligible to use the BooleanFieldListFilter, that'd be much
# more appropriate, and the AllValuesFieldListFilter won't get used for it.
class AllValuesFieldListFilter(FieldListFilter):
    def __init__(self, field, request, params, model, model_admin, field_path):
        self.lookup_kwarg = field_path
        self.lookup_kwarg_isnull = '%s__isnull' % field_path
        self.lookup_val = request.GET.get(self.lookup_kwarg, None)
        self.lookup_val_isnull = request.GET.get(self.lookup_kwarg_isnull,
                                                 None)
        parent_model, reverse_path = reverse_field_path(model, field_path)
        queryset = parent_model._default_manager.all()
        # optional feature: limit choices base on existing relationships
        # queryset = queryset.complex_filter(
        #    {'%s__isnull' % reverse_path: False})
        limit_choices_to = get_limit_choices_to_from_path(model, field_path)
        queryset = queryset.filter(limit_choices_to)

        self.lookup_choices = (queryset
                               .distinct()
                               .order_by(field.name)
                               .values_list(field.name, flat=True))
        super(AllValuesFieldListFilter, self).__init__(
            field, request, params, model, model_admin, field_path)

    def expected_parameters(self):
        return [self.lookup_kwarg, self.lookup_kwarg_isnull]

    def choices(self, cl):
        from django.contrib.admin.views.main import EMPTY_CHANGELIST_VALUE
        yield {
            'selected': (self.lookup_val is None
                and self.lookup_val_isnull is None),
            'query_string': cl.get_query_string({},
                [self.lookup_kwarg, self.lookup_kwarg_isnull]),
            'display': _('All'),
        }
        include_none = False
        for val in self.lookup_choices:
            if val is None:
                include_none = True
                continue
            val = smart_text(val)
            yield {
                'selected': self.lookup_val == val,
                'query_string': cl.get_query_string({
                    self.lookup_kwarg: val,
                }, [self.lookup_kwarg_isnull]),
                'display': val,
            }
        if include_none:
            yield {
                'selected': bool(self.lookup_val_isnull),
                'query_string': cl.get_query_string({
                    self.lookup_kwarg_isnull: 'True',
                }, [self.lookup_kwarg]),
                'display': EMPTY_CHANGELIST_VALUE,
            }

FieldListFilter.register(lambda f: True, AllValuesFieldListFilter)

"""
Creates permissions for all installed apps that need permissions.
"""
from __future__ import unicode_literals

import getpass
import locale
import unicodedata

from django.contrib.auth import models as auth_app, get_user_model
from django.core import exceptions
from django.core.management.base import CommandError
from django.db import DEFAULT_DB_ALIAS, router
from django.db.models import get_models, signals
from django.utils import six
from django.utils.six.moves import input


def _get_permission_codename(action, opts):
    return '%s_%s' % (action, opts.object_name.lower())


def _get_all_permissions(opts, ctype):
    """
    Returns (codename, name) for all permissions in the given opts.
    """
    builtin = _get_builtin_permissions(opts)
    custom = list(opts.permissions)
    _check_permission_clashing(custom, builtin, ctype)
    return builtin + custom

def _get_builtin_permissions(opts):
    """
    Returns (codename, name) for all autogenerated permissions.
    """
    perms = []
    for action in ('add', 'change', 'delete'):
        perms.append((_get_permission_codename(action, opts),
            'Can %s %s' % (action, opts.verbose_name_raw)))
    return perms

def _check_permission_clashing(custom, builtin, ctype):
    """
    Check that permissions for a model do not clash. Raises CommandError if
    there are duplicate permissions.
    """
    pool = set()
    builtin_codenames = set(p[0] for p in builtin)
    for codename, _name in custom:
        if codename in pool:
            raise CommandError(
                "The permission codename '%s' is duplicated for model '%s.%s'." %
                (codename, ctype.app_label, ctype.model_class().__name__))
        elif codename in builtin_codenames:
            raise CommandError(
                "The permission codename '%s' clashes with a builtin permission "
                "for model '%s.%s'." %
                (codename, ctype.app_label, ctype.model_class().__name__))
        pool.add(codename)

def create_permissions(app, created_models, verbosity, db=DEFAULT_DB_ALIAS, **kwargs):
    if not router.allow_syncdb(db, auth_app.Permission):
        return

    from django.contrib.contenttypes.models import ContentType

    app_models = get_models(app)

    # This will hold the permissions we're looking for as
    # (content_type, (codename, name))
    searched_perms = list()
    # The codenames and ctypes that should exist.
    ctypes = set()
    for klass in app_models:
        # Force looking up the content types in the current database
        # before creating foreign keys to them.
        ctype = ContentType.objects.db_manager(db).get_for_model(klass)
        ctypes.add(ctype)
        for perm in _get_all_permissions(klass._meta, ctype):
            searched_perms.append((ctype, perm))

    # Find all the Permissions that have a context_type for a model we're
    # looking for.  We don't need to check for codenames since we already have
    # a list of the ones we're going to create.
    all_perms = set(auth_app.Permission.objects.using(db).filter(
        content_type__in=ctypes,
    ).values_list(
        "content_type", "codename"
    ))

    perms = [
        auth_app.Permission(codename=codename, name=name, content_type=ctype)
        for ctype, (codename, name) in searched_perms
        if (ctype.pk, codename) not in all_perms
    ]
    auth_app.Permission.objects.using(db).bulk_create(perms)
    if verbosity >= 2:
        for perm in perms:
            print("Adding permission '%s'" % perm)


def create_superuser(app, created_models, verbosity, db, **kwargs):
    from django.core.management import call_command

    UserModel = get_user_model()

    if UserModel in created_models and kwargs.get('interactive', True):
        msg = ("\nYou just installed Django's auth system, which means you "
            "don't have any superusers defined.\nWould you like to create one "
            "now? (yes/no): ")
        confirm = input(msg)
        while 1:
            if confirm not in ('yes', 'no'):
                confirm = input('Please enter either "yes" or "no": ')
                continue
            if confirm == 'yes':
                call_command("createsuperuser", interactive=True, database=db)
            break


def get_system_username():
    """
    Try to determine the current system user's username.

    :returns: The username as a unicode string, or an empty string if the
        username could not be determined.
    """
    try:
        result = getpass.getuser()
    except (ImportError, KeyError):
        # KeyError will be raised by os.getpwuid() (called by getuser())
        # if there is no corresponding entry in the /etc/passwd file
        # (a very restricted chroot environment, for example).
        return ''
    if not six.PY3:
        default_locale = locale.getdefaultlocale()[1]
        if not default_locale:
            return ''
        try:
            result = result.decode(default_locale)
        except UnicodeDecodeError:
            # UnicodeDecodeError - preventive treatment for non-latin Windows.
            return ''
    return result


def get_default_username(check_db=True):
    """
    Try to determine the current system user's username to use as a default.

    :param check_db: If ``True``, requires that the username does not match an
        existing ``auth.User`` (otherwise returns an empty string).
    :returns: The username, or an empty string if no username can be
        determined.
    """
    # If the User model has been swapped out, we can't make any assumptions
    # about the default user name.
    if auth_app.User._meta.swapped:
        return ''

    default_username = get_system_username()
    try:
        default_username = unicodedata.normalize('NFKD', default_username)\
            .encode('ascii', 'ignore').decode('ascii').replace(' ', '').lower()
    except UnicodeDecodeError:
        return ''

    # Run the username validator
    try:
        auth_app.User._meta.get_field('username').run_validators(default_username)
    except exceptions.ValidationError:
        return ''

    # Don't return the default username if it is already taken.
    if check_db and default_username:
        try:
            auth_app.User._default_manager.get(username=default_username)
        except auth_app.User.DoesNotExist:
            pass
        else:
            return ''
    return default_username

signals.post_syncdb.connect(create_permissions,
    dispatch_uid="django.contrib.auth.management.create_permissions")
signals.post_syncdb.connect(create_superuser,
    sender=auth_app, dispatch_uid="django.contrib.auth.management.create_superuser")

from django.contrib.syndication.views import Feed
from django.contrib.sites.models import get_current_site
from django.contrib import comments
from django.utils.translation import ugettext as _

class LatestCommentFeed(Feed):
    """Feed of latest comments on the current site."""

    def __call__(self, request, *args, **kwargs):
        self.site = get_current_site(request)
        return super(LatestCommentFeed, self).__call__(request, *args, **kwargs)

    def title(self):
        return _("%(site_name)s comments") % dict(site_name=self.site.name)

    def link(self):
        return "http://%s/" % (self.site.domain)

    def description(self):
        return _("Latest comments on %(site_name)s") % dict(site_name=self.site.name)

    def items(self):
        qs = comments.get_model().objects.filter(
            site__pk = self.site.pk,
            is_public = True,
            is_removed = False,
        )
        return qs.order_by('-submit_date')[:40]

    def item_pubdate(self, item):
        return item.submit_date

import os
from django.conf import settings
from django.contrib.auth.models import User
from django.contrib.flatpages.models import FlatPage
from django.test import TestCase
from django.test.utils import override_settings


@override_settings(
    LOGIN_URL='/accounts/login/',
    MIDDLEWARE_CLASSES=(
        'django.middleware.common.CommonMiddleware',
        'django.contrib.sessions.middleware.SessionMiddleware',
        'django.middleware.csrf.CsrfViewMiddleware',
        'django.contrib.auth.middleware.AuthenticationMiddleware',
        'django.contrib.messages.middleware.MessageMiddleware',
        # no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
    ),
    TEMPLATE_DIRS=(
        os.path.join(os.path.dirname(__file__), 'templates'),
    ),
    SITE_ID=1,
)
class FlatpageViewTests(TestCase):
    fixtures = ['sample_flatpages', 'example_site']
    urls = 'django.contrib.flatpages.tests.urls'

    def test_view_flatpage(self):
        "A flatpage can be served through a view"
        response = self.client.get('/flatpage_root/flatpage/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, "<p>Isn't it flat!</p>")

    def test_view_non_existent_flatpage(self):
        "A non-existent flatpage raises 404 when served through a view"
        response = self.client.get('/flatpage_root/no_such_flatpage/')
        self.assertEqual(response.status_code, 404)

    def test_view_authenticated_flatpage(self):
        "A flatpage served through a view can require authentication"
        response = self.client.get('/flatpage_root/sekrit/')
        self.assertRedirects(response, '/accounts/login/?next=/flatpage_root/sekrit/')
        User.objects.create_user('testuser', 'test@example.com', 's3krit')
        self.client.login(username='testuser',password='s3krit')
        response = self.client.get('/flatpage_root/sekrit/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, "<p>Isn't it sekrit!</p>")

    def test_fallback_flatpage(self):
        "A fallback flatpage won't be served if the middleware is disabled"
        response = self.client.get('/flatpage/')
        self.assertEqual(response.status_code, 404)

    def test_fallback_non_existent_flatpage(self):
        "A non-existent flatpage won't be served if the fallback middlware is disabled"
        response = self.client.get('/no_such_flatpage/')
        self.assertEqual(response.status_code, 404)

    def test_view_flatpage_special_chars(self):
        "A flatpage with special chars in the URL can be served through a view"
        fp = FlatPage.objects.create(
            url="/some.very_special~chars-here/",
            title="A very special page",
            content="Isn't it special!",
            enable_comments=False,
            registration_required=False,
        )
        fp.sites.add(settings.SITE_ID)

        response = self.client.get('/flatpage_root/some.very_special~chars-here/')
        self.assertEqual(response.status_code, 200)
        self.assertContains(response, "<p>Isn't it special!</p>")


@override_settings(
    APPEND_SLASH = True,
    LOGIN_URL='/accounts/login/',
    MIDDLEWARE_CLASSES=(
        'django.middleware.common.CommonMiddleware',
        'django.contrib.sessions.middleware.SessionMiddleware',
        'django.middleware.csrf.CsrfViewMiddleware',
        'django.contrib.auth.middleware.AuthenticationMiddleware',
        'django.contrib.messages.middleware.MessageMiddleware',
        # no 'django.contrib.flatpages.middleware.FlatpageFallbackMiddleware'
    ),
    TEMPLATE_DIRS=(
        os.path.join(os.path.dirname(__file__), 'templates'),
    ),
    SITE_ID=1,
)
class FlatpageViewAppendSlashTests(TestCase):
    fixtures = ['sample_flatpages', 'example_site']
    urls = 'django.contrib.flatpages.tests.urls'

    def test_redirect_view_flatpage(self):
        "A flatpage can be served through a view and should add a slash"
        response = self.client.get('/flatpage_root/flatpage')
        self.assertRedirects(response, '/flatpage_root/flatpage/', status_code=301)

    def test_redirect_view_non_existent_flatpage(self):
        "A non-existent flatpage raises 404 when served through a view and should not add a slash"
        response = self.client.get('/flatpage_root/no_such_flatpage')
        self.assertEqual(response.status_code, 404)

    def test_redirect_fallback_flatpage(self):
        "A fallback flatpage won't be served if the middleware is disabled and should not add a slash"
        response = self.client.get('/flatpage')
        self.assertEqual(response.status_code, 404)

    def test_redirect_fallback_non_existent_flatpage(self):
        "A non-existent flatpage won't be served if the fallback middlware is disabled and should not add a slash"
        response = self.client.get('/no_such_flatpage')
        self.assertEqual(response.status_code, 404)

    def test_redirect_view_flatpage_special_chars(self):
        "A flatpage with special chars in the URL can be served through a view and should add a slash"
        fp = FlatPage.objects.create(
            url="/some.very_special~chars-here/",
            title="A very special page",
            content="Isn't it special!",
            enable_comments=False,
            registration_required=False,
        )
        fp.sites.add(settings.SITE_ID)

        response = self.client.get('/flatpage_root/some.very_special~chars-here')
        self.assertRedirects(response, '/flatpage_root/some.very_special~chars-here/', status_code=301)

from django.contrib.gis.gdal import OGRGeomType
from django.db.backends.sqlite3.introspection import DatabaseIntrospection, FlexibleFieldLookupDict
from django.utils import six

class GeoFlexibleFieldLookupDict(FlexibleFieldLookupDict):
    """
    Sublcass that includes updates the `base_data_types_reverse` dict
    for geometry field types.
    """
    base_data_types_reverse = FlexibleFieldLookupDict.base_data_types_reverse.copy()
    base_data_types_reverse.update(
        {'point' : 'GeometryField',
         'linestring' : 'GeometryField',
         'polygon' : 'GeometryField',
         'multipoint' : 'GeometryField',
         'multilinestring' : 'GeometryField',
         'multipolygon' : 'GeometryField',
         'geometrycollection' : 'GeometryField',
         })

class SpatiaLiteIntrospection(DatabaseIntrospection):
    data_types_reverse = GeoFlexibleFieldLookupDict()

    def get_geometry_type(self, table_name, geo_col):
        cursor = self.connection.cursor()
        try:
            # Querying the `geometry_columns` table to get additional metadata.
            cursor.execute('SELECT "coord_dimension", "srid", "type" '
                           'FROM "geometry_columns" '
                           'WHERE "f_table_name"=%s AND "f_geometry_column"=%s',
                           (table_name, geo_col))
            row = cursor.fetchone()
            if not row:
                raise Exception('Could not find a geometry column for "%s"."%s"' %
                                (table_name, geo_col))

            # OGRGeomType does not require GDAL and makes it easy to convert
            # from OGC geom type name to Django field.
            field_type = OGRGeomType(row[2]).django

            # Getting any GeometryField keyword arguments that are not the default.
            dim = row[0]
            srid = row[1]
            field_params = {}
            if srid != 4326:
                field_params['srid'] = srid
            if isinstance(dim, six.string_types) and 'Z' in dim:
                field_params['dim'] = 3
        finally:
            cursor.close()

        return field_type, field_params

from ctypes import c_char_p, c_double, c_int, c_void_p, POINTER
from django.contrib.gis.gdal.envelope import OGREnvelope
from django.contrib.gis.gdal.libgdal import lgdal
from django.contrib.gis.gdal.prototypes.errcheck import check_bool, check_envelope
from django.contrib.gis.gdal.prototypes.generation import (const_string_output,
    double_output, geom_output, int_output, srs_output, string_output, void_output)

### Generation routines specific to this module ###
def env_func(f, argtypes):
    "For getting OGREnvelopes."
    f.argtypes = argtypes
    f.restype = None
    f.errcheck = check_envelope
    return f

def pnt_func(f):
    "For accessing point information."
    return double_output(f, [c_void_p, c_int])

def topology_func(f):
    f.argtypes = [c_void_p, c_void_p]
    f.restype = c_int
    f.errchck = check_bool
    return f

### OGR_G ctypes function prototypes ###

# GeoJSON routines.
from_json = geom_output(lgdal.OGR_G_CreateGeometryFromJson, [c_char_p])
to_json = string_output(lgdal.OGR_G_ExportToJson, [c_void_p], str_result=True, decoding='ascii')
to_kml = string_output(lgdal.OGR_G_ExportToKML, [c_void_p, c_char_p], str_result=True, decoding='ascii')

# GetX, GetY, GetZ all return doubles.
getx = pnt_func(lgdal.OGR_G_GetX)
gety = pnt_func(lgdal.OGR_G_GetY)
getz = pnt_func(lgdal.OGR_G_GetZ)
    
# Geometry creation routines.
from_wkb = geom_output(lgdal.OGR_G_CreateFromWkb, [c_char_p, c_void_p, POINTER(c_void_p), c_int], offset=-2)
from_wkt = geom_output(lgdal.OGR_G_CreateFromWkt, [POINTER(c_char_p), c_void_p, POINTER(c_void_p)], offset=-1)
create_geom = geom_output(lgdal.OGR_G_CreateGeometry, [c_int])
clone_geom = geom_output(lgdal.OGR_G_Clone, [c_void_p])
get_geom_ref = geom_output(lgdal.OGR_G_GetGeometryRef, [c_void_p, c_int])
get_boundary = geom_output(lgdal.OGR_G_GetBoundary, [c_void_p])
geom_convex_hull = geom_output(lgdal.OGR_G_ConvexHull, [c_void_p])
geom_diff = geom_output(lgdal.OGR_G_Difference, [c_void_p, c_void_p])
geom_intersection = geom_output(lgdal.OGR_G_Intersection, [c_void_p, c_void_p])
geom_sym_diff = geom_output(lgdal.OGR_G_SymmetricDifference, [c_void_p, c_void_p])
geom_union = geom_output(lgdal.OGR_G_Union, [c_void_p, c_void_p])

# Geometry modification routines.
add_geom = void_output(lgdal.OGR_G_AddGeometry, [c_void_p, c_void_p])
import_wkt = void_output(lgdal.OGR_G_ImportFromWkt, [c_void_p, POINTER(c_char_p)])

# Destroys a geometry
destroy_geom = void_output(lgdal.OGR_G_DestroyGeometry, [c_void_p], errcheck=False)

# Geometry export routines.
to_wkb = void_output(lgdal.OGR_G_ExportToWkb, None, errcheck=True) # special handling for WKB.
to_wkt = string_output(lgdal.OGR_G_ExportToWkt, [c_void_p, POINTER(c_char_p)], decoding='ascii')
to_gml = string_output(lgdal.OGR_G_ExportToGML, [c_void_p], str_result=True, decoding='ascii')
get_wkbsize = int_output(lgdal.OGR_G_WkbSize, [c_void_p])

# Geometry spatial-reference related routines.
assign_srs = void_output(lgdal.OGR_G_AssignSpatialReference, [c_void_p, c_void_p], errcheck=False)
get_geom_srs = srs_output(lgdal.OGR_G_GetSpatialReference, [c_void_p])

# Geometry properties
get_area = double_output(lgdal.OGR_G_GetArea, [c_void_p])
get_centroid = void_output(lgdal.OGR_G_Centroid, [c_void_p, c_void_p])
get_dims = int_output(lgdal.OGR_G_GetDimension, [c_void_p])
get_coord_dim = int_output(lgdal.OGR_G_GetCoordinateDimension, [c_void_p])
set_coord_dim = void_output(lgdal.OGR_G_SetCoordinateDimension, [c_void_p, c_int], errcheck=False)

get_geom_count = int_output(lgdal.OGR_G_GetGeometryCount, [c_void_p])
get_geom_name = const_string_output(lgdal.OGR_G_GetGeometryName, [c_void_p], decoding='ascii')
get_geom_type = int_output(lgdal.OGR_G_GetGeometryType, [c_void_p])
get_point_count = int_output(lgdal.OGR_G_GetPointCount, [c_void_p])
get_point = void_output(lgdal.OGR_G_GetPoint, [c_void_p, c_int, POINTER(c_double), POINTER(c_double), POINTER(c_double)], errcheck=False)
geom_close_rings = void_output(lgdal.OGR_G_CloseRings, [c_void_p], errcheck=False)

# Topology routines.
ogr_contains = topology_func(lgdal.OGR_G_Contains)
ogr_crosses = topology_func(lgdal.OGR_G_Crosses)
ogr_disjoint = topology_func(lgdal.OGR_G_Disjoint)
ogr_equals = topology_func(lgdal.OGR_G_Equals)
ogr_intersects = topology_func(lgdal.OGR_G_Intersects)
ogr_overlaps = topology_func(lgdal.OGR_G_Overlaps)
ogr_touches = topology_func(lgdal.OGR_G_Touches)
ogr_within = topology_func(lgdal.OGR_G_Within)

# Transformation routines.
geom_transform = void_output(lgdal.OGR_G_Transform, [c_void_p, c_void_p])
geom_transform_to = void_output(lgdal.OGR_G_TransformTo, [c_void_p, c_void_p])

# For retrieving the envelope of the geometry.
get_envelope = env_func(lgdal.OGR_G_GetEnvelope, [c_void_p, POINTER(OGREnvelope)])


from __future__ import unicode_literals

import binascii
import unittest

from django.contrib.gis import memoryview
from django.contrib.gis.geos import GEOSGeometry, WKTReader, WKTWriter, WKBReader, WKBWriter, geos_version_info
from django.utils import six


class GEOSIOTest(unittest.TestCase):

    def test01_wktreader(self):
        # Creating a WKTReader instance
        wkt_r = WKTReader()
        wkt = 'POINT (5 23)'

        # read() should return a GEOSGeometry
        ref = GEOSGeometry(wkt)
        g1 = wkt_r.read(wkt.encode())
        g2 = wkt_r.read(wkt)

        for geom in (g1, g2):
            self.assertEqual(ref, geom)

        # Should only accept six.string_types objects.
        self.assertRaises(TypeError, wkt_r.read, 1)
        self.assertRaises(TypeError, wkt_r.read, memoryview(b'foo'))

    def test02_wktwriter(self):
        # Creating a WKTWriter instance, testing its ptr property.
        wkt_w = WKTWriter()
        self.assertRaises(TypeError, wkt_w._set_ptr, WKTReader.ptr_type())

        ref = GEOSGeometry('POINT (5 23)')
        ref_wkt = 'POINT (5.0000000000000000 23.0000000000000000)'
        self.assertEqual(ref_wkt, wkt_w.write(ref).decode())

    def test03_wkbreader(self):
        # Creating a WKBReader instance
        wkb_r = WKBReader()

        hex = b'000000000140140000000000004037000000000000'
        wkb = memoryview(binascii.a2b_hex(hex))
        ref = GEOSGeometry(hex)

        # read() should return a GEOSGeometry on either a hex string or
        # a WKB buffer.
        g1 = wkb_r.read(wkb)
        g2 = wkb_r.read(hex)
        for geom in (g1, g2):
            self.assertEqual(ref, geom)

        bad_input = (1, 5.23, None, False)
        for bad_wkb in bad_input:
            self.assertRaises(TypeError, wkb_r.read, bad_wkb)

    def test04_wkbwriter(self):
        wkb_w = WKBWriter()

        # Representations of 'POINT (5 23)' in hex -- one normal and
        # the other with the byte order changed.
        g = GEOSGeometry('POINT (5 23)')
        hex1 = b'010100000000000000000014400000000000003740'
        wkb1 = memoryview(binascii.a2b_hex(hex1))
        hex2 = b'000000000140140000000000004037000000000000'
        wkb2 = memoryview(binascii.a2b_hex(hex2))

        self.assertEqual(hex1, wkb_w.write_hex(g))
        self.assertEqual(wkb1, wkb_w.write(g))

        # Ensuring bad byteorders are not accepted.
        for bad_byteorder in (-1, 2, 523, 'foo', None):
            # Equivalent of `wkb_w.byteorder = bad_byteorder`
            self.assertRaises(ValueError, wkb_w._set_byteorder, bad_byteorder)

        # Setting the byteorder to 0 (for Big Endian)
        wkb_w.byteorder = 0
        self.assertEqual(hex2, wkb_w.write_hex(g))
        self.assertEqual(wkb2, wkb_w.write(g))

        # Back to Little Endian
        wkb_w.byteorder = 1

        # Now, trying out the 3D and SRID flags.
        g = GEOSGeometry('POINT (5 23 17)')
        g.srid = 4326

        hex3d = b'0101000080000000000000144000000000000037400000000000003140'
        wkb3d = memoryview(binascii.a2b_hex(hex3d))
        hex3d_srid = b'01010000A0E6100000000000000000144000000000000037400000000000003140'
        wkb3d_srid = memoryview(binascii.a2b_hex(hex3d_srid))

        # Ensuring bad output dimensions are not accepted
        for bad_outdim in (-1, 0, 1, 4, 423, 'foo', None):
            # Equivalent of `wkb_w.outdim = bad_outdim`
            self.assertRaises(ValueError, wkb_w._set_outdim, bad_outdim)

        # These tests will fail on 3.0.0 because of a bug that was fixed in 3.1:
        # http://trac.osgeo.org/geos/ticket/216
        if not geos_version_info()['version'].startswith('3.0.'):
            # Now setting the output dimensions to be 3
            wkb_w.outdim = 3

            self.assertEqual(hex3d, wkb_w.write_hex(g))
            self.assertEqual(wkb3d, wkb_w.write(g))

            # Telling the WKBWriter to include the srid in the representation.
            wkb_w.srid = True
            self.assertEqual(hex3d_srid, wkb_w.write_hex(g))
            self.assertEqual(wkb3d_srid, wkb_w.write(g))

def suite():
    s = unittest.TestSuite()
    s.addTest(unittest.makeSuite(GEOSIOTest))
    return s

def run(verbosity=2):
    unittest.TextTestRunner(verbosity=verbosity).run(suite())

from django.conf import settings
from django.db import DEFAULT_DB_ALIAS

# function that will pass a test.
def pass_test(*args): return

def no_backend(test_func, backend):
    "Use this decorator to disable test on specified backend."
    if settings.DATABASES[DEFAULT_DB_ALIAS]['ENGINE'].rsplit('.')[-1] == backend:
        return pass_test
    else:
        return test_func

# Decorators to disable entire test functions for specific
# spatial backends.
def no_oracle(func): return no_backend(func, 'oracle')
def no_postgis(func): return no_backend(func, 'postgis')
def no_mysql(func): return no_backend(func, 'mysql')
def no_spatialite(func): return no_backend(func, 'spatialite')

# Shortcut booleans to omit only portions of tests.
_default_db = settings.DATABASES[DEFAULT_DB_ALIAS]['ENGINE'].rsplit('.')[-1]
oracle  = _default_db == 'oracle'
postgis = _default_db == 'postgis'
mysql   = _default_db == 'mysql'
spatialite = _default_db == 'spatialite'

HAS_SPATIALREFSYS = True
if oracle and 'gis' in settings.DATABASES[DEFAULT_DB_ALIAS]['ENGINE']:
    from django.contrib.gis.db.backends.oracle.models import SpatialRefSys
elif postgis:
    from django.contrib.gis.db.backends.postgis.models import SpatialRefSys
elif spatialite:
    from django.contrib.gis.db.backends.spatialite.models import SpatialRefSys
else:
    HAS_SPATIALREFSYS = False
    SpatialRefSys = None

# -*- coding: utf-8 -*-
from django.utils.translation import ugettext_lazy as _

PROVINCE_CHOICES = (
    ('01', _('Araba')),
    ('02', _('Albacete')),
    ('03', _('Alacant')),
    ('04', _('Almeria')),
    ('05', _('Avila')),
    ('06', _('Badajoz')),
    ('07', _('Illes Balears')),
    ('08', _('Barcelona')),
    ('09', _('Burgos')),
    ('10', _('Caceres')),
    ('11', _('Cadiz')),
    ('12', _('Castello')),
    ('13', _('Ciudad Real')),
    ('14', _('Cordoba')),
    ('15', _('A Coruna')),
    ('16', _('Cuenca')),
    ('17', _('Girona')),
    ('18', _('Granada')),
    ('19', _('Guadalajara')),
    ('20', _('Guipuzkoa')),
    ('21', _('Huelva')),
    ('22', _('Huesca')),
    ('23', _('Jaen')),
    ('24', _('Leon')),
    ('25', _('Lleida')),
    ('26', _('La Rioja')),
    ('27', _('Lugo')),
    ('28', _('Madrid')),
    ('29', _('Malaga')),
    ('30', _('Murcia')),
    ('31', _('Navarre')),
    ('32', _('Ourense')),
    ('33', _('Asturias')),
    ('34', _('Palencia')),
    ('35', _('Las Palmas')),
    ('36', _('Pontevedra')),
    ('37', _('Salamanca')),
    ('38', _('Santa Cruz de Tenerife')),
    ('39', _('Cantabria')),
    ('40', _('Segovia')),
    ('41', _('Seville')),
    ('42', _('Soria')),
    ('43', _('Tarragona')),
    ('44', _('Teruel')),
    ('45', _('Toledo')),
    ('46', _('Valencia')),
    ('47', _('Valladolid')),
    ('48', _('Bizkaia')),
    ('49', _('Zamora')),
    ('50', _('Zaragoza')),
    ('51', _('Ceuta')),
    ('52', _('Melilla')),
)


from django.contrib.messages import constants
from django.contrib.messages.tests.base import BaseTest
from django.contrib.messages.storage.base import Message
from django.contrib.messages.storage.session import SessionStorage
from django.utils.safestring import SafeData, mark_safe


def set_session_data(storage, messages):
    """
    Sets the messages into the backend request's session and remove the
    backend's loaded data cache.
    """
    storage.request.session[storage.session_key] = messages
    if hasattr(storage, '_loaded_data'):
        del storage._loaded_data


def stored_session_messages_count(storage):
    data = storage.request.session.get(storage.session_key, [])
    return len(data)


class SessionTest(BaseTest):
    storage_class = SessionStorage

    def get_request(self):
        self.session = {}
        request = super(SessionTest, self).get_request()
        request.session = self.session
        return request

    def stored_messages_count(self, storage, response):
        return stored_session_messages_count(storage)

    def test_get(self):
        storage = self.storage_class(self.get_request())
        # Set initial data.
        example_messages = ['test', 'me']
        set_session_data(storage, example_messages)
        # Test that the message actually contains what we expect.
        self.assertEqual(list(storage), example_messages)

    def test_safedata(self):
        """
        Tests that a message containing SafeData is keeping its safe status when
        retrieved from the message storage.
        """
        storage = self.get_storage()

        message = Message(constants.DEBUG, mark_safe("<b>Hello Django!</b>"))
        set_session_data(storage, [message])
        self.assertIsInstance(list(storage)[0].message, SafeData)

from optparse import make_option

from django.conf import settings
from django.core.management.commands.runserver import Command as RunserverCommand

from django.contrib.staticfiles.handlers import StaticFilesHandler

class Command(RunserverCommand):
    option_list = RunserverCommand.option_list + (
        make_option('--nostatic', action="store_false", dest='use_static_handler', default=True,
            help='Tells Django to NOT automatically serve static files at STATIC_URL.'),
        make_option('--insecure', action="store_true", dest='insecure_serving', default=False,
            help='Allows serving static files even if DEBUG is False.'),
    )
    help = "Starts a lightweight Web server for development and also serves static files."

    def get_handler(self, *args, **options):
        """
        Returns the static files serving handler wrapping the default handler,
        if static files should be served. Otherwise just returns the default
        handler.

        """
        handler = super(Command, self).get_handler(*args, **options)
        use_static_handler = options.get('use_static_handler', True)
        insecure_serving = options.get('insecure_serving', False)
        if use_static_handler and (settings.DEBUG or insecure_serving):
            return StaticFilesHandler(handler)
        return handler

"""SMTP email backend class."""
import smtplib
import ssl
import threading

from django.conf import settings
from django.core.mail.backends.base import BaseEmailBackend
from django.core.mail.utils import DNS_NAME
from django.core.mail.message import sanitize_address
from django.utils.encoding import force_bytes


class EmailBackend(BaseEmailBackend):
    """
    A wrapper that manages the SMTP network connection.
    """
    def __init__(self, host=None, port=None, username=None, password=None,
                 use_tls=None, fail_silently=False, **kwargs):
        super(EmailBackend, self).__init__(fail_silently=fail_silently)
        self.host = host or settings.EMAIL_HOST
        self.port = port or settings.EMAIL_PORT
        if username is None:
            self.username = settings.EMAIL_HOST_USER
        else:
            self.username = username
        if password is None:
            self.password = settings.EMAIL_HOST_PASSWORD
        else:
            self.password = password
        if use_tls is None:
            self.use_tls = settings.EMAIL_USE_TLS
        else:
            self.use_tls = use_tls
        self.connection = None
        self._lock = threading.RLock()

    def open(self):
        """
        Ensures we have a connection to the email server. Returns whether or
        not a new connection was required (True or False).
        """
        if self.connection:
            # Nothing to do if the connection is already open.
            return False
        try:
            # If local_hostname is not specified, socket.getfqdn() gets used.
            # For performance, we use the cached FQDN for local_hostname.
            self.connection = smtplib.SMTP(self.host, self.port,
                                           local_hostname=DNS_NAME.get_fqdn())
            if self.use_tls:
                self.connection.ehlo()
                self.connection.starttls()
                self.connection.ehlo()
            if self.username and self.password:
                self.connection.login(self.username, self.password)
            return True
        except:
            if not self.fail_silently:
                raise

    def close(self):
        """Closes the connection to the email server."""
        if self.connection is None:
            return
        try:
            try:
                self.connection.quit()
            except (ssl.SSLError, smtplib.SMTPServerDisconnected):
                # This happens when calling quit() on a TLS connection
                # sometimes, or when the connection was already disconnected
                # by the server.
                self.connection.close()
            except:
                if self.fail_silently:
                    return
                raise
        finally:
            self.connection = None

    def send_messages(self, email_messages):
        """
        Sends one or more EmailMessage objects and returns the number of email
        messages sent.
        """
        if not email_messages:
            return
        with self._lock:
            new_conn_created = self.open()
            if not self.connection:
                # We failed silently on open().
                # Trying to send would be pointless.
                return
            num_sent = 0
            for message in email_messages:
                sent = self._send(message)
                if sent:
                    num_sent += 1
            if new_conn_created:
                self.close()
        return num_sent

    def _send(self, email_message):
        """A helper method that does the actual sending."""
        if not email_message.recipients():
            return False
        from_email = sanitize_address(email_message.from_email, email_message.encoding)
        recipients = [sanitize_address(addr, email_message.encoding)
                      for addr in email_message.recipients()]
        message = email_message.message()
        charset = message.get_charset().get_output_charset() if message.get_charset() else 'utf-8'
        try:
            self.connection.sendmail(from_email, recipients,
                    force_bytes(message.as_string(), charset))
        except:
            if not self.fail_silently:
                raise
            return False
        return True

"""
Module for abstract serializer/unserializer base classes.
"""

from django.db import models
from django.utils.encoding import smart_text
from django.utils import six

class SerializerDoesNotExist(KeyError):
    """The requested serializer was not found."""
    pass

class SerializationError(Exception):
    """Something bad happened during serialization."""
    pass

class DeserializationError(Exception):
    """Something bad happened during deserialization."""
    pass

class Serializer(object):
    """
    Abstract serializer base class.
    """

    # Indicates if the implemented serializer is only available for
    # internal Django use.
    internal_use_only = False

    def serialize(self, queryset, **options):
        """
        Serialize a queryset.
        """
        self.options = options

        self.stream = options.pop("stream", six.StringIO())
        self.selected_fields = options.pop("fields", None)
        self.use_natural_keys = options.pop("use_natural_keys", False)

        self.start_serialization()
        self.first = True
        for obj in queryset:
            self.start_object(obj)
            # Use the concrete parent class' _meta instead of the object's _meta
            # This is to avoid local_fields problems for proxy models. Refs #17717.
            concrete_model = obj._meta.concrete_model
            for field in concrete_model._meta.local_fields:
                if field.serialize:
                    if field.rel is None:
                        if self.selected_fields is None or field.attname in self.selected_fields:
                            self.handle_field(obj, field)
                    else:
                        if self.selected_fields is None or field.attname[:-3] in self.selected_fields:
                            self.handle_fk_field(obj, field)
            for field in concrete_model._meta.many_to_many:
                if field.serialize:
                    if self.selected_fields is None or field.attname in self.selected_fields:
                        self.handle_m2m_field(obj, field)
            self.end_object(obj)
            if self.first:
                self.first = False
        self.end_serialization()
        return self.getvalue()

    def start_serialization(self):
        """
        Called when serializing of the queryset starts.
        """
        raise NotImplementedError

    def end_serialization(self):
        """
        Called when serializing of the queryset ends.
        """
        pass

    def start_object(self, obj):
        """
        Called when serializing of an object starts.
        """
        raise NotImplementedError

    def end_object(self, obj):
        """
        Called when serializing of an object ends.
        """
        pass

    def handle_field(self, obj, field):
        """
        Called to handle each individual (non-relational) field on an object.
        """
        raise NotImplementedError

    def handle_fk_field(self, obj, field):
        """
        Called to handle a ForeignKey field.
        """
        raise NotImplementedError

    def handle_m2m_field(self, obj, field):
        """
        Called to handle a ManyToManyField.
        """
        raise NotImplementedError

    def getvalue(self):
        """
        Return the fully serialized queryset (or None if the output stream is
        not seekable).
        """
        if callable(getattr(self.stream, 'getvalue', None)):
            return self.stream.getvalue()

class Deserializer(six.Iterator):
    """
    Abstract base deserializer class.
    """

    def __init__(self, stream_or_string, **options):
        """
        Init this serializer given a stream or a string
        """
        self.options = options
        if isinstance(stream_or_string, six.string_types):
            self.stream = six.StringIO(stream_or_string)
        else:
            self.stream = stream_or_string
        # hack to make sure that the models have all been loaded before
        # deserialization starts (otherwise subclass calls to get_model()
        # and friends might fail...)
        models.get_apps()

    def __iter__(self):
        return self

    def __next__(self):
        """Iteration iterface -- return the next item in the stream"""
        raise NotImplementedError

class DeserializedObject(object):
    """
    A deserialized model.

    Basically a container for holding the pre-saved deserialized data along
    with the many-to-many data saved with the object.

    Call ``save()`` to save the object (with the many-to-many data) to the
    database; call ``save(save_m2m=False)`` to save just the object fields
    (and not touch the many-to-many stuff.)
    """

    def __init__(self, obj, m2m_data=None):
        self.object = obj
        self.m2m_data = m2m_data

    def __repr__(self):
        return "<DeserializedObject: %s.%s(pk=%s)>" % (
            self.object._meta.app_label, self.object._meta.object_name, self.object.pk)

    def save(self, save_m2m=True, using=None):
        # Call save on the Model baseclass directly. This bypasses any
        # model-defined save. The save is also forced to be raw.
        # This ensures that the data that is deserialized is literally
        # what came from the file, not post-processed by pre_save/save
        # methods.
        models.Model.save_base(self.object, using=using, raw=True)
        if self.m2m_data and save_m2m:
            for accessor_name, object_list in self.m2m_data.items():
                setattr(self.object, accessor_name, object_list)

        # prevent a second (possibly accidental) call to save() from saving
        # the m2m data twice.
        self.m2m_data = None

import re
from django.db.backends import BaseDatabaseIntrospection

field_size_re = re.compile(r'^\s*(?:var)?char\s*\(\s*(\d+)\s*\)\s*$')

def get_field_size(name):
    """ Extract the size number from a "varchar(11)" type name """
    m = field_size_re.search(name)
    return int(m.group(1)) if m else None


# This light wrapper "fakes" a dictionary interface, because some SQLite data
# types include variables in them -- e.g. "varchar(30)" -- and can't be matched
# as a simple dictionary lookup.
class FlexibleFieldLookupDict(object):
    # Maps SQL types to Django Field types. Some of the SQL types have multiple
    # entries here because SQLite allows for anything and doesn't normalize the
    # field type; it uses whatever was given.
    base_data_types_reverse = {
        'bool': 'BooleanField',
        'boolean': 'BooleanField',
        'smallint': 'SmallIntegerField',
        'smallint unsigned': 'PositiveSmallIntegerField',
        'smallinteger': 'SmallIntegerField',
        'int': 'IntegerField',
        'integer': 'IntegerField',
        'bigint': 'BigIntegerField',
        'integer unsigned': 'PositiveIntegerField',
        'decimal': 'DecimalField',
        'real': 'FloatField',
        'text': 'TextField',
        'char': 'CharField',
        'date': 'DateField',
        'datetime': 'DateTimeField',
        'time': 'TimeField',
    }

    def __getitem__(self, key):
        key = key.lower()
        try:
            return self.base_data_types_reverse[key]
        except KeyError:
            size = get_field_size(key)
            if size is not None:
                return ('CharField', {'max_length': size})
            raise KeyError

class DatabaseIntrospection(BaseDatabaseIntrospection):
    data_types_reverse = FlexibleFieldLookupDict()

    def get_table_list(self, cursor):
        "Returns a list of table names in the current database."
        # Skip the sqlite_sequence system table used for autoincrement key
        # generation.
        cursor.execute("""
            SELECT name FROM sqlite_master
            WHERE type='table' AND NOT name='sqlite_sequence'
            ORDER BY name""")
        return [row[0] for row in cursor.fetchall()]

    def get_table_description(self, cursor, table_name):
        "Returns a description of the table, with the DB-API cursor.description interface."
        return [(info['name'], info['type'], None, info['size'], None, None,
                 info['null_ok']) for info in self._table_info(cursor, table_name)]

    def get_relations(self, cursor, table_name):
        """
        Returns a dictionary of {field_index: (field_index_other_table, other_table)}
        representing all relationships to the given table. Indexes are 0-based.
        """

        # Dictionary of relations to return
        relations = {}

        # Schema for this table
        cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s", [table_name, "table"])
        results = cursor.fetchone()[0].strip()
        results = results[results.index('(')+1:results.rindex(')')]

        # Walk through and look for references to other tables. SQLite doesn't
        # really have enforced references, but since it echoes out the SQL used
        # to create the table we can look for REFERENCES statements used there.
        for field_index, field_desc in enumerate(results.split(',')):
            field_desc = field_desc.strip()
            if field_desc.startswith("UNIQUE"):
                continue

            m = re.search('references (.*) \(["|](.*)["|]\)', field_desc, re.I)
            if not m:
                continue

            table, column = [s.strip('"') for s in m.groups()]

            cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s", [table])
            result = cursor.fetchall()[0]
            other_table_results = result[0].strip()
            li, ri = other_table_results.index('('), other_table_results.rindex(')')
            other_table_results = other_table_results[li+1:ri]


            for other_index, other_desc in enumerate(other_table_results.split(',')):
                other_desc = other_desc.strip()
                if other_desc.startswith('UNIQUE'):
                    continue

                name = other_desc.split(' ', 1)[0].strip('"')
                if name == column:
                    relations[field_index] = (other_index, table)
                    break

        return relations

    def get_key_columns(self, cursor, table_name):
        """
        Returns a list of (column_name, referenced_table_name, referenced_column_name) for all
        key columns in given table.
        """
        key_columns = []

        # Schema for this table
        cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s", [table_name, "table"])
        results = cursor.fetchone()[0].strip()
        results = results[results.index('(')+1:results.rindex(')')]

        # Walk through and look for references to other tables. SQLite doesn't
        # really have enforced references, but since it echoes out the SQL used
        # to create the table we can look for REFERENCES statements used there.
        for field_index, field_desc in enumerate(results.split(',')):
            field_desc = field_desc.strip()
            if field_desc.startswith("UNIQUE"):
                continue

            m = re.search('"(.*)".*references (.*) \(["|](.*)["|]\)', field_desc, re.I)
            if not m:
                continue

            # This will append (column_name, referenced_table_name, referenced_column_name) to key_columns
            key_columns.append(tuple([s.strip('"') for s in m.groups()]))

        return key_columns

    def get_indexes(self, cursor, table_name):
        indexes = {}
        for info in self._table_info(cursor, table_name):
            if info['pk'] != 0:
                indexes[info['name']] = {'primary_key': True,
                                         'unique': False}
        cursor.execute('PRAGMA index_list(%s)' % self.connection.ops.quote_name(table_name))
        # seq, name, unique
        for index, unique in [(field[1], field[2]) for field in cursor.fetchall()]:
            cursor.execute('PRAGMA index_info(%s)' % self.connection.ops.quote_name(index))
            info = cursor.fetchall()
            # Skip indexes across multiple fields
            if len(info) != 1:
                continue
            name = info[0][2] # seqno, cid, name
            indexes[name] = {'primary_key': False,
                             'unique': unique}
        return indexes

    def get_primary_key_column(self, cursor, table_name):
        """
        Get the column name of the primary key for the given table.
        """
        # Don't use PRAGMA because that causes issues with some transactions
        cursor.execute("SELECT sql FROM sqlite_master WHERE tbl_name = %s AND type = %s", [table_name, "table"])
        results = cursor.fetchone()[0].strip()
        results = results[results.index('(')+1:results.rindex(')')]
        for field_desc in results.split(','):
            field_desc = field_desc.strip()
            m = re.search('"(.*)".*PRIMARY KEY$', field_desc)
            if m:
                return m.groups()[0]
        return None

    def _table_info(self, cursor, name):
        cursor.execute('PRAGMA table_info(%s)' % self.connection.ops.quote_name(name))
        # cid, name, type, notnull, dflt_value, pk
        return [{'name': field[1],
                 'type': field[2],
                 'size': get_field_size(field[2]),
                 'null_ok': not field[3],
                 'pk': field[5]     # undocumented
                 } for field in cursor.fetchall()]

"""
"Safe weakrefs", originally from pyDispatcher.

Provides a way to safely weakref any function, including bound methods (which
aren't handled by the core weakref module).
"""

import traceback
import weakref

def safeRef(target, onDelete = None):
    """Return a *safe* weak reference to a callable target

    target -- the object to be weakly referenced, if it's a
        bound method reference, will create a BoundMethodWeakref,
        otherwise creates a simple weakref.
    onDelete -- if provided, will have a hard reference stored
        to the callable to be called after the safe reference
        goes out of scope with the reference object, (either a
        weakref or a BoundMethodWeakref) as argument.
    """
    if hasattr(target, '__self__'):
        if target.__self__ is not None:
            # Turn a bound method into a BoundMethodWeakref instance.
            # Keep track of these instances for lookup by disconnect().
            assert hasattr(target, '__func__'), """safeRef target %r has __self__, but no __func__, don't know how to create reference"""%( target,)
            reference = get_bound_method_weakref(
                target=target,
                onDelete=onDelete
            )
            return reference
    if callable(onDelete):
        return weakref.ref(target, onDelete)
    else:
        return weakref.ref( target )

class BoundMethodWeakref(object):
    """'Safe' and reusable weak references to instance methods

    BoundMethodWeakref objects provide a mechanism for
    referencing a bound method without requiring that the
    method object itself (which is normally a transient
    object) is kept alive.  Instead, the BoundMethodWeakref
    object keeps weak references to both the object and the
    function which together define the instance method.

    Attributes:
        key -- the identity key for the reference, calculated
            by the class's calculateKey method applied to the
            target instance method
        deletionMethods -- sequence of callable objects taking
            single argument, a reference to this object which
            will be called when *either* the target object or
            target function is garbage collected (i.e. when
            this object becomes invalid).  These are specified
            as the onDelete parameters of safeRef calls.
        weakSelf -- weak reference to the target object
        weakFunc -- weak reference to the target function

    Class Attributes:
        _allInstances -- class attribute pointing to all live
            BoundMethodWeakref objects indexed by the class's
            calculateKey(target) method applied to the target
            objects.  This weak value dictionary is used to
            short-circuit creation so that multiple references
            to the same (object, function) pair produce the
            same BoundMethodWeakref instance.

    """

    _allInstances = weakref.WeakValueDictionary()

    def __new__( cls, target, onDelete=None, *arguments,**named ):
        """Create new instance or return current instance

        Basically this method of construction allows us to
        short-circuit creation of references to already-
        referenced instance methods.  The key corresponding
        to the target is calculated, and if there is already
        an existing reference, that is returned, with its
        deletionMethods attribute updated.  Otherwise the
        new instance is created and registered in the table
        of already-referenced methods.
        """
        key = cls.calculateKey(target)
        current =cls._allInstances.get(key)
        if current is not None:
            current.deletionMethods.append( onDelete)
            return current
        else:
            base = super( BoundMethodWeakref, cls).__new__( cls )
            cls._allInstances[key] = base
            base.__init__( target, onDelete, *arguments,**named)
            return base

    def __init__(self, target, onDelete=None):
        """Return a weak-reference-like instance for a bound method

        target -- the instance-method target for the weak
            reference, must have __self__ and __func__ attributes
            and be reconstructable via:
                target.__func__.__get__( target.__self__ )
            which is true of built-in instance methods.
        onDelete -- optional callback which will be called
            when this weak reference ceases to be valid
            (i.e. either the object or the function is garbage
            collected).  Should take a single argument,
            which will be passed a pointer to this object.
        """
        def remove(weak, self=self):
            """Set self.isDead to true when method or instance is destroyed"""
            methods = self.deletionMethods[:]
            del self.deletionMethods[:]
            try:
                del self.__class__._allInstances[ self.key ]
            except KeyError:
                pass
            for function in methods:
                try:
                    if callable( function ):
                        function( self )
                except Exception as e:
                    try:
                        traceback.print_exc()
                    except AttributeError:
                        print('Exception during saferef %s cleanup function %s: %s' % (
                            self, function, e)
                        )
        self.deletionMethods = [onDelete]
        self.key = self.calculateKey( target )
        self.weakSelf = weakref.ref(target.__self__, remove)
        self.weakFunc = weakref.ref(target.__func__, remove)
        self.selfName = str(target.__self__)
        self.funcName = str(target.__func__.__name__)

    def calculateKey( cls, target ):
        """Calculate the reference key for this reference

        Currently this is a two-tuple of the id()'s of the
        target object and the target function respectively.
        """
        return (id(target.__self__),id(target.__func__))
    calculateKey = classmethod( calculateKey )

    def __str__(self):
        """Give a friendly representation of the object"""
        return """%s( %s.%s )"""%(
            self.__class__.__name__,
            self.selfName,
            self.funcName,
        )

    __repr__ = __str__

    def __hash__(self):
        return hash(self.key)

    def __bool__( self ):
        """Whether we are still a valid reference"""
        return self() is not None

    def __nonzero__(self):      # Python 2 compatibility
        return type(self).__bool__(self)

    def __eq__(self, other):
        """Compare with another reference"""
        if not isinstance(other, self.__class__):
            return self.__class__ == type(other)
        return self.key == other.key

    def __call__(self):
        """Return a strong reference to the bound method

        If the target cannot be retrieved, then will
        return None, otherwise returns a bound instance
        method for our object and function.

        Note:
            You may call this method any number of times,
            as it does not invalidate the reference.
        """
        target = self.weakSelf()
        if target is not None:
            function = self.weakFunc()
            if function is not None:
                return function.__get__(target)
        return None

class BoundNonDescriptorMethodWeakref(BoundMethodWeakref):
    """A specialized BoundMethodWeakref, for platforms where instance methods
    are not descriptors.

    It assumes that the function name and the target attribute name are the
    same, instead of assuming that the function is a descriptor. This approach
    is equally fast, but not 100% reliable because functions can be stored on an
    attribute named differenty than the function's name such as in:

    class A: pass
    def foo(self): return "foo"
    A.bar = foo

    But this shouldn't be a common use case. So, on platforms where methods
    aren't descriptors (such as Jython) this implementation has the advantage
    of working in the most cases.
    """
    def __init__(self, target, onDelete=None):
        """Return a weak-reference-like instance for a bound method

        target -- the instance-method target for the weak
            reference, must have __self__ and __func__ attributes
            and be reconstructable via:
                target.__func__.__get__( target.__self__ )
            which is true of built-in instance methods.
        onDelete -- optional callback which will be called
            when this weak reference ceases to be valid
            (i.e. either the object or the function is garbage
            collected).  Should take a single argument,
            which will be passed a pointer to this object.
        """
        assert getattr(target.__self__, target.__name__) == target, \
               ("method %s isn't available as the attribute %s of %s" %
                (target, target.__name__, target.__self__))
        super(BoundNonDescriptorMethodWeakref, self).__init__(target, onDelete)

    def __call__(self):
        """Return a strong reference to the bound method

        If the target cannot be retrieved, then will
        return None, otherwise returns a bound instance
        method for our object and function.

        Note:
            You may call this method any number of times,
            as it does not invalidate the reference.
        """
        target = self.weakSelf()
        if target is not None:
            function = self.weakFunc()
            if function is not None:
                # Using partial() would be another option, but it erases the
                # "signature" of the function. That is, after a function is
                # curried, the inspect module can't be used to determine how
                # many arguments the function expects, nor what keyword
                # arguments it supports, and pydispatcher needs this
                # information.
                return getattr(target, function.__name__)
        return None

def get_bound_method_weakref(target, onDelete):
    """Instantiates the appropiate BoundMethodWeakRef, depending on the details of
    the underlying class method implementation"""
    if hasattr(target, '__get__'):
        # target method is a descriptor, so the default implementation works:
        return BoundMethodWeakref(target=target, onDelete=onDelete)
    else:
        # no luck, use the alternative implementation:
        return BoundNonDescriptorMethodWeakref(target=target, onDelete=onDelete)

"""Default tags used by the template system, available to all templates."""
from __future__ import unicode_literals

import sys
import re
from datetime import datetime
from itertools import groupby, cycle as itertools_cycle

from django.conf import settings
from django.template.base import (Node, NodeList, Template, Context, Library,
    TemplateSyntaxError, VariableDoesNotExist, InvalidTemplateLibrary,
    BLOCK_TAG_START, BLOCK_TAG_END, VARIABLE_TAG_START, VARIABLE_TAG_END,
    SINGLE_BRACE_START, SINGLE_BRACE_END, COMMENT_TAG_START, COMMENT_TAG_END,
    VARIABLE_ATTRIBUTE_SEPARATOR, get_library, token_kwargs, kwarg_re)
from django.template.smartif import IfParser, Literal
from django.template.defaultfilters import date
from django.utils.encoding import smart_text
from django.utils.safestring import mark_safe
from django.utils.html import format_html
from django.utils import six
from django.utils import timezone

register = Library()

class AutoEscapeControlNode(Node):
    """Implements the actions of the autoescape tag."""
    def __init__(self, setting, nodelist):
        self.setting, self.nodelist = setting, nodelist

    def render(self, context):
        old_setting = context.autoescape
        context.autoescape = self.setting
        output = self.nodelist.render(context)
        context.autoescape = old_setting
        if self.setting:
            return mark_safe(output)
        else:
            return output

class CommentNode(Node):
    def render(self, context):
        return ''

class CsrfTokenNode(Node):
    def render(self, context):
        csrf_token = context.get('csrf_token', None)
        if csrf_token:
            if csrf_token == 'NOTPROVIDED':
                return format_html("")
            else:
                return format_html("<input type='hidden' name='csrfmiddlewaretoken' value='{0}' />", csrf_token)
        else:
            # It's very probable that the token is missing because of
            # misconfiguration, so we raise a warning
            from django.conf import settings
            if settings.DEBUG:
                import warnings
                warnings.warn("A {% csrf_token %} was used in a template, but the context did not provide the value.  This is usually caused by not using RequestContext.")
            return ''

class CycleNode(Node):
    def __init__(self, cyclevars, variable_name=None, silent=False):
        self.cyclevars = cyclevars
        self.variable_name = variable_name
        self.silent = silent

    def render(self, context):
        if self not in context.render_context:
            # First time the node is rendered in template
            context.render_context[self] = itertools_cycle(self.cyclevars)
        cycle_iter = context.render_context[self]
        value = next(cycle_iter).resolve(context)
        if self.variable_name:
            context[self.variable_name] = value
        if self.silent:
            return ''
        return value

class DebugNode(Node):
    def render(self, context):
        from pprint import pformat
        output = [pformat(val) for val in context]
        output.append('\n\n')
        output.append(pformat(sys.modules))
        return ''.join(output)

class FilterNode(Node):
    def __init__(self, filter_expr, nodelist):
        self.filter_expr, self.nodelist = filter_expr, nodelist

    def render(self, context):
        output = self.nodelist.render(context)
        # Apply filters.
        context.update({'var': output})
        filtered = self.filter_expr.resolve(context)
        context.pop()
        return filtered

class FirstOfNode(Node):
    def __init__(self, vars):
        self.vars = vars

    def render(self, context):
        for var in self.vars:
            value = var.resolve(context, True)
            if value:
                return smart_text(value)
        return ''

class ForNode(Node):
    child_nodelists = ('nodelist_loop', 'nodelist_empty')

    def __init__(self, loopvars, sequence, is_reversed, nodelist_loop, nodelist_empty=None):
        self.loopvars, self.sequence = loopvars, sequence
        self.is_reversed = is_reversed
        self.nodelist_loop = nodelist_loop
        if nodelist_empty is None:
            self.nodelist_empty = NodeList()
        else:
            self.nodelist_empty = nodelist_empty

    def __repr__(self):
        reversed_text = self.is_reversed and ' reversed' or ''
        return "<For Node: for %s in %s, tail_len: %d%s>" % \
            (', '.join(self.loopvars), self.sequence, len(self.nodelist_loop),
             reversed_text)

    def __iter__(self):
        for node in self.nodelist_loop:
            yield node
        for node in self.nodelist_empty:
            yield node

    def render(self, context):
        if 'forloop' in context:
            parentloop = context['forloop']
        else:
            parentloop = {}
        context.push()
        try:
            values = self.sequence.resolve(context, True)
        except VariableDoesNotExist:
            values = []
        if values is None:
            values = []
        if not hasattr(values, '__len__'):
            values = list(values)
        len_values = len(values)
        if len_values < 1:
            context.pop()
            return self.nodelist_empty.render(context)
        nodelist = NodeList()
        if self.is_reversed:
            values = reversed(values)
        unpack = len(self.loopvars) > 1
        # Create a forloop value in the context.  We'll update counters on each
        # iteration just below.
        loop_dict = context['forloop'] = {'parentloop': parentloop}
        for i, item in enumerate(values):
            # Shortcuts for current loop iteration number.
            loop_dict['counter0'] = i
            loop_dict['counter'] = i+1
            # Reverse counter iteration numbers.
            loop_dict['revcounter'] = len_values - i
            loop_dict['revcounter0'] = len_values - i - 1
            # Boolean values designating first and last times through loop.
            loop_dict['first'] = (i == 0)
            loop_dict['last'] = (i == len_values - 1)

            pop_context = False
            if unpack:
                # If there are multiple loop variables, unpack the item into
                # them.
                try:
                    unpacked_vars = dict(zip(self.loopvars, item))
                except TypeError:
                    pass
                else:
                    pop_context = True
                    context.update(unpacked_vars)
            else:
                context[self.loopvars[0]] = item
            # In TEMPLATE_DEBUG mode provide source of the node which
            # actually raised the exception
            if settings.TEMPLATE_DEBUG:
                for node in self.nodelist_loop:
                    try:
                        nodelist.append(node.render(context))
                    except Exception as e:
                        if not hasattr(e, 'django_template_source'):
                            e.django_template_source = node.source
                        raise
            else:
                for node in self.nodelist_loop:
                    nodelist.append(node.render(context))
            if pop_context:
                # The loop variables were pushed on to the context so pop them
                # off again. This is necessary because the tag lets the length
                # of loopvars differ to the length of each set of items and we
                # don't want to leave any vars from the previous loop on the
                # context.
                context.pop()
        context.pop()
        return nodelist.render(context)

class IfChangedNode(Node):
    child_nodelists = ('nodelist_true', 'nodelist_false')

    def __init__(self, nodelist_true, nodelist_false, *varlist):
        self.nodelist_true, self.nodelist_false = nodelist_true, nodelist_false
        self._last_seen = None
        self._varlist = varlist
        self._id = str(id(self))

    def render(self, context):
        if 'forloop' in context and self._id not in context['forloop']:
            self._last_seen = None
            context['forloop'][self._id] = 1
        try:
            if self._varlist:
                # Consider multiple parameters.  This automatically behaves
                # like an OR evaluation of the multiple variables.
                compare_to = [var.resolve(context, True) for var in self._varlist]
            else:
                compare_to = self.nodelist_true.render(context)
        except VariableDoesNotExist:
            compare_to = None

        if compare_to != self._last_seen:
            self._last_seen = compare_to
            content = self.nodelist_true.render(context)
            return content
        elif self.nodelist_false:
            return self.nodelist_false.render(context)
        return ''

class IfEqualNode(Node):
    child_nodelists = ('nodelist_true', 'nodelist_false')

    def __init__(self, var1, var2, nodelist_true, nodelist_false, negate):
        self.var1, self.var2 = var1, var2
        self.nodelist_true, self.nodelist_false = nodelist_true, nodelist_false
        self.negate = negate

    def __repr__(self):
        return "<IfEqualNode>"

    def render(self, context):
        val1 = self.var1.resolve(context, True)
        val2 = self.var2.resolve(context, True)
        if (self.negate and val1 != val2) or (not self.negate and val1 == val2):
            return self.nodelist_true.render(context)
        return self.nodelist_false.render(context)

class IfNode(Node):

    def __init__(self, conditions_nodelists):
        self.conditions_nodelists = conditions_nodelists

    def __repr__(self):
        return "<IfNode>"

    def __iter__(self):
        for _, nodelist in self.conditions_nodelists:
            for node in nodelist:
                yield node

    @property
    def nodelist(self):
        return NodeList(node for _, nodelist in self.conditions_nodelists for node in nodelist)

    def render(self, context):
        for condition, nodelist in self.conditions_nodelists:

            if condition is not None:           # if / elif clause
                try:
                    match = condition.eval(context)
                except VariableDoesNotExist:
                    match = None
            else:                               # else clause
                match = True

            if match:
                return nodelist.render(context)

        return ''

class RegroupNode(Node):
    def __init__(self, target, expression, var_name):
        self.target, self.expression = target, expression
        self.var_name = var_name

    def resolve_expression(self, obj, context):
        # This method is called for each object in self.target. See regroup()
        # for the reason why we temporarily put the object in the context.
        context[self.var_name] = obj
        return self.expression.resolve(context, True)

    def render(self, context):
        obj_list = self.target.resolve(context, True)
        if obj_list == None:
            # target variable wasn't found in context; fail silently.
            context[self.var_name] = []
            return ''
        # List of dictionaries in the format:
        # {'grouper': 'key', 'list': [list of contents]}.
        context[self.var_name] = [
            {'grouper': key, 'list': list(val)}
            for key, val in
            groupby(obj_list, lambda obj: self.resolve_expression(obj, context))
        ]
        return ''

def include_is_allowed(filepath):
    for root in settings.ALLOWED_INCLUDE_ROOTS:
        if filepath.startswith(root):
            return True
    return False

class SsiNode(Node):
    def __init__(self, filepath, parsed):
        self.filepath = filepath
        self.parsed = parsed

    def render(self, context):
        filepath = self.filepath.resolve(context)

        if not include_is_allowed(filepath):
            if settings.DEBUG:
                return "[Didn't have permission to include file]"
            else:
                return '' # Fail silently for invalid includes.
        try:
            with open(filepath, 'r') as fp:
                output = fp.read()
        except IOError:
            output = ''
        if self.parsed:
            try:
                t = Template(output, name=filepath)
                return t.render(context)
            except TemplateSyntaxError as e:
                if settings.DEBUG:
                    return "[Included template had syntax error: %s]" % e
                else:
                    return '' # Fail silently for invalid included templates.
        return output

class LoadNode(Node):
    def render(self, context):
        return ''

class NowNode(Node):
    def __init__(self, format_string):
        self.format_string = format_string

    def render(self, context):
        tzinfo = timezone.get_current_timezone() if settings.USE_TZ else None
        return date(datetime.now(tz=tzinfo), self.format_string)

class SpacelessNode(Node):
    def __init__(self, nodelist):
        self.nodelist = nodelist

    def render(self, context):
        from django.utils.html import strip_spaces_between_tags
        return strip_spaces_between_tags(self.nodelist.render(context).strip())

class TemplateTagNode(Node):
    mapping = {'openblock': BLOCK_TAG_START,
               'closeblock': BLOCK_TAG_END,
               'openvariable': VARIABLE_TAG_START,
               'closevariable': VARIABLE_TAG_END,
               'openbrace': SINGLE_BRACE_START,
               'closebrace': SINGLE_BRACE_END,
               'opencomment': COMMENT_TAG_START,
               'closecomment': COMMENT_TAG_END,
               }

    def __init__(self, tagtype):
        self.tagtype = tagtype

    def render(self, context):
        return self.mapping.get(self.tagtype, '')

class URLNode(Node):
    def __init__(self, view_name, args, kwargs, asvar):
        self.view_name = view_name
        self.args = args
        self.kwargs = kwargs
        self.asvar = asvar

    def render(self, context):
        from django.core.urlresolvers import reverse, NoReverseMatch
        args = [arg.resolve(context) for arg in self.args]
        kwargs = dict([(smart_text(k, 'ascii'), v.resolve(context))
                       for k, v in self.kwargs.items()])

        view_name = self.view_name.resolve(context)

        if not view_name:
            raise NoReverseMatch("'url' requires a non-empty first argument. "
                "The syntax changed in Django 1.5, see the docs.")

        # Try to look up the URL twice: once given the view name, and again
        # relative to what we guess is the "main" app. If they both fail,
        # re-raise the NoReverseMatch unless we're using the
        # {% url ... as var %} construct in which case return nothing.
        url = ''
        try:
            url = reverse(view_name, args=args, kwargs=kwargs, current_app=context.current_app)
        except NoReverseMatch as e:
            if settings.SETTINGS_MODULE:
                project_name = settings.SETTINGS_MODULE.split('.')[0]
                try:
                    url = reverse(project_name + '.' + view_name,
                              args=args, kwargs=kwargs,
                              current_app=context.current_app)
                except NoReverseMatch:
                    if self.asvar is None:
                        # Re-raise the original exception, not the one with
                        # the path relative to the project. This makes a
                        # better error message.
                        raise e
            else:
                if self.asvar is None:
                    raise e

        if self.asvar:
            context[self.asvar] = url
            return ''
        else:
            return url

class VerbatimNode(Node):
    def __init__(self, content):
        self.content = content

    def render(self, context):
        return self.content

class WidthRatioNode(Node):
    def __init__(self, val_expr, max_expr, max_width):
        self.val_expr = val_expr
        self.max_expr = max_expr
        self.max_width = max_width

    def render(self, context):
        try:
            value = self.val_expr.resolve(context)
            max_value = self.max_expr.resolve(context)
            max_width = int(self.max_width.resolve(context))
        except VariableDoesNotExist:
            return ''
        except (ValueError, TypeError):
            raise TemplateSyntaxError("widthratio final argument must be an number")
        try:
            value = float(value)
            max_value = float(max_value)
            ratio = (value / max_value) * max_width
        except ZeroDivisionError:
            return '0'
        except (ValueError, TypeError):
            return ''
        return str(int(round(ratio)))

class WithNode(Node):
    def __init__(self, var, name, nodelist, extra_context=None):
        self.nodelist = nodelist
        # var and name are legacy attributes, being left in case they are used
        # by third-party subclasses of this Node.
        self.extra_context = extra_context or {}
        if name:
            self.extra_context[name] = var

    def __repr__(self):
        return "<WithNode>"

    def render(self, context):
        values = dict([(key, val.resolve(context)) for key, val in
                       six.iteritems(self.extra_context)])
        context.update(values)
        output = self.nodelist.render(context)
        context.pop()
        return output

@register.tag
def autoescape(parser, token):
    """
    Force autoescape behavior for this block.
    """
    args = token.contents.split()
    if len(args) != 2:
        raise TemplateSyntaxError("'autoescape' tag requires exactly one argument.")
    arg = args[1]
    if arg not in ('on', 'off'):
        raise TemplateSyntaxError("'autoescape' argument should be 'on' or 'off'")
    nodelist = parser.parse(('endautoescape',))
    parser.delete_first_token()
    return AutoEscapeControlNode((arg == 'on'), nodelist)

@register.tag
def comment(parser, token):
    """
    Ignores everything between ``{% comment %}`` and ``{% endcomment %}``.
    """
    parser.skip_past('endcomment')
    return CommentNode()

@register.tag
def cycle(parser, token):
    """
    Cycles among the given strings each time this tag is encountered.

    Within a loop, cycles among the given strings each time through
    the loop::

        {% for o in some_list %}
            <tr class="{% cycle 'row1' 'row2' %}">
                ...
            </tr>
        {% endfor %}

    Outside of a loop, give the values a unique name the first time you call
    it, then use that name each sucessive time through::

            <tr class="{% cycle 'row1' 'row2' 'row3' as rowcolors %}">...</tr>
            <tr class="{% cycle rowcolors %}">...</tr>
            <tr class="{% cycle rowcolors %}">...</tr>

    You can use any number of values, separated by spaces. Commas can also
    be used to separate values; if a comma is used, the cycle values are
    interpreted as literal strings.

    The optional flag "silent" can be used to prevent the cycle declaration
    from returning any value::

        {% for o in some_list %}
            {% cycle 'row1' 'row2' as rowcolors silent %}
            <tr class="{{ rowcolors }}">{% include "subtemplate.html " %}</tr>
        {% endfor %}

    """

    # Note: This returns the exact same node on each {% cycle name %} call;
    # that is, the node object returned from {% cycle a b c as name %} and the
    # one returned from {% cycle name %} are the exact same object. This
    # shouldn't cause problems (heh), but if it does, now you know.
    #
    # Ugly hack warning: This stuffs the named template dict into parser so
    # that names are only unique within each template (as opposed to using
    # a global variable, which would make cycle names have to be unique across
    # *all* templates.

    args = token.split_contents()

    if len(args) < 2:
        raise TemplateSyntaxError("'cycle' tag requires at least two arguments")

    if ',' in args[1]:
        # Backwards compatibility: {% cycle a,b %} or {% cycle a,b as foo %}
        # case.
        args[1:2] = ['"%s"' % arg for arg in args[1].split(",")]

    if len(args) == 2:
        # {% cycle foo %} case.
        name = args[1]
        if not hasattr(parser, '_namedCycleNodes'):
            raise TemplateSyntaxError("No named cycles in template. '%s' is not defined" % name)
        if not name in parser._namedCycleNodes:
            raise TemplateSyntaxError("Named cycle '%s' does not exist" % name)
        return parser._namedCycleNodes[name]

    as_form = False

    if len(args) > 4:
        # {% cycle ... as foo [silent] %} case.
        if args[-3] == "as":
            if args[-1] != "silent":
                raise TemplateSyntaxError("Only 'silent' flag is allowed after cycle's name, not '%s'." % args[-1])
            as_form = True
            silent = True
            args = args[:-1]
        elif args[-2] == "as":
            as_form = True
            silent = False

    if as_form:
        name = args[-1]
        values = [parser.compile_filter(arg) for arg in args[1:-2]]
        node = CycleNode(values, name, silent=silent)
        if not hasattr(parser, '_namedCycleNodes'):
            parser._namedCycleNodes = {}
        parser._namedCycleNodes[name] = node
    else:
        values = [parser.compile_filter(arg) for arg in args[1:]]
        node = CycleNode(values)
    return node

@register.tag
def csrf_token(parser, token):
    return CsrfTokenNode()

@register.tag
def debug(parser, token):
    """
    Outputs a whole load of debugging information, including the current
    context and imported modules.

    Sample usage::

        <pre>
            {% debug %}
        </pre>
    """
    return DebugNode()

@register.tag('filter')
def do_filter(parser, token):
    """
    Filters the contents of the block through variable filters.

    Filters can also be piped through each other, and they can have
    arguments -- just like in variable syntax.

    Sample usage::

        {% filter force_escape|lower %}
            This text will be HTML-escaped, and will appear in lowercase.
        {% endfilter %}

    Note that the ``escape`` and ``safe`` filters are not acceptable arguments.
    Instead, use the ``autoescape`` tag to manage autoescaping for blocks of
    template code.
    """
    _, rest = token.contents.split(None, 1)
    filter_expr = parser.compile_filter("var|%s" % (rest))
    for func, unused in filter_expr.filters:
        if getattr(func, '_decorated_function', func).__name__ in ('escape', 'safe'):
            raise TemplateSyntaxError('"filter %s" is not permitted.  Use the "autoescape" tag instead.' % func.__name__)
    nodelist = parser.parse(('endfilter',))
    parser.delete_first_token()
    return FilterNode(filter_expr, nodelist)

@register.tag
def firstof(parser, token):
    """
    Outputs the first variable passed that is not False, without escaping.

    Outputs nothing if all the passed variables are False.

    Sample usage::

        {% firstof var1 var2 var3 %}

    This is equivalent to::

        {% if var1 %}
            {{ var1|safe }}
        {% else %}{% if var2 %}
            {{ var2|safe }}
        {% else %}{% if var3 %}
            {{ var3|safe }}
        {% endif %}{% endif %}{% endif %}

    but obviously much cleaner!

    You can also use a literal string as a fallback value in case all
    passed variables are False::

        {% firstof var1 var2 var3 "fallback value" %}

    If you want to escape the output, use a filter tag::

        {% filter force_escape %}
            {% firstof var1 var2 var3 "fallback value" %}
        {% endfilter %}

    """
    bits = token.split_contents()[1:]
    if len(bits) < 1:
        raise TemplateSyntaxError("'firstof' statement requires at least one argument")
    return FirstOfNode([parser.compile_filter(bit) for bit in bits])

@register.tag('for')
def do_for(parser, token):
    """
    Loops over each item in an array.

    For example, to display a list of athletes given ``athlete_list``::

        <ul>
        {% for athlete in athlete_list %}
            <li>{{ athlete.name }}</li>
        {% endfor %}
        </ul>

    You can loop over a list in reverse by using
    ``{% for obj in list reversed %}``.

    You can also unpack multiple values from a two-dimensional array::

        {% for key,value in dict.items %}
            {{ key }}: {{ value }}
        {% endfor %}

    The ``for`` tag can take an optional ``{% empty %}`` clause that will
    be displayed if the given array is empty or could not be found::

        <ul>
          {% for athlete in athlete_list %}
            <li>{{ athlete.name }}</li>
          {% empty %}
            <li>Sorry, no athletes in this list.</li>
          {% endfor %}
        <ul>

    The above is equivalent to -- but shorter, cleaner, and possibly faster
    than -- the following::

        <ul>
          {% if althete_list %}
            {% for athlete in athlete_list %}
              <li>{{ athlete.name }}</li>
            {% endfor %}
          {% else %}
            <li>Sorry, no athletes in this list.</li>
          {% endif %}
        </ul>

    The for loop sets a number of variables available within the loop:

        ==========================  ================================================
        Variable                    Description
        ==========================  ================================================
        ``forloop.counter``         The current iteration of the loop (1-indexed)
        ``forloop.counter0``        The current iteration of the loop (0-indexed)
        ``forloop.revcounter``      The number of iterations from the end of the
                                    loop (1-indexed)
        ``forloop.revcounter0``     The number of iterations from the end of the
                                    loop (0-indexed)
        ``forloop.first``           True if this is the first time through the loop
        ``forloop.last``            True if this is the last time through the loop
        ``forloop.parentloop``      For nested loops, this is the loop "above" the
                                    current one
        ==========================  ================================================

    """
    bits = token.contents.split()
    if len(bits) < 4:
        raise TemplateSyntaxError("'for' statements should have at least four"
                                  " words: %s" % token.contents)

    is_reversed = bits[-1] == 'reversed'
    in_index = is_reversed and -3 or -2
    if bits[in_index] != 'in':
        raise TemplateSyntaxError("'for' statements should use the format"
                                  " 'for x in y': %s" % token.contents)

    loopvars = re.split(r' *, *', ' '.join(bits[1:in_index]))
    for var in loopvars:
        if not var or ' ' in var:
            raise TemplateSyntaxError("'for' tag received an invalid argument:"
                                      " %s" % token.contents)

    sequence = parser.compile_filter(bits[in_index+1])
    nodelist_loop = parser.parse(('empty', 'endfor',))
    token = parser.next_token()
    if token.contents == 'empty':
        nodelist_empty = parser.parse(('endfor',))
        parser.delete_first_token()
    else:
        nodelist_empty = None
    return ForNode(loopvars, sequence, is_reversed, nodelist_loop, nodelist_empty)

def do_ifequal(parser, token, negate):
    bits = list(token.split_contents())
    if len(bits) != 3:
        raise TemplateSyntaxError("%r takes two arguments" % bits[0])
    end_tag = 'end' + bits[0]
    nodelist_true = parser.parse(('else', end_tag))
    token = parser.next_token()
    if token.contents == 'else':
        nodelist_false = parser.parse((end_tag,))
        parser.delete_first_token()
    else:
        nodelist_false = NodeList()
    val1 = parser.compile_filter(bits[1])
    val2 = parser.compile_filter(bits[2])
    return IfEqualNode(val1, val2, nodelist_true, nodelist_false, negate)

@register.tag
def ifequal(parser, token):
    """
    Outputs the contents of the block if the two arguments equal each other.

    Examples::

        {% ifequal user.id comment.user_id %}
            ...
        {% endifequal %}

        {% ifnotequal user.id comment.user_id %}
            ...
        {% else %}
            ...
        {% endifnotequal %}
    """
    return do_ifequal(parser, token, False)

@register.tag
def ifnotequal(parser, token):
    """
    Outputs the contents of the block if the two arguments are not equal.
    See ifequal.
    """
    return do_ifequal(parser, token, True)

class TemplateLiteral(Literal):
    def __init__(self, value, text):
        self.value = value
        self.text = text # for better error messages

    def display(self):
        return self.text

    def eval(self, context):
        return self.value.resolve(context, ignore_failures=True)

class TemplateIfParser(IfParser):
    error_class = TemplateSyntaxError

    def __init__(self, parser, *args, **kwargs):
        self.template_parser = parser
        super(TemplateIfParser, self).__init__(*args, **kwargs)

    def create_var(self, value):
        return TemplateLiteral(self.template_parser.compile_filter(value), value)

@register.tag('if')
def do_if(parser, token):
    """
    The ``{% if %}`` tag evaluates a variable, and if that variable is "true"
    (i.e., exists, is not empty, and is not a false boolean value), the
    contents of the block are output:

    ::

        {% if athlete_list %}
            Number of athletes: {{ athlete_list|count }}
        {% elif athlete_in_locker_room_list %}
            Athletes should be out of the locker room soon!
        {% else %}
            No athletes.
        {% endif %}

    In the above, if ``athlete_list`` is not empty, the number of athletes will
    be displayed by the ``{{ athlete_list|count }}`` variable.

    As you can see, the ``if`` tag may take one or several `` {% elif %}``
    clauses, as well as an ``{% else %}`` clause that will be displayed if all
    previous conditions fail. These clauses are optional.

    ``if`` tags may use ``or``, ``and`` or ``not`` to test a number of
    variables or to negate a given variable::

        {% if not athlete_list %}
            There are no athletes.
        {% endif %}

        {% if athlete_list or coach_list %}
            There are some athletes or some coaches.
        {% endif %}

        {% if athlete_list and coach_list %}
            Both atheletes and coaches are available.
        {% endif %}

        {% if not athlete_list or coach_list %}
            There are no athletes, or there are some coaches.
        {% endif %}

        {% if athlete_list and not coach_list %}
            There are some athletes and absolutely no coaches.
        {% endif %}

    Comparison operators are also available, and the use of filters is also
    allowed, for example::

        {% if articles|length >= 5 %}...{% endif %}

    Arguments and operators _must_ have a space between them, so
    ``{% if 1>2 %}`` is not a valid if tag.

    All supported operators are: ``or``, ``and``, ``in``, ``not in``
    ``==`` (or ``=``), ``!=``, ``>``, ``>=``, ``<`` and ``<=``.

    Operator precedence follows Python.
    """
    # {% if ... %}
    bits = token.split_contents()[1:]
    condition = TemplateIfParser(parser, bits).parse()
    nodelist = parser.parse(('elif', 'else', 'endif'))
    conditions_nodelists = [(condition, nodelist)]
    token = parser.next_token()

    # {% elif ... %} (repeatable)
    while token.contents.startswith('elif'):
        bits = token.split_contents()[1:]
        condition = TemplateIfParser(parser, bits).parse()
        nodelist = parser.parse(('elif', 'else', 'endif'))
        conditions_nodelists.append((condition, nodelist))
        token = parser.next_token()

    # {% else %} (optional)
    if token.contents == 'else':
        nodelist = parser.parse(('endif',))
        conditions_nodelists.append((None, nodelist))
        token = parser.next_token()

    # {% endif %}
    assert token.contents == 'endif'

    return IfNode(conditions_nodelists)


@register.tag
def ifchanged(parser, token):
    """
    Checks if a value has changed from the last iteration of a loop.

    The ``{% ifchanged %}`` block tag is used within a loop. It has two
    possible uses.

    1. Checks its own rendered contents against its previous state and only
       displays the content if it has changed. For example, this displays a
       list of days, only displaying the month if it changes::

            <h1>Archive for {{ year }}</h1>

            {% for date in days %}
                {% ifchanged %}<h3>{{ date|date:"F" }}</h3>{% endifchanged %}
                <a href="{{ date|date:"M/d"|lower }}/">{{ date|date:"j" }}</a>
            {% endfor %}

    2. If given one or more variables, check whether any variable has changed.
       For example, the following shows the date every time it changes, while
       showing the hour if either the hour or the date has changed::

            {% for date in days %}
                {% ifchanged date.date %} {{ date.date }} {% endifchanged %}
                {% ifchanged date.hour date.date %}
                    {{ date.hour }}
                {% endifchanged %}
            {% endfor %}
    """
    bits = token.contents.split()
    nodelist_true = parser.parse(('else', 'endifchanged'))
    token = parser.next_token()
    if token.contents == 'else':
        nodelist_false = parser.parse(('endifchanged',))
        parser.delete_first_token()
    else:
        nodelist_false = NodeList()
    values = [parser.compile_filter(bit) for bit in bits[1:]]
    return IfChangedNode(nodelist_true, nodelist_false, *values)

@register.tag
def ssi(parser, token):
    """
    Outputs the contents of a given file into the page.

    Like a simple "include" tag, the ``ssi`` tag includes the contents
    of another file -- which must be specified using an absolute path --
    in the current page::

        {% ssi "/home/html/ljworld.com/includes/right_generic.html" %}

    If the optional "parsed" parameter is given, the contents of the included
    file are evaluated as template code, with the current context::

        {% ssi "/home/html/ljworld.com/includes/right_generic.html" parsed %}
    """
    bits = token.split_contents()
    parsed = False
    if len(bits) not in (2, 3):
        raise TemplateSyntaxError("'ssi' tag takes one argument: the path to"
                                  " the file to be included")
    if len(bits) == 3:
        if bits[2] == 'parsed':
            parsed = True
        else:
            raise TemplateSyntaxError("Second (optional) argument to %s tag"
                                      " must be 'parsed'" % bits[0])
    filepath = parser.compile_filter(bits[1])
    return SsiNode(filepath, parsed)

@register.tag
def load(parser, token):
    """
    Loads a custom template tag set.

    For example, to load the template tags in
    ``django/templatetags/news/photos.py``::

        {% load news.photos %}

    Can also be used to load an individual tag/filter from
    a library::

        {% load byline from news %}

    """
    bits = token.contents.split()
    if len(bits) >= 4 and bits[-2] == "from":
        try:
            taglib = bits[-1]
            lib = get_library(taglib)
        except InvalidTemplateLibrary as e:
            raise TemplateSyntaxError("'%s' is not a valid tag library: %s" %
                                      (taglib, e))
        else:
            temp_lib = Library()
            for name in bits[1:-2]:
                if name in lib.tags:
                    temp_lib.tags[name] = lib.tags[name]
                    # a name could be a tag *and* a filter, so check for both
                    if name in lib.filters:
                        temp_lib.filters[name] = lib.filters[name]
                elif name in lib.filters:
                    temp_lib.filters[name] = lib.filters[name]
                else:
                    raise TemplateSyntaxError("'%s' is not a valid tag or filter in tag library '%s'" %
                                              (name, taglib))
            parser.add_library(temp_lib)
    else:
        for taglib in bits[1:]:
            # add the library to the parser
            try:
                lib = get_library(taglib)
                parser.add_library(lib)
            except InvalidTemplateLibrary as e:
                raise TemplateSyntaxError("'%s' is not a valid tag library: %s" %
                                          (taglib, e))
    return LoadNode()

@register.tag
def now(parser, token):
    """
    Displays the date, formatted according to the given string.

    Uses the same format as PHP's ``date()`` function; see http://php.net/date
    for all the possible values.

    Sample usage::

        It is {% now "jS F Y H:i" %}
    """
    bits = token.split_contents()
    if len(bits) != 2:
        raise TemplateSyntaxError("'now' statement takes one argument")
    format_string = bits[1][1:-1]
    return NowNode(format_string)

@register.tag
def regroup(parser, token):
    """
    Regroups a list of alike objects by a common attribute.

    This complex tag is best illustrated by use of an example:  say that
    ``people`` is a list of ``Person`` objects that have ``first_name``,
    ``last_name``, and ``gender`` attributes, and you'd like to display a list
    that looks like:

        * Male:
            * George Bush
            * Bill Clinton
        * Female:
            * Margaret Thatcher
            * Colendeeza Rice
        * Unknown:
            * Pat Smith

    The following snippet of template code would accomplish this dubious task::

        {% regroup people by gender as grouped %}
        <ul>
        {% for group in grouped %}
            <li>{{ group.grouper }}
            <ul>
                {% for item in group.list %}
                <li>{{ item }}</li>
                {% endfor %}
            </ul>
        {% endfor %}
        </ul>

    As you can see, ``{% regroup %}`` populates a variable with a list of
    objects with ``grouper`` and ``list`` attributes.  ``grouper`` contains the
    item that was grouped by; ``list`` contains the list of objects that share
    that ``grouper``.  In this case, ``grouper`` would be ``Male``, ``Female``
    and ``Unknown``, and ``list`` is the list of people with those genders.

    Note that ``{% regroup %}`` does not work when the list to be grouped is not
    sorted by the key you are grouping by!  This means that if your list of
    people was not sorted by gender, you'd need to make sure it is sorted
    before using it, i.e.::

        {% regroup people|dictsort:"gender" by gender as grouped %}

    """
    firstbits = token.contents.split(None, 3)
    if len(firstbits) != 4:
        raise TemplateSyntaxError("'regroup' tag takes five arguments")
    target = parser.compile_filter(firstbits[1])
    if firstbits[2] != 'by':
        raise TemplateSyntaxError("second argument to 'regroup' tag must be 'by'")
    lastbits_reversed = firstbits[3][::-1].split(None, 2)
    if lastbits_reversed[1][::-1] != 'as':
        raise TemplateSyntaxError("next-to-last argument to 'regroup' tag must"
                                  " be 'as'")
    var_name = lastbits_reversed[0][::-1]
    # RegroupNode will take each item in 'target', put it in the context under
    # 'var_name', evaluate 'var_name'.'expression' in the current context, and
    # group by the resulting value. After all items are processed, it will
    # save the final result in the context under 'var_name', thus clearing the
    # temporary values. This hack is necessary because the template engine
    # doesn't provide a context-aware equivalent of Python's getattr.
    expression = parser.compile_filter(var_name +
                                       VARIABLE_ATTRIBUTE_SEPARATOR +
                                       lastbits_reversed[2][::-1])
    return RegroupNode(target, expression, var_name)

@register.tag
def spaceless(parser, token):
    """
    Removes whitespace between HTML tags, including tab and newline characters.

    Example usage::

        {% spaceless %}
            <p>
                <a href="foo/">Foo</a>
            </p>
        {% endspaceless %}

    This example would return this HTML::

        <p><a href="foo/">Foo</a></p>

    Only space between *tags* is normalized -- not space between tags and text.
    In this example, the space around ``Hello`` won't be stripped::

        {% spaceless %}
            <strong>
                Hello
            </strong>
        {% endspaceless %}
    """
    nodelist = parser.parse(('endspaceless',))
    parser.delete_first_token()
    return SpacelessNode(nodelist)

@register.tag
def templatetag(parser, token):
    """
    Outputs one of the bits used to compose template tags.

    Since the template system has no concept of "escaping", to display one of
    the bits used in template tags, you must use the ``{% templatetag %}`` tag.

    The argument tells which template bit to output:

        ==================  =======
        Argument            Outputs
        ==================  =======
        ``openblock``       ``{%``
        ``closeblock``      ``%}``
        ``openvariable``    ``{{``
        ``closevariable``   ``}}``
        ``openbrace``       ``{``
        ``closebrace``      ``}``
        ``opencomment``     ``{#``
        ``closecomment``    ``#}``
        ==================  =======
    """
    bits = token.contents.split()
    if len(bits) != 2:
        raise TemplateSyntaxError("'templatetag' statement takes one argument")
    tag = bits[1]
    if tag not in TemplateTagNode.mapping:
        raise TemplateSyntaxError("Invalid templatetag argument: '%s'."
                                  " Must be one of: %s" %
                                  (tag, list(TemplateTagNode.mapping)))
    return TemplateTagNode(tag)

@register.tag
def url(parser, token):
    """
    Returns an absolute URL matching given view with its parameters.

    This is a way to define links that aren't tied to a particular URL
    configuration::

        {% url "path.to.some_view" arg1 arg2 %}

        or

        {% url "path.to.some_view" name1=value1 name2=value2 %}

    The first argument is a path to a view. It can be an absolute Python path
    or just ``app_name.view_name`` without the project name if the view is
    located inside the project.

    Other arguments are space-separated values that will be filled in place of
    positional and keyword arguments in the URL. Don't mix positional and
    keyword arguments.

    All arguments for the URL should be present.

    For example if you have a view ``app_name.client`` taking client's id and
    the corresponding line in a URLconf looks like this::

        ('^client/(\d+)/$', 'app_name.client')

    and this app's URLconf is included into the project's URLconf under some
    path::

        ('^clients/', include('project_name.app_name.urls'))

    then in a template you can create a link for a certain client like this::

        {% url "app_name.client" client.id %}

    The URL will look like ``/clients/client/123/``.

    The first argument can also be a named URL instead of the Python path to
    the view callable. For example if the URLconf entry looks like this::

        url('^client/(\d+)/$', name='client-detail-view')

    then in the template you can use::

        {% url "client-detail-view" client.id %}

    There is even another possible value type for the first argument. It can be
    the name of a template variable that will be evaluated to obtain the view
    name or the URL name, e.g.::

        {% with view_path="app_name.client" %}
        {% url view_path client.id %}
        {% endwith %}

        or,

        {% with url_name="client-detail-view" %}
        {% url url_name client.id %}
        {% endwith %}

    """
    bits = token.split_contents()
    if len(bits) < 2:
        raise TemplateSyntaxError("'%s' takes at least one argument"
                                  " (path to a view)" % bits[0])
    try:
        viewname = parser.compile_filter(bits[1])
    except TemplateSyntaxError as exc:
        exc.args = (exc.args[0] + ". "
                "The syntax of 'url' changed in Django 1.5, see the docs."),
        raise
    args = []
    kwargs = {}
    asvar = None
    bits = bits[2:]
    if len(bits) >= 2 and bits[-2] == 'as':
        asvar = bits[-1]
        bits = bits[:-2]

    if len(bits):
        for bit in bits:
            match = kwarg_re.match(bit)
            if not match:
                raise TemplateSyntaxError("Malformed arguments to url tag")
            name, value = match.groups()
            if name:
                kwargs[name] = parser.compile_filter(value)
            else:
                args.append(parser.compile_filter(value))

    return URLNode(viewname, args, kwargs, asvar)

@register.tag
def verbatim(parser, token):
    """
    Stops the template engine from rendering the contents of this block tag.

    Usage::

        {% verbatim %}
            {% don't process this %}
        {% endverbatim %}

    You can also designate a specific closing tag block (allowing the
    unrendered use of ``{% endverbatim %}``)::

        {% verbatim myblock %}
            ...
        {% endverbatim myblock %}
    """
    nodelist = parser.parse(('endverbatim',))
    parser.delete_first_token()
    return VerbatimNode(nodelist.render(Context()))

@register.tag
def widthratio(parser, token):
    """
    For creating bar charts and such, this tag calculates the ratio of a given
    value to a maximum value, and then applies that ratio to a constant.

    For example::

        <img src='bar.gif' height='10' width='{% widthratio this_value max_value max_width %}' />

    If ``this_value`` is 175, ``max_value`` is 200, and ``max_width`` is 100,
    the image in the above example will be 88 pixels wide
    (because 175/200 = .875; .875 * 100 = 87.5 which is rounded up to 88).
    """
    bits = token.contents.split()
    if len(bits) != 4:
        raise TemplateSyntaxError("widthratio takes three arguments")
    tag, this_value_expr, max_value_expr, max_width = bits

    return WidthRatioNode(parser.compile_filter(this_value_expr),
                          parser.compile_filter(max_value_expr),
                          parser.compile_filter(max_width))

@register.tag('with')
def do_with(parser, token):
    """
    Adds one or more values to the context (inside of this block) for caching
    and easy access.

    For example::

        {% with total=person.some_sql_method %}
            {{ total }} object{{ total|pluralize }}
        {% endwith %}

    Multiple values can be added to the context::

        {% with foo=1 bar=2 %}
            ...
        {% endwith %}

    The legacy format of ``{% with person.some_sql_method as total %}`` is
    still accepted.
    """
    bits = token.split_contents()
    remaining_bits = bits[1:]
    extra_context = token_kwargs(remaining_bits, parser, support_legacy=True)
    if not extra_context:
        raise TemplateSyntaxError("%r expected at least one variable "
                                  "assignment" % bits[0])
    if remaining_bits:
        raise TemplateSyntaxError("%r received an invalid token: %r" %
                                  (bits[0], remaining_bits[0]))
    nodelist = parser.parse(('endwith',))
    parser.delete_first_token()
    return WithNode(None, None, nodelist, extra_context=extra_context)

"""
Fixes Python 2.4's failure to deepcopy unbound functions.
"""

import copy
import types
import warnings

warnings.warn("django.utils.copycompat is deprecated; use the native copy module instead",
              DeprecationWarning)

# Monkeypatch copy's deepcopy registry to handle functions correctly.
if (hasattr(copy, '_deepcopy_dispatch') and types.FunctionType not in copy._deepcopy_dispatch):
    copy._deepcopy_dispatch[types.FunctionType] = copy._deepcopy_atomic

# Pose as the copy module now.
del copy, types
from copy import *

from __future__ import unicode_literals

import datetime

from django.utils.timezone import is_aware, utc
from django.utils.translation import ungettext, ugettext

def timesince(d, now=None, reversed=False):
    """
    Takes two datetime objects and returns the time between d and now
    as a nicely formatted string, e.g. "10 minutes".  If d occurs after now,
    then "0 minutes" is returned.

    Units used are years, months, weeks, days, hours, and minutes.
    Seconds and microseconds are ignored.  Up to two adjacent units will be
    displayed.  For example, "2 weeks, 3 days" and "1 year, 3 months" are
    possible outputs, but "2 weeks, 3 hours" and "1 year, 5 days" are not.

    Adapted from http://blog.natbat.co.uk/archive/2003/Jun/14/time_since
    """
    chunks = (
      (60 * 60 * 24 * 365, lambda n: ungettext('year', 'years', n)),
      (60 * 60 * 24 * 30, lambda n: ungettext('month', 'months', n)),
      (60 * 60 * 24 * 7, lambda n : ungettext('week', 'weeks', n)),
      (60 * 60 * 24, lambda n : ungettext('day', 'days', n)),
      (60 * 60, lambda n: ungettext('hour', 'hours', n)),
      (60, lambda n: ungettext('minute', 'minutes', n))
    )
    # Convert datetime.date to datetime.datetime for comparison.
    if not isinstance(d, datetime.datetime):
        d = datetime.datetime(d.year, d.month, d.day)
    if now and not isinstance(now, datetime.datetime):
        now = datetime.datetime(now.year, now.month, now.day)

    if not now:
        now = datetime.datetime.now(utc if is_aware(d) else None)

    delta = (d - now) if reversed else (now - d)
    # ignore microseconds
    since = delta.days * 24 * 60 * 60 + delta.seconds
    if since <= 0:
        # d is in the future compared to now, stop processing.
        return '0 ' + ugettext('minutes')
    for i, (seconds, name) in enumerate(chunks):
        count = since // seconds
        if count != 0:
            break
    s = ugettext('%(number)d %(type)s') % {'number': count, 'type': name(count)}
    if i + 1 < len(chunks):
        # Now get the second item
        seconds2, name2 = chunks[i + 1]
        count2 = (since - (seconds * count)) // seconds2
        if count2 != 0:
            s += ugettext(', %(number)d %(type)s') % {'number': count2, 'type': name2(count2)}
    return s

def timeuntil(d, now=None):
    """
    Like timesince, but returns a string measuring the time until
    the given time.
    """
    return timesince(d, now, reversed=True)

#!/usr/bin/env python
#
# This python file contains utility scripts to manage Django translations.
# It has to be run inside the django git root directory.
#
# The following commands are available:
#
# * update_catalogs: check for new strings in core and contrib catalogs, and
#                    output how much strings are new/changed.
#
# * lang_stats: output statistics for each catalog/language combination
#
# * fetch: fetch translations from transifex.com
#
# Each command support the --languages and --resources options to limit their
# operation to the specified language or resource. For example, to get stats
# for Spanish in contrib.admin, run:
#
#  $ python scripts/manage_translations.py lang_stats --language=es --resources=admin

import os
from optparse import OptionParser
from subprocess import call, Popen, PIPE

from django.core.management import call_command


HAVE_JS = ['admin']

def _get_locale_dirs(include_core=True):
    """
    Return a tuple (contrib name, absolute path) for all locale directories,
    optionally including the django core catalog.
    """
    contrib_dir = os.path.join(os.getcwd(), 'django', 'contrib')
    dirs = []
    for contrib_name in os.listdir(contrib_dir):
        path = os.path.join(contrib_dir, contrib_name, 'locale')
        if os.path.isdir(path):
            dirs.append((contrib_name, path))
            if contrib_name in HAVE_JS:
                dirs.append(("%s-js" % contrib_name, path))
    if include_core:
        dirs.insert(0, ('core', os.path.join(os.getcwd(), 'django', 'conf', 'locale')))
    return dirs

def _tx_resource_for_name(name):
    """ Return the Transifex resource name """
    if name == 'core':
        return "django.core"
    else:
        return "django.contrib-%s" % name

def _check_diff(cat_name, base_path):
    """
    Output the approximate number of changed/added strings in the en catalog.
    """
    po_path = '%(path)s/en/LC_MESSAGES/django%(ext)s.po' % {
        'path': base_path, 'ext': 'js' if cat_name.endswith('-js') else ''}
    p = Popen("git diff -U0 %s | egrep -v '^@@|^[-+]#|^..POT-Creation' | wc -l" % po_path,
              stdout=PIPE, stderr=PIPE, shell=True)
    output, errors = p.communicate()
    num_changes = int(output.strip()) - 4
    print("%d changed/added messages in '%s' catalog." % (num_changes, cat_name))


def update_catalogs(resources=None, languages=None):
    """
    Update the en/LC_MESSAGES/django.po (main and contrib) files with
    new/updated translatable strings.
    """
    contrib_dirs = _get_locale_dirs(include_core=False)

    os.chdir(os.path.join(os.getcwd(), 'django'))
    print("Updating main en catalog")
    call_command('makemessages', locale='en')
    _check_diff('core', os.path.join(os.getcwd(), 'conf', 'locale'))

    # Contrib catalogs
    for name, dir_ in contrib_dirs:
        if resources and not name in resources:
            continue
        os.chdir(os.path.join(dir_, '..'))
        print("Updating en catalog in %s" % dir_)
        if name.endswith('-js'):
            call_command('makemessages', locale='en', domain='djangojs')
        else:
            call_command('makemessages', locale='en')
        _check_diff(name, dir_)


def lang_stats(resources=None, languages=None):
    """
    Output language statistics of committed translation files for each
    Django catalog.
    If resources is provided, it should be a list of translation resource to
    limit the output (e.g. ['core', 'gis']).
    """
    locale_dirs = _get_locale_dirs()

    for name, dir_ in locale_dirs:
        if resources and not name in resources:
            continue
        print("\nShowing translations stats for '%s':" % name) 
        langs = sorted([d for d in os.listdir(dir_) if not d.startswith('_')])
        for lang in langs:
            if languages and not lang in languages:
                continue
            # TODO: merge first with the latest en catalog
            p = Popen("msgfmt -vc -o /dev/null %(path)s/%(lang)s/LC_MESSAGES/django%(ext)s.po" % {
                'path': dir_, 'lang': lang, 'ext': 'js' if name.endswith('-js') else ''},
                stdout=PIPE, stderr=PIPE, shell=True)
            output, errors = p.communicate()
            if p.returncode == 0:
                # msgfmt output stats on stderr
                print("%s: %s" % (lang, errors.strip()))


def fetch(resources=None, languages=None):
    """
    Fetch translations from Transifex, wrap long lines, generate mo files.
    """
    locale_dirs = _get_locale_dirs()

    for name, dir_ in locale_dirs:
        if resources and not name in resources:
            continue

        # Transifex pull
        if languages is None:
            call('tx pull -r %(res)s -a -f' % {'res': _tx_resource_for_name(name)}, shell=True)
            languages = sorted([d for d in os.listdir(dir_) if not d.startswith('_')])
        else:
            for lang in languages:
                call('tx pull -r %(res)s -f -l %(lang)s' % {
                    'res': _tx_resource_for_name(name), 'lang': lang}, shell=True)

        # msgcat to wrap lines and msgfmt for compilation of .mo file
        for lang in languages:
            po_path = '%(path)s/%(lang)s/LC_MESSAGES/django%(ext)s.po' % {
                'path': dir_, 'lang': lang, 'ext': 'js' if name.endswith('-js') else ''}
            call('msgcat -o %s %s' % (po_path, po_path), shell=True)
            mo_path = '%s.mo' % po_path[:-3]
            call('msgfmt -o %s %s' % (mo_path, po_path), shell=True)


if __name__ == "__main__":
    RUNABLE_SCRIPTS = ('update_catalogs', 'lang_stats', 'fetch')

    parser = OptionParser(usage="usage: %prog [options] cmd")
    parser.add_option("-r", "--resources", action='append',
        help="limit operation to the specified resources")
    parser.add_option("-l", "--languages", action='append',
        help="limit operation to the specified languages")
    options, args = parser.parse_args()

    if not args:
        parser.print_usage()
        exit(1)

    if args[0] in RUNABLE_SCRIPTS:
        eval(args[0])(options.resources, options.languages)
    else:
        print("Available commands are: %s" % ", ".join(RUNABLE_SCRIPTS))

"""
Tests for field subclassing.
"""

from __future__ import absolute_import

from django.db import models
from django.utils.encoding import force_text

from .fields import SmallField, SmallerField, JSONField
from django.utils.encoding import python_2_unicode_compatible


@python_2_unicode_compatible
class MyModel(models.Model):
    name = models.CharField(max_length=10)
    data = SmallField('small field')

    def __str__(self):
        return force_text(self.name)

class OtherModel(models.Model):
    data = SmallerField()

class DataModel(models.Model):
    data = JSONField()

"""
5. Many-to-many relationships

To define a many-to-many relationship, use ``ManyToManyField()``.

In this example, an ``Article`` can be published in multiple ``Publication``
objects, and a ``Publication`` has multiple ``Article`` objects.
"""

from django.db import models
from django.utils.encoding import python_2_unicode_compatible


@python_2_unicode_compatible
class Publication(models.Model):
    title = models.CharField(max_length=30)

    def __str__(self):
        return self.title

    class Meta:
        ordering = ('title',)

@python_2_unicode_compatible
class Article(models.Model):
    headline = models.CharField(max_length=100)
    publications = models.ManyToManyField(Publication)

    def __str__(self):
        return self.headline

    class Meta:
        ordering = ('headline',)

"""
13. Adding hooks before/after saving and deleting

To execute arbitrary code around ``save()`` and ``delete()``, just subclass
the methods.
"""
from __future__ import unicode_literals

from django.db import models
from django.utils.encoding import python_2_unicode_compatible


@python_2_unicode_compatible
class Person(models.Model):
    first_name = models.CharField(max_length=20)
    last_name = models.CharField(max_length=20)

    def __init__(self, *args, **kwargs):
        super(Person, self).__init__(*args, **kwargs)
        self.data = []

    def __str__(self):
        return "%s %s" % (self.first_name, self.last_name)

    def save(self, *args, **kwargs):
        self.data.append("Before save")
         # Call the "real" save() method
        super(Person, self).save(*args, **kwargs)
        self.data.append("After save")

    def delete(self):
        self.data.append("Before deletion")
        # Call the "real" delete() method
        super(Person, self).delete()
        self.data.append("After deletion")

from __future__ import absolute_import, unicode_literals

import datetime

from django.core.exceptions import ValidationError
from django.test import TestCase
from django.utils import unittest

from .models import (CustomPKModel, UniqueTogetherModel, UniqueFieldsModel,
    UniqueForDateModel, ModelToValidate, Post, FlexibleDatePost,
    UniqueErrorsModel)


class GetUniqueCheckTests(unittest.TestCase):
    def test_unique_fields_get_collected(self):
        m = UniqueFieldsModel()
        self.assertEqual(
            ([(UniqueFieldsModel, ('id',)),
              (UniqueFieldsModel, ('unique_charfield',)),
              (UniqueFieldsModel, ('unique_integerfield',))],
             []),
            m._get_unique_checks()
        )

    def test_unique_together_gets_picked_up_and_converted_to_tuple(self):
        m = UniqueTogetherModel()
        self.assertEqual(
            ([(UniqueTogetherModel, ('ifield', 'cfield',)),
              (UniqueTogetherModel, ('ifield', 'efield')),
              (UniqueTogetherModel, ('id',)), ],
             []),
            m._get_unique_checks()
        )

    def test_primary_key_is_considered_unique(self):
        m = CustomPKModel()
        self.assertEqual(([(CustomPKModel, ('my_pk_field',))], []), m._get_unique_checks())

    def test_unique_for_date_gets_picked_up(self):
        m = UniqueForDateModel()
        self.assertEqual((
            [(UniqueForDateModel, ('id',))],
            [(UniqueForDateModel, 'date', 'count', 'start_date'),
             (UniqueForDateModel, 'year', 'count', 'end_date'),
             (UniqueForDateModel, 'month', 'order', 'end_date')]
            ), m._get_unique_checks()
        )

    def test_unique_for_date_exclusion(self):
        m = UniqueForDateModel()
        self.assertEqual((
            [(UniqueForDateModel, ('id',))],
            [(UniqueForDateModel, 'year', 'count', 'end_date'),
             (UniqueForDateModel, 'month', 'order', 'end_date')]
            ), m._get_unique_checks(exclude='start_date')
        )

class PerformUniqueChecksTest(TestCase):
    def test_primary_key_unique_check_not_performed_when_adding_and_pk_not_specified(self):
        # Regression test for #12560
        with self.assertNumQueries(0):
            mtv = ModelToValidate(number=10, name='Some Name')
            setattr(mtv, '_adding', True)
            mtv.full_clean()

    def test_primary_key_unique_check_performed_when_adding_and_pk_specified(self):
        # Regression test for #12560
        with self.assertNumQueries(1):
            mtv = ModelToValidate(number=10, name='Some Name', id=123)
            setattr(mtv, '_adding', True)
            mtv.full_clean()

    def test_primary_key_unique_check_not_performed_when_not_adding(self):
        # Regression test for #12132
        with self.assertNumQueries(0):
            mtv = ModelToValidate(number=10, name='Some Name')
            mtv.full_clean()

    def test_unique_for_date(self):
        p1 = Post.objects.create(title="Django 1.0 is released",
            slug="Django 1.0", subtitle="Finally", posted=datetime.date(2008, 9, 3))

        p = Post(title="Django 1.0 is released", posted=datetime.date(2008, 9, 3))
        with self.assertRaises(ValidationError) as cm:
            p.full_clean()
        self.assertEqual(cm.exception.message_dict, {'title': ['Title must be unique for Posted date.']})

        # Should work without errors
        p = Post(title="Work on Django 1.1 begins", posted=datetime.date(2008, 9, 3))
        p.full_clean()

        # Should work without errors
        p = Post(title="Django 1.0 is released", posted=datetime.datetime(2008, 9,4))
        p.full_clean()

        p = Post(slug="Django 1.0", posted=datetime.datetime(2008, 1, 1))
        with self.assertRaises(ValidationError) as cm:
            p.full_clean()
        self.assertEqual(cm.exception.message_dict, {'slug': ['Slug must be unique for Posted year.']})

        p = Post(subtitle="Finally", posted=datetime.datetime(2008, 9, 30))
        with self.assertRaises(ValidationError) as cm:
            p.full_clean()
        self.assertEqual(cm.exception.message_dict, {'subtitle': ['Subtitle must be unique for Posted month.']})

        p = Post(title="Django 1.0 is released")
        with self.assertRaises(ValidationError) as cm:
            p.full_clean()
        self.assertEqual(cm.exception.message_dict, {'posted': ['This field cannot be null.']})

    def test_unique_for_date_with_nullable_date(self):
        p1 = FlexibleDatePost.objects.create(title="Django 1.0 is released",
            slug="Django 1.0", subtitle="Finally", posted=datetime.date(2008, 9, 3))

        p = FlexibleDatePost(title="Django 1.0 is released")
        try:
            p.full_clean()
        except ValidationError:
            self.fail("unique_for_date checks shouldn't trigger when the associated DateField is None.")

        p = FlexibleDatePost(slug="Django 1.0")
        try:
            p.full_clean()
        except ValidationError:
            self.fail("unique_for_year checks shouldn't trigger when the associated DateField is None.")

        p = FlexibleDatePost(subtitle="Finally")
        try:
            p.full_clean()
        except ValidationError:
            self.fail("unique_for_month checks shouldn't trigger when the associated DateField is None.")

    def test_unique_errors(self):
        m1 = UniqueErrorsModel.objects.create(name='Some Name', no=10)
        m = UniqueErrorsModel(name='Some Name', no=11)
        with self.assertRaises(ValidationError) as cm:
            m.full_clean()
        self.assertEqual(cm.exception.message_dict, {'name': ['Custom unique name message.']})

        m = UniqueErrorsModel(name='Some Other Name', no=10)
        with self.assertRaises(ValidationError) as cm:
            m.full_clean()
        self.assertEqual(cm.exception.message_dict, {'no': ['Custom unique number message.']})


# -*- coding: utf-8 -*-
from __future__ import absolute_import, unicode_literals

import tempfile
import os

from django import forms
from django.contrib import admin
from django.contrib.admin.views.main import ChangeList
from django.core.files.storage import FileSystemStorage
from django.core.mail import EmailMessage
from django.conf.urls import patterns, url
from django.db import models
from django.forms.models import BaseModelFormSet
from django.http import HttpResponse
from django.contrib.admin import BooleanFieldListFilter

from .models import (Article, Chapter, Account, Media, Child, Parent, Picture,
    Widget, DooHickey, Grommet, Whatsit, FancyDoodad, Category, Link,
    PrePopulatedPost, PrePopulatedSubPost, CustomArticle, Section,
    ModelWithStringPrimaryKey, Color, Thing, Actor, Inquisition, Sketch, Person,
    Persona, Subscriber, ExternalSubscriber, OldSubscriber, Vodcast, EmptyModel,
    Fabric, Gallery, Language, Recommendation, Recommender, Collector, Post,
    Gadget, Villain, SuperVillain, Plot, PlotDetails, CyclicOne, CyclicTwo,
    WorkHour, Reservation, FoodDelivery, RowLevelChangePermissionModel, Paper,
    CoverLetter, Story, OtherStory, Book, Promo, ChapterXtra1, Pizza, Topping,
    Album, Question, Answer, ComplexSortedPerson, PrePopulatedPostLargeSlug,
    AdminOrderedField, AdminOrderedModelMethod, AdminOrderedAdminMethod,
    AdminOrderedCallable, Report, Color2, UnorderedObject, MainPrepopulated,
    RelatedPrepopulated, UndeletableObject, UserMessenger, Simple, Choice,
    ShortMessage, Telegram)


def callable_year(dt_value):
    try:
        return dt_value.year
    except AttributeError:
        return None
callable_year.admin_order_field = 'date'


class ArticleInline(admin.TabularInline):
    model = Article
    prepopulated_fields = {
        'title' : ('content',)
    }
    fieldsets=(
        ('Some fields', {
            'classes': ('collapse',),
            'fields': ('title', 'content')
        }),
        ('Some other fields', {
            'classes': ('wide',),
            'fields': ('date', 'section')
        })
    )

class ChapterInline(admin.TabularInline):
    model = Chapter


class ChapterXtra1Admin(admin.ModelAdmin):
    list_filter = ('chap',
                   'chap__title',
                   'chap__book',
                   'chap__book__name',
                   'chap__book__promo',
                   'chap__book__promo__name',)


class ArticleAdmin(admin.ModelAdmin):
    list_display = ('content', 'date', callable_year, 'model_year', 'modeladmin_year')
    list_filter = ('date', 'section')

    def changelist_view(self, request):
        "Test that extra_context works"
        return super(ArticleAdmin, self).changelist_view(
            request, extra_context={
                'extra_var': 'Hello!'
            }
        )

    def modeladmin_year(self, obj):
        return obj.date.year
    modeladmin_year.admin_order_field = 'date'
    modeladmin_year.short_description = None

    def delete_model(self, request, obj):
        EmailMessage(
            'Greetings from a deleted object',
            'I hereby inform you that some user deleted me',
            'from@example.com',
            ['to@example.com']
        ).send()
        return super(ArticleAdmin, self).delete_model(request, obj)

    def save_model(self, request, obj, form, change=True):
        EmailMessage(
            'Greetings from a created object',
            'I hereby inform you that some user created me',
            'from@example.com',
            ['to@example.com']
        ).send()
        return super(ArticleAdmin, self).save_model(request, obj, form, change)


class RowLevelChangePermissionModelAdmin(admin.ModelAdmin):
    def has_change_permission(self, request, obj=None):
        """ Only allow changing objects with even id number """
        return request.user.is_staff and (obj is not None) and (obj.id % 2 == 0)


class CustomArticleAdmin(admin.ModelAdmin):
    """
    Tests various hooks for using custom templates and contexts.
    """
    change_list_template = 'custom_admin/change_list.html'
    change_form_template = 'custom_admin/change_form.html'
    add_form_template = 'custom_admin/add_form.html'
    object_history_template = 'custom_admin/object_history.html'
    delete_confirmation_template = 'custom_admin/delete_confirmation.html'
    delete_selected_confirmation_template = 'custom_admin/delete_selected_confirmation.html'

    def changelist_view(self, request):
        "Test that extra_context works"
        return super(CustomArticleAdmin, self).changelist_view(
            request, extra_context={
                'extra_var': 'Hello!'
            }
        )


class ThingAdmin(admin.ModelAdmin):
    list_filter = ('color__warm', 'color__value', 'pub_date',)


class InquisitionAdmin(admin.ModelAdmin):
    list_display = ('leader', 'country', 'expected')


class SketchAdmin(admin.ModelAdmin):
    raw_id_fields = ('inquisition',)


class FabricAdmin(admin.ModelAdmin):
    list_display = ('surface',)
    list_filter = ('surface',)


class BasePersonModelFormSet(BaseModelFormSet):
    def clean(self):
        for person_dict in self.cleaned_data:
            person = person_dict.get('id')
            alive = person_dict.get('alive')
            if person and alive and person.name == "Grace Hopper":
                raise forms.ValidationError("Grace is not a Zombie")


class PersonAdmin(admin.ModelAdmin):
    list_display = ('name', 'gender', 'alive')
    list_editable = ('gender', 'alive')
    list_filter = ('gender',)
    search_fields = ('^name',)
    save_as = True

    def get_changelist_formset(self, request, **kwargs):
        return super(PersonAdmin, self).get_changelist_formset(request,
            formset=BasePersonModelFormSet, **kwargs)

    def queryset(self, request):
        # Order by a field that isn't in list display, to be able to test
        # whether ordering is preserved.
        return super(PersonAdmin, self).queryset(request).order_by('age')


class FooAccount(Account):
    """A service-specific account of type Foo."""
    servicename = 'foo'


class BarAccount(Account):
    """A service-specific account of type Bar."""
    servicename = 'bar'


class FooAccountAdmin(admin.StackedInline):
    model = FooAccount
    extra = 1


class BarAccountAdmin(admin.StackedInline):
    model = BarAccount
    extra = 1


class PersonaAdmin(admin.ModelAdmin):
    inlines = (
        FooAccountAdmin,
        BarAccountAdmin
    )


class SubscriberAdmin(admin.ModelAdmin):
    actions = ['mail_admin']

    def mail_admin(self, request, selected):
        EmailMessage(
            'Greetings from a ModelAdmin action',
            'This is the test email from a admin action',
            'from@example.com',
            ['to@example.com']
        ).send()


def external_mail(modeladmin, request, selected):
    EmailMessage(
        'Greetings from a function action',
        'This is the test email from a function action',
        'from@example.com',
        ['to@example.com']
    ).send()
external_mail.short_description = 'External mail (Another awesome action)'


def redirect_to(modeladmin, request, selected):
    from django.http import HttpResponseRedirect
    return HttpResponseRedirect('/some-where-else/')
redirect_to.short_description = 'Redirect to (Awesome action)'


class ExternalSubscriberAdmin(admin.ModelAdmin):
    actions = [redirect_to, external_mail]


class Podcast(Media):
    release_date = models.DateField()

    class Meta:
        ordering = ('release_date',) # overridden in PodcastAdmin


class PodcastAdmin(admin.ModelAdmin):
    list_display = ('name', 'release_date')
    list_editable = ('release_date',)
    date_hierarchy = 'release_date'
    ordering = ('name',)


class VodcastAdmin(admin.ModelAdmin):
    list_display = ('name', 'released')
    list_editable = ('released',)

    ordering = ('name',)


class ChildInline(admin.StackedInline):
    model = Child


class ParentAdmin(admin.ModelAdmin):
    model = Parent
    inlines = [ChildInline]

    list_editable = ('name',)

    def save_related(self, request, form, formsets, change):
        super(ParentAdmin, self).save_related(request, form, formsets, change)
        first_name, last_name = form.instance.name.split()
        for child in form.instance.child_set.all():
            if len(child.name.split()) < 2:
                child.name = child.name + ' ' + last_name
                child.save()


class EmptyModelAdmin(admin.ModelAdmin):
    def queryset(self, request):
        return super(EmptyModelAdmin, self).queryset(request).filter(pk__gt=1)


class OldSubscriberAdmin(admin.ModelAdmin):
    actions = None


temp_storage = FileSystemStorage(tempfile.mkdtemp(dir=os.environ['DJANGO_TEST_TEMP_DIR']))
UPLOAD_TO = os.path.join(temp_storage.location, 'test_upload')


class PictureInline(admin.TabularInline):
    model = Picture
    extra = 1


class GalleryAdmin(admin.ModelAdmin):
    inlines = [PictureInline]


class PictureAdmin(admin.ModelAdmin):
    pass


class LanguageAdmin(admin.ModelAdmin):
    list_display = ['iso', 'shortlist', 'english_name', 'name']
    list_editable = ['shortlist']


class RecommendationAdmin(admin.ModelAdmin):
    search_fields = ('=titletranslation__text', '=recommender__titletranslation__text',)


class WidgetInline(admin.StackedInline):
    model = Widget


class DooHickeyInline(admin.StackedInline):
    model = DooHickey


class GrommetInline(admin.StackedInline):
    model = Grommet


class WhatsitInline(admin.StackedInline):
    model = Whatsit


class FancyDoodadInline(admin.StackedInline):
    model = FancyDoodad


class CategoryAdmin(admin.ModelAdmin):
    list_display = ('id', 'collector', 'order')
    list_editable = ('order',)


class CategoryInline(admin.StackedInline):
    model = Category


class CollectorAdmin(admin.ModelAdmin):
    inlines = [
        WidgetInline, DooHickeyInline, GrommetInline, WhatsitInline,
        FancyDoodadInline, CategoryInline
    ]


class LinkInline(admin.TabularInline):
    model = Link
    extra = 1

    readonly_fields = ("posted", "multiline")

    def multiline(self, instance):
        return "InlineMultiline\ntest\nstring"


class SubPostInline(admin.TabularInline):
    model = PrePopulatedSubPost

    prepopulated_fields = {
        'subslug' : ('subtitle',)
    }

    def get_readonly_fields(self, request, obj=None):
        if obj and obj.published:
            return ('subslug',)
        return self.readonly_fields

    def get_prepopulated_fields(self, request, obj=None):
        if obj and obj.published:
            return {}
        return self.prepopulated_fields


class PrePopulatedPostAdmin(admin.ModelAdmin):
    list_display = ['title', 'slug']
    prepopulated_fields = {
        'slug' : ('title',)
    }

    inlines = [SubPostInline]

    def get_readonly_fields(self, request, obj=None):
        if obj and obj.published:
            return ('slug',)
        return self.readonly_fields

    def get_prepopulated_fields(self, request, obj=None):
        if obj and obj.published:
            return {}
        return self.prepopulated_fields


class PostAdmin(admin.ModelAdmin):
    list_display = ['title', 'public']
    readonly_fields = (
        'posted', 'awesomeness_level', 'coolness', 'value', 'multiline',
        lambda obj: "foo"
    )

    inlines = [
        LinkInline
    ]

    def coolness(self, instance):
        if instance.pk:
            return "%d amount of cool." % instance.pk
        else:
            return "Unkown coolness."

    def value(self, instance):
        return 1000

    def multiline(self, instance):
        return "Multiline\ntest\nstring"

    value.short_description = 'Value in $US'


class CustomChangeList(ChangeList):
    def get_query_set(self, request):
        return self.root_query_set.filter(pk=9999) # Does not exist


class GadgetAdmin(admin.ModelAdmin):
    def get_changelist(self, request, **kwargs):
        return CustomChangeList


class PizzaAdmin(admin.ModelAdmin):
    readonly_fields = ('toppings',)


class WorkHourAdmin(admin.ModelAdmin):
    list_display = ('datum', 'employee')
    list_filter = ('employee',)


class FoodDeliveryAdmin(admin.ModelAdmin):
    list_display=('reference', 'driver', 'restaurant')
    list_editable = ('driver', 'restaurant')


class CoverLetterAdmin(admin.ModelAdmin):
    """
    A ModelAdmin with a custom queryset() method that uses defer(), to test
    verbose_name display in messages shown after adding/editing CoverLetter
    instances.
    Note that the CoverLetter model defines a __unicode__ method.
    For testing fix for ticket #14529.
    """

    def queryset(self, request):
        return super(CoverLetterAdmin, self).queryset(request).defer('date_written')


class PaperAdmin(admin.ModelAdmin):
    """
    A ModelAdmin with a custom queryset() method that uses only(), to test
    verbose_name display in messages shown after adding/editing Paper
    instances.
    For testing fix for ticket #14529.
    """

    def queryset(self, request):
        return super(PaperAdmin, self).queryset(request).only('title')


class ShortMessageAdmin(admin.ModelAdmin):
    """
    A ModelAdmin with a custom queryset() method that uses defer(), to test
    verbose_name display in messages shown after adding/editing ShortMessage
    instances.
    For testing fix for ticket #14529.
    """

    def queryset(self, request):
        return super(ShortMessageAdmin, self).queryset(request).defer('timestamp')


class TelegramAdmin(admin.ModelAdmin):
    """
    A ModelAdmin with a custom queryset() method that uses only(), to test
    verbose_name display in messages shown after adding/editing Telegram
    instances.
    Note that the Telegram model defines a __unicode__ method.
    For testing fix for ticket #14529.
    """

    def queryset(self, request):
        return super(TelegramAdmin, self).queryset(request).only('title')


class StoryForm(forms.ModelForm):
    class Meta:
        widgets = {'title': forms.HiddenInput}


class StoryAdmin(admin.ModelAdmin):
    list_display = ('id', 'title', 'content')
    list_display_links = ('title',) # 'id' not in list_display_links
    list_editable = ('content', )
    form = StoryForm
    ordering = ["-pk"]


class OtherStoryAdmin(admin.ModelAdmin):
    list_display = ('id', 'title', 'content')
    list_display_links = ('title', 'id') # 'id' in list_display_links
    list_editable = ('content', )
    ordering = ["-pk"]


class ComplexSortedPersonAdmin(admin.ModelAdmin):
    list_display = ('name', 'age', 'is_employee', 'colored_name')
    ordering = ('name',)

    def colored_name(self, obj):
        return '<span style="color: #%s;">%s</span>' % ('ff00ff', obj.name)
    colored_name.allow_tags = True
    colored_name.admin_order_field = 'name'


class AlbumAdmin(admin.ModelAdmin):
    list_filter = ['title']


class WorkHourAdmin(admin.ModelAdmin):
    list_display = ('datum', 'employee')
    list_filter = ('employee',)


class PrePopulatedPostLargeSlugAdmin(admin.ModelAdmin):
    prepopulated_fields = {
        'slug' : ('title',)
    }


class AdminOrderedFieldAdmin(admin.ModelAdmin):
    ordering = ('order',)
    list_display = ('stuff', 'order')

class AdminOrderedModelMethodAdmin(admin.ModelAdmin):
    ordering = ('order',)
    list_display = ('stuff', 'some_order')

class AdminOrderedAdminMethodAdmin(admin.ModelAdmin):
    def some_admin_order(self, obj):
        return obj.order
    some_admin_order.admin_order_field = 'order'
    ordering = ('order',)
    list_display = ('stuff', 'some_admin_order')

def admin_ordered_callable(obj):
    return obj.order
admin_ordered_callable.admin_order_field = 'order'
class AdminOrderedCallableAdmin(admin.ModelAdmin):
    ordering = ('order',)
    list_display = ('stuff', admin_ordered_callable)

class ReportAdmin(admin.ModelAdmin):
    def extra(self, request):
        return HttpResponse()

    def get_urls(self):
        # Corner case: Don't call parent implementation
        return patterns('',
            url(r'^extra/$',
                self.extra,
                name='cable_extra'),
        )


class CustomTemplateBooleanFieldListFilter(BooleanFieldListFilter):
    template = 'custom_filter_template.html'

class CustomTemplateFilterColorAdmin(admin.ModelAdmin):
    list_filter = (('warm', CustomTemplateBooleanFieldListFilter),)


# For Selenium Prepopulated tests -------------------------------------
class RelatedPrepopulatedInline1(admin.StackedInline):
    fieldsets = (
        (None, {
            'fields': (('pubdate', 'status'), ('name', 'slug1', 'slug2',),)
        }),
    )
    model = RelatedPrepopulated
    extra = 1
    prepopulated_fields = {'slug1': ['name', 'pubdate'],
                           'slug2': ['status', 'name']}

class RelatedPrepopulatedInline2(admin.TabularInline):
    model = RelatedPrepopulated
    extra = 1
    prepopulated_fields = {'slug1': ['name', 'pubdate'],
                           'slug2': ['status', 'name']}

class MainPrepopulatedAdmin(admin.ModelAdmin):
    inlines = [RelatedPrepopulatedInline1, RelatedPrepopulatedInline2]
    fieldsets = (
        (None, {
            'fields': (('pubdate', 'status'), ('name', 'slug1', 'slug2',),)
        }),
    )
    prepopulated_fields = {'slug1': ['name', 'pubdate'],
                           'slug2': ['status', 'name']}


class UnorderedObjectAdmin(admin.ModelAdmin):
    list_display = ['name']
    list_editable = ['name']
    list_per_page = 2


class UndeletableObjectAdmin(admin.ModelAdmin):
    def change_view(self, *args, **kwargs):
        kwargs['extra_context'] = {'show_delete': False}
        return super(UndeletableObjectAdmin, self).change_view(*args, **kwargs)


def callable_on_unknown(obj):
    return obj.unknown


class AttributeErrorRaisingAdmin(admin.ModelAdmin):
    list_display = [callable_on_unknown, ]

class MessageTestingAdmin(admin.ModelAdmin):
    actions = ["message_debug", "message_info", "message_success",
               "message_warning", "message_error", "message_extra_tags"]

    def message_debug(self, request, selected):
        self.message_user(request, "Test debug", level="debug")

    def message_info(self, request, selected):
        self.message_user(request, "Test info", level="info")

    def message_success(self, request, selected):
        self.message_user(request, "Test success", level="success")

    def message_warning(self, request, selected):
        self.message_user(request, "Test warning", level="warning")

    def message_error(self, request, selected):
        self.message_user(request, "Test error", level="error")

    def message_extra_tags(self, request, selected):
        self.message_user(request, "Test tags", extra_tags="extra_tag")


class ChoiceList(admin.ModelAdmin):
    list_display = ['choice']
    readonly_fields = ['choice']
    fields = ['choice']


site = admin.AdminSite(name="admin")
site.register(Article, ArticleAdmin)
site.register(CustomArticle, CustomArticleAdmin)
site.register(Section, save_as=True, inlines=[ArticleInline])
site.register(ModelWithStringPrimaryKey)
site.register(Color)
site.register(Thing, ThingAdmin)
site.register(Actor)
site.register(Inquisition, InquisitionAdmin)
site.register(Sketch, SketchAdmin)
site.register(Person, PersonAdmin)
site.register(Persona, PersonaAdmin)
site.register(Subscriber, SubscriberAdmin)
site.register(ExternalSubscriber, ExternalSubscriberAdmin)
site.register(OldSubscriber, OldSubscriberAdmin)
site.register(Podcast, PodcastAdmin)
site.register(Vodcast, VodcastAdmin)
site.register(Parent, ParentAdmin)
site.register(EmptyModel, EmptyModelAdmin)
site.register(Fabric, FabricAdmin)
site.register(Gallery, GalleryAdmin)
site.register(Picture, PictureAdmin)
site.register(Language, LanguageAdmin)
site.register(Recommendation, RecommendationAdmin)
site.register(Recommender)
site.register(Collector, CollectorAdmin)
site.register(Category, CategoryAdmin)
site.register(Post, PostAdmin)
site.register(Gadget, GadgetAdmin)
site.register(Villain)
site.register(SuperVillain)
site.register(Plot)
site.register(PlotDetails)
site.register(CyclicOne)
site.register(CyclicTwo)
site.register(WorkHour, WorkHourAdmin)
site.register(Reservation)
site.register(FoodDelivery, FoodDeliveryAdmin)
site.register(RowLevelChangePermissionModel, RowLevelChangePermissionModelAdmin)
site.register(Paper, PaperAdmin)
site.register(CoverLetter, CoverLetterAdmin)
site.register(ShortMessage, ShortMessageAdmin)
site.register(Telegram, TelegramAdmin)
site.register(Story, StoryAdmin)
site.register(OtherStory, OtherStoryAdmin)
site.register(Report, ReportAdmin)
site.register(MainPrepopulated, MainPrepopulatedAdmin)
site.register(UnorderedObject, UnorderedObjectAdmin)
site.register(UndeletableObject, UndeletableObjectAdmin)

# We intentionally register Promo and ChapterXtra1 but not Chapter nor ChapterXtra2.
# That way we cover all four cases:
#     related ForeignKey object registered in admin
#     related ForeignKey object not registered in admin
#     related OneToOne object registered in admin
#     related OneToOne object not registered in admin
# when deleting Book so as exercise all four troublesome (w.r.t escaping
# and calling force_text to avoid problems on Python 2.3) paths through
# contrib.admin.util's get_deleted_objects function.
site.register(Book, inlines=[ChapterInline])
site.register(Promo)
site.register(ChapterXtra1, ChapterXtra1Admin)
site.register(Pizza, PizzaAdmin)
site.register(Topping)
site.register(Album, AlbumAdmin)
site.register(Question)
site.register(Answer)
site.register(PrePopulatedPost, PrePopulatedPostAdmin)
site.register(ComplexSortedPerson, ComplexSortedPersonAdmin)
site.register(PrePopulatedPostLargeSlug, PrePopulatedPostLargeSlugAdmin)
site.register(AdminOrderedField, AdminOrderedFieldAdmin)
site.register(AdminOrderedModelMethod, AdminOrderedModelMethodAdmin)
site.register(AdminOrderedAdminMethod, AdminOrderedAdminMethodAdmin)
site.register(AdminOrderedCallable, AdminOrderedCallableAdmin)
site.register(Color2, CustomTemplateFilterColorAdmin)
site.register(Simple, AttributeErrorRaisingAdmin)
site.register(UserMessenger, MessageTestingAdmin)
site.register(Choice, ChoiceList)

# Register core models we need in our tests
from django.contrib.auth.models import User, Group
from django.contrib.auth.admin import UserAdmin, GroupAdmin
site.register(User, UserAdmin)
site.register(Group, GroupAdmin)

# -*- coding:utf-8 -*-
from __future__ import unicode_literals

from datetime import datetime

from django.test import TestCase


FULL_RESPONSE = 'Test conditional get response'
LAST_MODIFIED = datetime(2007, 10, 21, 23, 21, 47)
LAST_MODIFIED_STR = 'Sun, 21 Oct 2007 23:21:47 GMT'
LAST_MODIFIED_NEWER_STR = 'Mon, 18 Oct 2010 16:56:23 GMT'
LAST_MODIFIED_INVALID_STR = 'Mon, 32 Oct 2010 16:56:23 GMT'
EXPIRED_LAST_MODIFIED_STR = 'Sat, 20 Oct 2007 23:21:47 GMT'
ETAG = 'b4246ffc4f62314ca13147c9d4f76974'
EXPIRED_ETAG = '7fae4cd4b0f81e7d2914700043aa8ed6'

class ConditionalGet(TestCase):
    urls = 'regressiontests.conditional_processing.urls'

    def assertFullResponse(self, response, check_last_modified=True, check_etag=True):
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.content, FULL_RESPONSE.encode())
        if check_last_modified:
            self.assertEqual(response['Last-Modified'], LAST_MODIFIED_STR)
        if check_etag:
            self.assertEqual(response['ETag'], '"%s"' % ETAG)

    def assertNotModified(self, response):
        self.assertEqual(response.status_code, 304)
        self.assertEqual(response.content, b'')

    def testWithoutConditions(self):
        response = self.client.get('/condition/')
        self.assertFullResponse(response)

    def testIfModifiedSince(self):
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_STR
        response = self.client.get('/condition/')
        self.assertNotModified(response)
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_NEWER_STR
        response = self.client.get('/condition/')
        self.assertNotModified(response)
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_INVALID_STR
        response = self.client.get('/condition/')
        self.assertFullResponse(response)
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = EXPIRED_LAST_MODIFIED_STR
        response = self.client.get('/condition/')
        self.assertFullResponse(response)

    def testIfNoneMatch(self):
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % ETAG
        response = self.client.get('/condition/')
        self.assertNotModified(response)
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % EXPIRED_ETAG
        response = self.client.get('/condition/')
        self.assertFullResponse(response)

        # Several etags in If-None-Match is a bit exotic but why not?
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s", "%s"' % (ETAG, EXPIRED_ETAG)
        response = self.client.get('/condition/')
        self.assertNotModified(response)

    def testIfMatch(self):
        self.client.defaults['HTTP_IF_MATCH'] = '"%s"' % ETAG
        response = self.client.put('/condition/etag/')
        self.assertEqual(response.status_code, 200)
        self.client.defaults['HTTP_IF_MATCH'] = '"%s"' % EXPIRED_ETAG
        response = self.client.put('/condition/etag/')
        self.assertEqual(response.status_code, 412)

    def testBothHeaders(self):
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_STR
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % ETAG
        response = self.client.get('/condition/')
        self.assertNotModified(response)

        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = EXPIRED_LAST_MODIFIED_STR
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % ETAG
        response = self.client.get('/condition/')
        self.assertFullResponse(response)

        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_STR
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % EXPIRED_ETAG
        response = self.client.get('/condition/')
        self.assertFullResponse(response)

    def testSingleCondition1(self):
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_STR
        response = self.client.get('/condition/last_modified/')
        self.assertNotModified(response)
        response = self.client.get('/condition/etag/')
        self.assertFullResponse(response, check_last_modified=False)

    def testSingleCondition2(self):
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % ETAG
        response = self.client.get('/condition/etag/')
        self.assertNotModified(response)
        response = self.client.get('/condition/last_modified/')
        self.assertFullResponse(response, check_etag=False)

    def testSingleCondition3(self):
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = EXPIRED_LAST_MODIFIED_STR
        response = self.client.get('/condition/last_modified/')
        self.assertFullResponse(response, check_etag=False)

    def testSingleCondition4(self):
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % EXPIRED_ETAG
        response = self.client.get('/condition/etag/')
        self.assertFullResponse(response, check_last_modified=False)

    def testSingleCondition5(self):
        self.client.defaults['HTTP_IF_MODIFIED_SINCE'] = LAST_MODIFIED_STR
        response = self.client.get('/condition/last_modified2/')
        self.assertNotModified(response)
        response = self.client.get('/condition/etag2/')
        self.assertFullResponse(response, check_last_modified=False)

    def testSingleCondition6(self):
        self.client.defaults['HTTP_IF_NONE_MATCH'] = '"%s"' % ETAG
        response = self.client.get('/condition/etag2/')
        self.assertNotModified(response)
        response = self.client.get('/condition/last_modified2/')
        self.assertFullResponse(response, check_etag=False)

    def testInvalidETag(self):
        self.client.defaults['HTTP_IF_NONE_MATCH'] = r'"\"'
        response = self.client.get('/condition/etag/')
        self.assertFullResponse(response, check_last_modified=False)

from __future__ import absolute_import

from .error_messages import (FormsErrorMessagesTestCase,
    ModelChoiceFieldErrorMessagesTestCase)
from .extra import FormsExtraTestCase, FormsExtraL10NTestCase
from .fields import FieldsTests
from .forms import FormsTestCase
from .formsets import (FormsFormsetTestCase, FormsetAsFooTests,
    TestIsBoundBehavior, TestEmptyFormSet)
from .input_formats import (LocalizedTimeTests, CustomTimeInputFormatsTests,
    SimpleTimeFormatTests, LocalizedDateTests, CustomDateInputFormatsTests,
    SimpleDateFormatTests, LocalizedDateTimeTests,
    CustomDateTimeInputFormatsTests, SimpleDateTimeFormatTests)
from .media import FormsMediaTestCase, StaticFormsMediaTestCase
from .models import (TestTicket12510, ModelFormCallableModelDefault,
    FormsModelTestCase, RelatedModelFormTests)
from .regressions import FormsRegressionsTestCase
from .util import FormsUtilTestCase
from .validators import TestFieldWithValidators
from .widgets import (FormsWidgetTestCase, FormsI18NWidgetsTestCase,
    WidgetTests, LiveWidgetTests, ClearableFileInputTests)

"""
Various edge-cases for model managers.
"""

from django.db import models
from django.utils.encoding import python_2_unicode_compatible


class OnlyFred(models.Manager):
    def get_query_set(self):
        return super(OnlyFred, self).get_query_set().filter(name='fred')


class OnlyBarney(models.Manager):
    def get_query_set(self):
        return super(OnlyBarney, self).get_query_set().filter(name='barney')


class Value42(models.Manager):
    def get_query_set(self):
        return super(Value42, self).get_query_set().filter(value=42)


class AbstractBase1(models.Model):
    name = models.CharField(max_length=50)

    class Meta:
        abstract = True

    # Custom managers
    manager1 = OnlyFred()
    manager2 = OnlyBarney()
    objects = models.Manager()


class AbstractBase2(models.Model):
    value = models.IntegerField()

    class Meta:
        abstract = True

    # Custom manager
    restricted = Value42()


# No custom manager on this class to make sure the default case doesn't break.
class AbstractBase3(models.Model):
    comment = models.CharField(max_length=50)

    class Meta:
        abstract = True


@python_2_unicode_compatible
class Parent(models.Model):
    name = models.CharField(max_length=50)

    manager = OnlyFred()

    def __str__(self):
        return self.name


# Managers from base classes are inherited and, if no manager is specified
# *and* the parent has a manager specified, the first one (in the MRO) will
# become the default.
@python_2_unicode_compatible
class Child1(AbstractBase1):
    data = models.CharField(max_length=25)

    def __str__(self):
        return self.data


@python_2_unicode_compatible
class Child2(AbstractBase1, AbstractBase2):
    data = models.CharField(max_length=25)

    def __str__(self):
        return self.data


@python_2_unicode_compatible
class Child3(AbstractBase1, AbstractBase3):
    data = models.CharField(max_length=25)

    def __str__(self):
        return self.data


@python_2_unicode_compatible
class Child4(AbstractBase1):
    data = models.CharField(max_length=25)

    # Should be the default manager, although the parent managers are
    # inherited.
    default = models.Manager()

    def __str__(self):
        return self.data


@python_2_unicode_compatible
class Child5(AbstractBase3):
    name = models.CharField(max_length=25)

    default = OnlyFred()
    objects = models.Manager()

    def __str__(self):
        return self.name


# Will inherit managers from AbstractBase1, but not Child4.
class Child6(Child4):
    value = models.IntegerField()


# Will not inherit default manager from parent.
class Child7(Parent):
    pass

"""
Regression tests for proper working of ForeignKey(null=True). Tests these bugs:

    * #7512: including a nullable foreign key reference in Meta ordering has un
xpected results

"""
from __future__ import unicode_literals

from django.db import models
from django.utils.encoding import python_2_unicode_compatible


# The first two models represent a very simple null FK ordering case.
class Author(models.Model):
    name = models.CharField(max_length=150)

@python_2_unicode_compatible
class Article(models.Model):
    title = models.CharField(max_length=150)
    author = models.ForeignKey(Author, null=True)

    def __str__(self):
        return 'Article titled: %s' % (self.title, )

    class Meta:
        ordering = ['author__name', ]


# These following 4 models represent a far more complex ordering case.
class SystemInfo(models.Model):
    system_name = models.CharField(max_length=32)

class Forum(models.Model):
    system_info = models.ForeignKey(SystemInfo)
    forum_name = models.CharField(max_length=32)

@python_2_unicode_compatible
class Post(models.Model):
    forum = models.ForeignKey(Forum, null=True)
    title = models.CharField(max_length=32)

    def __str__(self):
        return self.title

@python_2_unicode_compatible
class Comment(models.Model):
    post = models.ForeignKey(Post, null=True)
    comment_text = models.CharField(max_length=250)

    class Meta:
        ordering = ['post__forum__system_info__system_name', 'comment_text']

    def __str__(self):
        return self.comment_text

from django.core.xheaders import populate_xheaders
from django.http import HttpResponse
from django.utils.decorators import decorator_from_middleware
from django.views.generic import View
from django.middleware.doc import XViewMiddleware

from .models import Article

xview_dec = decorator_from_middleware(XViewMiddleware)

def xview(request):
    return HttpResponse()

def xview_xheaders(request, object_id):
    response = HttpResponse()
    populate_xheaders(request, response, Article, 1)
    return response

class XViewClass(View):
    def get(self, request):
        return HttpResponse()

# Copyright (C) 2010 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Utilities for OAuth.

Utilities for making it easier to work with OAuth 1.0 credentials.
"""

__author__ = 'jcgregorio@google.com (Joe Gregorio)'

import pickle
import threading

from apiclient.oauth import Storage as BaseStorage


class Storage(BaseStorage):
  """Store and retrieve a single credential to and from a file."""

  def __init__(self, filename):
    self._filename = filename
    self._lock = threading.Lock()

  def get(self):
    """Retrieve Credential from file.

    Returns:
      apiclient.oauth.Credentials
    """
    self._lock.acquire()
    try:
      f = open(self._filename, 'r')
      credentials = pickle.loads(f.read())
      f.close()
      credentials.set_store(self.put)
    except:
      credentials = None
    self._lock.release()

    return credentials

  def put(self, credentials):
    """Write a pickled Credentials to file.

    Args:
      credentials: Credentials, the credentials to store.
    """
    self._lock.acquire()
    f = open(self._filename, 'w')
    f.write(pickle.dumps(credentials))
    f.close()
    self._lock.release()

#!/usr/bin/env python
# ---------------------------------------------------------------------------

"""
Provides a front-end to the Python standard ``optparse`` module. The
``CommandLineParser`` class makes two changes to the standard behavior.

  - The output for the '-h' option is slightly different.
  - A bad option causes the parser to generate the entire usage output,
    not just an error message.

It also provides a couple extra utility modules.
"""

__docformat__ = "restructuredtext en"

# ---------------------------------------------------------------------------
# Imports
# ---------------------------------------------------------------------------

from optparse import OptionParser
import sys

# ---------------------------------------------------------------------------
# Exports
# ---------------------------------------------------------------------------

__all__ = ['CommandLineParser']

# ---------------------------------------------------------------------------
# Classes
# ---------------------------------------------------------------------------

class CommandLineParser(OptionParser):
    """Custom version of command line option parser."""

    def __init__(self, *args, **kw):
        """ Create a new instance. """

        OptionParser.__init__(self, *args, **kw)

        # I like my help option message better than the default...

        self.remove_option('-h')
        self.add_option('-h', '--help', action='help',
                        help='Show this message and exit.')
        
        self.epilogue = None

    def print_help(self, out=sys.stderr):
        """
        Print the help message, followed by the epilogue (if set), to the
        specified output file. You can define an epilogue by setting the
        ``epilogue`` field.
        
        :Parameters:
            out : file
                where to write the usage message
        """
        OptionParser.print_help(self, out)
        if self.epilogue:
            import textwrap
            print >> out, '\n%s' % textwrap.fill(self.epilogue, 80)
            out.flush()

    def die_with_usage(self, msg=None, exit_code=2):
        """
        Display a usage message and exit.

        :Parameters:
            msg : str
                If not set to ``None`` (the default), this message will be
                displayed before the usage message
                
            exit_code : int
                The process exit code. Defaults to 2.
        """
        if msg != None:
            print >> sys.stderr, msg
        self.print_help(sys.stderr)
        sys.exit(exit_code)

    def error(self, msg):
        """
        Overrides parent ``OptionParser`` class's ``error()`` method and
        forces the full usage message on error.
        """
        sys.stderr.write("%s: error: %s\n" % (self.get_prog_name(), msg))
        self.die_with_usage(msg)

# Nose program for testing grizzled.proxy class.

from __future__ import absolute_import

# ---------------------------------------------------------------------------
# Imports
# ---------------------------------------------------------------------------

from grizzled.proxy import Forwarder
import tempfile
from grizzled.file import unlink_quietly
from .test_helpers import exception_expected

# ---------------------------------------------------------------------------
# Globals
# ---------------------------------------------------------------------------

# ---------------------------------------------------------------------------
# Classes
# ---------------------------------------------------------------------------

class ForwardToFile(Forwarder):
    def __init__(self, file, *exceptions):
        Forwarder.__init__(self, file, exceptions)

class TestProxyPackage(object):

    def test_forward_all(self):
        path = self._create_file()
        try:
            with open(path) as f:
                contents = ''.join(f.readlines())

            with open(path) as f:
                fwd = ForwardToFile(f)
                contents2 = ''.join(fwd.readlines())

            assert contents2 == contents

        finally:
            unlink_quietly(path)

    def test_forward_all_but_name(self):
        path = self._create_file()
        try:
            with exception_expected(AttributeError):
                with open(path) as f:
                    fwd = ForwardToFile(f, 'name', 'foo')
                    fwd.name
        finally:
            unlink_quietly(path)

    def test_forward_all_but_name_mode(self):
        path = self._create_file()
        try:
            with open(path) as f:
                fwd = ForwardToFile(f, 'name', 'mode')
                fwd.closed # should not fail
                with exception_expected(AttributeError):
                    fwd.name
                with exception_expected(AttributeError):
                    fwd.mode
        finally:
            unlink_quietly(path)

    def _create_file(self):
        temp = tempfile.NamedTemporaryFile(prefix="fwdtest", delete=False)
        temp.write(', '.join([str(x) for x in range(1, 81)]))
        temp.write(', '.join([str(x) for x in range(1, 21)]))
        temp.close
        return temp.name

# -*- coding: utf-8 -*-
"""
Jinja2
~~~~~~

Jinja2 is a template engine written in pure Python.  It provides a
`Django`_ inspired non-XML syntax but supports inline expressions and
an optional `sandboxed`_ environment.

Nutshell
--------

Here a small example of a Jinja template::

    {% extends 'base.html' %}
    {% block title %}Memberlist{% endblock %}
    {% block content %}
      <ul>
      {% for user in users %}
        <li><a href="{{ user.url }}">{{ user.username }}</a></li>
      {% endfor %}
      </ul>
    {% endblock %}

Philosophy
----------

Application logic is for the controller but don't try to make the life
for the template designer too hard by giving him too few functionality.

For more informations visit the new `Jinja2 webpage`_ and `documentation`_.

.. _sandboxed: http://en.wikipedia.org/wiki/Sandbox_(computer_security)
.. _Django: http://www.djangoproject.com/
.. _Jinja2 webpage: http://jinja.pocoo.org/
.. _documentation: http://jinja.pocoo.org/2/documentation/
"""
import sys

from setuptools import setup, Extension, Feature

debugsupport = Feature(
    'optional C debug support',
    standard=False,
    ext_modules = [
        Extension('jinja2._debugsupport', ['jinja2/_debugsupport.c']),
    ],
)


# tell distribute to use 2to3 with our own fixers.
extra = {}
if sys.version_info >= (3, 0):
    extra.update(
        use_2to3=True,
        use_2to3_fixers=['custom_fixers']
    )

# ignore the old '--with-speedups' flag
try:
    speedups_pos = sys.argv.index('--with-speedups')
except ValueError:
    pass
else:
    sys.argv[speedups_pos] = '--with-debugsupport'
    sys.stderr.write('*' * 74 + '\n')
    sys.stderr.write('WARNING:\n')
    sys.stderr.write('  the --with-speedups flag is deprecated, assuming '
                     '--with-debugsupport\n')
    sys.stderr.write('  For the actual speedups install the MarkupSafe '
                     'package.\n')
    sys.stderr.write('*' * 74 + '\n')


setup(
    name='Jinja2',
    version='2.6',
    url='http://jinja.pocoo.org/',
    license='BSD',
    author='Armin Ronacher',
    author_email='armin.ronacher@active-4.com',
    description='A small but fast and easy to use stand-alone template '
                'engine written in pure python.',
    long_description=__doc__,
    # jinja is egg safe. But we hate eggs
    zip_safe=False,
    classifiers=[
        'Development Status :: 5 - Production/Stable',
        'Environment :: Web Environment',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: BSD License',
        'Operating System :: OS Independent',
        'Programming Language :: Python',
        'Programming Language :: Python :: 3',
        'Topic :: Internet :: WWW/HTTP :: Dynamic Content',
        'Topic :: Software Development :: Libraries :: Python Modules',
        'Topic :: Text Processing :: Markup :: HTML'
    ],
    packages=['jinja2', 'jinja2.testsuite', 'jinja2.testsuite.res',
              'jinja2._markupsafe'],
    extras_require={'i18n': ['Babel>=0.8']},
    test_suite='jinja2.testsuite.suite',
    include_package_data=True,
    entry_points="""
    [babel.extractors]
    jinja2 = jinja2.ext:babel_extract[i18n]
    """,
    features={'debugsupport': debugsupport},
    **extra
)

#!/usr/bin/env python
#
# Copyright 2011 Google Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

__author__ = 'rafek@google.com (Rafe Kaplan)'

import os.path, sys, fnmatch
from distutils.command.build_py import build_py as _build_py
from distutils.util import convert_path
from glob import glob

class build_py(_build_py):
    """Enhanced 'build_py' command that includes data files with packages

    The data files are specified via a 'package_data' argument to 'setup()'.
    See 'setuptools.dist.Distribution' for more details.

    Also, this version of the 'build_py' command allows you to specify both
    'py_modules' and 'packages' in the same setup operation.
    """
    def finalize_options(self):
        _build_py.finalize_options(self)
        self.package_data = self.distribution.package_data
        self.exclude_package_data = self.distribution.exclude_package_data or {}
        if 'data_files' in self.__dict__: del self.__dict__['data_files']

    def run(self):
        """Build modules, packages, and copy data files to build directory"""
        if not self.py_modules and not self.packages:
            return

        if self.py_modules:
            self.build_modules()

        if self.packages:
            self.build_packages()
            self.build_package_data()

        # Only compile actual .py files, using our base class' idea of what our
        # output files are.
        self.byte_compile(_build_py.get_outputs(self, include_bytecode=0))

    def __getattr__(self,attr):
        if attr=='data_files':  # lazily compute data files
            self.data_files = files = self._get_data_files(); return files
        return _build_py.__getattr__(self,attr)

    def _get_data_files(self):
        """Generate list of '(package,src_dir,build_dir,filenames)' tuples"""
        self.analyze_manifest()
        data = []
        for package in self.packages or ():
            # Locate package source directory
            src_dir = self.get_package_dir(package)

            # Compute package build directory
            build_dir = os.path.join(*([self.build_lib] + package.split('.')))

            # Length of path to strip from found files
            plen = len(src_dir)+1

            # Strip directory from globbed filenames
            filenames = [
                file[plen:] for file in self.find_data_files(package, src_dir)
                ]
            data.append( (package, src_dir, build_dir, filenames) )
        return data

    def find_data_files(self, package, src_dir):
        """Return filenames for package's data files in 'src_dir'"""
        globs = (self.package_data.get('', [])
                 + self.package_data.get(package, []))
        files = self.manifest_files.get(package, [])[:]
        for pattern in globs:
            # Each pattern has to be converted to a platform-specific path
            files.extend(glob(os.path.join(src_dir, convert_path(pattern))))
        return self.exclude_data_files(package, src_dir, files)

    def build_package_data(self):
        """Copy data files into build directory"""
        lastdir = None
        for package, src_dir, build_dir, filenames in self.data_files:
            for filename in filenames:
                target = os.path.join(build_dir, filename)
                self.mkpath(os.path.dirname(target))
                self.copy_file(os.path.join(src_dir, filename), target)


    def analyze_manifest(self):
        self.manifest_files = mf = {}
        if not self.distribution.include_package_data:
            return
        src_dirs = {}
        for package in self.packages or ():
            # Locate package source directory
            src_dirs[assert_relative(self.get_package_dir(package))] = package

        self.run_command('egg_info')
        ei_cmd = self.get_finalized_command('egg_info')
        for path in ei_cmd.filelist.files:
            d,f = os.path.split(assert_relative(path))
            prev = None
            oldf = f
            while d and d!=prev and d not in src_dirs:
                prev = d
                d, df = os.path.split(d)
                f = os.path.join(df, f)
            if d in src_dirs:
                if path.endswith('.py') and f==oldf:
                    continue    # it's a module, not data
                mf.setdefault(src_dirs[d],[]).append(path)

    def get_data_files(self): pass  # kludge 2.4 for lazy computation

    if sys.version<"2.4":    # Python 2.4 already has this code
        def get_outputs(self, include_bytecode=1):
            """Return complete list of files copied to the build directory

            This includes both '.py' files and data files, as well as '.pyc'
            and '.pyo' files if 'include_bytecode' is true.  (This method is
            needed for the 'install_lib' command to do its job properly, and to
            generate a correct installation manifest.)
            """
            return _build_py.get_outputs(self, include_bytecode) + [
                os.path.join(build_dir, filename)
                for package, src_dir, build_dir,filenames in self.data_files
                for filename in filenames
                ]

    def check_package(self, package, package_dir):
        """Check namespace packages' __init__ for declare_namespace"""
        try:
            return self.packages_checked[package]
        except KeyError:
            pass

        init_py = _build_py.check_package(self, package, package_dir)
        self.packages_checked[package] = init_py

        if not init_py or not self.distribution.namespace_packages:
            return init_py

        for pkg in self.distribution.namespace_packages:
            if pkg==package or pkg.startswith(package+'.'):
                break
        else:
            return init_py

        f = open(init_py,'rU')
        if 'declare_namespace' not in f.read():
            from distutils import log
            log.warn(
               "WARNING: %s is a namespace package, but its __init__.py does\n"
               "not declare_namespace(); setuptools 0.7 will REQUIRE this!\n"
               '(See the setuptools manual under "Namespace Packages" for '
               "details.)\n", package
            )
        f.close()
        return init_py

    def initialize_options(self):
        self.packages_checked={}
        _build_py.initialize_options(self)







    def exclude_data_files(self, package, src_dir, files):
        """Filter filenames for package's data files in 'src_dir'"""
        globs = (self.exclude_package_data.get('', [])
                 + self.exclude_package_data.get(package, []))
        bad = []
        for pattern in globs:
            bad.extend(
                fnmatch.filter(
                    files, os.path.join(src_dir, convert_path(pattern))
                )
            )
        bad = dict.fromkeys(bad)
        seen = {}
        return [
            f for f in files if f not in bad
                and f not in seen and seen.setdefault(f,1)  # ditch dupes
        ]


def assert_relative(path):
    if not os.path.isabs(path):
        return path
    from distutils.errors import DistutilsSetupError
    raise DistutilsSetupError(
"""Error: setup script specifies an absolute path:

    %s

setup() arguments must *always* be /-separated paths relative to the
setup.py directory, *never* absolute paths.
""" % path
    )










import cgi, warnings
from webob.headers import _trans_key

def html_escape(s):
    """HTML-escape a string or object

    This converts any non-string objects passed into it to strings
    (actually, using ``unicode()``).  All values returned are
    non-unicode strings (using ``&#num;`` entities for all non-ASCII
    characters).

    None is treated specially, and returns the empty string.
    """
    if s is None:
        return ''
    if hasattr(s, '__html__'):
        return s.__html__()
    if not isinstance(s, basestring):
        if hasattr(s, '__unicode__'):
            s = unicode(s)
        else:
            s = str(s)
    s = cgi.escape(s, True)
    if isinstance(s, unicode):
        s = s.encode('ascii', 'xmlcharrefreplace')
    return s

def header_docstring(header, rfc_section):
    if header.isupper():
        header = _trans_key(header)
    major_section = rfc_section.split('.')[0]
    link = 'http://www.w3.org/Protocols/rfc2616/rfc2616-sec%s.html#sec%s' % (major_section, rfc_section)
    return "Gets and sets the ``%s`` header (`HTTP spec section %s <%s>`_)." \
        % (header, rfc_section, link)

def warn_deprecation(text, version, stacklevel):
    # version specifies when to start raising exceptions instead of warnings
    if version == '1.2':
        cls = DeprecationWarning
    elif version == '1.3':
        cls = PendingDeprecationWarning
    else:
        cls = DeprecationWarning
        warnings.warn("Unknown warn_deprecation version arg: %r" % version,
            RuntimeWarning,
            stacklevel=1
        )
    warnings.warn(text, cls, stacklevel=stacklevel+1)

status_reasons = {
    # Status Codes
    # Informational
    100: 'Continue',
    101: 'Switching Protocols',
    102: 'Processing',

    # Successful
    200: 'OK',
    201: 'Created',
    202: 'Accepted',
    203: 'Non-Authoritative Information',
    204: 'No Content',
    205: 'Reset Content',
    206: 'Partial Content',
    207: 'Multi Status',
    226: 'IM Used',

    # Redirection
    300: 'Multiple Choices',
    301: 'Moved Permanently',
    302: 'Found',
    303: 'See Other',
    304: 'Not Modified',
    305: 'Use Proxy',
    307: 'Temporary Redirect',

    # Client Error
    400: 'Bad Request',
    401: 'Unauthorized',
    402: 'Payment Required',
    403: 'Forbidden',
    404: 'Not Found',
    405: 'Method Not Allowed',
    406: 'Not Acceptable',
    407: 'Proxy Authentication Required',
    408: 'Request Timeout',
    409: 'Conflict',
    410: 'Gone',
    411: 'Length Required',
    412: 'Precondition Failed',
    413: 'Request Entity Too Large',
    414: 'Request URI Too Long',
    415: 'Unsupported Media Type',
    416: 'Requested Range Not Satisfiable',
    417: 'Expectation Failed',
    422: 'Unprocessable Entity',
    423: 'Locked',
    424: 'Failed Dependency',
    426: 'Upgrade Required',

    # Server Error
    500: 'Internal Server Error',
    501: 'Not Implemented',
    502: 'Bad Gateway',
    503: 'Service Unavailable',
    504: 'Gateway Timeout',
    505: 'HTTP Version Not Supported',
    507: 'Insufficient Storage',
    510: 'Not Extended',
}

"""
Contains some data structures.
"""

from webob.util.dictmixin import DictMixin

class EnvironHeaders(DictMixin):
    """An object that represents the headers as present in a
    WSGI environment.

    This object is a wrapper (with no internal state) for a WSGI
    request object, representing the CGI-style HTTP_* keys as a
    dictionary.  Because a CGI environment can only hold one value for
    each key, this dictionary is single-valued (unlike outgoing
    headers).
    """

    def __init__(self, environ):
        self.environ = environ

    def _trans_name(self, name):
        key = 'HTTP_'+name.replace('-', '_').upper()
        if key == 'HTTP_CONTENT_LENGTH':
            key = 'CONTENT_LENGTH'
        elif key == 'HTTP_CONTENT_TYPE':
            key = 'CONTENT_TYPE'
        return key

    def _trans_key(self, key):
        if key == 'CONTENT_TYPE':
            return 'Content-Type'
        elif key == 'CONTENT_LENGTH':
            return 'Content-Length'
        elif key.startswith('HTTP_'):
            return key[5:].replace('_', '-').title()
        else:
            return None
        
    def __getitem__(self, item):
        return self.environ[self._trans_name(item)]

    def __setitem__(self, item, value):
        self.environ[self._trans_name(item)] = value

    def __delitem__(self, item):
        del self.environ[self._trans_name(item)]

    def __iter__(self):
        for key in self.environ:
            name = self._trans_key(key)
            if name is not None:
                yield name

    def keys(self):
        return list(iter(self))

    def __contains__(self, item):
        return self._trans_name(item) in self.environ


class Token(object):
    def __init__(self, start_mark, end_mark):
        self.start_mark = start_mark
        self.end_mark = end_mark
    def __repr__(self):
        attributes = [key for key in self.__dict__
                if not key.endswith('_mark')]
        attributes.sort()
        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))
                for key in attributes])
        return '%s(%s)' % (self.__class__.__name__, arguments)

#class BOMToken(Token):
#    id = '<byte order mark>'

class DirectiveToken(Token):
    id = '<directive>'
    def __init__(self, name, value, start_mark, end_mark):
        self.name = name
        self.value = value
        self.start_mark = start_mark
        self.end_mark = end_mark

class DocumentStartToken(Token):
    id = '<document start>'

class DocumentEndToken(Token):
    id = '<document end>'

class StreamStartToken(Token):
    id = '<stream start>'
    def __init__(self, start_mark=None, end_mark=None,
            encoding=None):
        self.start_mark = start_mark
        self.end_mark = end_mark
        self.encoding = encoding

class StreamEndToken(Token):
    id = '<stream end>'

class BlockSequenceStartToken(Token):
    id = '<block sequence start>'

class BlockMappingStartToken(Token):
    id = '<block mapping start>'

class BlockEndToken(Token):
    id = '<block end>'

class FlowSequenceStartToken(Token):
    id = '['

class FlowMappingStartToken(Token):
    id = '{'

class FlowSequenceEndToken(Token):
    id = ']'

class FlowMappingEndToken(Token):
    id = '}'

class KeyToken(Token):
    id = '?'

class ValueToken(Token):
    id = ':'

class BlockEntryToken(Token):
    id = '-'

class FlowEntryToken(Token):
    id = ','

class AliasToken(Token):
    id = '<alias>'
    def __init__(self, value, start_mark, end_mark):
        self.value = value
        self.start_mark = start_mark
        self.end_mark = end_mark

class AnchorToken(Token):
    id = '<anchor>'
    def __init__(self, value, start_mark, end_mark):
        self.value = value
        self.start_mark = start_mark
        self.end_mark = end_mark

class TagToken(Token):
    id = '<tag>'
    def __init__(self, value, start_mark, end_mark):
        self.value = value
        self.start_mark = start_mark
        self.end_mark = end_mark

class ScalarToken(Token):
    id = '<scalar>'
    def __init__(self, value, plain, start_mark, end_mark, style=None):
        self.value = value
        self.plain = plain
        self.start_mark = start_mark
        self.end_mark = end_mark
        self.style = style


#!/usr/bin/env python

import os
import Queue
import sys
import threading
import tornado.httpclient
import unittest
from flexmock import flexmock

sys.path.append(os.path.join(os.path.dirname(__file__), "../../"))
import helper
import hermes_constants
from custom_hermes_exceptions import MissingRequestArgs

sys.path.append(os.path.join(os.path.dirname(__file__), "../../../lib"))
import appscale_info

sys.path.append(os.path.join(os.path.dirname(__file__), '../../../AppServer'))
from google.appengine.api.appcontroller_client import AppControllerClient

class FakeAppControllerClient():
  def __init__(self, registered):
    self.registered = registered
  def deployment_id_exists(self):
    return self.registered
  def get_deployment_id(self):
    return 'fake_id'

class FakeAsyncClient():
  def fetch(self):
    pass

class FakeClient():
  def fetch(self):
    pass

class FakeLock():
  def __init__(self, wrapped_class):
    pass
  def acquire(self):
    pass
  def release(self):
    pass

class FakeRequest():
  def __init__(self):
    self.url = fake_url
    self.body = fake_data

class FakeResponse():
  def __init__(self, request, code):
    self.request = request
    self.code = code

fake_url = 'http://some.url'
fake_data = 'some data'
fake_node_info = [
  {
    'host': fake_url,
    'role': 'db_master',
    'index': None
  },
  {
    'host': fake_url,
    'role': 'zk',
    'index': 0
  }
]

class TestHelper(unittest.TestCase):
  """ A set of test cases for Hermes helper functions. """

  def test_create_request(self):
    # Test with no args.
    self.assertRaises(MissingRequestArgs, helper.create_request)
    # Test GET.
    self.assertIsNotNone(helper.create_request, ['some url', 'some method'])
    # Test POST.
    self.assertIsNotNone(helper.create_request, ['some url', 'some method',
      'some data'])

  def test_urlfetch(self):
    fake_request = FakeRequest()
    fake_response = FakeResponse(fake_request, 200)
    fake_client = flexmock(tornado.httpclient.HTTPClient())

    fake_client.should_receive('fetch').and_return(fake_response)
    self.assertIsNotNone(helper.urlfetch, fake_request)

  def test_urlfetch_async(self):
    fake_request = FakeRequest()
    fake_response = FakeResponse(fake_request, 200)
    fake_client = flexmock(tornado.httpclient.AsyncHTTPClient())

    fake_client.should_receive('fetch').and_return(fake_response)
    self.assertIsNotNone(helper.urlfetch, fake_request)

  def test_get_br_service_url(self):
    fake_url = 'http://host:{0}{1}'.format(hermes_constants.BR_SERVICE_PORT,
      hermes_constants.BR_SERVICE_PATH)
    self.assertEquals(fake_url, helper.get_br_service_url('host'))

  def test_get_deployment_id(self):
    # Test with a registered AppScale deployment.
    fake_acc = FakeAppControllerClient(True)
    flexmock(appscale_info).should_receive('get_appcontroller_client').\
      and_return(fake_acc)
    flexmock(AppControllerClient).should_receive('deployment_id_exists').\
      and_return(True)
    flexmock(AppControllerClient).should_receive('get_deployment_id').\
      and_return('fake_id')
    self.assertEquals('fake_id', helper.get_deployment_id())

    # Test with an AppScale deployment that's not registered.
    fake_acc = FakeAppControllerClient(False)
    flexmock(appscale_info).should_receive('get_appcontroller_client').\
      and_return(fake_acc)
    flexmock(AppControllerClient).should_receive('deployment_id_exists').\
      and_return(False)
    self.assertIsNone(helper.get_deployment_id())

  def test_get_node_info(self):
    flexmock(appscale_info).should_receive('get_db_master_ip').and_return(
      'foo')
    flexmock(appscale_info).should_receive('get_db_slave_ips').and_return(
      ['bar'])
    flexmock(appscale_info).should_receive('get_zk_node_ips').and_return(
      ['baz'])
    flexmock(helper).should_receive('get_br_service_url').and_return(
      'http://some.url').at_least().times(2)
    self.assertEquals(fake_node_info, helper.get_node_info())

  def test_create_br_json_data(self):
    pass

  def test_delete_task_from_mem(self):
    flexmock(FakeLock(threading.Lock())).should_receive('acquire').\
      and_return()
    flexmock(FakeLock(threading.Lock())).should_receive('release').\
      and_return()
    helper.delete_task_from_mem('foo')

  def test_report_status(self):
    pass

  def test_send_remote_request(self):
    flexmock(Queue.Queue).should_receive('put').and_return().at_least().times(1)
    flexmock(helper).should_receive('urlfetch').and_return('qux').at_least().\
      times(1)

    helper.send_remote_request(FakeRequest(), Queue.Queue())

if __name__ == "__main__":
  unittest.main()

""" Top level server for the Search API. """
from search_api import SearchService

import logging

import tornado.httpserver
import tornado.httputil
import tornado.ioloop
import tornado.web
import time

# Default port for the search API web server.
DEFAULT_PORT = 53423

class MainHandler(tornado.web.RequestHandler):
  """ Main handler class. """
  
  def initialize(self, search_service):
    """ Class for initializing search service web handler. """
    self.search_service = search_service

  @tornado.web.asynchronous
  def post(self):
    """ A POST handler for request to this server. """
    request = self.request
    http_request_data = request.body
    pb_type = request.headers['protocolbuffertype']
    if pb_type == "Request":
      response = self.search_service.remote_request(http_request_data)
    else:
      response = self.search_service.unknown_request(pb_type)

    request.connection.write_headers(
      tornado.httputil.ResponseStartLine('HTTP/1.1', 200, 'OK'),
      tornado.httputil.HTTPHeaders({"Content-Length": str(len(response))}))
    request.connection.write(response)
    request.connection.finish()


def get_application():
  """ Retrieves the application to feed into tornado. """
  return tornado.web.Application([
    (r"/?", MainHandler, dict(search_service=SearchService())),
    ], )

if __name__ == "__main__":
  logging.getLogger().setLevel(logging.INFO) 
  logging.info("Starting server on port {0}".format(DEFAULT_PORT))
  http_server = tornado.httpserver.HTTPServer(get_application())
  http_server.bind(DEFAULT_PORT)
  http_server.start(0)
  tornado.ioloop.IOLoop.instance().start()

'''
Created on Apr 21, 2011

@author: Mark V Systems Limited
(c) Copyright 2011 Mark V Systems Limited, All rights reserved.
'''
import xml.dom, math, re
from arelle.ModelValue import qname
from arelle import XPathContext, XbrlUtil
from arelle.ModelInstanceObject import ModelDimensionValue
from decimal import Decimal
    
class fnFunctionNotAvailable(Exception):
    def __init__(self):
        self.args =  ("custom function not available",)
    def __repr__(self):
        return self.args[0]
    
def call(xc, p, qname, contextItem, args):
    try:
        cfSig = xc.modelXbrl.modelCustomFunctionSignatures[qname, len(args)]
        if cfSig is not None and cfSig.customFunctionImplementation is not None:
            return callCfi(xc, p, qname, cfSig, contextItem, args)
        elif qname in xc.customFunctions: # plug in method custom functions 
            return xc.customFunctions[qname](xc, p, contextItem, args) # use plug-in's method
        elif qname not in customFunctions: # compiled functions in this module
            raise fnFunctionNotAvailable
        return customFunctions[qname](xc, p, contextItem, args)
    except (fnFunctionNotAvailable, KeyError):
        raise XPathContext.FunctionNotAvailable("custom function:{0}".format(str(qname)))

def callCfi(xc, p, qname, cfSig, contextItem, args):
    if len(args) != len(cfSig.inputTypes): 
        raise XPathContext.FunctionNumArgs()

    cfi = cfSig.customFunctionImplementation
    overriddenInScopeVars = {}
    traceSource = xc.formulaOptions.traceSource(xc.traceType)
    traceEvaluation = xc.formulaOptions.traceEvaluation(xc.traceType)
    inputNames = cfi.inputNames
    for i, argName in enumerate(inputNames):
        if argName in xc.inScopeVars:
            overriddenInScopeVars[argName] = xc.inScopeVars[argName]
        xc.inScopeVars[argName] = args[i]
        
    if traceEvaluation:
        xc.modelXbrl.info("formula:trace",
                            _("%(cfi)s(%(arguments)s)"),
                            modelObject=cfi,
                            cfi=qname, 
                            arguments=', '.join("{}={}".format(argName, args[i])
                                                for i, argName in enumerate(inputNames)))

    for i, step in enumerate(cfi.stepExpressions):
        stepQname, stepExpression = step
        stepProg = cfi.stepProgs[i]
        if traceSource:
            xc.modelXbrl.info("formula:trace",
                                _("%(cfi)s step %(step)s \nExpression: \n%(expression)s"),
                                modelObject=cfi,
                                cfi=qname, step=stepQname, expression=stepExpression)
        result = xc.evaluate(stepProg)
        if traceEvaluation:
            xc.modelXbrl.info("formula:trace",
                                _("%(cfi)s step %(step)s \nResult: \n%(expression)s"),
                                modelObject=cfi,
                                cfi=qname, step=stepQname, expression=result)
        if stepQname in xc.inScopeVars:
            overriddenInScopeVars[stepQname] = xc.inScopeVars[stepQname]
        xc.inScopeVars[stepQname] = result

    if traceSource:
        xc.modelXbrl.info("formula:trace",
                            _("%(cfi)s output \nExpression: \n%(expression)s"),
                            modelObject=cfi,
                            cfi=qname, expression=cfi.outputExpression)
    result = xc.evaluateAtomicValue(cfi.outputProg, cfSig.outputType)
    if traceEvaluation:
        xc.modelXbrl.info("formula:trace",
                            _("%(cfi)s output \nResult: \n%(expression)s"),
                            modelObject=cfi,
                            cfi=qname, expression=result)

    for step in cfi.stepExpressions:
        stepQname = step[0]
        if stepQname in overriddenInScopeVars:
            xc.inScopeVars[stepQname] = overriddenInScopeVars[stepQname]

    for i, argName in enumerate(inputNames):
        if argName in overriddenInScopeVars:
            xc.inScopeVars[argName] = overriddenInScopeVars[argName]
        else:
            del xc.inScopeVars[argName]

    if result is None:  # atomic value failed the result cast expression
        raise XPathContext.FunctionArgType("output",cfSig.outputType,result)
    return result

# for test case 22015 v01        
def  my_fn_PDxEV(xc, p, contextItem, args):
    if len(args) != 2: raise XPathContext.FunctionNumArgs()
    PDseq = args[0] if isinstance(args[0],(list,tuple)) else (args[0],)
    EVseq = args[1] if isinstance(args[1],(list,tuple)) else (args[1],)
    dimQname = qname("{http://www.example.com/wgt-avg}ExposuresDimension")
    PDxEV = []
    for pd in PDseq:
        if pd.context is not None:
            pdDim = pd.context.dimValue(dimQname)
            for ev in EVseq:
                if ev.context is not None:
                    evDim = ev.context.dimValue(dimQname)
                    if pdDim is not None and isinstance(pdDim,ModelDimensionValue):
                        dimEqual =  pdDim.isEqualTo(evDim, equalMode=XbrlUtil.S_EQUAL2)
                    elif evDim is not None and isinstance(evDim,ModelDimensionValue):
                        dimEqual =  evDim.isEqualTo(pdDim, equalMode=XbrlUtil.S_EQUAL2)
                    else:
                        dimEqual = (pdDim == evDim)
                    if dimEqual:
                        pdX = pd.xValue
                        evX = ev.xValue
                        # type promotion required
                        if isinstance(pdX,Decimal) and isinstance(evX,float):
                            pdX = float(pdX)
                        elif isinstance(evX,Decimal) and isinstance(pdX,float):
                            pdX = float(evX)
                        PDxEV.append(pdX * evX)
                        break
    return PDxEV


customFunctions = {
    qname("{http://www.example.com/wgt-avg/function}my-fn:PDxEV"): my_fn_PDxEV
}

'''
Created on Oct 3, 2010

@author: Mark V Systems Limited
(c) Copyright 2010 Mark V Systems Limited, All rights reserved.
'''
from collections import defaultdict
import os, sys, traceback, uuid
import logging
from decimal import Decimal
from arelle import UrlUtil, XmlUtil, ModelValue, XbrlConst, XmlValidate
from arelle.FileSource import FileNamedStringIO
from arelle.ModelObject import ModelObject, ObjectPropertyViewWrapper
from arelle.Locale import format_string
from arelle.PluginManager import pluginClassMethods
from arelle.PrototypeInstanceObject import FactPrototype, DimValuePrototype
from arelle.PythonUtil import flattenSequence
from arelle.UrlUtil import isHttpUrl
from arelle.ValidateXbrlDimensions import isFactDimensionallyValid
ModelRelationshipSet = None # dynamic import
ModelFact = None

profileStatNumber = 0

AUTO_LOCATE_ELEMENT = '771407c0-1d0c-11e1-be5e-028037ec0200' # singleton meaning choose best location for new element
DEFAULT = sys.intern(_STR_8BIT("default"))
NONDEFAULT = sys.intern(_STR_8BIT("non-default"))
DEFAULTorNONDEFAULT = sys.intern(_STR_8BIT("default-or-non-default"))
    

def load(modelManager, url, nextaction=None, base=None, useFileSource=None, errorCaptureLevel=None, **kwargs):
    """Each loaded instance, DTS, testcase, testsuite, versioning report, or RSS feed, is represented by an 
    instance of a ModelXbrl object. The ModelXbrl object has a collection of ModelDocument objects, each 
    representing an XML document (for now, with SQL whenever its time comes). One of the modelDocuments of 
    the ModelXbrl is the entry point (of discovery or of the test suite).
    
    :param url: may be a filename or FileSource object
    :type url: str or FileSource
    :param nextaction: text to use as status line prompt on conclusion of loading and discovery
    :type nextaction: str
    :param base: the base URL if any (such as a versioning report's URL when loading to/from DTS modelXbrl).
    :type base: str
    :param useFileSource: for internal use (when an entry point is in a FileSource archive and discovered files expected to also be in the entry point's archive.
    :type useFileSource: bool
    :returns: ModelXbrl -- a new modelXbrl, performing DTS discovery for instance, inline XBRL, schema, linkbase, and versioning report entry urls
   """
    if nextaction is None: nextaction = _("loading")
    from arelle import (ModelDocument, FileSource)
    modelXbrl = create(modelManager, errorCaptureLevel=errorCaptureLevel)
    if useFileSource is not None:
        modelXbrl.fileSource = useFileSource
        modelXbrl.closeFileSource = False
        url = url
    elif isinstance(url,FileSource.FileSource):
        modelXbrl.fileSource = url
        modelXbrl.closeFileSource= True
        url = modelXbrl.fileSource.url
    else:
        modelXbrl.fileSource = FileSource.FileSource(url, modelManager.cntlr)
        modelXbrl.closeFileSource= True
    modelXbrl.modelDocument = ModelDocument.load(modelXbrl, url, base, isEntry=True, **kwargs)
    del modelXbrl.entryLoadingUrl
    loadSchemalocatedSchemas(modelXbrl)
    
    #from arelle import XmlValidate
    #uncomment for trial use of lxml xml schema validation of entry document
    #XmlValidate.xmlValidate(modelXbrl.modelDocument)
    modelManager.cntlr.webCache.saveUrlCheckTimes()
    modelManager.showStatus(_("xbrl loading finished, {0}...").format(nextaction))
    return modelXbrl

def create(modelManager, newDocumentType=None, url=None, schemaRefs=None, createModelDocument=True, isEntry=False, errorCaptureLevel=None, initialXml=None, initialComment=None, base=None, discover=True):
    from arelle import (ModelDocument, FileSource)
    modelXbrl = ModelXbrl(modelManager, errorCaptureLevel=errorCaptureLevel)
    modelXbrl.locale = modelManager.locale
    if newDocumentType:
        modelXbrl.fileSource = FileSource.FileSource(url, modelManager.cntlr) # url may be an open file handle, use str(url) below
        modelXbrl.closeFileSource= True
        if createModelDocument:
            modelXbrl.modelDocument = ModelDocument.create(modelXbrl, newDocumentType, str(url), schemaRefs=schemaRefs, isEntry=isEntry, initialXml=initialXml, initialComment=initialComment, base=base, discover=discover)
            if isEntry:
                del modelXbrl.entryLoadingUrl
                loadSchemalocatedSchemas(modelXbrl)
    return modelXbrl
    
def loadSchemalocatedSchemas(modelXbrl):
    from arelle import ModelDocument
    if modelXbrl.modelDocument is not None and modelXbrl.modelDocument.type < ModelDocument.Type.DTSENTRIES:
        # at this point DTS is fully discovered but schemaLocated xsd's are not yet loaded
        modelDocumentsSchemaLocated = set()
        while True: # need this logic because each new pass may add new urlDocs
            modelDocuments = set(modelXbrl.urlDocs.values()) - modelDocumentsSchemaLocated
            if not modelDocuments:
                break
            modelDocument = modelDocuments.pop()
            modelDocumentsSchemaLocated.add(modelDocument)
            modelDocument.loadSchemalocatedSchemas()
        
class ModelXbrl:
    """
    .. class:: ModelXbrl(modelManager)
    
    ModelXbrl objects represent loaded instances and inline XBRL instances and their DTSes, DTSes 
    (without instances), versioning reports, testcase indexes, testcase variation documents, and 
    other document-centric loadable objects.
    
    :param modelManager: The controller's modelManager object for the current session or command line process.
    :type modelManager: ModelManager

        .. attribute:: urlDocs
        
        Dict, by URL, of loaded modelDocuments
        
        .. attribute:: errorCaptureLevel
        
        Minimum logging level to capture in errors list (default is INCONSISTENCY)
        
        .. attribute:: errors
        
        Captured error codes (at or over minimum error capture logging level) and assertion results, which were sent to logger, via log() methods, used for validation and post-processing
        
        .. attribute:: logErrorCount, logWarningCoutn, logInfoCount
        
        Counts of respective error levels processed by modelXbrl logger

        .. attribute:: arcroleTypes

        Dict by arcrole of defining modelObjects
        
        .. attribute:: roleTypes

        Dict by role of defining modelObjects

        .. attribute:: qnameConcepts

        Dict by qname (QName) of all top level schema elements, regardless of whether discovered or not discoverable (not in DTS)
        
        .. attribute:: qnameAttributes
        
        Dict by qname of all top level schema attributes

        .. attribute:: qnameAttributeGroups

        Dict by qname of all top level schema attribute groups

        .. attribute:: qnameTypes

        Dict by qname of all top level and anonymous types

        .. attribute:: baseSets
        
        Dict of base sets by (arcrole, linkrole, arc qname, link qname), (arcrole, linkrole, *, *), (arcrole, *, *, *), and in addition, collectively for dimensions, formula,  and rendering, as arcroles 'XBRL-dimensions', 'XBRL-formula', and 'Table-rendering'.

        .. attribute:: relationshipSets

        Dict of effective relationship sets indexed same as baseSets (including collective indices), but lazily resolved when requested.

        .. attribute:: qnameDimensionDefaults

        Dict of dimension defaults by qname of dimension

        .. attribute:: facts

        List of top level facts (not nested in tuples), document order

        .. attribute:: factsInInstance

        List of all facts in instance (including nested in tuples), document order

        .. attribute:: contexts

        Dict of contexts by id

        .. attribute:: units

        Dict of units by id

        .. attribute:: modelObjects

        Model objects in loaded order, allowing object access by ordinal index (for situations, such as tkinter, where a reference to an object would create a memory freeing difficulty).

        .. attribute:: qnameParameters

        Dict of formula parameters by their qname

        .. attribute:: modelVariableSets

        Set of variableSets in formula linkbases

        .. attribute:: modelCustomFunctionSignatures

        Dict of custom function signatures by qname and by qname,arity

        .. attribute:: modelCustomFunctionImplementations

        Dict of custom function implementations by qname

        .. attribute:: views

        List of view objects

        .. attribute:: langs

        Set of langs in use by modelXbrl

        .. attribute:: labelRoles

        Set of label roles in use by modelXbrl's linkbases

        .. attribute:: hasXDT

        True if dimensions discovered

        .. attribute:: hasTableRendering

        True if table rendering discovered

        .. attribute:: hasTableIndexing

        True if table indexing discovered

        .. attribute:: hasFormulae

        True if formulae discovered

        .. attribute:: formulaOutputInstance

        Standard output instance if formulae produce one. 

        .. attribute:: hasRendering

        True if rendering tables are discovered

        .. attribute:: Log
        
        Logger for modelXbrl

    """
    
    def __init__(self, modelManager, errorCaptureLevel=None):
        self.modelManager = modelManager
        self.skipDTS = modelManager.skipDTS
        self.init(errorCaptureLevel=errorCaptureLevel)
        
    def init(self, keepViews=False, errorCaptureLevel=None):
        self.uuid = uuid.uuid1().urn
        self.namespaceDocs = defaultdict(list)
        self.urlDocs = {}
        self.urlUnloadableDocs = {}  # if entry is True, entry is blocked and unloadable, False means loadable but warned
        self.errorCaptureLevel = (errorCaptureLevel or logging._checkLevel("INCONSISTENCY"))
        self.errors = []
        self.logCount = {}
        self.arcroleTypes = defaultdict(list)
        self.roleTypes = defaultdict(list)
        self.qnameConcepts = {} # indexed by qname of element
        self.nameConcepts = defaultdict(list) # contains ModelConcepts by name 
        self.qnameAttributes = {}
        self.qnameAttributeGroups = {}
        self.qnameGroupDefinitions = {}
        self.qnameTypes = {} # contains ModelTypes by qname key of type
        self.baseSets = defaultdict(list) # contains ModelLinks for keys arcrole, arcrole#linkrole
        self.relationshipSets = {} # contains ModelRelationshipSets by bas set keys
        self.qnameDimensionDefaults = {} # contains qname of dimension (index) and default member(value)
        self.facts = []
        self.factsInInstance = set()
        self.undefinedFacts = [] # elements presumed to be facts but not defined
        self.contexts = {}
        self.units = {}
        self.modelObjects = []
        self.qnameParameters = {}
        self.modelVariableSets = set()
        self.modelCustomFunctionSignatures = {}
        self.modelCustomFunctionImplementations = set()
        self.modelRenderingTables = set()
        if not keepViews:
            self.views = []
        self.langs = {self.modelManager.defaultLang}
        from arelle.XbrlConst import standardLabel
        self.labelroles = {standardLabel}
        self.hasXDT = False
        self.hasTableRendering = False
        self.hasTableIndexing = False
        self.hasFormulae = False
        self.formulaOutputInstance = None
        self.logger = logging.getLogger("arelle")
        self.logRefObjectProperties = getattr(self.logger, "logRefObjectProperties", False)
        self.logRefHasPluginAttrs = any(True for m in pluginClassMethods("Logging.Ref.Attributes"))
        self.logRefHasPluginProperties = any(True for m in pluginClassMethods("Logging.Ref.Properties"))
        self.profileStats = {}
        self.schemaDocsToValidate = set()
        self.modelXbrl = self # for consistency in addressing modelXbrl
        self.arelleUnitTests = {} # unit test entries (usually from processing instructions
        for pluginXbrlMethod in pluginClassMethods("ModelXbrl.Init"):
            pluginXbrlMethod(self)


    def close(self):
        """Closes any views, formula output instances, modelDocument(s), and dereferences all memory used 
        """
        if not self.isClosed:
            self.closeViews()
            if self.formulaOutputInstance:
                self.formulaOutputInstance.close()
            if hasattr(self,"fileSource") and self.closeFileSource:
                self.fileSource.close()
            modelDocument = getattr(self,"modelDocument",None)
            urlDocs = getattr(self,"urlDocs",None)
            for relSet in self.relationshipSets.values():
                relSet.clear()
            self.__dict__.clear() # dereference everything before closing document
            if modelDocument:
                modelDocument.close(urlDocs=urlDocs)
            
    @property
    def isClosed(self):
        """
        :returns:  bool -- True if closed (python object has deferenced and deleted all attributes after closing)
        """
        return not bool(self.__dict__)  # closed when dict is empty
            
    def reload(self,nextaction,reloadCache=False):
        """Reloads all model objects from their original entry point URL, preserving any open views (which are reloaded).
        
        :param nextAction: status line text string, if any, to show upon completion
        :type nextAction: str
        :param reloadCache: True to force clearing and reloading of web cache, if working online.
        :param reloadCache: bool
        """
        from arelle import ModelDocument
        self.init(keepViews=True)
        self.modelDocument = ModelDocument.load(self, self.fileSource.url, isEntry=True, reloadCache=reloadCache)
        self.modelManager.showStatus(_("xbrl loading finished, {0}...").format(nextaction),5000)
        self.modelManager.reloadViews(self)
            
    def closeViews(self):
        """Close views associated with this modelXbrl
        """
        if not self.isClosed:
            for view in range(len(self.views)):
                if len(self.views) > 0:
                    self.views[0].close()
        
    def relationshipSet(self, arcrole, linkrole=None, linkqname=None, arcqname=None, includeProhibits=False):
        """Returns a relationship set matching specified parameters (only arcrole is required).
        
        Resolve and determine relationship set.  If a relationship set of the same parameters was previously resolved, it is returned from a cache.
        
        :param arcrole: Required arcrole, or special collective arcroles 'XBRL-dimensions', 'XBRL-formula', and 'Table-rendering'
        :type arcrole: str
        :param linkrole: Linkrole (wild if None)
        :type linkrole: str
        :param arcqname: Arc element qname (wild if None)
        :type arcqname: QName
        :param includeProhibits: True to include prohibiting arc elements as relationships
        :type includeProhibits: bool
        :returns: [ModelRelationship] -- Ordered list of effective relationship objects per parameters
        """
        global ModelRelationshipSet
        if ModelRelationshipSet is None:
            from arelle import ModelRelationshipSet
        key = (arcrole, linkrole, linkqname, arcqname, includeProhibits)
        if key not in self.relationshipSets:
            ModelRelationshipSet.create(self, arcrole, linkrole, linkqname, arcqname, includeProhibits)
        return self.relationshipSets[key]
    
    def baseSetModelLink(self, linkElement):
        for modelLink in self.baseSets[("XBRL-footnotes",None,None,None)]:
            if modelLink == linkElement:
                return modelLink
        return None
    
    def roleTypeDefinition(self, roleURI):
        modelRoles = self.roleTypes.get(roleURI, ())
        if modelRoles:
            return modelRoles[0].definition or roleURI
        return roleURI
    
    def roleTypeName(self, roleURI):
        # authority-specific role type name
        for pluginXbrlMethod in pluginClassMethods("ModelXbrl.RoleTypeName"):
            _roleTypeName = pluginXbrlMethod(self, roleURI)
            if _roleTypeName:
                return _roleTypeName
        return self.roleTypeDefinition(roleURI)
    
    def matchSubstitutionGroup(self, elementQname, subsGrpMatchTable):
        """Resolve a subsitutionGroup for the elementQname from the match table
        
        Used by ModelObjectFactory to return Class type for new ModelObject subclass creation, and isInSubstitutionGroup
        
        :param elementQname: Element/Concept QName to find substitution group
        :type elementQname: QName
        :param subsGrpMatchTable: Table of substitutions used to determine xml proxy object class for xml elements and substitution group membership
        :type subsGrpMatchTable: dict
        :returns: object -- value matching subsGrpMatchTable key
        """
        if elementQname in subsGrpMatchTable:
            return subsGrpMatchTable[elementQname] # head of substitution group
        elementMdlObj = self.qnameConcepts.get(elementQname)
        if elementMdlObj is not None:
            subsGrpMdlObj = elementMdlObj.substitutionGroup
            while subsGrpMdlObj is not None:
                subsGrpQname = subsGrpMdlObj.qname
                if subsGrpQname in subsGrpMatchTable:
                    return subsGrpMatchTable[subsGrpQname]
                subsGrpMdlObj = subsGrpMdlObj.substitutionGroup
        return subsGrpMatchTable.get(None)
    
    def isInSubstitutionGroup(self, elementQname, subsGrpQnames):
        """Determine if element is in substitution group(s)
        
        Used by ModelObjectFactory to return Class type for new ModelObject subclass creation, and isInSubstitutionGroup
        
        :param elementQname: Element/Concept QName to determine if in substitution group(s)
        :type elementQname: QName
        :param subsGrpQnames: QName or list of QNames
        :type subsGrpMatchTable: QName or [QName]
        :returns: bool -- True if element is in any substitution group
        """
        return self.matchSubstitutionGroup(elementQname, {
                  qn:(qn is not None) for qn in (subsGrpQnames if hasattr(subsGrpQnames, '__iter__') else (subsGrpQnames,)) + (None,)})
    
    def createInstance(self, url=None):
        """Creates an instance document for a DTS which didn't have an instance document, such as
        to create a new instance for a DTS which was loaded from a taxonomy or linkbase entry point.
        
        :param url: File name to save the new instance document
        :type url: str
        """
        from arelle import (ModelDocument, FileSource)
        if self.modelDocument.type == ModelDocument.Type.INSTANCE: 
            # entry already is an instance, delete facts etc.
            del self.facts[:]
            self.factsInInstance.clear()
            del self.undefinedFacts[:]
            self.contexts.clear()
            self.units.clear()
            self.modelDocument.idObjects.clear
            del self.modelDocument.hrefObjects[:]
            self.modelDocument.schemaLocationElements.clear()
            self.modelDocument.referencedNamespaces.clear()
            for child in list(self.modelDocument.xmlRootElement):
                if not (isinstance(child, ModelObject) and child.namespaceURI == XbrlConst.link and 
                        child.localName.endswith("Ref")): # remove contexts, facts, footnotes
                    self.modelDocument.xmlRootElement.remove(child)
        else:
            priorFileSource = self.fileSource
            self.fileSource = FileSource.FileSource(url, self.modelManager.cntlr)
            if isHttpUrl(self.uri):
                schemaRefUri = self.uri
            else:   # relativize local paths
                schemaRefUri = os.path.relpath(self.uri, os.path.dirname(url))
            self.modelDocument = ModelDocument.create(self, ModelDocument.Type.INSTANCE, url, schemaRefs=[schemaRefUri], isEntry=True)
            if priorFileSource:
                priorFileSource.close()
            self.closeFileSource= True
            del self.entryLoadingUrl
        # reload dts views
        from arelle import ViewWinDTS
        for view in self.views:
            if isinstance(view, ViewWinDTS.ViewDTS):
                self.modelManager.cntlr.uiThreadQueue.put((view.view, []))
                
    def saveInstance(self, **kwargs):
        """Saves current instance document file.
        
        :param overrideFilepath: specify to override saving in instance's modelDocument.filepath
        """
        self.modelDocument.save(**kwargs)
            
    @property    
    def prefixedNamespaces(self):
        """Dict of prefixes for namespaces defined in DTS
        """
        prefixedNamespaces = {}
        for nsDocs in self.namespaceDocs.values():
            for nsDoc in nsDocs:
                ns = nsDoc.targetNamespace
                if ns:
                    prefix = XmlUtil.xmlnsprefix(nsDoc.xmlRootElement, ns)
                    if prefix and prefix not in prefixedNamespaces:
                        prefixedNamespaces[prefix] = ns
        return prefixedNamespaces 
    
    def matchContext(self, entityIdentScheme, entityIdentValue, periodType, periodStart, periodEndInstant, dims, segOCCs, scenOCCs):
        """Finds matching context, by aspects, as in formula usage, if any
        
        :param entityIdentScheme: Scheme to match
        :type entityIdentScheme: str
        :param entityIdentValue: Entity identifier value to match
        :type entityIdentValue: str
        :param periodType: Period type to match ("instant", "duration", or "forever")
        :type periodType: str
        :param periodStart: Date or dateTime of period start
        :type periodStart: ModelValue.DateTime, datetime.date or datetime.datetime
        :param periodEndInstant: Date or dateTime of period send
        :type periodEndInstant: ModelValue.DateTime, datetime.date or datetime.datetime
        :param dims: Dimensions
        :type dims: ModelDimension or QName
        :param segOCCs: Segment non-dimensional nodes
        :type segOCCs: lxml element
        :param scenOCCs: Scenario non-dimensional nodes
        :type scenOCCs: lxml element
        :returns: ModelContext -- Matching context or None
        """
        from arelle.ModelFormulaObject import Aspect
        from arelle.ModelValue import dateUnionEqual
        from arelle.XbrlUtil import sEqual
        if dims: segAspect, scenAspect = (Aspect.NON_XDT_SEGMENT, Aspect.NON_XDT_SCENARIO)
        else: segAspect, scenAspect = (Aspect.COMPLETE_SEGMENT, Aspect.COMPLETE_SCENARIO)
        for c in self.contexts.values():
            if (c.entityIdentifier == (entityIdentScheme, entityIdentValue) and
                ((c.isInstantPeriod and periodType == "instant" and dateUnionEqual(c.instantDatetime, periodEndInstant, instantEndDate=True)) or
                 (c.isStartEndPeriod and periodType == "duration" and dateUnionEqual(c.startDatetime, periodStart) and dateUnionEqual(c.endDatetime, periodEndInstant, instantEndDate=True)) or
                 (c.isForeverPeriod and periodType == "forever")) and
                 # dimensions match if dimensional model
                 (dims is None or (
                    (c.qnameDims.keys() == dims.keys()) and
                        all([cDim.isEqualTo(dims[cDimQn]) for cDimQn, cDim in c.qnameDims.items()]))) and
                 # OCCs match for either dimensional or non-dimensional modle
                 all(
                   all([sEqual(self, cOCCs[i], mOCCs[i]) for i in range(len(mOCCs))])
                     if len(cOCCs) == len(mOCCs) else False
                        for cOCCs,mOCCs in ((c.nonDimValues(segAspect),segOCCs),
                                            (c.nonDimValues(scenAspect),scenOCCs)))
                ):
                    return c
        return None
                 
    def createContext(self, entityIdentScheme, entityIdentValue, periodType, periodStart, periodEndInstant, priItem, dims, segOCCs, scenOCCs,
                      afterSibling=None, beforeSibling=None, id=None):
        """Creates a new ModelContext and validates (integrates into modelDocument object model).
        
        :param entityIdentScheme: Scheme to match
        :type entityIdentScheme: str
        :param entityIdentValue: Entity identifier value to match
        :type entityIdentValue: str
        :param periodType: Period type to match ("instant", "duration", or "forever")
        :type periodType: str
        :param periodStart: Date or dateTime of period start
        :type periodStart: ModelValue.DateTime, datetime.date or datetime.datetime
        :param periodEndInstant: Date or dateTime of period send
        :type periodEndInstant: ModelValue.DateTime, datetime.date or datetime.datetime
        :param dims: Dimensions
        :type dims: ModelDimension or QName
        :param segOCCs: Segment non-dimensional nodes
        :type segOCCs: lxml element
        :param scenOCCs: Scenario non-dimensional nodes
        :type scenOCCs: lxml element
        :param beforeSibling: lxml element in instance to insert new concept before
        :type beforeSibling: ModelObject
        :param afterSibling: lxml element in instance to insert new concept after
        :type afterSibling: ModelObject
        :param id: id to assign to new context, if absent an id will be generated
        :type id: str
        :returns: ModelContext -- New model context object
        """
        xbrlElt = self.modelDocument.xmlRootElement
        if afterSibling == AUTO_LOCATE_ELEMENT:
            afterSibling = XmlUtil.lastChild(xbrlElt, XbrlConst.xbrli, ("schemaLocation", "roleType", "arcroleType", "context"))
        cntxId = id if id else 'c-{0:02n}'.format( len(self.contexts) + 1)
        newCntxElt = XmlUtil.addChild(xbrlElt, XbrlConst.xbrli, "context", attributes=("id", cntxId),
                                      afterSibling=afterSibling, beforeSibling=beforeSibling)
        entityElt = XmlUtil.addChild(newCntxElt, XbrlConst.xbrli, "entity")
        XmlUtil.addChild(entityElt, XbrlConst.xbrli, "identifier",
                            attributes=("scheme", entityIdentScheme),
                            text=entityIdentValue)
        periodElt = XmlUtil.addChild(newCntxElt, XbrlConst.xbrli, "period")
        if periodType == "forever":
            XmlUtil.addChild(periodElt, XbrlConst.xbrli, "forever")
        elif periodType == "instant":
            XmlUtil.addChild(periodElt, XbrlConst.xbrli, "instant", 
                             text=XmlUtil.dateunionValue(periodEndInstant, subtractOneDay=True))
        elif periodType == "duration":
            XmlUtil.addChild(periodElt, XbrlConst.xbrli, "startDate", 
                             text=XmlUtil.dateunionValue(periodStart))
            XmlUtil.addChild(periodElt, XbrlConst.xbrli, "endDate", 
                             text=XmlUtil.dateunionValue(periodEndInstant, subtractOneDay=True))
        segmentElt = None
        scenarioElt = None
        from arelle.ModelInstanceObject import ModelDimensionValue
        if dims: # requires primary item to determin ambiguous concepts
            ''' in theory we have to check full set of dimensions for validity in source or any other
                context element, but for shortcut will see if each dimension is already reported in an
                unambiguous valid contextElement
            '''
            if priItem is not None: # creating concept for a specific fact
                dims[2] = priItem # Aspect.CONCEPT: prototype needs primary item as an aspect
                fp = FactPrototype(self, dims)
                del dims[2] # Aspect.CONCEPT
                # force trying a valid prototype's context Elements
                if not isFactDimensionallyValid(self, fp, setPrototypeContextElements=True):
                    self.info("arelle:info",
                        _("Create context for %(priItem)s, cannot determine valid context elements, no suitable hypercubes"), 
                        modelObject=self, priItem=priItem)
                fpDims = fp.context.qnameDims
            else:
                fpDims = dims # dims known to be valid (such as for inline extraction) 
            for dimQname in sorted(fpDims.keys()):
                dimValue = fpDims[dimQname]
                if isinstance(dimValue, (DimValuePrototype,ModelDimensionValue)):
                    dimMemberQname = dimValue.memberQname  # None if typed dimension
                    contextEltName = dimValue.contextElement
                else: # qname for explicit or node for typed
                    dimMemberQname = None
                    contextEltName = None
                if contextEltName == "segment":
                    if segmentElt is None: 
                        segmentElt = XmlUtil.addChild(entityElt, XbrlConst.xbrli, "segment")
                    contextElt = segmentElt
                elif contextEltName == "scenario":
                    if scenarioElt is None: 
                        scenarioElt = XmlUtil.addChild(newCntxElt, XbrlConst.xbrli, "scenario")
                    contextElt = scenarioElt
                else:
                    self.info("arelleLinfo",
                        _("Create context, %(dimension)s, cannot determine context element, either no all relationship or validation issue"), 
                        modelObject=self, dimension=dimQname),
                    continue
                dimAttr = ("dimension", XmlUtil.addQnameValue(xbrlElt, dimQname))
                if dimValue.isTyped:
                    dimElt = XmlUtil.addChild(contextElt, XbrlConst.xbrldi, "xbrldi:typedMember", 
                                              attributes=dimAttr)
                    if isinstance(dimValue, (ModelDimensionValue, DimValuePrototype)) and dimValue.isTyped:
                        XmlUtil.copyNodes(dimElt, dimValue.typedMember) 
                elif dimMemberQname:
                    dimElt = XmlUtil.addChild(contextElt, XbrlConst.xbrldi, "xbrldi:explicitMember",
                                              attributes=dimAttr,
                                              text=XmlUtil.addQnameValue(xbrlElt, dimMemberQname))
        if segOCCs:
            if segmentElt is None: 
                segmentElt = XmlUtil.addChild(entityElt, XbrlConst.xbrli, "segment")
            XmlUtil.copyNodes(segmentElt, segOCCs)
        if scenOCCs:
            if scenarioElt is None: 
                scenarioElt = XmlUtil.addChild(newCntxElt, XbrlConst.xbrli, "scenario")
            XmlUtil.copyNodes(scenarioElt, scenOCCs)
                
        XmlValidate.validate(self, newCntxElt)
        self.modelDocument.contextDiscover(newCntxElt)
        return newCntxElt
        
        
    def matchUnit(self, multiplyBy, divideBy):
        """Finds matching unit, by measures, as in formula usage, if any
        
        :param multiplyBy: List of multiply-by measure QNames (or top level measures if no divideBy)
        :type multiplyBy: [QName]
        :param divideBy: List of multiply-by measure QNames (or empty list if no divideBy)
        :type divideBy: [QName]
        :returns: ModelUnit -- Matching unit object or None
        """
        multiplyBy.sort()
        divideBy.sort()
        for u in self.units.values():
            if u.measures == (multiplyBy,divideBy):
                return u
        return None

    def createUnit(self, multiplyBy, divideBy, afterSibling=None, beforeSibling=None, id=None):
        """Creates new unit, by measures, as in formula usage, if any
        
        :param multiplyBy: List of multiply-by measure QNames (or top level measures if no divideBy)
        :type multiplyBy: [QName]
        :param divideBy: List of multiply-by measure QNames (or empty list if no divideBy)
        :type divideBy: [QName]
        :param beforeSibling: lxml element in instance to insert new concept before
        :type beforeSibling: ModelObject
        :param afterSibling: lxml element in instance to insert new concept after
        :type afterSibling: ModelObject
        :param id: id to assign to new unit, if absent an id will be generated
        :type id: str
        :returns: ModelUnit -- New unit object
        """
        xbrlElt = self.modelDocument.xmlRootElement
        if afterSibling == AUTO_LOCATE_ELEMENT:
            afterSibling = XmlUtil.lastChild(xbrlElt, XbrlConst.xbrli, ("schemaLocation", "roleType", "arcroleType", "context", "unit"))
        unitId = id if id else 'u-{0:02n}'.format( len(self.units) + 1)
        newUnitElt = XmlUtil.addChild(xbrlElt, XbrlConst.xbrli, "unit", attributes=("id", unitId),
                                      afterSibling=afterSibling, beforeSibling=beforeSibling)
        if len(divideBy) == 0:
            for multiply in multiplyBy:
                XmlUtil.addChild(newUnitElt, XbrlConst.xbrli, "measure", text=XmlUtil.addQnameValue(xbrlElt, multiply))
        else:
            divElt = XmlUtil.addChild(newUnitElt, XbrlConst.xbrli, "divide")
            numElt = XmlUtil.addChild(divElt, XbrlConst.xbrli, "unitNumerator")
            denElt = XmlUtil.addChild(divElt, XbrlConst.xbrli, "unitDenominator")
            for multiply in multiplyBy:
                XmlUtil.addChild(numElt, XbrlConst.xbrli, "measure", text=XmlUtil.addQnameValue(xbrlElt, multiply))
            for divide in divideBy:
                XmlUtil.addChild(denElt, XbrlConst.xbrli, "measure", text=XmlUtil.addQnameValue(xbrlElt, divide))
        XmlValidate.validate(self, newUnitElt)
        self.modelDocument.unitDiscover(newUnitElt)
        return newUnitElt
    
    @property
    def nonNilFactsInInstance(self): # indexed by fact (concept) qname
        """Facts in the instance which are not nil, cached
        
        :returns: set -- non-nil facts in instance
        """
        try:
            return self._nonNilFactsInInstance
        except AttributeError:
            self._nonNilFactsInInstance = set(f for f in self.factsInInstance if not f.isNil)
            return self._nonNilFactsInInstance
        
    @property
    def factsByQname(self): # indexed by fact (concept) qname
        """Facts in the instance indexed by their QName, cached
        
        :returns: dict -- indexes are QNames, values are ModelFacts
        """
        try:
            return self._factsByQname
        except AttributeError:
            self._factsByQname = fbqn = defaultdict(set)
            for f in self.factsInInstance: 
                if f.qname is not None:
                    fbqn[f.qname].add(f)
            return fbqn
        
    def factsByDatatype(self, notStrict, typeQname): # indexed by fact (concept) qname
        """Facts in the instance indexed by data type QName, cached as types are requested

        :param notSctrict: if True, fact may be derived
        :type notStrict: bool
        :returns: set -- ModelFacts that have specified type or (if nonStrict) derived from specified type
        """
        try:
            return self._factsByDatatype[notStrict, typeQname]
        except AttributeError:
            self._factsByDatatype = {}
            return self.factsByDatatype(notStrict, typeQname)
        except KeyError:
            self._factsByDatatype[notStrict, typeQname] = fbdt = set()
            for f in self.factsInInstance:
                c = f.concept
                if c.typeQname == typeQname or (notStrict and c.type.isDerivedFrom(typeQname)):
                    fbdt.add(f)
            return fbdt
        
    def factsByPeriodType(self, periodType): # indexed by fact (concept) qname
        """Facts in the instance indexed by periodType, cached

        :param periodType: Period type to match ("instant", "duration", or "forever")
        :type periodType: str
        :returns: set -- ModelFacts that have specified periodType
        """
        try:
            return self._factsByPeriodType[periodType]
        except AttributeError:
            self._factsByPeriodType = fbpt = defaultdict(set)
            for f in self.factsInInstance:
                p = f.concept.periodType
                if p:
                    fbpt[p].add(f)
            return self.factsByPeriodType(periodType)
        except KeyError:
            return set()  # no facts for this period type
        
    def factsByDimMemQname(self, dimQname, memQname=None): # indexed by fact (concept) qname
        """Facts in the instance indexed by their Dimension  and Member QName, cached
        
        :returns: dict -- indexes are (Dimension, Member) and (Dimension) QNames, values are ModelFacts
        If Member is None, returns facts that have the dimension (explicit or typed)
        If Member is NONDEFAULT, returns facts that have the dimension (explicit non-default or typed)
        If Member is DEFAULT, returns facts that have the dimension (explicit non-default or typed) defaulted
        """
        try:
            fbdq = self._factsByDimQname[dimQname]
            return fbdq[memQname]
        except AttributeError:
            self._factsByDimQname = {}
            return self.factsByDimMemQname(dimQname, memQname)
        except KeyError:
            self._factsByDimQname[dimQname] = fbdq = defaultdict(set)
            for fact in self.factsInInstance: 
                if fact.isItem and fact.context is not None:
                    dimValue = fact.context.dimValue(dimQname)
                    if isinstance(dimValue, ModelValue.QName):  # explicit dimension default value
                        fbdq[None].add(fact) # set of all facts that have default value for dimension
                        if dimQname in self.modelXbrl.qnameDimensionDefaults:
                            fbdq[self.qnameDimensionDefaults[dimQname]].add(fact) # set of facts that have this dim and mem
                            fbdq[DEFAULT].add(fact) # set of all facts that have default value for dimension
                    elif dimValue is not None: # not default
                        fbdq[None].add(fact) # set of all facts that have default value for dimension
                        fbdq[NONDEFAULT].add(fact) # set of all facts that have non-default value for dimension
                        if dimValue.isExplicit:
                            fbdq[dimValue.memberQname].add(fact) # set of facts that have this dim and mem
                    else: # default typed dimension
                        fbdq[DEFAULT].add(fact)
            return fbdq[memQname]
        
    def matchFact(self, otherFact, unmatchedFactsStack=None, deemP0inf=False):
        """Finds matching fact, by XBRL 2.1 duplicate definition (if tuple), or by
        QName and VEquality (if an item), lang and accuracy equality, as in formula and test case usage
        
        :param otherFact: Fact to match
        :type otherFact: ModelFact
        :deemP0inf: boolean for formula validation to deem P0 facts to be VEqual as if they were P=INF
        :returns: ModelFact -- Matching fact or None
        """
        for fact in self.facts:
            if (fact.isTuple):
                if otherFact.isDuplicateOf(fact, unmatchedFactsStack=unmatchedFactsStack):
                    return fact
            elif (fact.qname == otherFact.qname and fact.isVEqualTo(otherFact, deemP0inf=deemP0inf)):
                if not fact.isNumeric:
                    if fact.xmlLang == otherFact.xmlLang:
                        return fact
                else:
                    if (fact.decimals == otherFact.decimals and
                        fact.precision == otherFact.precision):
                        return fact
        return None
            
    def createFact(self, conceptQname, attributes=None, text=None, parent=None, afterSibling=None, beforeSibling=None, validate=True):
        """Creates new fact, as in formula output instance creation, and validates into object model
        
        :param conceptQname: QNames of concept
        :type conceptQname: QName
        :param attributes: Tuple of name, value, or tuples of name, value tuples (name,value) or ((name,value)[,(name,value...)]), where name is either QName or clark-notation name string
        :param text: Text content of fact (will be converted to xpath compatible str by FunctionXS.xsString)
        :type text: object
        :param parent: lxml element in instance to append as child of
        :type parent: ModelObject
        :param beforeSibling: lxml element in instance to insert new concept before
        :type beforeSibling: ModelObject
        :param afterSibling: lxml element in instance to insert new concept after
        :type afterSibling: ModelObject
        :param validate: specify False to block XML Validation (required when constructing a tuple which is invalid until after it's contents are created)
        :type validate: boolean
        :returns: ModelFact -- New fact object
        """
        if parent is None: parent = self.modelDocument.xmlRootElement
        self.makeelementParentModelObject = parent
        newFact = XmlUtil.addChild(parent, conceptQname, attributes=attributes, text=text,
                                   afterSibling=afterSibling, beforeSibling=beforeSibling)
        global ModelFact
        if ModelFact is None:
            from arelle.ModelInstanceObject import ModelFact
        if hasattr(self, "_factsByQname"):
            self._factsByQname[newFact.qname].add(newFact)
        if not isinstance(newFact, ModelFact):
            return newFact # unable to create fact for this concept OR DTS not loaded for target instance (e.g., inline extraction, summary output)
        del self.makeelementParentModelObject
        if validate:
            XmlValidate.validate(self, newFact)
        self.modelDocument.factDiscover(newFact, parentElement=parent)
        # update cached sets
        if not newFact.isNil and hasattr(self, "_nonNilFactsInInstance"):
            self._nonNilFactsInInstance.add(newFact)
        if newFact.concept is not None:
            if hasattr(self, "_factsByDatatype"):
                del self._factsByDatatype # would need to iterate derived type ancestry to populate
            if hasattr(self, "_factsByPeriodType"):
                self._factsByPeriodType[newFact.concept.periodType].add(newFact)
            if hasattr(self, "_factsByDimQname"):
                del self._factsByDimQname
        self.setIsModified()
        return newFact    
        
    def setIsModified(self):
        """Records that the underlying document has been modified.
        """
        self.modelDocument.isModified = True

    def isModified(self):
        """Check if the underlying document has been modified.
        """
        md = self.modelDocument
        if md is not None:
            return md.isModified
        else:
            return False

    def modelObject(self, objectId):
        """Finds a model object by an ordinal ID which may be buried in a tkinter view id string (e.g., 'somedesignation_ordinalnumber').
        
        :param objectId: string which includes _ordinalNumber, produced by ModelObject.objectId(), or integer object index
        :type objectId: str or int
        :returns: ModelObject
        """
        if isinstance(objectId, _INT_TYPES):  # may be long or short in 2.7
            return self.modelObjects[objectId]
        # assume it is a string with ID in a tokenized representation, like xyz_33
        try:
            return self.modelObjects[_INT(objectId.rpartition("_")[2])]
        except (IndexError, ValueError):
            return None
    
    # UI thread viewModelObject
    def viewModelObject(self, objectId):
        """Finds model object, if any, and synchronizes any views displaying it to bring the model object into scrollable view region and highlight it
        :param objectId: string which includes _ordinalNumber, produced by ModelObject.objectId(), or integer object index
        :type objectId: str or int
        """
        modelObject = ""
        try:
            if isinstance(objectId, (ModelObject,FactPrototype)):
                modelObject = objectId
            elif isinstance(objectId, str) and objectId.startswith("_"):
                modelObject = self.modelObject(objectId)
            if modelObject is not None:
                for view in self.views:
                    view.viewModelObject(modelObject)
        except (IndexError, ValueError, AttributeError)as err:
            self.modelManager.addToLog(_("Exception viewing properties {0} {1} at {2}").format(
                            modelObject,
                            err, traceback.format_tb(sys.exc_info()[2])))

    def effectiveMessageCode(self, messageCodes):        
        effectiveMessageCode = None
        _validationType = self.modelManager.disclosureSystem.validationType
        _exclusiveTypesPattern = self.modelManager.disclosureSystem.exclusiveTypesPattern
        
        for argCode in messageCodes if isinstance(messageCodes,tuple) else (messageCodes,):
            if (isinstance(argCode, ModelValue.QName) or
                (_validationType and argCode.startswith(_validationType)) or
                (not _exclusiveTypesPattern or _exclusiveTypesPattern.match(argCode) == None)):
                effectiveMessageCode = argCode
                break
        return effectiveMessageCode

    # isLoggingEffectiveFor( messageCodes= messageCode= level= )
    def isLoggingEffectiveFor(self, **kwargs): # args can be messageCode(s) and level
        logger = self.logger
        if "messageCodes" in kwargs or "messageCode" in kwargs:
            if "messageCodes" in kwargs:
                messageCodes = kwargs["messageCodes"]
            else:
                messageCodes = kwargs["messageCode"]
            messageCode = self.effectiveMessageCode(messageCodes)
            codeEffective = (messageCode and
                             (not logger.messageCodeFilter or logger.messageCodeFilter.match(messageCode))) 
        else:
            codeEffective = True
        if "level" in kwargs and logger.messageLevelFilter:
            levelEffective = logger.messageLevelFilter.match(kwargs["level"].lower())
        else:
            levelEffective = True
        return codeEffective and levelEffective

    def logArguments(self, codes, msg, codedArgs):
        """ Prepares arguments for logger function as per info() below.
        
        If codes includes EFM, GFM, HMRC, or SBR-coded error then the code chosen (if a sequence)
        corresponds to whether EFM, GFM, HMRC, or SBR validation is in effect.
        """
        def propValues(properties):
            # deref objects in properties
            return [(p[0],str(p[1])) if len(p) == 2 else (p[0],str(p[1]),propValues(p[2]))
                    for p in properties if 2 <= len(p) <= 3]
        # determine logCode
        messageCode = self.effectiveMessageCode(codes)
        
        # determine message and extra arguments
        fmtArgs = {}
        extras = {"messageCode":messageCode}
        modelObjectArgs = ()

        for argName, argValue in codedArgs.items():
            if argName in ("modelObject", "modelXbrl", "modelDocument"):
                try:
                    entryUrl = self.modelDocument.uri
                except AttributeError:
                    try:
                        entryUrl = self.entryLoadingUrl
                    except AttributeError:
                        entryUrl = self.fileSource.url
                refs = []
                modelObjectArgs = argValue if isinstance(argValue, (tuple,list,set)) else (argValue,)
                for arg in flattenSequence(modelObjectArgs):
                    if arg is not None:
                        if isinstance(arg, _STR_BASE):
                            objectUrl = arg
                        else:
                            try:
                                objectUrl = arg.modelDocument.uri
                            except AttributeError:
                                try:
                                    objectUrl = self.modelDocument.uri
                                except AttributeError:
                                    objectUrl = self.entryLoadingUrl
                        try:
                            file = UrlUtil.relativeUri(entryUrl, objectUrl)
                        except:
                            file = ""
                        ref = {}
                        if isinstance(arg,(ModelObject, ObjectPropertyViewWrapper)):
                            _arg = arg.modelObject if isinstance(arg, ObjectPropertyViewWrapper) else arg
                            ref["href"] = file + "#" + XmlUtil.elementFragmentIdentifier(_arg)
                            ref["sourceLine"] = _arg.sourceline
                            ref["objectId"] = _arg.objectId()
                            if self.logRefObjectProperties:
                                try:
                                    ref["properties"] = propValues(arg.propertyView)
                                except AttributeError:
                                    pass # is a default properties entry appropriate or needed?
                            if self.logRefHasPluginProperties:
                                refProperties = ref.get("properties", {})
                                for pluginXbrlMethod in pluginClassMethods("Logging.Ref.Properties"):
                                    pluginXbrlMethod(arg, refProperties, codedArgs)
                                if refProperties:
                                    ref["properties"] = refProperties
                        else:
                            ref["href"] = file
                            try:
                                ref["sourceLine"] = arg.sourceline
                            except AttributeError:
                                pass # arg may not have sourceline, ignore if so
                        if self.logRefHasPluginAttrs:
                            refAttributes = {}
                            for pluginXbrlMethod in pluginClassMethods("Logging.Ref.Attributes"):
                                pluginXbrlMethod(arg, refAttributes, codedArgs)
                            if refAttributes:
                                ref["customAttributes"] = refAttributes
                        refs.append(ref)
                extras["refs"] = refs
            elif argName == "sourceFileLine":
                # sourceFileLines is pairs of file and line numbers, e.g., ((file,line),(file2,line2),...)
                ref = {}
                if isinstance(argValue, (tuple,list)):
                    ref["href"] = str(argValue[0])
                    if len(argValue) > 1 and argValue[1]:
                        ref["sourceLine"] = str(argValue[1])
                else:
                    ref["href"] = str(argValue)
                extras["refs"] = [ref]
            elif argName == "sourceFileLines":
                # sourceFileLines is tuple/list of pairs of file and line numbers, e.g., ((file,line),(file2,line2),...)
                refs = []
                for arg in (argValue if isinstance(argValue, (tuple,list)) else (argValue,)):
                    ref = {}
                    if isinstance(arg, (tuple,list)):
                        ref["href"] = str(arg[0])
                        if len(arg) > 1 and arg[1]:
                            ref["sourceLine"] = str(arg[1])
                    else:
                        ref["href"] = str(arg)
                    refs.append(ref)
                extras["refs"] = refs
            elif argName == "sourceLine":
                if isinstance(argValue, _INT_TYPES):    # must be sortable with int's in logger
                    extras["sourceLine"] = argValue
            elif argName not in ("exc_info", "messageCodes"):
                if isinstance(argValue, (ModelValue.QName, ModelObject, bool, FileNamedStringIO,
                                         # might be a set of lxml objects not dereferencable at shutdown 
                                         tuple, list, set)):
                    fmtArgs[argName] = str(argValue)
                elif argValue is None:
                    fmtArgs[argName] = "(none)"
                elif isinstance(argValue, _INT_TYPES):
                    # need locale-dependent formatting
                    fmtArgs[argName] = format_string(self.modelManager.locale, '%i', argValue)
                elif isinstance(argValue,(float,Decimal)):
                    # need locale-dependent formatting
                    fmtArgs[argName] = format_string(self.modelManager.locale, '%f', argValue)
                elif isinstance(argValue, dict):
                    fmtArgs[argName] = argValue
                else:
                    fmtArgs[argName] = str(argValue)

        if "refs" not in extras:
            try:
                file = os.path.basename(self.modelDocument.uri)
            except AttributeError:
                try:
                    file = os.path.basename(self.entryLoadingUrl)
                except:
                    file = ""
            extras["refs"] = [{"href": file}]
        for pluginXbrlMethod in pluginClassMethods("Logging.Message.Parameters"):
            # plug in can rewrite msg string or return msg if not altering msg
            msg = pluginXbrlMethod(messageCode, msg, modelObjectArgs, fmtArgs) or msg
        return (messageCode, 
                (msg, fmtArgs) if fmtArgs else (msg,), 
                extras)

    def debug(self, codes, msg, **args):
        """Same as error(), but as info
        """
        """@messageCatalog=[]"""
        self.log('DEBUG', codes, msg, **args)
                    
    def info(self, codes, msg, **args):
        """Same as error(), but as info
        """
        """@messageCatalog=[]"""
        self.log('INFO', codes, msg, **args)
                    
    def warning(self, codes, msg, **args):
        """Same as error(), but as warning, and no error code saved for Validate
        """
        """@messageCatalog=[]"""
        self.log('WARNING', codes, msg, **args)
                    
    def log(self, level, codes, msg, **args):
        """Same as error(), but level passed in as argument
        """
        logger = self.logger
        messageCode, logArgs, extras = self.logArguments(codes, msg, args)
        if messageCode == "asrtNoLog":
            self.errors.append(args["assertionResults"])
        elif (messageCode and
              (not logger.messageCodeFilter or logger.messageCodeFilter.match(messageCode)) and
              (not logger.messageLevelFilter or logger.messageLevelFilter.match(level.lower()))):
            numericLevel = logging._checkLevel(level)
            self.logCount[numericLevel] = self.logCount.get(numericLevel, 0) + 1
            if numericLevel >= self.errorCaptureLevel:
                self.errors.append(messageCode)
            """@messageCatalog=[]"""
            logger.log(numericLevel, *logArgs, exc_info=args.get("exc_info"), extra=extras)
                    
    def error(self, codes, msg, **args):
        """Logs a message as info, by code, logging-system message text (using %(name)s named arguments 
        to compose string by locale language), resolving model object references (such as qname), 
        to prevent non-dereferencable memory usage.  Supports logging system parameters, and 
        special parameters modelObject, modelXbrl, or modelDocument, to provide trace 
        information to the file, source line, and href (XPath element scheme pointer).  
        Supports the logging exc_info argument.
        
        Args may include a specification of one or more ModelObjects that identify the source of the
        message, as modelObject={single-modelObject, (sequence-of-modelObjects)} or modelXbrl=modelXbrl or
        modelDocument=modelDocument.
        
        Args must include a named argument for each msg %(namedArg)s replacement.
        
        :param codes: Message code or tuple/list of message codes
        :type codes: str or [str]
        :param msg: Message text string to be formatted and replaced with named parameters in **args
        :param **args: Named arguments including modelObject, modelXbrl, or modelDocument, named arguments in msg string, and any exc_info argument.
        :param messageCodes: If first parameter codes, above, is dynamically formatted, this is a documentation string of the message codes only used for extraction of the message catalog document (not used in run-time processing).
        """
        """@messageCatalog=[]"""
        self.log('ERROR', codes, msg, **args)

    def exception(self, codes, msg, **args):
        """Same as error(), but as exception
        """
        """@messageCatalog=[]"""
        self.log('CRITICAL', codes, msg, **args)
        
    def logProfileStats(self):
        """Logs profile stats that were collected
        """
        timeTotal = format_string(self.modelManager.locale, _("%.3f secs"), self.profileStats.get("total", (0,0,0))[1])
        timeEFM = format_string(self.modelManager.locale, _("%.3f secs"), self.profileStats.get("validateEFM", (0,0,0))[1])
        self.info("info:profileStats",
                _("Profile statistics \n") +
                ' \n'.join(format_string(self.modelManager.locale, _("%s %.3f secs, %.0fK"), (statName, statValue[1], statValue[2]), grouping=True)
                           for statName, statValue in sorted(self.profileStats.items(), key=lambda item: item[1])) +
                " \n", # put instance reference on fresh line in traces
                modelObject=self.modelXbrl.modelDocument, profileStats=self.profileStats,
                timeTotal=timeTotal, timeEFM=timeEFM)
    
    def profileStat(self, name=None, stat=None):
        '''
        order 1xx - load, import, setup, etc
        order 2xx - views, 26x - table lb
        3xx diff, other utilities
        5xx validation
        6xx formula
        '''
        if self.modelManager.collectProfileStats:
            import time
            global profileStatNumber
            try:
                if name:
                    thisTime = stat if stat is not None else time.time() - self._startedTimeStat
                    mem = self.modelXbrl.modelManager.cntlr.memoryUsed
                    prevTime = self.profileStats.get(name, (0,0,0))[1]
                    self.profileStats[name] = (profileStatNumber, thisTime + prevTime, mem)
                    profileStatNumber += 1
            except AttributeError:
                pass
            if stat is None:
                self._startedTimeStat = time.time()
        
    def profileActivity(self, activityCompleted=None, minTimeToShow=0):
        """Used to provide interactive GUI messages of long-running processes.
        
        When the time between last profileActivity and this profileActivity exceeds minTimeToShow, then
        the time is logged (if it is shorter than it is not logged), thus providing feedback of long
        running (and possibly troublesome) processing steps.
        
        :param activityCompleted: Description of activity completed, or None if call is just to demark starting of a profiled activity.
        :type activityCompleted: str
        :param minTimeToShow: Seconds of elapsed time for activity, if longer then the profile message appears in the log.
        :type minTimeToShow: seconds
        """
        import time
        try:
            if activityCompleted:
                timeTaken = time.time() - self._startedProfiledActivity
                if timeTaken > minTimeToShow:
                    self.info("info:profileActivity",
                            _("%(activity)s %(time)s secs\n"),
                            modelObject=self.modelXbrl.modelDocument, activity=activityCompleted,
                            time=format_string(self.modelManager.locale, "%.3f", timeTaken, grouping=True))
        except AttributeError:
            pass
        self._startedProfiledActivity = time.time()

    def saveDTSpackage(self):
        """Contributed program to save DTS package as a zip file.  Refactored into a plug-in (and may be removed from main code).
        """ 
        if self.fileSource.isArchive:
            return
        from zipfile import ZipFile 
        import os 
        entryFilename = self.fileSource.url 
        pkgFilename = entryFilename + ".zip" 
        with ZipFile(pkgFilename, 'w') as zip:
            numFiles = 0
            for fileUri in sorted(self.urlDocs.keys()): 
                if not isHttpUrl(fileUri): 
                    numFiles += 1
                    # this has to be a relative path because the hrefs will break
                    zip.write(fileUri, os.path.basename(fileUri)) 
        self.info("info",
                  _("DTS of %(entryFile)s has %(numberOfFiles)s files packaged into %(packageOutputFile)s"), 
                modelObject=self,
                entryFile=os.path.basename(entryFilename), packageOutputFile=pkgFilename, numberOfFiles=numFiles)

'''
Created on Oct 17, 2010

@author: Mark V Systems Limited
(c) Copyright 2010 Mark V Systems Limited, All rights reserved.
'''
from collections import defaultdict
from math import (log10, isnan, isinf, fabs, trunc, fmod, floor, pow)
import decimal
try:
    from regex import compile as re_compile
except ImportError:
    from re import compile as re_compile
import hashlib
from arelle import Locale, XbrlConst, XbrlUtil
from arelle.ModelObject import ObjectPropertyViewWrapper
from arelle.XmlValidate import UNVALIDATED, VALID

numberPattern = re_compile("[-+]?[0]*([1-9]?[0-9]*)([.])?(0*)([1-9]?[0-9]*)?([eE])?([-+]?[0-9]*)?")
ZERO = decimal.Decimal(0)
ONE = decimal.Decimal(1)
NaN = decimal.Decimal("NaN")
floatNaN = float("NaN")
floatINF = float("INF")

def validate(modelXbrl, inferDecimals=False):
    ValidateXbrlCalcs(modelXbrl, inferDecimals).validate()
    
class ValidateXbrlCalcs:
    def __init__(self, modelXbrl, inferDecimals=False):
        self.modelXbrl = modelXbrl
        self.inferDecimals = inferDecimals
        self.mapContext = {}
        self.mapUnit = {}
        self.sumFacts = defaultdict(list)
        self.sumConceptBindKeys = defaultdict(set)
        self.itemFacts = defaultdict(list)
        self.itemConceptBindKeys = defaultdict(set)
        self.duplicateKeyFacts = {}
        self.duplicatedFacts = set()
        self.esAlFacts = defaultdict(list)
        self.esAlConceptBindKeys = defaultdict(set)
        self.conceptsInEssencesAlias = set()
        self.requiresElementFacts = defaultdict(list)
        self.conceptsInRequiresElement = set()
        
    def validate(self):
        if not self.modelXbrl.contexts and not self.modelXbrl.facts:
            return # skip if no contexts or facts
        
        if not self.inferDecimals: # infering precision is now contrary to XBRL REC section 5.2.5.2
            self.modelXbrl.info("xbrl.5.2.5.2:inferringPrecision","Validating calculations inferring precision.")
            
        # identify equal contexts
        self.modelXbrl.profileActivity()
        uniqueContextHashes = {}
        for context in self.modelXbrl.contexts.values():
            h = context.contextDimAwareHash
            if h in uniqueContextHashes:
                if context.isEqualTo(uniqueContextHashes[h]):
                    self.mapContext[context] = uniqueContextHashes[h]
            else:
                uniqueContextHashes[h] = context
        del uniqueContextHashes
        self.modelXbrl.profileActivity("... identify equal contexts", minTimeToShow=1.0)

        # identify equal contexts
        uniqueUnitHashes = {}
        for unit in self.modelXbrl.units.values():
            h = unit.hash
            if h in uniqueUnitHashes:
                if unit.isEqualTo(uniqueUnitHashes[h]):
                    self.mapUnit[unit] = uniqueUnitHashes[h]
            else:
                uniqueUnitHashes[h] = unit
        self.modelXbrl.profileActivity("... identify equal units", minTimeToShow=1.0)
                    
        # identify concepts participating in essence-alias relationships
        # identify calcluation & essence-alias base sets (by key)
        for baseSetKey in self.modelXbrl.baseSets.keys():
            arcrole, ELR, linkqname, arcqname = baseSetKey
            if ELR and linkqname and arcqname:
                if arcrole in (XbrlConst.essenceAlias, XbrlConst.requiresElement):
                    conceptsSet = {XbrlConst.essenceAlias:self.conceptsInEssencesAlias,
                                   XbrlConst.requiresElement:self.conceptsInRequiresElement}[arcrole]
                    for modelRel in self.modelXbrl.relationshipSet(arcrole,ELR,linkqname,arcqname).modelRelationships:
                        for concept in (modelRel.fromModelObject, modelRel.toModelObject):
                            if concept is not None and concept.qname is not None:
                                conceptsSet.add(concept)
        self.modelXbrl.profileActivity("... identify requires-element and esseance-aliased concepts", minTimeToShow=1.0)

        self.bindFacts(self.modelXbrl.facts,[self.modelXbrl.modelDocument.xmlRootElement])
        self.modelXbrl.profileActivity("... bind facts", minTimeToShow=1.0)
        
        # identify calcluation & essence-alias base sets (by key)
        for baseSetKey in self.modelXbrl.baseSets.keys():
            arcrole, ELR, linkqname, arcqname = baseSetKey
            if ELR and linkqname and arcqname:
                if arcrole in (XbrlConst.summationItem, XbrlConst.essenceAlias, XbrlConst.requiresElement):
                    relsSet = self.modelXbrl.relationshipSet(arcrole,ELR,linkqname,arcqname)
                    if arcrole == XbrlConst.summationItem:
                        fromRelationships = relsSet.fromModelObjects()
                        for sumConcept, modelRels in fromRelationships.items():
                            sumBindingKeys = self.sumConceptBindKeys[sumConcept]
                            dupBindingKeys = set()
                            boundSumKeys = set()
                            # determine boundSums
                            for modelRel in modelRels:
                                itemConcept = modelRel.toModelObject
                                if itemConcept is not None and itemConcept.qname is not None:
                                    itemBindingKeys = self.itemConceptBindKeys[itemConcept]
                                    boundSumKeys |= sumBindingKeys & itemBindingKeys
                            # add up rounded items
                            boundSums = defaultdict(decimal.Decimal) # sum of facts meeting factKey
                            boundSummationItems = defaultdict(list) # corresponding fact refs for messages
                            for modelRel in modelRels:
                                weight = modelRel.weightDecimal
                                itemConcept = modelRel.toModelObject
                                if itemConcept is not None:
                                    for itemBindKey in boundSumKeys:
                                        ancestor, contextHash, unit = itemBindKey
                                        factKey = (itemConcept, ancestor, contextHash, unit)
                                        if factKey in self.itemFacts:
                                            for fact in self.itemFacts[factKey]:
                                                if fact in self.duplicatedFacts:
                                                    dupBindingKeys.add(itemBindKey)
                                                else:
                                                    roundedValue = roundFact(fact, self.inferDecimals)
                                                    boundSums[itemBindKey] += roundedValue * weight
                                                    boundSummationItems[itemBindKey].append(wrappedFactWithWeight(fact,weight,roundedValue))
                            for sumBindKey in boundSumKeys:
                                ancestor, contextHash, unit = sumBindKey
                                factKey = (sumConcept, ancestor, contextHash, unit)
                                if factKey in self.sumFacts:
                                    sumFacts = self.sumFacts[factKey]
                                    for fact in sumFacts:
                                        if fact in self.duplicatedFacts:
                                            dupBindingKeys.add(sumBindKey)
                                        elif sumBindKey not in dupBindingKeys:
                                            roundedSum = roundFact(fact, self.inferDecimals)
                                            roundedItemsSum = roundFact(fact, self.inferDecimals, vDecimal=boundSums[sumBindKey])
                                            if roundedItemsSum  != roundFact(fact, self.inferDecimals):
                                                d = inferredDecimals(fact)
                                                if isnan(d) or isinf(d): d = 4
                                                _boundSummationItems = boundSummationItems[sumBindKey]
                                                unreportedContribingItemQnames = [] # list the missing/unreported contributors in relationship order
                                                for modelRel in modelRels:
                                                    itemConcept = modelRel.toModelObject
                                                    if (itemConcept is not None and 
                                                        (itemConcept, ancestor, contextHash, unit) not in self.itemFacts):
                                                        unreportedContribingItemQnames.append(str(itemConcept.qname))
                                                self.modelXbrl.log('INCONSISTENCY', "xbrl.5.2.5.2:calcInconsistency",
                                                    _("Calculation inconsistent from %(concept)s in link role %(linkrole)s reported sum %(reportedSum)s computed sum %(computedSum)s context %(contextID)s unit %(unitID)s unreportedContributingItems %(unreportedContributors)s"),
                                                    modelObject=wrappedSummationAndItems(fact, roundedSum, _boundSummationItems),
                                                    concept=sumConcept.qname, linkrole=ELR, 
                                                    linkroleDefinition=self.modelXbrl.roleTypeDefinition(ELR),
                                                    reportedSum=Locale.format_decimal(self.modelXbrl.locale, roundedSum, 1, max(d,0)),
                                                    computedSum=Locale.format_decimal(self.modelXbrl.locale, roundedItemsSum, 1, max(d,0)), 
                                                    contextID=fact.context.id, unitID=fact.unit.id,
                                                    unreportedContributors=", ".join(unreportedContribingItemQnames) or "none")
                                                del unreportedContribingItemQnames[:]
                            boundSummationItems.clear() # dereference facts in list
                    elif arcrole == XbrlConst.essenceAlias:
                        for modelRel in relsSet.modelRelationships:
                            essenceConcept = modelRel.fromModelObject
                            aliasConcept = modelRel.toModelObject
                            essenceBindingKeys = self.esAlConceptBindKeys[essenceConcept]
                            aliasBindingKeys = self.esAlConceptBindKeys[aliasConcept]
                            for esAlBindKey in essenceBindingKeys & aliasBindingKeys:
                                ancestor, contextHash = esAlBindKey
                                essenceFactsKey = (essenceConcept, ancestor, contextHash)
                                aliasFactsKey = (aliasConcept, ancestor, contextHash)
                                if essenceFactsKey in self.esAlFacts and aliasFactsKey in self.esAlFacts:
                                    for eF in self.esAlFacts[essenceFactsKey]:
                                        for aF in self.esAlFacts[aliasFactsKey]:
                                            essenceUnit = self.mapUnit.get(eF.unit,eF.unit)
                                            aliasUnit = self.mapUnit.get(aF.unit,aF.unit)
                                            if essenceUnit != aliasUnit:
                                                self.modelXbrl.log('INCONSISTENCY', "xbrl.5.2.6.2.2:essenceAliasUnitsInconsistency",
                                                    _("Essence-Alias inconsistent units from %(essenceConcept)s to %(aliasConcept)s in link role %(linkrole)s context %(contextID)s"),
                                                    modelObject=(modelRel, eF, aF), 
                                                    essenceConcept=essenceConcept.qname, aliasConcept=aliasConcept.qname, 
                                                    linkrole=ELR, 
                                                    linkroleDefinition=self.modelXbrl.roleTypeDefinition(ELR),
                                                    contextID=eF.context.id)
                                            if not XbrlUtil.vEqual(eF, aF):
                                                self.modelXbrl.log('INCONSISTENCY', "xbrl.5.2.6.2.2:essenceAliasUnitsInconsistency",
                                                    _("Essence-Alias inconsistent value from %(essenceConcept)s to %(aliasConcept)s in link role %(linkrole)s context %(contextID)s"),
                                                    modelObject=(modelRel, eF, aF), 
                                                    essenceConcept=essenceConcept.qname, aliasConcept=aliasConcept.qname, 
                                                    linkrole=ELR,
                                                    linkroleDefinition=self.modelXbrl.roleTypeDefinition(ELR),
                                                    contextID=eF.context.id)
                    elif arcrole == XbrlConst.requiresElement:
                        for modelRel in relsSet.modelRelationships:
                            sourceConcept = modelRel.fromModelObject
                            requiredConcept = modelRel.toModelObject
                            if sourceConcept in self.requiresElementFacts and \
                               not requiredConcept in self.requiresElementFacts:
                                    self.modelXbrl.log('INCONSISTENCY', "xbrl.5.2.6.2.4:requiresElementInconsistency",
                                        _("Requires-Element %(requiringConcept)s missing required fact for %(requiredConcept)s in link role %(linkrole)s"),
                                        modelObject=sourceConcept, 
                                        requiringConcept=sourceConcept.qname, requiredConcept=requiredConcept.qname, 
                                        linkrole=ELR,
                                        linkroleDefinition=self.modelXbrl.roleTypeDefinition(ELR))
        self.modelXbrl.profileActivity("... find inconsistencies", minTimeToShow=1.0)
        self.modelXbrl.profileActivity() # reset
    
    def bindFacts(self, facts, ancestors):
        for f in facts:
            concept = f.concept
            if concept is not None:
                # index facts by their calc relationship set
                if concept.isNumeric:
                    for ancestor in ancestors:
                        # tbd: uniqify context and unit
                        context = self.mapContext.get(f.context,f.context)
                        # must use nonDimAwareHash to achieve s-equal comparison of contexts
                        contextHash = context.contextNonDimAwareHash if context is not None else hash(None)
                        unit = self.mapUnit.get(f.unit,f.unit)
                        calcKey = (concept, ancestor, contextHash, unit)
                        if not f.isNil:
                            self.itemFacts[calcKey].append(f)
                            bindKey = (ancestor, contextHash, unit)
                            self.itemConceptBindKeys[concept].add(bindKey)
                    if not f.isNil:
                        self.sumFacts[calcKey].append(f) # sum only for immediate parent
                        self.sumConceptBindKeys[concept].add(bindKey)
                    # calcKey is the last ancestor added (immediate parent of fact)
                    if calcKey in self.duplicateKeyFacts:
                        self.duplicatedFacts.add(f)
                        self.duplicatedFacts.add(self.duplicateKeyFacts[calcKey])
                    else:
                        self.duplicateKeyFacts[calcKey] = f
                elif concept.isTuple:
                    self.bindFacts(f.modelTupleFacts, ancestors + [f])

                # index facts by their essence alias relationship set
                if concept in self.conceptsInEssencesAlias and not f.isNil:
                    ancestor = ancestors[-1]    # only care about direct parent
                    context = self.mapContext.get(f.context,f.context)
                    contextHash = context.contextNonDimAwareHash if context is not None else hash(None)
                    esAlKey = (concept, ancestor, contextHash)
                    self.esAlFacts[esAlKey].append(f)
                    bindKey = (ancestor, contextHash)
                    self.esAlConceptBindKeys[concept].add(bindKey)
                # index facts by their requires element usage
                if concept in self.conceptsInRequiresElement:
                    self.requiresElementFacts[concept].append(f)

def roundFact(fact, inferDecimals=False, vDecimal=None):
    if vDecimal is None:
        vStr = fact.value
        try:
            vDecimal = decimal.Decimal(vStr)
            vFloatFact = float(vStr)
        except (decimal.InvalidOperation, ValueError): # would have been a schema error reported earlier
            vDecimal = NaN
            vFloatFact = floatNaN
    else: #only vFloat is defined, may not need vStr unless inferring precision from decimals
        if vDecimal.is_nan():
            return vDecimal
        vStr = None
        try:
            vFloatFact = float(fact.value)
        except ValueError:
            vFloatFact = floatNaN
    dStr = fact.decimals
    pStr = fact.precision
    if dStr == "INF" or pStr == "INF":
        vRounded = vDecimal
    elif inferDecimals: #infer decimals, round per 4.6.7.2, e.g., half-down
        if pStr:
            p = int(pStr)
            if p == 0:
                vRounded = NaN
            elif vDecimal == 0:
                vRounded = ZERO
            else:
                vAbs = fabs(vFloatFact)
                d = p - int(floor(log10(vAbs))) - 1
                # defeat binary rounding to nearest even
                #if trunc(fmod(vFloat * (10 ** d),2)) != 0:
                #    vFloat += 10 ** (-d - 1) * (1.0 if vFloat > 0 else -1.0)
                #vRounded = round(vFloat, d)
                vRounded = decimalRound(vDecimal,d,decimal.ROUND_HALF_EVEN)
        elif dStr:
            d = int(dStr)
            # defeat binary rounding to nearest even
            #if trunc(fmod(vFloat * (10 ** d),2)) != 0:
            #    vFloat += 10 ** (-d - 1) * (-1.0 if vFloat > 0 else 1.0)
            #vRounded = round(vFloat, d)
            #vRounded = round(vFloat,d)
            vRounded = decimalRound(vDecimal,d,decimal.ROUND_HALF_EVEN)
        else: # no information available to do rounding (other errors xbrl.4.6.3 error)
            vRounded = vDecimal
    else: # infer precision
        if dStr:
            match = numberPattern.match(vStr if vStr else str(vDecimal))
            if match:
                nonZeroInt, period, zeroDec, nonZeroDec, e, exp = match.groups()
                p = (len(nonZeroInt) if nonZeroInt and (len(nonZeroInt)) > 0 else -len(zeroDec)) + \
                    (int(exp) if exp and (len(exp) > 0) else 0) + \
                    (int(dStr))
            else:
                p = 0
        elif pStr:
            p = int(pStr)
        else: # no rounding information
            p = None
        if p == 0:
            vRounded = NaN
        elif vDecimal == 0:
            vRounded = vDecimal
        elif p is not None:  # round per 4.6.7.1, half-up
            vAbs = vDecimal.copy_abs()
            log = vAbs.log10()
            # defeat rounding to nearest even
            d = p - int(log) - (1 if vAbs >= 1 else 0)
            #if trunc(fmod(vFloat * (10 ** d),2)) != 0:
            #    vFloat += 10 ** (-d - 1) * (1.0 if vFloat > 0 else -1.0)
            #vRounded = round(vFloat, d)
            vRounded = decimalRound(vDecimal,d,decimal.ROUND_HALF_UP)
        else: # no information available to do rounding (other errors xbrl.4.6.3 error)
            vRounded = vDecimal
    return vRounded
    
def decimalRound(x, d, rounding):
    if x.is_normal() and -28 <= d <= 28: # prevent exception with excessive quantization digits
        if d >= 0:
            return x.quantize(ONE.scaleb(-d),rounding)
        else: # quantize only seems to work on fractional part, convert integer to fraction at scaled point    
            return x.scaleb(d).quantize(ONE,rounding).scaleb(-d)
    return x # infinite, NaN, zero, or excessive decimal digits ( > 28 )

def inferredPrecision(fact):
    vStr = fact.value
    dStr = fact.decimals
    pStr = fact.precision
    if dStr == "INF" or pStr == "INF":
        return floatINF
    try:
        vFloat = float(vStr)
        if dStr:
            match = numberPattern.match(vStr if vStr else str(vFloat))
            if match:
                nonZeroInt, period, zeroDec, nonZeroDec, e, exp = match.groups()
                p = (len(nonZeroInt) if nonZeroInt else (-len(zeroDec) if nonZeroDec else 0)) + \
                    (int(exp) if exp else 0) + \
                    (int(dStr))
                if p < 0:
                    p = 0 # "pathological case" 2.1 spec example 13 line 7
            else:
                p = 0
        else:
            return int(pStr)
    except ValueError:
        return floatNaN
    if p == 0:
        return 0
    elif vFloat == 0:
        return 0
    else:
        return p
    
def inferredDecimals(fact):
    vStr = fact.value
    dStr = fact.decimals
    pStr = fact.precision
    if dStr == "INF" or pStr == "INF":
        return floatINF
    try:
        if pStr:
            p = int(pStr)
            if p == 0:
                return floatNaN # =0 cannot be determined
            vFloat = float(vStr)
            if vFloat == 0:
                return floatINF # =0 cannot be determined
            else:
                vAbs = fabs(vFloat)
                return p - int(floor(log10(vAbs))) - 1
        elif dStr:
            return int(dStr)
    except ValueError:
        pass
    return floatNaN
    
def roundValue(value, precision=None, decimals=None, scale=None):
    try:
        vDecimal = decimal.Decimal(value)
        if scale:
            iScale = int(scale)
            vDecimal = vDecimal.scaleb(iScale)
        if precision is not None:
            vFloat = float(value)
            if scale:
                vFloat = pow(vFloat, iScale)
    except (decimal.InvalidOperation, ValueError): # would have been a schema error reported earlier
        return NaN
    if precision is not None:
        if not isinstance(precision, (int,float)):
            if precision == "INF":
                precision = floatINF
            else:
                try:
                    precision = int(precision)
                except ValueError: # would be a schema error
                    precision = floatNaN
        if isinf(precision):
            vRounded = vDecimal
        elif precision == 0 or isnan(precision):
            vRounded = NaN
        elif vFloat == 0:
            vRounded = ZERO
        else:
            vAbs = fabs(vFloat)
            log = log10(vAbs)
            d = precision - int(log) - (1 if vAbs >= 1 else 0)
            vRounded = decimalRound(vDecimal,d,decimal.ROUND_HALF_UP)
    elif decimals is not None:
        if not isinstance(decimals, (int,float)):
            if decimals == "INF":
                decimals = floatINF
            else:
                try:
                    decimals = int(decimals)
                except ValueError: # would be a schema error
                    decimals = floatNaN
        if isinf(decimals):
            vRounded = vDecimal
        elif isnan(decimals):
            vRounded = NaN
        else:
            vRounded = decimalRound(vDecimal,decimals,decimal.ROUND_HALF_EVEN)
    else:
        vRounded = vDecimal
    return vRounded

def insignificantDigits(value, precision=None, decimals=None, scale=None):
    try:
        vDecimal = decimal.Decimal(value)
        if scale:
            iScale = int(scale)
            vDecimal = vDecimal.scaleb(iScale)
        if precision is not None:
            vFloat = float(value)
            if scale:
                vFloat = pow(vFloat, iScale)
    except (decimal.InvalidOperation, ValueError): # would have been a schema error reported earlier
        return None
    if precision is not None:
        if not isinstance(precision, (int,float)):
            if precision == "INF":
                return None
            else:
                try:
                    precision = int(precision)
                except ValueError: # would be a schema error
                    return None
        if isinf(precision) or precision == 0 or isnan(precision) or vFloat == 0: 
            return None
        else:
            vAbs = fabs(vFloat)
            log = log10(vAbs)
            decimals = precision - int(log) - (1 if vAbs >= 1 else 0)
    elif decimals is not None:
        if not isinstance(decimals, (int,float)):
            if decimals == "INF":
                return None
            else:
                try:
                    decimals = int(decimals)
                except ValueError: # would be a schema error
                    return None
        if isinf(decimals) or isnan(decimals):
            return None
    else:
        return None
    if vDecimal.is_normal() and -28 <= decimals <= 28: # prevent exception with excessive quantization digits
        if decimals > 0:
            divisor = ONE.scaleb(-decimals) # fractional scaling doesn't produce scientific notation
        else:  # extra quantize step to prevent scientific notation for decimal number
            divisor = ONE.scaleb(-decimals).quantize(ONE, decimal.ROUND_HALF_UP) # should never round
        insignificantDigits = abs(vDecimal) % divisor
        if insignificantDigits:
            return (vDecimal // divisor * divisor,  # truncated portion of number
                    insignificantDigits)   # nsignificant digits portion of number
    return None


def wrappedFactWithWeight(fact, weight, roundedValue):
    return ObjectPropertyViewWrapper(fact, ( ("weight", weight), ("roundedValue", roundedValue)) )

def wrappedSummationAndItems(fact, roundedSum, boundSummationItems):
    # need hash of facts and their values from boundSummationItems
    ''' ARELLE-281, replace: faster python-based hash (replace with hashlib for fewer collisions)
    itemValuesHash = hash( tuple(( hash(b.modelObject.qname), hash(b.extraProperties[1][1]) )
                                 # sort by qname so we don't care about reordering of summation terms
                                 for b in sorted(boundSummationItems,
                                                       key=lambda b: b.modelObject.qname)) )
    sumValueHash = hash( (hash(fact.qname), hash(roundedSum)) )
    '''
    sha256 = hashlib.sha256()
    # items hash: sort by qname so we don't care about reordering of summation terms in linkbase updates
    for b in sorted(boundSummationItems, key=lambda b: b.modelObject.qname):
        sha256.update(b.modelObject.qname.namespaceURI.encode('utf-8','replace')) #qname of erroneous submission may not be utf-8 perfectly encodable
        sha256.update(b.modelObject.qname.localName.encode('utf-8','replace'))
        sha256.update(str(b.extraProperties[1][1]).encode('utf-8','replace'))
    itemValuesHash = sha256.hexdigest()
    # summation value hash
    sha256 = hashlib.sha256()
    sha256.update(fact.qname.namespaceURI.encode('utf-8','replace'))
    sha256.update(fact.qname.localName.encode('utf-8','replace'))
    sha256.update(str(roundedSum).encode('utf-8','replace'))
    sumValueHash = sha256.hexdigest()
    # return list of bound summation followed by bound contributing items
    return [ObjectPropertyViewWrapper(fact,
                                      ( ("sumValueHash", sumValueHash),
                                        ("itemValuesHash", itemValuesHash),
                                        ("roundedSum", roundedSum) ))] + \
            boundSummationItems
                    
'''
Created on Apr 5, 2015

@author: Acsone S. A.
(c) Copyright 2015 Mark V Systems Limited, All rights reserved.
'''
from tkinter import *
try:
    from tkinter.ttk import *
except ImportError:
    from ttk import *
from arelle.CntlrWinTooltip import ToolTip

class ViewPane:
    def __init__(self, modelXbrl, tabWin, tabTitle,
                 contentView, hasToolTip=False, lang=None):
        self.blockViewModelObject = 0
        self.tabWin = tabWin

        self.viewFrame = contentView
        self.viewFrame.view = self

        tabWin.add(self.viewFrame,text=tabTitle)
        self.modelXbrl = modelXbrl
        self.hasToolTip = hasToolTip
        self.toolTipText = StringVar()
        if hasToolTip:
            self.toolTipText = StringVar()
            self.toolTip = ToolTip(self.gridBody, 
                                   textvariable=self.toolTipText, 
                                   wraplength=480, 
                                   follow_mouse=True,
                                   state="disabled")
            self.toolTipColId = None
            self.toolTipRowId = None
        self.modelXbrl = modelXbrl
        modelManager = self.modelXbrl.modelManager
        self.contextMenuClick = modelManager.cntlr.contextMenuClick
        self.lang = lang
        if modelXbrl:
            modelXbrl.views.append(self)
            if not lang: 
                self.lang = modelXbrl.modelManager.defaultLang
        
    def close(self):
        del self.viewFrame.view
        self.tabWin.forget(self.viewFrame)
        if self in self.modelXbrl.views:
            self.modelXbrl.views.remove(self)
        self.modelXbrl = None
        
    def select(self):
        self.tabWin.select(self.viewFrame)

    def onClick(self, *args):
        if self.modelXbrl:
            self.modelXbrl.modelManager.cntlr.currentView = self
        
    def leave(self, *args):
        self.toolTipColId = None
        self.toolTipRowId = None

    def motion(self, *args):
        pass
                

    def contextMenu(self):
        try:
            return self.menu
        except AttributeError:
            self.menu = Menu( self.viewFrame, tearoff = 0 )
            return self.menu

    def bindContextMenu(self, widget):
        if not widget.bind(self.contextMenuClick): 
            widget.bind( self.contextMenuClick, self.popUpMenu )

    def popUpMenu(self, event):
        self.menu.post( event.x_root, event.y_root )

    def menuAddLangs(self):
        langsMenu = Menu(self.viewFrame, tearoff=0)
        self.menu.add_cascade(label=_("Language"), menu=langsMenu, underline=0)
        for lang in sorted(self.modelXbrl.langs):
            langsMenu.add_cascade(label=lang, underline=0,
                                  command=lambda l=lang: self.setLang(l))

    def setLang(self, lang):
        self.lang = lang
        self.view()


'''
This module is an example to convert Html Tables into Xlsx (Excel) tables
Preconfigured here to use SEC Edgar Rendering R files as input

@author: Mark V Systems Limited
(c) Copyright 2014 Mark V Systems Limited, All rights reserved.
'''
import os, sys, re
from lxml import etree, html
from openpyxl.workbook import Workbook 
from openpyxl.worksheet import ColumnDimension
from openpyxl.cell import get_column_letter
from openpyxl.style import Alignment
    
class Report():
    def __init__(self, longName, shortName, htmlFileName):
        self.longName = longName
        self.shortName = shortName
        self.htmlFileName = htmlFileName
    def __repr__(self):
        return ("report(longName='{}', shortName='{}', htmlFileName='{}')"
                .format(self.longName, self.shortName, self.htmlFileName))
        
def intCol(elt, attrName, default=None):
    try:
        return int(elt.get(attrName, default))
    except (TypeError, ValueError):
        return default

numberPattern = re.compile(r"\s*([$]\s*)?[(]?\s*[+-]?[0-9,]+([.][0-9]*)?[)-]?\s*$")
displayNonePattern = re.compile(r"\s*display:\s*none;")

def saveTableToExelle(rFilesDir):
    
    # get reports from FilingSummary
    reports = []
    try:
        fsdoc = etree.parse(os.path.join(rFilesDir, "FilingSummary.xml"))
        for rElt in fsdoc.iter(tag="Report"):
            reports.append(Report(rElt.findtext("LongName"),
                                  rElt.findtext("ShortName"),
                                  rElt.findtext("HtmlFileName")))
    except (EnvironmentError,
            etree.LxmlError) as err:
        print("FilingSummary.xml: directory {0} error: {1}".format(rFilesDir, err))
        
    wb = Workbook(encoding='utf-8')
    # remove predefined sheets
    for sheetName in wb.get_sheet_names():
        ws = wb.get_sheet_by_name(sheetName)
        if ws is not None:
            wb.remove_sheet(ws)
            
    sheetNames = set() # prevent duplicates
    
    for reportNum, report in enumerate(reports):
        sheetName = report.shortName[:31]  # max length 31 for excel title
        if sheetName in sheetNames:
            sheetName = sheetName[:31-len(str(reportNum))] + str(reportNum)
        sheetNames.add(sheetName)
        ws = wb.create_sheet(title=sheetName)

        try:
            # doesn't detect utf-8 encoding the normal way, pass it a string
            #htmlSource = ''
            #with open(os.path.join(rFilesDir, report.htmlFileName), 'rt', encoding='utf-8') as fh:
            #    htmlSource = fh.read()
            #rdoc = html.document_fromstring(htmlSource)
            rdoc = html.parse(os.path.join(rFilesDir, report.htmlFileName))
            row = -1
            mergedAreas = {}  # colNumber: (colspan,lastrow)
            for tableElt in rdoc.iter(tag="table"):
                # skip pop up tables
                if tableElt.get("class") ==  "authRefData":
                    continue
                if tableElt.getparent().tag == "div":
                    style = tableElt.getparent().get("style")
                    if style and displayNonePattern.match(style):
                        continue
                colWidths = {}
                for rowNum, trElt in enumerate(tableElt.iter(tag="tr")):
                    # remove passed mergedAreas
                    for mergeCol in [col
                                     for col, mergedArea in mergedAreas.items()
                                     if mergedArea[1] > rowNum]:
                        del mergedAreas[mergeCol]
                    col = 0
                    for coltag in ("th", "td"):
                        for cellElt in trElt.iter(tag=coltag):
                            if col == 0:
                                row += 1 # new row
                            if col in mergedAreas:
                                col += mergedAreas[col][0] - 1
                            text = cellElt.text_content()
                            colspan = intCol(cellElt, "colspan", 1)
                            rowspan = intCol(cellElt, "rowspan", 1)
                            #if col not in colWidths:
                            #    colWidths[col] = 10.0 # some kind of default width
                            for elt in cellElt.iter():
                                style = elt.get("style")
                                if style and "width:" in style:
                                    try:
                                        kw, sep, width = style.partition("width:")
                                        if "px" in width:
                                            width, sep, kw = width.partition("px")
                                            width = float(width) * 0.67777777
                                        else:
                                            width = float(width)
                                        colWidths[col] = width
                                    except ValueError:
                                        pass
                            if rowspan > 1:
                                mergedAreas[col] = (colspan, row + rowspan - 1)
                            cell = ws.cell(row=row,column=col)
                            if text:
                                cell.value = text
                                if numberPattern.match(text):
                                    cell.style.alignment.horizontal = Alignment.HORIZONTAL_RIGHT
                                else:
                                    cell.style.alignment.wrap_text = True
                            if colspan > 1 or rowspan > 1:
                                ws.merge_cells(start_row=row, end_row=row+rowspan-1, start_column=col, end_column=col+colspan-1)
                            cell.style.alignment.vertical = Alignment.VERTICAL_TOP
                            if coltag == "th":
                                cell.style.alignment.horizontal = Alignment.HORIZONTAL_CENTER
                                cell.style.font.bold = True
                            cell.style.font.size = 9  # some kind of default size
                            col += colspan
                for col, width in colWidths.items():
                    ws.column_dimensions[get_column_letter(col+1)].width = width
        except (EnvironmentError, 
                etree.LxmlError) as err:
            print("{0}: directory {1} error: {2}".format(report.htmlFileName, rFilesDir, err))
    
    wb.save(os.path.join(rFilesDir, "exelleOut.xlsx"))
    
if __name__ == "__main__":
    
    # test directory
    saveTableToExelle(r"C:\Users\Herm Fischer\Documents\mvsl\projects\SEC\14.1\R-files\wpoRfiles")

'''
This is an example of a plug-in to both GUI menu and command line/web service
that will provide an option to replace behavior of table linkbase validation to 
generate vs diff table linkbase infoset files.

(c) Copyright 2012 Mark V Systems Limited, All rights reserved.
'''

def validateTableInfosetMenuEntender(cntlr, validateMenu):
    # Extend menu with an item for the save infoset plugin
    cntlr.modelManager.generateTableInfoset = cntlr.config.setdefault("generateTableInfoset",False)
    from tkinter import BooleanVar
    generateTableInfoset = BooleanVar(value=cntlr.modelManager.generateTableInfoset)
    def setTableInfosetOption(*args):
        cntlr.config["generateTableInfoset"] = cntlr.modelManager.generateTableInfoset = generateTableInfoset.get()
    generateTableInfoset.trace("w", setTableInfosetOption)
    validateMenu.add_checkbutton(label=_("Generate table infosets (instead of diffing them)"), 
                                 underline=0, 
                                 variable=generateTableInfoset, onvalue=True, offvalue=False)

def validateTableInfosetCommandLineOptionExtender(parser):
    # extend command line options with a save DTS option
    parser.add_option("--generate-table-infoset", 
                      action="store_true", 
                      dest="generateTableInfoset", 
                      help=_("Generate table instance infosets (instead of diffing them)."))

def validateTableInfosetCommandLineXbrlLoaded(cntlr, options, modelXbrl, *args):
    cntlr.modelManager.generateTableInfoset = getattr(options, "generateTableInfoset", False)

def validateTableInfoset(modelXbrl, resultTableUri):
    diffToFile = not getattr(modelXbrl.modelManager, 'generateTableInfoset', False)
    from arelle import ViewFileRenderedGrid
    ViewFileRenderedGrid.viewRenderedGrid(modelXbrl, 
                                          resultTableUri, 
                                          diffToFile=diffToFile)  # false to save infoset files
    return True # blocks standard behavior in validate.py

__pluginInfo__ = {
    'name': 'Validate Table Infoset (Optional behavior)',
    'version': '0.9',
    'description': "This plug-in adds a feature modify batch validation of table linkbase to save, versus diff, infoset files.  ",
    'license': 'Apache-2',
    'author': 'Mark V Systems Limited',
    'copyright': '(c) Copyright 2012 Mark V Systems Limited, All rights reserved.',
    # classes of mount points (required)
    'CntlrWinMain.Menu.Validation': validateTableInfosetMenuEntender,
    'CntlrCmdLine.Options': validateTableInfosetCommandLineOptionExtender,
    'CntlrCmdLine.Xbrl.Loaded': validateTableInfosetCommandLineXbrlLoaded,
    'Validate.TableInfoset': validateTableInfoset,
}

'''
sphinxEvaluator processes the Sphinx language in the context of an XBRL DTS and instance.

(c) Copyright 2013 Mark V Systems Limited, California US, All rights reserved.  
Mark V copyright applies to this software, which is licensed according to the terms of Arelle(r).

Sphinx is a Rules Language for XBRL described by a Sphinx 2 Primer 
(c) Copyright 2012 CoreFiling, Oxford UK. 
Sphinx copyright applies to the Sphinx language, not to this software.
Mark V Systems conveys neither rights nor license for the Sphinx language. 
'''

import operator
from .SphinxContext import HyperspaceBindings, HyperspaceBinding
from .SphinxParser import (astFunctionReference, astHyperspaceExpression, astNode, 
                           astFormulaRule, astReportRule,
                           astVariableReference)
from .SphinxMethods import (methodImplementation, functionImplementation, 
                            aggreateFunctionImplementation, aggreateFunctionAcceptsFactArgs,
                            moduleInit as SphinxMethodsModuleInit)
from arelle.ModelFormulaObject import Aspect
from arelle.ModelValue import QName
from arelle.ModelInstanceObject import ModelFact
from arelle.ModelXbrl import DEFAULT, NONDEFAULT, DEFAULTorNONDEFAULT
from arelle import XbrlConst, XmlUtil

class SphinxException(Exception):
    def __init__(self, node, code, message, **kwargs ):
        self.node = node
        self.code = code
        self.message = message
        self.kwargs = kwargs
        self.args = ( self.__repr__(), )
    def __repr__(self):
        return _('[{0}] exception: {1} at {2}').format(self.code, self.message % self.kwargs, self.node.sourceFileLine)
            
class SphinxSpecialValue:
    def __init__(self, name):
        self.name = name
    def __repr__(self):
        return self.name

UNBOUND = SphinxSpecialValue("unbound")
NONE = SphinxSpecialValue("none")


def evaluateRuleBase(sphinxContext):
    
    # clear any residual values
    for constantNode in sphinxContext.constants.values():
        constantNode.value = None
        
    clearEvaluation(sphinxContext)
    
    # check any rule-base preconditions
    for preconditionNode in sphinxContext.ruleBasePreconditionNodes:
        preconditionPasses = evaluate(preconditionNode, sphinxContext)
        clearEvaluation(sphinxContext)
        if not preconditionPasses:
            return
        
    # evaluate rules
    for ruleProg in sphinxContext.rules:
        evaluate(ruleProg, sphinxContext)
        clearEvaluation(sphinxContext)
        
    # dereference constants
    for constantNode in sphinxContext.constants.values():
        constantNode.value = None
        
def clearEvaluation(sphinxContext):
    sphinxContext.tags.clear()
    sphinxContext.localVariables.clear()
    while sphinxContext.hyperspaceBindings:
        sphinxContext.hyperspaceBindings.close() # resets sphinxContext.hyperspaceBindings to parent bindings
        
def evaluate(node, sphinxContext, value=False, fallback=None, hsBoundFact=False):
    if isinstance(node, astNode):
        if fallback is None:
            result = evaluator[node.__class__.__name__](node, sphinxContext)
        else:
            try:
                result = evaluator[node.__class__.__name__](node, sphinxContext)
            except StopIteration:
                if sphinxContext.formulaOptions.traceVariableSetExpressionEvaluation:
                    sphinxContext.modelXbrl.info("sphinx:trace",
                         _("%(node)s has unbound evaluation"), 
                         sourceFileLine=node.sourceFileLine, node=str(node))
                return fallback
        if sphinxContext.formulaOptions.traceVariableSetExpressionEvaluation:
            sphinxContext.modelXbrl.info("sphinx:trace",
                 _("%(node)s evaluation: %(value)s"), 
                 sourceFileLine=node.sourceFileLine, node=str(node), value=result)
        if result is not None:
            if isinstance(result, HyperspaceBinding):
                if hsBoundFact:  # return fact, not the value of fact
                    return result.yieldedFact
                elif value:
                    return result.value
            # dereference nodes to their value
            if (value or hsBoundFact) and isinstance(result, astNode):
                return evaluate(result, sphinxContext, value, fallback, hsBoundFact)
            return result
        return result
    elif isinstance(node, (tuple,list)):
        return [evaluate(item, sphinxContext, value, fallback, hsBoundFact)
                for item in node]
    elif isinstance(node, set):
        return set(evaluate(item, sphinxContext, value, fallback, hsBoundFact)
                   for item in node)
    else:
        return node

def evaluateAnnotationDeclaration(node, sphinxContext):
    return None

def evaluateBinaryOperation(node, sphinxContext):
    leftValue = evaluate(node.leftExpr, sphinxContext, value=True, fallback=UNBOUND)
    rightValue = evaluate(node.rightExpr, sphinxContext, value=True, fallback=UNBOUND)
    op = node.op
    if sphinxContext.formulaOptions.traceVariableExpressionEvaluation:
        sphinxContext.modelXbrl.info("sphinx:trace",
             _("Binary op %(op)s v1: %(leftValue)s, v2: %(rightValue)s"), 
             sourceFileLine=node.sourceFileLine, op=op, leftValue=leftValue, rightValue=rightValue)
    if op == ":=":
        if sphinxContext.ruleNode.bind  == "left":
            if rightValue is UNBOUND: raise StopIteration
        elif sphinxContext.ruleNode.bind  == "right":
            if leftValue is UNBOUND: raise StopIteration
        elif sphinxContext.ruleNode.bind  == "either":
            if leftValue is UNBOUND and rightValue is UNBOUND: raise StopIteration
        else: # both or default
            if leftValue is UNBOUND or rightValue is UNBOUND: raise StopIteration
        return (leftValue, rightValue)
    elif op in {"|+|", "|+", "+|", "+", "|-|", "|-", "-|", "-"}:
        if leftValue is UNBOUND: 
            if op[0] == '|':
                raise StopIteration
            else:
                leftValue = 0
        if rightValue is UNBOUND:
            if op[-1] == '|':
                raise StopIteration
            else:
                rightValue = 0
    else:
        if leftValue is UNBOUND:
            return UNBOUND
        if rightValue is UNBOUND:
            if op == "or" and leftValue:
                return True
            return UNBOUND
        if op == "/" and rightValue == 0:  # prevent divide by zero
            return UNBOUND
    try:
        result = {'+': operator.add, '-': operator.sub, '*': operator.mul, '/': operator.truediv,
                  '<': operator.lt, '>': operator.gt, '<=': operator.le, '>=': operator.ge,
                  '==': operator.eq, '!=': operator.ne,
                  'and': operator.and_, 'or': operator.or_,
                  }[op](leftValue, rightValue)
        return result
    except KeyError:
        sphinxContext.modelXbrl.error("sphinx:error",
             _("Operation \"%(op)s\" not implemented for %(node)s"), 
             sourceFileLine=node.sourceFileLine, op=op, node=str(node))
    except (TypeError, ZeroDivisionError) as err:
        sphinxContext.modelXbrl.error("sphinx:error",
             _("Operation \"%(op)s\" raises exception %(error)s for %(node)s"), 
             sourceFileLine=node.sourceFileLine, op=op, node=str(node), error=str(err))
    return None

def evaluateConstant(node, sphinxContext):
    if node.value is None: # first time
        hsBindings = HyperspaceBindings(sphinxContext)  # must have own hsBindings from caller
        previousLocalVariables = sphinxContext.localVariables # save local variables
        sphinxContext.localVariables = {}
        node.value = evaluate(node.expr, sphinxContext)
        if sphinxContext.formulaOptions.traceVariableSetExpressionEvaluation:
            sphinxContext.modelXbrl.info("sphinx:trace",
                 _("Constant %(name)s assigned value: %(value)s"), 
                 sourceFileLine=node.sourceFileLine, name=node.constantName, value=node.value)
        hsBindings.close()
        sphinxContext.localVariables = previousLocalVariables
    return node.value

def evaluateFor(node, sphinxContext):
    # add a hyperspaceBinding to sphinxContext for this node
    hsBindings = sphinxContext.hyperspaceBindings
    forBinding = hsBindings.forBinding(node)
    # set variable here because although needed for next() operation, will be cleared outside of for's context
    sphinxContext.localVariables[node.name] = forBinding.yieldedValue
    return evaluate(node.expr, sphinxContext)

def evaluateFunctionDeclaration(node, sphinxContext, args):
    overriddenVariables = {}

    if isinstance(args, dict):
        # args may not all be used in the function declaration, just want used ones
        argDict = dict((name, value)
                       for name, value in args.items()
                       if name in node.params)
    else:  # purely positional args      
        # positional parameters named according to function prototype
        if len(args) != len(node.params):
            sphinxContext.modelXbrl.log("ERROR", "sphinx.functionArgumentsMismatch",
                                        _("Function %(name)s requires %(required)s parameters but %(provided)s are provided"),
                                        sourceFileLine=node.sourceFileLine,
                                        name=node.name, required=len(node.params), provided=len(args))
            return None
        argDict = dict((paramName, args[i])
                       for i, paramName in enumerate(node.params))
    for name, value in argDict.items():
        if name in sphinxContext.localVariables:
            overriddenVariables[name] = sphinxContext.localVariables[name]
        sphinxContext.localVariables[name] = value
    
    def clearFunctionArgs():
        for name in argDict.keys():
            del sphinxContext.localVariables[name]
        sphinxContext.localVariables.update(overriddenVariables)
        overriddenVariables.clear()

    try:
        result = evaluate(node.expr, sphinxContext)
        clearFunctionArgs()
        return result
    except StopIteration as ex:
        clearFunctionArgs()
        raise ex  # reraise exception

def evaluateFunctionReference(node, sphinxContext):
    name = node.name
    if name in ("error", "warning", "info", "pass"):
        sphinxContext.dynamicSeverity = node.name
    elif name == "unbound":
        return UNBOUND
    
    if name in aggreateFunctionImplementation:
        return evaluateAggregateFunction(node, sphinxContext, name)

    if name in sphinxContext.functions:  # user defined function
        resolveValues = sphinxContext.functions[name].functionType == "function"
        namedParametersAssignedTo = sphinxContext.localVariables
    else:
        resolveValues = True
        if name in ("error", "warning", "info", "pass"):
            namedParametersAssignedTo = sphinxContext.tags
        else:
            namedParametersAssignedTo = sphinxContext.localVariables
    
    # evaluate local variables
    for localVar in node.localVariables:
        evaluate(localVar, sphinxContext)
    # evaluate args
    args = []
    tagName = None
    l = len(node.args)
    for i in range(l):
        arg = node.args[i]
        if arg == "=":
            if i > 0:
                tagName = node.args[i-1]
        elif i == l - 1 or node.args[i+1] != "=":
            if resolveValues: # macros pass in the argument, not value
                arg = evaluate(arg, sphinxContext, value=True)
            elif (isinstance(arg, astVariableReference) and 
                  getattr(sphinxContext.localVariables.get(arg.variableName),
                          "isMacroParameter", False)):
                # pass original macro parameter, not a reference to it (otherwise causes looping)
                arg = sphinxContext.localVariables[arg.variableName]
            elif isinstance(arg, astNode):
                arg.isMacroParameter = True
            args.append(arg)
            if tagName:
                namedParametersAssignedTo[tagName] = arg
            tagName = None
            
    if name in ("error", "warning", "info", "pass"):
        result = None
    
    # call function here
    elif name in sphinxContext.functions:  # user defined function
        result = evaluateFunctionDeclaration(sphinxContext.functions[name], sphinxContext, args)
        
    # call built-in functions
    elif name in functionImplementation:
        result = functionImplementation[name](node, sphinxContext, args)
    
    else:
        raise SphinxException(node, 
                              "sphinx:functionName", 
                              _("unassigned function name %(name)s"),
                              name=name)
        
    # remove local variables
    for localVar in node.localVariables:
        del sphinxContext.localVariables[localVar.name]
    return result
    
def evaluateAggregateFunction(node, sphinxContext, name):
    # determine if evaluating args found hyperspace (first time)
    args = []
    iterateAbove, bindingsLen = getattr(node, "aggregationHsBindings", (None, None))
    firstTime = bindingsLen is None
    hsBindings = sphinxContext.hyperspaceBindings
    parentAggregationNode = hsBindings.aggregationNode
    parentIsValuesIteration = hsBindings.isValuesIteration
    hsBindings.aggregationNode = node # block removing nested aspect bindings
    hsBindings.isValuesIteration = False
    prevHsBindingsLen = len(hsBindings.hyperspaceBindings)
    hsBoundFact = aggreateFunctionAcceptsFactArgs[name]
    arg = node.args[0]
    try:
        while (True):   # possibly multiple bindings
            # evaluate local variables
            for localVar in node.localVariables:
                evaluate(localVar, sphinxContext)
                
            value = evaluate(arg, sphinxContext, value=True, hsBoundFact=hsBoundFact)
            if isinstance(value, (list,set)):
                for listArg in value:
                    if value is not UNBOUND:
                        args.append(evaluate(listArg, sphinxContext, value=True))
            elif value is not UNBOUND:
                args.append(value)
            if firstTime:
                if len(hsBindings.hyperspaceBindings) == prevHsBindingsLen:
                    # no hs bindings, just scalar
                    break
                else:    # has hs bindings, evaluate rest of them
                    firstTime = False
                    iterateAbove = prevHsBindingsLen - 1
                    bindingsLen = len(hsBindings.hyperspaceBindings)
                    node.aggregationHsBindings = (iterateAbove, bindingsLen)
            hsBindings.next(iterateAbove, bindingsLen)
    except StopIteration:
        pass # no more bindings
    hsBindings.isValuesIteration = parentIsValuesIteration
    hsBindings.aggregationNode = parentAggregationNode
    # remove local variables
    for localVar in node.localVariables:
        if localVar in sphinxContext.localVariables:
            del sphinxContext.localVariables[localVar.name]
    if sphinxContext.formulaOptions.traceVariableExpressionEvaluation:
        sphinxContext.modelXbrl.info("sphinx:trace",
             _("Aggregative function %(name)s arguments: %(args)s"), 
             sourceFileLine=node.sourceFileLine, name=name, 
             args=",".join(str(a) for a in args))
    try:
        return aggreateFunctionImplementation[name](node, sphinxContext, args)
    except (TypeError, ZeroDivisionError) as err:
        sphinxContext.modelXbrl.error("sphinx:error",
             _("Function %(name)s raises exception %(error)s in %(node)s"), 
             sourceFileLine=node.sourceFileLine, name=name, node=str(node), error=str(err))
        return None

def evaluateHyperspaceExpression(node, sphinxContext):
    # add a hyperspaceBinding to sphinxContext for this node
    hsBindings = sphinxContext.hyperspaceBindings
    nodeBinding = hsBindings.nodeBinding(node)
    return nodeBinding

def evaluateIf(node, sphinxContext):
    condition = evaluate(node.condition, sphinxContext, value=True)
    if condition:
        expr = node.thenExpr
    else:
        expr = node.elseExpr
    return evaluate(expr, sphinxContext)

def evaluateMessage(node, sphinxContext, resultTags, hsBindings):
    def evaluateTagExpr(tagExpr, modifier):
        if modifier == "value":
            value = evaluate(tagExpr, sphinxContext, value=True)
        elif modifier == "context":
            value = contextView(sphinxContext, tagExpr)
        else:
            value = "{0} {1}".format(evaluate(tagExpr, sphinxContext, value=True), 
                                     contextView(sphinxContext))
        return value
    
    msgstr = evaluate(node.message, sphinxContext, value=True)
    text = []
    args = []
    i = 0
    while True:
        j = msgstr.find("${", i)
        if j >= 0:
            text.append(msgstr[i:j]) # previous part of string
            k = msgstr.find("}", j+2)
            if k > j:
                text.append("{" + str(len(args)) + "}")
                tag, sep, modifier = msgstr[j+2:k].strip().partition(".")
                if tag == "context":
                    value = contextView(sphinxContext),
                elif tag in resultTags:
                    value = evaluateTagExpr(resultTags.tags[tag], modifier)
                elif tag in sphinxContext.tags:
                    value = evaluateTagExpr(sphinxContext.tags[tag], modifier)
                elif tag in sphinxContext.taggedConstants:
                    value = evaluateTagExpr(evaluateConstant(sphinxContext.taggedConstants[tag], sphinxContext), modifier)
                elif tag in ("trace", "left", "right", "difference"):
                    value = 'Tag "{0}" is not yet supported'.format(tag)
                else:
                    sphinxContext.modelXbrl.log("ERROR", "sphinx.unboundMessageTag",
                                                _("Validation rule tag %(tag)s is not Bound"),
                                                sourceFileLine=node.sourceFileLine,
                                                tag=tag)
                    value = "${" + tag + "}"
                args.append(value)
                
                i = k + 1
        else:
            text.append(msgstr[i:])
            break
    messageStr = ''.join(text)
    return messageStr.format(*args)

def evaluateMethodReference(node, sphinxContext):
    args = []
    for i, nodeArg in enumerate(node.args):
        arg = evaluate(nodeArg, 
                       sphinxContext, 
                       value=True,
                       hsBoundFact=(i == 0)) # don't deref arg 0
        args.append(arg)
    return methodImplementation.get(node.name,                       # requested method
                                    methodImplementation["unknown"]  # default if missing method
                                    )(node, sphinxContext, args)

def evaluateNoOp(node, sphinxContext):
    return None

def evaluateNumericLiteral(node, sphinxContext):
    return node.value

def evaluatePreconditionDeclaration(node, sphinxContext):
    hsBindings = HyperspaceBindings(sphinxContext)
    result = evaluate(node.expr, sphinxContext, value=True)
    hsBindings.close()
    return result

def evaluatePreconditionReference(node, sphinxContext):
    preconditionPasses = True
    for name in node.names:
        if name in sphinxContext.preconditionNodes:
            if not evaluate(sphinxContext.preconditionNodes[name], sphinxContext, value=True):
                preconditionPasses = False
            clearEvaluation(sphinxContext)
            if not preconditionPasses:
                break
    return preconditionPasses

def evaluateQnameLiteral(node, sphinxContext):
    return node.value

def evaluateReportRule(node, sphinxContext):
    return None

def evaluateRuleBasePrecondition(node, sphinxContext):
    if node.precondition:
        return evaluate(node.precondition, sphinxContext, value=True)
    return True

def evaluateStringLiteral(node, sphinxContext):
    return node.text

def evaluateTagAssignment(node, sphinxContext):
    result = evaluate(node.expr, sphinxContext, value=True)
    sphinxContext.tags[node.tagName] = result
    return result

def evaluateTagReference(node, sphinxContext):
    try:
        return sphinxContext.tags[node.name]
    except KeyError:
        raise SphinxException(node, 
                              "sphinx:tagName", 
                              _("unassigned tag name %(name)s"),
                              name=node.name )

def evaluateRule(node, sphinxContext):
    isFormulaRule = isinstance(node, astFormulaRule)
    isReportRule = isinstance(node, astReportRule)
    name = (node.name or ("sphinx.report" if isReportRule else "sphinx.raise"))
    nodeId = node.nodeTypeName + ' ' + name
    if node.precondition:
        result = evaluate(node.precondition, sphinxContext, value=True)
        if sphinxContext.formulaOptions.traceVariableSetExpressionResult:
            sphinxContext.modelXbrl.info("sphinx:trace",
                 _("%(node)s precondition evaluation: %(value)s"), 
                 sourceFileLine=node.sourceFileLine, node=nodeId, value=result)
        if not result:
            return None 
    # nest hyperspace binding
    sphinxContext.ruleNode = node
    hsBindings = None
    ruleIteration = 0
    try:
        hsBindings = HyperspaceBindings(sphinxContext)
        while True:
            ruleIteration += 1
            sphinxContext.dynamicSeverity = None
            sphinxContext.tags.clear()
            sphinxContext.localVariables.clear()
            if sphinxContext.formulaOptions.traceVariableSetExpressionResult:
                sphinxContext.modelXbrl.info("sphinx:trace",
                     _("%(node)s starting iteration %(iteration)s"), 
                     sourceFileLine=node.sourceFileLine, node=nodeId, iteration=ruleIteration)
            for varAssignNode in node.variableAssignments:
                evaluateVariableAssignment(varAssignNode, sphinxContext)
            result = evaluate(node.expr, sphinxContext, value=True)
            if result is UNBOUND:
                result = None # nothing to do for this pass
            elif isFormulaRule:
                left, right = result
                if left is UNBOUND:
                    difference = UNBOUND
                elif right is UNBOUND:
                    difference = UNBOUND
                else:
                    difference = abs(left - right)
                result = difference != 0
                resultTags = {"left": left, "right": right, "difference": difference}
                sphinxContext.dynamicSeverity = None
                if node.severity in sphinxContext.functions:
                    evaluateFunctionDeclaration(sphinxContext.functions[node.severity],
                                                sphinxContext,
                                                {"difference": difference, "left": left, "right": right})
                    if sphinxContext.dynamicSeverity is None or sphinxContext.dynamicSeverity == "pass": # don't process pass
                        sphinxContext.dynamicSeverity = None
                        result = False
            else:
                if isReportRule:
                    resultTags = {"value": result}
                else:
                    resultTags = {}
            if sphinxContext.formulaOptions.traceVariableSetExpressionResult:
                sphinxContext.modelXbrl.info("sphinx:trace",
                     _("%(node)s result %(result)s %(severity)s iteration %(iteration)s"), 
                     sourceFileLine=node.sourceFileLine, node=nodeId, iteration=ruleIteration,
                     result=result,
                     severity=(sphinxContext.dynamicSeverity or node.severity or 
                               ("info" if isReportRule else "error")))
            if ((result or isReportRule) or 
                (sphinxContext.dynamicSeverity and sphinxContext.dynamicSeverity != "pass")):
                severity = (sphinxContext.dynamicSeverity or node.severity or 
                            ("info" if isReportRule else "error"))
                if isinstance(severity, astFunctionReference):
                    severity = severity.name
                logSeverity = {"error" : "ERROR", "warning": "WARNING", "info": "INFO"}[severity]
                if node.message:
                    sphinxContext.modelXbrl.log(logSeverity, name, 
                                                evaluateMessage(node.message, sphinxContext, resultTags, hsBindings),
                                                sourceFileLine=[node.sourceFileLine] + 
                                                [(fact.modelDocument.uri, fact.sourceline) for fact in hsBindings.boundFacts],
                                                severity=severity)
                elif isFormulaRule:
                    sphinxContext.modelXbrl.log(logSeverity,
                                                name,
                                                _("Formula %(severity)s difference %(value)s for %(aspects)s"),
                                                sourceFileLine=[node.sourceFileLine] + 
                                                [(fact.modelDocument.uri, fact.sourceline) for fact in hsBindings.boundFacts],
                                                severity=severity,
                                                value=difference,
                                                aspects=contextView(sphinxContext))
                elif isReportRule:
                    sphinxContext.modelXbrl.log(logSeverity,
                                                name,
                                                _("Report %(severity)s %(value)s for %(aspects)s"),
                                                sourceFileLine=[node.sourceFileLine] + 
                                                [(fact.modelDocument.uri, fact.sourceline) for fact in hsBindings.boundFacts],
                                                severity=severity,
                                                value=result,
                                                aspects=contextView(sphinxContext))
                else:
                    sphinxContext.modelXbrl.log(logSeverity,
                                                name,
                                                _("Validation rule %(severity)s for %(aspects)s"),
                                                sourceFileLine=[node.sourceFileLine] + 
                                                [(fact.modelDocument.uri, fact.sourceline) for fact in hsBindings.boundFacts],
                                                severity=severity,
                                                aspects=contextView(sphinxContext))
            hsBindings.next() # raises StopIteration when done
    except StopIteration:
        if sphinxContext.formulaOptions.traceVariableSetExpressionResult:
            sphinxContext.modelXbrl.info("sphinx:trace",
                 _("%(node)s StopIteration"), 
                 sourceFileLine=node.sourceFileLine, node=nodeId)
    except SphinxException as ex:
        sphinxContext.modelXbrl.log("ERROR",
                                    ex.code,
                                    _("Exception in %(node)s: %(exception)s"),
                                    node=nodeId,
                                    ruleName=name,
                                    exception=ex.message % ex.kwargs,
                                    sourceFileLine=[node.sourceFileLine] + ([ex.node.sourceFileLine] if ex.node is not node else []),
                                    **ex.kwargs)
    if hsBindings is not None:
        hsBindings.close()
    return None

def noop(arg):
    return arg

def evaluateUnaryOperation(node, sphinxContext):
    if node.op == "brackets":  # parentheses around an expression
        return node.expr
    value = evaluate(node.expr, sphinxContext, value=True, fallback=UNBOUND)
    if value is UNBOUND:
        return UNBOUND
    try:
        result = {'+': operator.pos, '-': operator.neg, 'not': operator.not_,
                  'values': noop,
                  }[node.op](value)
        return result
    except KeyError:
        sphinxContext.modelXbrl.error("sphinx:error",
             _("%(node)s operation %(op)s not implemented"), 
             modelObject=node, op=node.op)
    return None

def evaluateValuesIteration(node, sphinxContext):
    hsBindings = sphinxContext.hyperspaceBindings
    if hsBindings.aggregationNode is None:
        sphinxContext.modelXbrl.error("sphinx:warning",
             _("Values iteration expected to be nested in an aggregating function"), 
             modelObject=node)
    else:
        hsBindings.isValuesIteration = True
    return evaluate(node.expr, sphinxContext)

def evaluateVariableAssignment(node, sphinxContext):
    result = evaluate(node.expr, sphinxContext)
    sphinxContext.localVariables[node.variableName] = result
    if node.tagName:
        sphinxContext.tags[node.tagName] = result
    return result

def evaluateVariableReference(node, sphinxContext):
    try:
        return sphinxContext.localVariables[node.variableName]
    except KeyError:
        if node.variableName in sphinxContext.constants:
            return evaluateConstant(sphinxContext.constants[node.variableName], sphinxContext)
        raise SphinxException(node, 
                              "sphinx:variableName", 
                              _("unassigned variable name %(name)s"),
                              name=node.variableName)

def evaluateWith(node, sphinxContext):
    # covered clauses of withExpr match uncovered aspects of expr
    hsBindings = sphinxContext.hyperspaceBindings
    withRestrictionBinding = hsBindings.nodeBinding(node.restrictionExpr, isWithRestrictionNode=True)
    hsBindings.withRestrictionBindings.append(withRestrictionBinding)
    try:
        for varAssignNode in node.variableAssignments:
            evaluateVariableAssignment(varAssignNode, sphinxContext)
        result = evaluate(node.bodyExpr, sphinxContext)
    except Exception as ex:
        del hsBindings.withRestrictionBindings[-1]
        raise ex    # re-throw the exception after removing withstack entry
    del hsBindings.withRestrictionBindings[-1]
    return result

def contextView(sphinxContext, fact=None):
    if isinstance(fact, ModelFact):
        return "{0}[{1}]".format(fact.qname,
                                 ", ".join("{2}={1}".format(aspectName(aspect), 
                                   factAspectValue(fact, aspect, view=True))
                                   for aspect, fact in sphinxContext.hyperspaceBindings.aspectBoundFacts.items()
                                   if factAspectValue(fact, aspect) and aspect != Aspect.CONCEPT))
    else:
        return "[{0}]".format(", ".join("{0}={1}".format(aspectName(aspect), 
                                   factAspectValue(fact, aspect, view=True))
                                   for aspect, fact in sphinxContext.hyperspaceBindings.aspectBoundFacts.items()
                                   if factAspectValue(fact, aspect)))
    
def aspectName(aspect):
    if isinstance(aspect, QName):
        return aspect
    return {Aspect.LOCATION: "tuple",
            Aspect.CONCEPT: "primary",
            Aspect.ENTITY_IDENTIFIER: "entity",  
            Aspect.PERIOD: "period", 
            Aspect.UNIT: "unit", 
            Aspect.NON_XDT_SEGMENT: "segment",
            Aspect.NON_XDT_SCENARIO: "scenario",
            }.get(aspect)
    if aspect in Aspect.label:
        return Aspect.label[aspect]
    else:
        return str(aspect)

def factAspectValue(fact, aspect, view=False):
    if fact is DEFAULT:
        return 'none'
    elif fact is NONDEFAULT:
        return '*'
    elif fact is DEFAULTorNONDEFAULT:
        return '**'
    elif aspect == Aspect.LOCATION:
        parentQname = fact.getparent().qname
        if parentQname == XbrlConst.qnXbrliXbrl: # not tuple
            return NONE
        return parentQname # tuple
    elif aspect == Aspect.CONCEPT:
        return fact.qname
    elif fact.isTuple or fact.context is None:
        return NONE     #subsequent aspects don't exist for tuples
    elif aspect == Aspect.UNIT:
        if fact.unit is None:
            return NONE
        measures = fact.unit.measures
        if measures[1]:
            return "{0} / {1}".format(' '.join(str(m) for m in measures[0]),
                                      ' '.join(str(m) for m in measures[1]))
        else:
            return ' '.join(str(m) for m in measures[0])
    else:
        context = fact.context
        if aspect == Aspect.PERIOD:
            return ("forever" if context.isForeverPeriod else
                XmlUtil.dateunionValue(context.instantDatetime, subtractOneDay=True) if context.isInstantPeriod else
                XmlUtil.dateunionValue(context.startDatetime) + "-" + XmlUtil.dateunionValue(context.endDatetime, subtractOneDay=True))
        elif aspect == Aspect.ENTITY_IDENTIFIER:
            if view:
                return context.entityIdentifier[1]
            else:
                return context.entityIdentifier  # (scheme, identifier)
        elif aspect in (Aspect.COMPLETE_SEGMENT, Aspect.COMPLETE_SCENARIO,
                        Aspect.NON_XDT_SEGMENT, Aspect.NON_XDT_SCENARIO):
            return ''.join(XmlUtil.xmlstring(elt, stripXmlns=True, prettyPrint=True)
                           for elt in context.nonDimValues(aspect))
        elif aspect == Aspect.DIMENSIONS:
            return context.dimAspects(fact.xpCtx.defaultDimensionAspects)
        elif isinstance(aspect, QName):
            dimValue = context.dimValue(aspect)
            if dimValue is None:
                return NONE
            else:
                if isinstance(dimValue, QName): #default dim
                    return dimValue
                elif dimValue.isExplicit:
                    return dimValue.memberQname
                else: # explicit
                    return dimValue.typedMember.xValue # typed element value



evaluator = {
    "astAnnotationDeclaration":   evaluateAnnotationDeclaration,
    "astBinaryOperation":         evaluateBinaryOperation,
    "astComment":                 evaluateNoOp,
    "astFor":                     evaluateFor,
    "astFormulaRule":             evaluateRule,
    "astFunctionDeclaration":     evaluateFunctionDeclaration,
    "astFunctionReference":       evaluateFunctionReference,
    "astHyperspaceExpression":    evaluateHyperspaceExpression,
    "astIf":                      evaluateIf,
    "astMessage":                 evaluateMessage,
    "astMethodReference":         evaluateMethodReference,
    "astNamespaceDeclaration":    evaluateNoOp,
    "astNode":                    evaluateNoOp,
    "astNoOp":                    evaluateNoOp,
    "astNumericLiteral":          evaluateNumericLiteral,
    "astPreconditionDeclaration": evaluatePreconditionDeclaration,
    "astQnameLiteral":            evaluateQnameLiteral,
    "astReportRule":              evaluateRule,
    "astSourceFile":              evaluateNoOp,
    "astRuleBasePrecondition":    evaluateRuleBasePrecondition,
    "astPreconditionReference":   evaluatePreconditionReference,
    "astStringLiteral":           evaluateStringLiteral,
    "astTagAssignment":           evaluateTagAssignment,
    "astTagReference":            evaluateTagReference,
    "astValidationRule":          evaluateRule,
    "astValuesIteration":         evaluateValuesIteration,
    "astVariableAssignment":      evaluateVariableAssignment,
    "astVariableReference":       evaluateVariableReference,
    "astUnaryOperation":          evaluateUnaryOperation,
    "astWith":                    evaluateWith,
          }
        
SphinxMethodsModuleInit()
'''
DialogRssWatchExtender extends DialogRssWatch for XBRL databases.

It is separate from the xbrlDB __init__.py module so that it can be removed when 
compiling server versions where Python has no GUI facilities.  The imports of GUI
facilities would cause compilation of the server-related modules to fail, otherwise.

(c) Copyright 2013 Mark V Systems Limited, California US, All rights reserved.  
Mark V copyright applies to this software, which is licensed according to the terms of Arelle(r).
and does not apply to the XBRL US Database schema and description.

'''

def dialogRssWatchDBextender(dialog, frame, row, options, cntlr, openFileImage, openDatabaseImage):
    from tkinter import PhotoImage, N, S, E, W
    from tkinter.simpledialog import askstring
    from arelle.CntlrWinTooltip import ToolTip
    from arelle.UiUtil import gridCell, label
    try:
        from tkinter.ttk import Button
    except ImportError:
        from ttk import Button
        
    def enterConnectionString():
        from arelle.DialogUserPassword import askDatabase
        # (user, password, host, port, database)
        db = askDatabase(cntlr.parent, dialog.cellDBconnection.value.split(',') if dialog.cellDBconnection.value else None)
        if db:
            dbConnectionString = ','.join(db)
            dialog.options["xbrlDBconnection"] = dbConnectionString 
            dialog.cellDBconnection.setValue(dbConnectionString)
        else:  # deleted
            dialog.options.pop("xbrlDBconnection", "")  # remove entry
    label(frame, 1, row, "DB Connection:")
    dialog.cellDBconnection = gridCell(frame,2, row, options.get("xbrlDBconnection",""))
    ToolTip(dialog.cellDBconnection, text=_("Enter an XBRL Database (Postgres) connection string.  "
                                           "E.g., host,port,user,password,db[,timeout].  "), wraplength=240)
    enterDBconnectionButton = Button(frame, image=openDatabaseImage, width=12, command=enterConnectionString)
    enterDBconnectionButton.grid(row=row, column=3, sticky=W)

#!/usr/bin/env python
#
# this script generates a testcase variations file for entry point checking
#

import os, fnmatch, xml.dom.minidom, datetime

def main():
    # the top directory where to generate the test case (and relative file names in the variations)
    topDirectory = "C:\\temp\\editaxonomy20110314"
    testcaseName = "EDInet test cases"
    ownerName = "Hugh Wallis"
    ownerEmail = "hughwallis@xbrl.org"
    
    entryRelativeFilePaths = []
    for root, dirs, files in os.walk(topDirectory):
        for fileName in files:
            if fnmatch.fnmatch(fileName, '*.xsd'):
                fullFilePath = os.path.join(root, fileName)
                entryRelativeFilePaths.append( os.path.relpath(fullFilePath, topDirectory) )

    lines = [
        '<?xml version="1.0" encoding="UTF-8"?>',
        '<!-- Copyright 2011 XBRL International.  All Rights Reserved. -->',
        '<?xml-stylesheet type="text/xsl" href="http://www.xbrl.org/Specification/formula/REC-2009-06-22/conformance/infrastructure/test.xsl"?>',
        '<testcase name="{0}" date="{1}" '.format(testcaseName,datetime.date.today()),
        ' xmlns="http://xbrl.org/2008/conformance"',
        ' xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"',
        ' xsi:schemaLocation="http://xbrl.org/2008/conformance http://www.xbrl.org/Specification/formula/REC-2009-06-22/conformance/infrastructure/test.xsd">',
        '  <creator>',
        '  <name>{0}</name>'.format(ownerName),
        '  <email>{0}</email>'.format(ownerEmail),
        '  </creator>',
        '  <name>{0}</name>'.format(ownerEmail),
        '  <description>{0}</description>'.format(testcaseName),
        ]
    
    num = 1
    for entryFile in entryRelativeFilePaths:
        fileName = os.path.basename(entryFile)
        lines.append("  <variation name='{0}' id='V-{1}'>".format(fileName, num))
        num += 1
        lines.append("    <description>{0}</description>".format(fileName))
        lines.append("    <data>")
        lines.append("       <xsd readMeFirst='true'>{0}</xsd>".format(entryFile.replace("\\","/")))
        lines.append("    </data>")
        lines.append("    <result expected='valid'/>")
        lines.append("  </variation>")
        
    lines.append('</testcase>')
        
    with open( os.path.join(topDirectory, "testcase.xml"), "w") as fh:
        fh.write('\n'.join(lines))

if __name__ == "__main__":
    main()


from .gen.tags import _Tags

class Tags(_Tags):
    """Tags resource"""
    pass

#!/usr/bin/python

''' SAX parser implementation to prepare an Ordnance Survey
    GML file (.gml or .gz) so that it is ready to be loaded by OGR 1.9
    or above.
    The parser promotes the fid attribute to a child element.
    Output is via stdout and is UTF-8 encoded.

    usage: python prepgml4ogr.py file.gml
'''

import sys
import os.path
import gzip
import zipfile
from xml.sax import make_parser
from xml.sax.handler import ContentHandler
from xml.sax import saxutils


class gmlhandler(ContentHandler):

    def __init__(self, preparer):
        # The class that will prepare the features
        self.preparer = preparer
        # Flag to indicate if we have encountered the first element yet
        self.first_elm = True
        self.feat = None
        self.recording = False

    def startElement(self, name, attrs):
        if self.first_elm:
            # Output the xml declaration prior to the first element,
            # done here instead of in startDocument to allow us to avoid
            # outputting the declaration when we try and parse non XML content
            # as can happen when we parse all files in a zip archive
            self.first_elm = False
            output('<?xml version="1.0" ?>')
        try:
            name = name.split(':')[1]
        except IndexError:
            pass
        # Determine if we are interested
        # in starting to record the raw
        # XML string so we can prepare
        # the feature when the feature ends
        if name in self.preparer.feat_types:
            self.buffer = []
            self.recording = True
        # Process the attributes
        tmp = '<' + name
        for (name, value) in attrs.items():
            try:
                name = name.split(':')[1]
            except IndexError:
                pass
            tmp += ' %s=%s' % (name, saxutils.quoteattr(value))
        tmp += '>'
        if self.recording:
            self.buffer.append(tmp)
        else:
            output(tmp)
        return

    def characters(self, ch):
        if len(ch.strip()) > 0:
            if self.recording:
                self.buffer.append(saxutils.escape(ch))
            else:
                output(saxutils.escape(ch))

    def endElement(self, name):
        try:
            name = name.split(':')[1]
        except IndexError:
            pass
        if self.recording:
            self.buffer.append('</' + name + '>')
        else:
            output('</' + name + '>')
        if name in self.preparer.feat_types:
            self.recording = False
            output(self.preparer.prepare_feature(''.join(self.buffer)))
            self.buffer = []


def output(str):
    try:
        sys.stdout.write(str.encode('utf_8', 'xmlcharrefreplace').decode('utf_8'))
    except UnicodeEncodeError:
        sys.stdout.write(str.encode('utf_8', 'xmlcharrefreplace'))


class prep_gml():

    def __init__(self, inputfile):
        self.feat_types = []

    def get_feat_types(self):
        return self.feat_types

    def prepare_feature(self, feat_str):
        return feat_str


def main():
    if len(sys.argv) < 2:
        print('usage: python prepgml4ogr.py file [[prep_module.]prep_class]')
        sys.exit(1)

    inputfile = sys.argv[1]
    if os.path.exists(inputfile):

        # Create an instance of a preparer
        # class which is used to prepare
        # features as they are read
        prep_class = 'prep_gml'
        try:
            prep_class = sys.argv[2]
        except IndexError:
            pass
        prep_class = get_preparer(prep_class)
        preparer = prep_class(inputfile)

        parser = make_parser()
        parser.setContentHandler(gmlhandler(preparer))

        if os.path.splitext(inputfile)[1].lower() == '.zip':
            archive = zipfile.ZipFile(inputfile, 'r')
            for filename in archive.namelist():
                file = archive.open(filename)
                try:
                    parser.parse(file)
                except:
                    # Ignore any files that can't be parsed
                    pass
        else:
            if os.path.splitext(inputfile)[1].lower() == '.gz':
                file = gzip.open(inputfile, 'r')
            else:
                # Assume non compressed gml, xml or no extension
                file = open(inputfile, 'r')
            parser.parse(file)

    else:
        print('Could not find input file: ' + inputfile)


def get_preparer(prep_class):
    parts = prep_class.split('.')
    if len(parts) > 1:
        prep_module = parts[0]
        prep_module = __import__(prep_module)
        prep_class = getattr(prep_module, parts[1])
    else:
        prep_class = globals()[prep_class]
    return prep_class

if __name__ == '__main__':
    main()

from django.contrib import admin

# Register your models here.

import csv
from . import eigen

import axelrod.interaction_utils as iu

from numpy import mean, nanmedian, std

try:
    # Python 2
    from StringIO import StringIO
except ImportError:
    # Python 3
    from io import StringIO


class ResultSet(object):
    """A class to hold the results of a tournament."""

    def __init__(self, players, interactions, with_morality=True):
        """
        Parameters
        ----------
            players : list
                a list of player objects.
            interactions : list
                a list of dictionaries mapping tuples of player indices to
                interactions (1 for each repetition)
            with_morality : bool
                a flag to determine whether morality metrics should be
                calculated.
        """
        self.players = players
        self.nplayers = len(players)
        self.interactions = interactions
        self.nrepetitions = len(interactions)

        # Calculate all attributes:
        self.build_all(with_morality)

    def build_all(self, with_morality):
        """Build all the results. In a seperate method to make inheritance more
        straightforward"""
        self.wins = self.build_wins()
        self.match_lengths = self.build_match_lengths()

        self.scores = self.build_scores()
        self.normalised_scores = self.build_normalised_scores()
        self.ranking = self.build_ranking()
        self.ranked_names = self.build_ranked_names()
        self.payoffs = self.build_payoffs()
        self.payoff_matrix = self.build_payoff_matrix()
        self.payoff_stddevs = self.build_payoff_stddevs()
        self.score_diffs = self.build_score_diffs()
        self.payoff_diffs_means = self.build_payoff_diffs_means()

        if with_morality:
            self.cooperation = self.build_cooperation()
            self.normalised_cooperation = self.build_normalised_cooperation()
            self.vengeful_cooperation = self.build_vengeful_cooperation()
            self.cooperating_rating = self.build_cooperating_rating()
            self.good_partner_matrix = self.build_good_partner_matrix()
            self.good_partner_rating = self.build_good_partner_rating()
            self.eigenmoses_rating = self.build_eigenmoses_rating()
            self.eigenjesus_rating = self.build_eigenjesus_rating()

    @property
    def _null_results_matrix(self):
        """
        Returns:
        --------
            A null matrix (i.e. fully populated with zero values) using
            lists of the form required for the results dictionary.

            i.e. one row per player, containing one element per opponent (in
            order of player index) which lists values for each repetition.
        """
        plist = list(range(self.nplayers))
        replist = list(range(self.nrepetitions))
        return [[[0 for j in plist] for i in plist] for r in replist]

    def build_match_lengths(self):
        """
        Returns:
        --------
            The match lengths. List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of repetitions and MLi is a list of the form:

            [Pli1, PLi2, Pli3, ..., Plim]

            Where m is the number of players and Plij is of the form:

            [aij1, aij2, aij3, ..., aijk]

            Where k is the number of players and aijk is the length of the match
            between player j and k in repetition i.
        """
        match_lengths = self._null_results_matrix

        for rep in range(self.nrepetitions):

            for player_pair_index, interactions in self.interactions[rep].items():
                player, opponent = player_pair_index
                match_lengths[rep][player][opponent] = len(interactions)

                if player != opponent:  # Match lengths are symmetric
                    match_lengths[rep][opponent][player] = len(interactions)

        return match_lengths

    def build_scores(self):
        """
        Returns:
        --------
            The total scores per player for each repetition lengths.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of repetitions and pij is the total score
            obtained by each player in repetition j.

            In Axelrod's original tournament, there were no self-interactions
            (e.g. player 1 versus player 1) and so these are also ignored.
        """
        scores = [[0 for rep in range(self.nrepetitions)] for _ in
                  range(self.nplayers)]

        for rep, inter_dict in enumerate(self.interactions):
            for index_pair, interactions in inter_dict.items():
                if index_pair[0] != index_pair[1]: # Ignoring self interactions
                    final_scores = iu.compute_final_score(interactions)
                    for player in range(2):
                        player_index = index_pair[player]
                        player_score = final_scores[player]
                        scores[player_index][rep] += player_score

        return scores

    def build_ranked_names(self):
        """
        Returns:
        --------
            Returns the ranked names. A list of names as calculated by
            self.ranking.
        """
        return [str(self.players[i]) for i in self.ranking]

    def build_wins(self):
        """
        Returns:
        --------

            The total wins per player for each repetition lengths.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of repetitions and pij is the total wins
            obtained by each player in repetition j.

            In Axelrod's original tournament, there were no self-interactions
            (e.g. player 1 versus player 1) and so these are also ignored.
        """
        wins = [[0 for rep in range(self.nrepetitions)] for _ in
                range(self.nplayers)]

        for rep, inter_dict in enumerate(self.interactions):
            for index_pair, interactions in inter_dict.items():
                if index_pair[0] != index_pair[1]:  # Ignore self interactions
                    for player in range(2):
                        player_index = index_pair[player]

                        winner_index = iu.compute_winner_index(interactions)
                        if winner_index is not False and player == winner_index:
                            wins[player_index][rep] += 1

        return wins

    def build_normalised_scores(self):
        """
        Returns:
        --------

            The total mean scores per turn per layer for each repetition
            lengths.  List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of repetitions and pij is the mean scores per
            turn obtained by each player in repetition j.

            In Axelrod's original tournament, there were no self-interactions
            (e.g. player 1 versus player 1) and so these are also ignored.
        """
        normalised_scores = [
            [[] for rep in range(self.nrepetitions)] for _ in
            range(self.nplayers)]

        # Getting list of all per turn scores for each player for each rep
        for rep, inter_dict in enumerate(self.interactions):
            for index_pair, interactions in inter_dict.items():
                if index_pair[0] != index_pair[1]:  # Ignore self interactions
                    scores_per_turn = iu.compute_final_score_per_turn(interactions)
                    for player in range(2):
                        player_index = index_pair[player]
                        score_per_turn = scores_per_turn[player]
                        normalised_scores[player_index][rep].append(score_per_turn)

        # Obtaining mean scores and overwriting corresponding entry in
        # normalised scores
        for i, rep in enumerate(normalised_scores):
            for j, player_scores in enumerate(rep):
                normalised_scores[i][j] = mean(player_scores)

        return normalised_scores

    def build_ranking(self):
        """
        Returns:
        --------

            The ranking. List of the form:

            [R1, R2, R3..., Rn]

            Where n is the number of players and Rj is the rank of the jth player
            (based on median normalised score).
        """
        return sorted(range(self.nplayers),
                      key=lambda i: -nanmedian(self.normalised_scores[i]))

    def build_payoffs(self):
        """
        Returns:
        --------

            The list of per turn payoffs.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of players and pij is a list of the form:

            [uij1, uij2, ..., uijk]

            Where k is the number of repetitions and uijk is the list of utilities
            obtained by player i against player j in each repetition.
        """
        plist = list(range(self.nplayers))
        payoffs = [[[] for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                utilities = []
                for rep in self.interactions:

                    if (player, opponent) in rep:
                        interactions = rep[(player, opponent)]
                        utilities.append(iu.compute_final_score_per_turn(interactions)[0])
                    if (opponent, player) in rep:
                        interactions = rep[(opponent, player)]
                        utilities.append(iu.compute_final_score_per_turn(interactions)[1])

                    payoffs[player][opponent] = utilities
        return payoffs

    def build_payoff_matrix(self):
        """
        Returns:
        --------
            The mean of per turn payoffs.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of players and pij is a list of the form:

            [uij1, uij2, ..., uijk]

            Where k is the number of repetitions and u is the mean utility (over
            all repetitions) obtained by player i against player j.
        """
        plist = list(range(self.nplayers))
        payoff_matrix = [[[] for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                utilities = self.payoffs[player][opponent]

                if utilities:
                    payoff_matrix[player][opponent] = mean(utilities)
                else:
                    payoff_matrix[player][opponent] = 0

        return payoff_matrix

    def build_payoff_stddevs(self):
        """
        Returns:
        --------

            The mean of per turn payoffs.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of players and pij is a list of the form:

            [uij1, uij2, ..., uijk]

            Where k is the number of repetitions and u is the standard
            deviation of the utility (over all repetitions) obtained by player
            i against player j.
        """
        plist = list(range(self.nplayers))
        payoff_stddevs = [[[0] for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                utilities = self.payoffs[player][opponent]

                if utilities:
                    payoff_stddevs[player][opponent] = std(utilities)
                else:
                    payoff_stddevs[player][opponent] = 0

        return payoff_stddevs

    def build_score_diffs(self):
        """
        Returns:
        --------

            Returns the score differences between players.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where m is the number of players and pij is a list of the form:

            [uij1, uij2, ..., uijk]

            Where k is the number of repetitions and uijm is the difference of the
            scores per turn between player i and j in repetition m.
        """
        plist = list(range(self.nplayers))
        score_diffs = [[[0] * self.nrepetitions for opponent in plist]
                       for player in plist]

        for player in plist:
            for opponent in plist:
                for r, rep in enumerate(self.interactions):
                    if (player, opponent) in rep:
                        scores = iu.compute_final_score_per_turn(rep[(player,
                                                                      opponent)])
                        diff = (scores[0] - scores[1])
                        score_diffs[player][opponent][r] = diff
                    if (opponent, player) in rep:
                        scores = iu.compute_final_score_per_turn(rep[(opponent,
                                                                      player)])
                        diff = (scores[1] - scores[0])
                        score_diffs[player][opponent][r] = diff
        return score_diffs

    def build_payoff_diffs_means(self):
        """
        Returns:
        --------

            The score differences between players.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where pij is the mean difference of the
            scores per turn between player i and j in repetition m.
        """
        plist = list(range(self.nplayers))
        payoff_diffs_means = [[0 for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                diffs = []
                for rep in self.interactions:
                    if (player, opponent) in rep:
                        scores = iu.compute_final_score_per_turn(rep[(player,
                                                                      opponent)])
                        diffs.append(scores[0] - scores[1])
                    if (opponent, player) in rep:
                        scores = iu.compute_final_score_per_turn(rep[(opponent,
                                                                      player)])
                        diffs.append(scores[1] - scores[0])
                if diffs:
                    payoff_diffs_means[player][opponent] = mean(diffs)
                else:
                    payoff_diffs_means[player][opponent] = 0
        return payoff_diffs_means

    def build_cooperation(self):
        """
        Returns:
        --------

            The list of cooperation counts.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where pij is the total number of cooperations over all repetitions
            played by player i against player j.
        """
        plist = list(range(self.nplayers))
        cooperations = [[0 for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                if player != opponent:
                    for rep in self.interactions:
                        coop_count = 0

                        if (player, opponent) in rep:
                            interactions = rep[(player, opponent)]
                            coop_count = iu.compute_cooperations(interactions)[0]
                        if (opponent, player) in rep:
                            interactions = rep[(opponent, player)]
                            coop_count = iu.compute_cooperations(interactions)[1]

                        cooperations[player][opponent] += coop_count
        return cooperations

    def build_normalised_cooperation(self):
        """
        Returns:
        --------

            The list of per turn cooperation counts.
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pin]

            Where pij is the mean number of
            cooperations per turn played by player i against player j in each
            repetition.
        """
        plist = list(range(self.nplayers))
        normalised_cooperations = [[0 for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                coop_counts = []
                for rep in self.interactions:

                    if (player, opponent) in rep:
                        interactions = rep[(player, opponent)]
                        coop_counts.append(iu.compute_normalised_cooperation(interactions)[0])

                    if (opponent, player) in rep:
                        interactions = rep[(opponent, player)]
                        coop_counts.append(iu.compute_normalised_cooperation(interactions)[1])

                    if ((player, opponent) not in rep) and ((opponent, player) not in rep):
                        coop_counts.append(0)

                    # Mean over all reps:
                    normalised_cooperations[player][opponent] = mean(coop_counts)
        return normalised_cooperations

    def build_vengeful_cooperation(self):
        """
        Returns:
        --------

            The vengeful cooperation matrix derived from the
            normalised cooperation matrix:

                Dij = 2(Cij - 0.5)
        """
        return [[2 * (element - 0.5) for element in row]
                for row in self.normalised_cooperation]

    def build_cooperating_rating(self):
        """
        Returns:
        --------

            The list of cooperation counts
            List of the form:

            [ML1, ML2, ML3..., MLn]

            Where n is the number of players and MLi is a list of the form:

            [pi1, pi2, pi3, ..., pim]

            Where pij is the total number of cooperations divided by the total
            number of turns over all repetitions played by player i against
            player j.
        """

        plist = list(range(self.nplayers))
        total_length_v_opponent = [zip(*[rep[player_index] for
                                         rep in self.match_lengths])
                                   for player_index in plist]
        lengths = [[sum(e) for j, e in enumerate(row) if i != j] for i, row in
                   enumerate(total_length_v_opponent)]

        # Max is to deal with edge cases of matches that have no turns
        return [sum(cs) / max(1, float(sum(ls))) for cs, ls
                in zip(self.cooperation, lengths)]

    def build_good_partner_matrix(self):
        """
        Returns:
        --------

            An n by n matrix of good partner ratings for n players i.e. an n by
            n matrix where n is the number of players. Each row (i) and column
            (j) represents an individual player and the value Pij is the sum of
            the number of repetitions where player i cooperated as often or
            more than opponent j.
        """

        plist = list(range(self.nplayers))
        good_partner_matrix = [[0 for opponent in plist] for player in plist]

        for player in plist:
            for opponent in plist:
                if player != opponent:
                    for rep in self.interactions:

                        if (player, opponent) in rep:
                            interaction = rep[(player, opponent)]
                            coops = iu.compute_cooperations(interaction)
                            if coops[0] >= coops[1]:
                                good_partner_matrix[player][opponent] += 1

                        if (opponent, player) in rep:
                            interaction = rep[(opponent, player)]
                            coops = iu.compute_cooperations(interaction)
                            if coops[0] <= coops[1]:
                                good_partner_matrix[player][opponent] += 1

        return good_partner_matrix

    def build_good_partner_rating(self):
        """
        Returns:
        --------

        A list of good partner ratings ordered by player index.
        """
        plist = list(range(self.nplayers))
        good_partner_rating = []

        for player_index in plist:
            total_interactions = 0
            for rep in self.interactions:
                total_interactions += len(
                    [pair for pair in rep.keys()
                     if player_index in pair and pair[0] != pair[1]])
            # Max is to deal with edge case of matchs with no turns
            rating = sum(self.good_partner_matrix[player_index]) / max(1, float(total_interactions))
            good_partner_rating.append(rating)

        return good_partner_rating

    def build_eigenjesus_rating(self):
        """
        Returns:
        --------

        The eigenjesus rating as defined in:
        http://www.scottaaronson.com/morality.pdf
        """
        eigenvector, eigenvalue = eigen.principal_eigenvector(
                self.normalised_cooperation)
        return eigenvector.tolist()

    def build_eigenmoses_rating(self):
        """
        Returns:
        --------

        The eigenmoses rating as defined in:
        http://www.scottaaronson.com/morality.pdf
        """
        eigenvector, eigenvalue = eigen.principal_eigenvector(
                self.vengeful_cooperation)
        return eigenvector.tolist()

    def csv(self):
        """
        Returns:
        --------

        The string of the total scores per player (columns) per repetition
        (rows).
        """
        csv_string = StringIO()
        header = ",".join(self.ranked_names) + "\n"
        csv_string.write(header)
        writer = csv.writer(csv_string, lineterminator="\n")
        for irep in range(self.nrepetitions):
            data = [self.normalised_scores[rank][irep]
                    for rank in self.ranking]
            writer.writerow(list(map(str, data)))
        return csv_string.getvalue()


class ResultSetFromFile(ResultSet):
    """A class to hold the results of a tournament.

    Initialised by a csv file of the format:


    [p1index, p2index, p1name, p2name, p1rep1ac1p2rep1ac1p1rep1ac2p2rep1ac2,
    ...]
    [0, 1, Defector, Cooperator, DCDCDC, DCDCDC, DCDCDC,...]
    [0, 2, Defector, Alternator, DCDDDC, DCDDDC, DCDDDC,...]
    [1, 2, Cooperator, Alternator, CCCDCC, CCCDCC, CCCDCC,...]
    """

    def __init__(self, filename, with_morality=True):
        """
        Parameters
        ----------
            filename : string
                name of a file of the correct file.
            with_morality : bool
                a flag to determine whether morality metrics should be
                calculated.
        """
        self.players, self.interactions = self._read_csv(filename)
        self.nplayers = len(self.players)
        self.nrepetitions = len(self.interactions)

        # Calculate all attributes:
        self.build_all(with_morality)

    def _read_csv(self, filename):
        """
        Reads from a csv file of the format:

        p1index, p2index, p1name, p2name, p1rep1ac1p2rep1ac1p1rep1ac2p2rep1ac2,
        ...
        0, 1, Defector, Cooperator, DCDCDC, DCDCDC, DCDCDC,...
        0, 2, Defector, Alternator, DCDDDC, DCDDDC, DCDDDC,...
        1, 2, Cooperator, Alternator, CCCDCC, CCCDCC, CCCDCC,...

        Returns
        -------

            A tuple:
                - First element: list of player names
                - Second element: interactions (list of dictionaries mapping
                  index indices to interactions)
        """
        players_d = {}
        interactions_d = {}
        with open(filename, 'r') as f:
            for row in csv.reader(f):
                index_pair = (int(row[0]), int(row[1]))
                players = (row[2], row[3])
                inters = row[4:]

                # Build a dictionary mapping indices to players
                # This is temporary to make sure the ordering of the players
                # matches the indices
                for index, player in zip(index_pair, players):
                    if index not in players:
                        players_d[index] = player

                # Build a dictionary mapping indices to list of interactions
                # This is temporary (as we do not know the number of
                # interactions at this point.
                interactions_d[index_pair] = [self._string_to_interactions(inter)
                                              for inter in inters]
        nreps = len(inters)

        # Create an ordered list of players
        players = []
        for i in range(len(players_d)):
            players.append(players_d[i])

        # Create a list of dictionaries
        interactions = []
        for rep in range(nreps):
            pair_to_interactions_d = {}
            for index_pair, inters in interactions_d.items():
                pair_to_interactions_d[index_pair] = inters[rep]
            interactions.append(pair_to_interactions_d)

        return players, interactions

    def _string_to_interactions(self, string):
        """
        Converts a compact string representation of an interaction to an
        interaction:

        'CDCDDD' -> [('C', 'D'), ('C', 'D'), ('D', 'D')]
        """
        return iu.string_to_interactions(string)

from axelrod import Actions, Player

C, D = Actions.C, Actions.D

class MindController(Player):
    """A player that changes the opponents strategy to cooperate."""

    name = 'Mind Controller'
    classifier = {
        'memory_depth': -10,
        'stochastic': False,
        'makes_use_of': set(),
        'inspects_source': False,
        'manipulates_source': True,  # Finds out what opponent will do
        'manipulates_state': False
    }

    @staticmethod
    def strategy(opponent):
        """
        Alters the opponents strategy method to be a lambda function which
        always returns C. This player will then always return D to take
        advantage of this
        """

        opponent.strategy = lambda opponent: C
        return D


class MindWarper(Player):
    """
    A player that changes the opponent's strategy but blocks changes to
    its own.
    """

    name = 'Mind Warper'
    classifier = {
        'memory_depth': -10,
        'stochastic': False,
        'makes_use_of': set(),
        'inspects_source': False,
        'manipulates_source': True,  # changes what opponent will do
        'manipulates_state': False
    }

    def __setattr__(self, name, val):
        if name == 'strategy':
            pass
        else:
            self.__dict__[name] = val

    @staticmethod
    def strategy(opponent):
        opponent.strategy = lambda opponent: C
        return D


class MindBender(MindWarper):
    """
    A player that changes the opponent's strategy by modifying the internal
    dictionary.
    """

    name = 'Mind Bender'
    classifier = {
        'memory_depth': -10,
        'makes_use_of': set(),
        'stochastic': False,
        'inspects_source': False,
        'manipulates_source': True,  # changes what opponent will do
        'manipulates_state': False
    }

    @staticmethod
    def strategy(opponent):
        opponent.__dict__['strategy'] = lambda opponent: C
        return D

"""Tests for the Ecosystem class"""

import unittest

import axelrod


class TestEcosystem(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        cooperators = axelrod.Tournament(players=[
            axelrod.Cooperator(),
            axelrod.Cooperator(),
            axelrod.Cooperator(),
            axelrod.Cooperator(),
        ])
        defector_wins = axelrod.Tournament(players=[
            axelrod.Cooperator(),
            axelrod.Cooperator(),
            axelrod.Cooperator(),
            axelrod.Defector(),
        ])
        cls.res_cooperators = cooperators.play()
        cls.res_defector_wins = defector_wins.play()

    def test_init(self):
        """Are the populations created correctly?"""

        # By default create populations of equal size
        eco = axelrod.Ecosystem(self.res_cooperators)
        pops = eco.population_sizes
        self.assertEqual(eco.nplayers, 4)
        self.assertEqual(len(pops), 1)
        self.assertEqual(len(pops[0]), 4)
        self.assertAlmostEqual(sum(pops[0]), 1.0)
        self.assertEqual(list(set(pops[0])), [0.25])

        # Can pass list of initial population distributions
        eco = axelrod.Ecosystem(self.res_cooperators, population=[.7, .25, .03, .02])
        pops = eco.population_sizes
        self.assertEqual(eco.nplayers, 4)
        self.assertEqual(len(pops), 1)
        self.assertEqual(len(pops[0]), 4)
        self.assertAlmostEqual(sum(pops[0]), 1.0)
        self.assertEqual(pops[0], [.7, .25, .03, .02])

        # Distribution will automatically normalise
        eco = axelrod.Ecosystem(self.res_cooperators, population=[70, 25, 3, 2])
        pops = eco.population_sizes
        self.assertEqual(eco.nplayers, 4)
        self.assertEqual(len(pops), 1)
        self.assertEqual(len(pops[0]), 4)
        self.assertAlmostEqual(sum(pops[0]), 1.0)
        self.assertEqual(pops[0], [.7, .25, .03, .02])

        # If passed list is of incorrect size get error
        self.assertRaises(TypeError, axelrod.Ecosystem, self.res_cooperators, population=[.7, .2, .03, .1, .1])

        # If passed list has negative values
        self.assertRaises(TypeError, axelrod.Ecosystem, self.res_cooperators, population=[.7, -.2, .03, .2])

    def test_fitness(self):
        fitness = lambda p: 2 * p
        eco = axelrod.Ecosystem(self.res_cooperators, fitness=fitness)
        self.assertTrue(eco.fitness(10), 20)

    def test_cooperators(self):
        """Are cooperators stable over time?"""

        eco = axelrod.Ecosystem(self.res_cooperators)
        eco.reproduce(100)
        pops = eco.population_sizes
        self.assertEqual(len(pops), 101)
        for p in pops:
            self.assertEqual(len(p), 4)
            self.assertEqual(sum(p), 1.0)
            self.assertEqual(list(set(p)), [0.25])

    def test_defector_wins(self):
        """Does one defector win over time?"""

        eco = axelrod.Ecosystem(self.res_defector_wins)
        eco.reproduce(1000)
        pops = eco.population_sizes
        self.assertEqual(len(pops), 1001)
        for p in pops:
            self.assertEqual(len(p), 4)
            self.assertAlmostEqual(sum(p), 1.0)
        last = pops[-1]
        self.assertAlmostEqual(last[0], 0.0)
        self.assertAlmostEqual(last[1], 0.0)
        self.assertAlmostEqual(last[2], 0.0)
        self.assertAlmostEqual(last[3], 1.0)

"""Test for the qlearner strategy."""

import random

import axelrod
from axelrod import simulate_play, Game

from .test_player import TestPlayer, test_responses

C, D = axelrod.Actions.C, axelrod.Actions.D


class TestRiskyQLearner(TestPlayer):

    name = 'Risky QLearner'
    player = axelrod.RiskyQLearner
    expected_classifier = {
        'memory_depth': float('inf'),
        'stochastic': True,
        'makes_use_of': set(["game"]),
        'inspects_source': False,
        'manipulates_source': False,
        'manipulates_state': False
    }

    def test_payoff_matrix(self):
        (R, P, S, T) = Game().RPST()
        payoff_matrix = {C: {C: R, D: S}, D: {C: T, D: P}}
        p1 = self.player()
        self.assertEqual(p1.payoff_matrix, payoff_matrix)

    def test_qs_update(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.RiskyQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs, {'': {C: 0, D: 0.9}, '0.0': {C: 0, D: 0}})
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs,{'': {C: 0, D: 0.9}, '0.0': {C: 2.7, D: 0}, 'C1.0': {C: 0, D: 0}})

    def test_vs_update(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.RiskyQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs, {'': 0.9, '0.0': 0})
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs,{'': 0.9, '0.0': 2.7, 'C1.0': 0})

    def test_prev_state_updates(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.RiskyQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, '0.0')
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, 'C1.0')

    def test_strategy(self):
        """Tests that it chooses the best strategy."""
        random.seed(5)
        p1 = axelrod.RiskyQLearner()
        p1.state = 'CCDC'
        p1.Qs = {'': {C: 0, D: 0}, 'CCDC': {C: 2, D: 6}}
        p2 = axelrod.Cooperator()
        test_responses(self, p1, p2, [], [], [C, D, C, C, D, C, C])

    def test_reset_method(self):
        """
        tests the reset method
        """
        P1 = axelrod.RiskyQLearner()
        P1.Qs = {'': {C: 0, D: -0.9}, '0.0': {C: 0, D: 0}}
        P1.Vs = {'': 0, '0.0': 0}
        P1.history = [C, D, D, D]
        P1.prev_state = C
        P1.reset()
        self.assertEqual(P1.prev_state, '')
        self.assertEqual(P1.history, [])
        self.assertEqual(P1.Vs, {'': 0})
        self.assertEqual(P1.Qs, {'': {C: 0, D: 0}})


class TestArrogantQLearner(TestPlayer):

    name = 'Arrogant QLearner'
    player = axelrod.ArrogantQLearner
    expected_classifier = {
        'memory_depth': float('inf'),  # Long memory
        'stochastic': True,
        'makes_use_of': set(["game"]),
        'inspects_source': False,
        'manipulates_source': False,
        'manipulates_state': False
    }

    def test_qs_update(self):
        """
        Test that the q and v values update
        """
        random.seed(5)
        p1 = axelrod.ArrogantQLearner()
        p2 = axelrod.Cooperator()
        play_1, play_2 = simulate_play(p1, p2)
        self.assertEqual(p1.Qs, {'': {C: 0, D: 0.9}, '0.0': {C: 0, D: 0}})
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs,{'': {C: 0, D: 0.9}, '0.0': {C: 2.7, D: 0}, 'C1.0': {C: 0, D: 0}})

    def test_vs_update(self):
        """
        Test that the q and v values update
        """
        random.seed(5)
        p1 = axelrod.ArrogantQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs, {'': 0.9, '0.0': 0})
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs,{'': 0.9, '0.0': 2.7, 'C1.0': 0})

    def test_prev_state_updates(self):
        """
        Test that the q and v values update
        """
        random.seed(5)
        p1 = axelrod.ArrogantQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, '0.0')
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, 'C1.0')

    def test_strategy(self):
        """Tests that it chooses the best strategy."""
        random.seed(9)
        p1 = axelrod.ArrogantQLearner()
        p1.state = 'CCDC'
        p1.Qs = {'': {C: 0, D: 0}, 'CCDC': {C: 2, D: 6}}
        p2 = axelrod.Cooperator()
        test_responses(self, p1, p2, [], [], [C, C, C, C, C, C, C])

    def test_reset_method(self):
        """Tests the reset method."""
        P1 = axelrod.ArrogantQLearner()
        P1.Qs = {'': {C: 0, D: -0.9}, '0.0': {C: 0, D: 0}}
        P1.Vs = {'': 0, '0.0': 0}
        P1.history = [C, D, D, D]
        P1.prev_state = C
        P1.reset()
        self.assertEqual(P1.prev_state, '')
        self.assertEqual(P1.history, [])
        self.assertEqual(P1.Vs, {'':0})
        self.assertEqual(P1.Qs, {'':{C:0, D:0}})


class TestHesitantQLearner(TestPlayer):

    name = 'Hesitant QLearner'
    player = axelrod.HesitantQLearner
    expected_classifier = {
        'memory_depth': float('inf'),  # Long memory
        'stochastic': True,
        'makes_use_of': set(["game"]),
        'inspects_source': False,
        'manipulates_source': False,
        'manipulates_state': False
    }

    def test_qs_update(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.HesitantQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs, {'': {C: 0, D: 0.1}, '0.0': {C: 0, D: 0}})
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs,{'': {C: 0, D: 0.1}, '0.0': {C: 0.30000000000000004, D: 0}, 'C1.0': {C: 0, D: 0}})

    def test_vs_update(self):
        """
        Test that the q and v values update
        """
        random.seed(5)
        p1 = axelrod.HesitantQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs, {'': 0.1, '0.0': 0})
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs,{'': 0.1, '0.0': 0.30000000000000004, 'C1.0': 0})

    def test_prev_state_updates(self):
        """
        Test that the q and v values update
        """
        random.seed(5)
        p1 = axelrod.HesitantQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, '0.0')
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, 'C1.0')

    def test_strategy(self):
        """Tests that it chooses the best strategy."""
        random.seed(9)
        p1 = axelrod.HesitantQLearner()
        p1.state = 'CCDC'
        p1.Qs = {'': {C: 0, D: 0}, 'CCDC': {C: 2, D: 6}}
        p2 = axelrod.Cooperator()
        test_responses(self, p1, p2, [], [], [C, C, C, C, C, C, C])

    def test_reset_method(self):
        """
        tests the reset method
        """
        P1 = axelrod.HesitantQLearner()
        P1.Qs = {'': {C: 0, D: -0.9}, '0.0': {C: 0, D: 0}}
        P1.Vs = {'': 0, '0.0': 0}
        P1.history = [C, D, D, D]
        P1.prev_state = C
        P1.reset()
        self.assertEqual(P1.prev_state, '')
        self.assertEqual(P1.history, [])
        self.assertEqual(P1.Vs, {'': 0})
        self.assertEqual(P1.Qs, {'': {C: 0, D: 0}})


class TestCautiousQLearner(TestPlayer):

    name = 'Cautious QLearner'
    player = axelrod.CautiousQLearner
    expected_classifier = {
        'memory_depth': float('inf'),  # Long memory
        'stochastic': True,
        'makes_use_of': set(["game"]),
        'inspects_source': False,
        'manipulates_source': False,
        'manipulates_state': False
    }

    def test_qs_update(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.CautiousQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs, {'': {C: 0, D: 0.1}, '0.0': {C: 0, D: 0}})
        simulate_play(p1, p2)
        self.assertEqual(p1.Qs,{'': {C: 0, D: 0.1}, '0.0': {C: 0.30000000000000004, D: 0}, 'C1.0': {C: 0, D: 0.0}})

    def test_vs_update(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.CautiousQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs, {'': 0.1, '0.0': 0})
        simulate_play(p1, p2)
        self.assertEqual(p1.Vs,{'': 0.1, '0.0': 0.30000000000000004, 'C1.0': 0})

    def test_prev_state_updates(self):
        """Test that the q and v values update."""
        random.seed(5)
        p1 = axelrod.CautiousQLearner()
        p2 = axelrod.Cooperator()
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, '0.0')
        simulate_play(p1, p2)
        self.assertEqual(p1.prev_state, 'C1.0')

    def test_strategy(self):
        """Tests that it chooses the best strategy."""
        random.seed(9)
        p1 = axelrod.CautiousQLearner()
        p1.state = 'CCDC'
        p1.Qs = {'': {C: 0, D: 0}, 'CCDC': {C: 2, D: 6}}
        p2 = axelrod.Cooperator()
        test_responses(self, p1, p2, [], [], [C, C, C, C, C, C, C])

    def test_reset_method(self):
        """Tests the reset method."""
        P1 = axelrod.CautiousQLearner()
        P1.Qs = {'': {C: 0, D: -0.9}, '0.0': {C: 0, D: 0}}
        P1.Vs = {'': 0, '0.0': 0}
        P1.history = [C, D, D, D]
        P1.prev_state = C
        P1.reset()
        self.assertEqual(P1.prev_state, '')
        self.assertEqual(P1.history, [])
        self.assertEqual(P1.Vs, {'': 0})
        self.assertEqual(P1.Qs, {'': {C: 0, D: 0}})

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from enum import Enum


class OSType(Enum):

    linux = "linux"
    windows = "windows"
    unmapped = "unmapped"


class CertificateState(Enum):

    active = "active"
    deleting = "deleting"
    deletefailed = "deletefailed"


class CertificateFormat(Enum):

    pfx = "pfx"
    cer = "cer"
    unmapped = "unmapped"


class ComputeNodeFillType(Enum):

    spread = "spread"
    pack = "pack"
    unmapped = "unmapped"


class CertificateStoreLocation(Enum):

    currentuser = "currentuser"
    localmachine = "localmachine"
    unmapped = "unmapped"


class CertificateVisibility(Enum):

    starttask = "starttask"
    task = "task"
    remoteuser = "remoteuser"
    unmapped = "unmapped"


class PoolLifetimeOption(Enum):

    jobschedule = "jobschedule"
    job = "job"
    unmapped = "unmapped"


class JobScheduleState(Enum):

    active = "active"
    completed = "completed"
    disabled = "disabled"
    terminating = "terminating"
    deleting = "deleting"


class SchedulingErrorCategory(Enum):

    usererror = "usererror"
    servererror = "servererror"
    unmapped = "unmapped"


class JobState(Enum):

    active = "active"
    disabling = "disabling"
    disabled = "disabled"
    enabling = "enabling"
    terminating = "terminating"
    completed = "completed"
    deleting = "deleting"


class JobPreparationTaskState(Enum):

    running = "running"
    completed = "completed"


class JobReleaseTaskState(Enum):

    running = "running"
    completed = "completed"


class PoolState(Enum):

    active = "active"
    deleting = "deleting"
    upgrading = "upgrading"


class AllocationState(Enum):

    steady = "steady"
    resizing = "resizing"
    stopping = "stopping"


class TaskState(Enum):

    active = "active"
    preparing = "preparing"
    running = "running"
    completed = "completed"


class TaskAddStatus(Enum):

    success = "success"
    clienterror = "clienterror"
    servererror = "servererror"
    unmapped = "unmapped"


class StartTaskState(Enum):

    running = "running"
    completed = "completed"


class ComputeNodeState(Enum):

    idle = "idle"
    rebooting = "rebooting"
    reimaging = "reimaging"
    running = "running"
    unusable = "unusable"
    creating = "creating"
    starting = "starting"
    waitingforstarttask = "waitingforstarttask"
    starttaskfailed = "starttaskfailed"
    unknown = "unknown"
    leavingpool = "leavingpool"
    offline = "offline"


class SchedulingState(Enum):

    enabled = "enabled"
    disabled = "disabled"


class DisableJobOption(Enum):

    requeue = "requeue"
    terminate = "terminate"
    wait = "wait"


class ComputeNodeDeallocationOption(Enum):

    requeue = "requeue"
    terminate = "terminate"
    taskcompletion = "taskcompletion"
    retaineddata = "retaineddata"


class ComputeNodeRebootOption(Enum):

    requeue = "requeue"
    terminate = "terminate"
    taskcompletion = "taskcompletion"
    retaineddata = "retaineddata"


class ComputeNodeReimageOption(Enum):

    requeue = "requeue"
    terminate = "terminate"
    taskcompletion = "taskcompletion"
    retaineddata = "retaineddata"


class DisableComputeNodeSchedulingOption(Enum):

    requeue = "requeue"
    terminate = "terminate"
    taskcompletion = "taskcompletion"

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class ComputeNodeGetRemoteLoginSettingsOptions(Model):
    """
    Additional parameters for the GetRemoteLoginSettings operation.

    :param timeout: Sets the maximum time that the server can spend
     processing the request, in seconds. The default is 30 seconds. Default
     value: 30 .
    :type timeout: int
    :param client_request_id: Caller generated request identity, in the form
     of a GUID with no decoration such as curly braces e.g.
     9C4D50EE-2D56-4CD3-8152-34347DC9F2B0.
    :type client_request_id: str
    :param return_client_request_id: Specifies if the server should return
     the client-request-id identifier in the response.
    :type return_client_request_id: bool
    :param ocp_date: The time the request was issued. If not specified, this
     header will be automatically populated with the current system clock
     time.
    :type ocp_date: datetime
    """ 

    def __init__(self, timeout=30, client_request_id=None, return_client_request_id=None, ocp_date=None):
        self.timeout = timeout
        self.client_request_id = client_request_id
        self.return_client_request_id = return_client_request_id
        self.ocp_date = ocp_date

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class JobEnableOptions(Model):
    """
    Additional parameters for the Enable operation.

    :param timeout: Sets the maximum time that the server can spend
     processing the request, in seconds. The default is 30 seconds. Default
     value: 30 .
    :type timeout: int
    :param client_request_id: Caller generated request identity, in the form
     of a GUID with no decoration such as curly braces e.g.
     9C4D50EE-2D56-4CD3-8152-34347DC9F2B0.
    :type client_request_id: str
    :param return_client_request_id: Specifies if the server should return
     the client-request-id identifier in the response.
    :type return_client_request_id: bool
    :param ocp_date: The time the request was issued. If not specified, this
     header will be automatically populated with the current system clock
     time.
    :type ocp_date: datetime
    :param if_match: An ETag is specified. Specify this header to perform the
     operation only if the resource's ETag is an exact match as specified.
    :type if_match: str
    :param if_none_match: An ETag is specified. Specify this header to
     perform the operation only if the resource's ETag does not match the
     specified ETag.
    :type if_none_match: str
    :param if_modified_since: Specify this header to perform the operation
     only if the resource has been modified since the specified date/time.
    :type if_modified_since: datetime
    :param if_unmodified_since: Specify this header to perform the operation
     only if the resource has not been modified since the specified date/time.
    :type if_unmodified_since: datetime
    """ 

    def __init__(self, timeout=30, client_request_id=None, return_client_request_id=None, ocp_date=None, if_match=None, if_none_match=None, if_modified_since=None, if_unmodified_since=None):
        self.timeout = timeout
        self.client_request_id = client_request_id
        self.return_client_request_id = return_client_request_id
        self.ocp_date = ocp_date
        self.if_match = if_match
        self.if_none_match = if_none_match
        self.if_modified_since = if_modified_since
        self.if_unmodified_since = if_unmodified_since

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class JobScheduleTerminateOptions(Model):
    """
    Additional parameters for the Terminate operation.

    :param timeout: Sets the maximum time that the server can spend
     processing the request, in seconds. The default is 30 seconds. Default
     value: 30 .
    :type timeout: int
    :param client_request_id: Caller generated request identity, in the form
     of a GUID with no decoration such as curly braces e.g.
     9C4D50EE-2D56-4CD3-8152-34347DC9F2B0.
    :type client_request_id: str
    :param return_client_request_id: Specifies if the server should return
     the client-request-id identifier in the response.
    :type return_client_request_id: bool
    :param ocp_date: The time the request was issued. If not specified, this
     header will be automatically populated with the current system clock
     time.
    :type ocp_date: datetime
    :param if_match: An ETag is specified. Specify this header to perform the
     operation only if the resource's ETag is an exact match as specified.
    :type if_match: str
    :param if_none_match: An ETag is specified. Specify this header to
     perform the operation only if the resource's ETag does not match the
     specified ETag.
    :type if_none_match: str
    :param if_modified_since: Specify this header to perform the operation
     only if the resource has been modified since the specified date/time.
    :type if_modified_since: datetime
    :param if_unmodified_since: Specify this header to perform the operation
     only if the resource has not been modified since the specified date/time.
    :type if_unmodified_since: datetime
    """ 

    def __init__(self, timeout=30, client_request_id=None, return_client_request_id=None, ocp_date=None, if_match=None, if_none_match=None, if_modified_since=None, if_unmodified_since=None):
        self.timeout = timeout
        self.client_request_id = client_request_id
        self.return_client_request_id = return_client_request_id
        self.ocp_date = ocp_date
        self.if_match = if_match
        self.if_none_match = if_none_match
        self.if_modified_since = if_modified_since
        self.if_unmodified_since = if_unmodified_since

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class PoolEvaluateAutoScaleOptions(Model):
    """
    Additional parameters for the EvaluateAutoScale operation.

    :param timeout: Sets the maximum time that the server can spend
     processing the request, in seconds. The default is 30 seconds. Default
     value: 30 .
    :type timeout: int
    :param client_request_id: Caller generated request identity, in the form
     of a GUID with no decoration such as curly braces e.g.
     9C4D50EE-2D56-4CD3-8152-34347DC9F2B0.
    :type client_request_id: str
    :param return_client_request_id: Specifies if the server should return
     the client-request-id identifier in the response.
    :type return_client_request_id: bool
    :param ocp_date: The time the request was issued. If not specified, this
     header will be automatically populated with the current system clock
     time.
    :type ocp_date: datetime
    """ 

    def __init__(self, timeout=30, client_request_id=None, return_client_request_id=None, ocp_date=None):
        self.timeout = timeout
        self.client_request_id = client_request_id
        self.return_client_request_id = return_client_request_id
        self.ocp_date = ocp_date

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class StartTaskInformation(Model):
    """
    Information about a start task running on a compute node.

    :param state: Gets or sets the state of the start task on the compute
     node. Possible values include: 'running', 'completed'
    :type state: str
    :param start_time: Gets or sets the time at which the start task started
     running.
    :type start_time: datetime
    :param end_time: Gets or sets the time at which the start task stopped
     running.
    :type end_time: datetime
    :param exit_code: Gets or sets the exit code of the start task.
    :type exit_code: int
    :param scheduling_error: Gets or sets any error encountered scheduling
     the start task.
    :type scheduling_error: :class:`TaskSchedulingError
     <azure.batch.models.TaskSchedulingError>`
    :param retry_count: Gets or sets the number of times the task has been
     retried by the Batch service.
    :type retry_count: int
    :param last_retry_time: Gets or sets the most recent time at which a
     retry of the task started running.
    :type last_retry_time: datetime
    """ 

    _validation = {
        'state': {'required': True},
        'start_time': {'required': True},
        'retry_count': {'required': True},
    }

    _attribute_map = {
        'state': {'key': 'state', 'type': 'StartTaskState'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'exit_code': {'key': 'exitCode', 'type': 'int'},
        'scheduling_error': {'key': 'schedulingError', 'type': 'TaskSchedulingError'},
        'retry_count': {'key': 'retryCount', 'type': 'int'},
        'last_retry_time': {'key': 'lastRetryTime', 'type': 'iso-8601'},
    }

    def __init__(self, state, start_time, retry_count, end_time=None, exit_code=None, scheduling_error=None, last_retry_time=None):
        self.state = state
        self.start_time = start_time
        self.end_time = end_time
        self.exit_code = exit_code
        self.scheduling_error = scheduling_error
        self.retry_count = retry_count
        self.last_retry_time = last_retry_time

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.pipeline import ClientRawResponse
import uuid

from .. import models


class ApplicationOperations(object):
    """ApplicationOperations operations.

    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An objec model deserializer.
    """

    def __init__(self, client, config, serializer, deserializer):

        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer

        self.config = config

    def list(
            self, application_list_options=None, custom_headers={}, raw=False, **operation_config):
        """
        Lists all of the applications available in the specified account.

        :param application_list_options: Additional parameters for the
         operation
        :type application_list_options: :class:`ApplicationListOptions
         <azure.batch.models.ApplicationListOptions>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`ApplicationSummaryPaged
         <azure.batch.models.ApplicationSummaryPaged>`
        """
        max_results = None
        if application_list_options is not None:
            max_results = application_list_options.max_results
        timeout = None
        if application_list_options is not None:
            timeout = application_list_options.timeout
        client_request_id = None
        if application_list_options is not None:
            client_request_id = application_list_options.client_request_id
        return_client_request_id = None
        if application_list_options is not None:
            return_client_request_id = application_list_options.return_client_request_id
        ocp_date = None
        if application_list_options is not None:
            ocp_date = application_list_options.ocp_date

        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/applications'

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')
                if max_results is not None:
                    query_parameters['maxresults'] = self._serialize.query("max_results", max_results, 'int')
                if timeout is not None:
                    query_parameters['timeout'] = self._serialize.query("timeout", timeout, 'int')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; odata=minimalmetadata; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')
            if client_request_id is not None:
                header_parameters['client-request-id'] = self._serialize.header("client_request_id", client_request_id, 'str')
            if return_client_request_id is not None:
                header_parameters['return-client-request-id'] = self._serialize.header("return_client_request_id", return_client_request_id, 'bool')
            if ocp_date is not None:
                header_parameters['ocp-date'] = self._serialize.header("ocp_date", ocp_date, 'rfc-1123')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                raise models.BatchErrorException(self._deserialize, response)

            return response

        # Deserialize response
        deserialized = models.ApplicationSummaryPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.ApplicationSummaryPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def get(
            self, application_id, application_get_options=None, custom_headers={}, raw=False, **operation_config):
        """
        Gets information about the specified application.

        :param application_id: The id of the application.
        :type application_id: str
        :param application_get_options: Additional parameters for the
         operation
        :type application_get_options: :class:`ApplicationGetOptions
         <azure.batch.models.ApplicationGetOptions>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`ApplicationSummary
         <azure.batch.models.ApplicationSummary>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        timeout = None
        if application_get_options is not None:
            timeout = application_get_options.timeout
        client_request_id = None
        if application_get_options is not None:
            client_request_id = application_get_options.client_request_id
        return_client_request_id = None
        if application_get_options is not None:
            return_client_request_id = application_get_options.return_client_request_id
        ocp_date = None
        if application_get_options is not None:
            ocp_date = application_get_options.ocp_date

        # Construct URL
        url = '/applications/{applicationId}'
        path_format_arguments = {
            'applicationId': self._serialize.url("application_id", application_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')
        if timeout is not None:
            query_parameters['timeout'] = self._serialize.query("timeout", timeout, 'int')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; odata=minimalmetadata; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')
        if client_request_id is not None:
            header_parameters['client-request-id'] = self._serialize.header("client_request_id", client_request_id, 'str')
        if return_client_request_id is not None:
            header_parameters['return-client-request-id'] = self._serialize.header("return_client_request_id", return_client_request_id, 'bool')
        if ocp_date is not None:
            header_parameters['ocp-date'] = self._serialize.header("ocp_date", ocp_date, 'rfc-1123')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            raise models.BatchErrorException(self._deserialize, response)

        deserialized = None
        header_dict = {}

        if response.status_code == 200:
            deserialized = self._deserialize('ApplicationSummary', response)
            header_dict = {
                'client-request-id': 'str',
                'request-id': 'str',
                'ETag': 'str',
                'Last-Modified': 'rfc-1123',
            }

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            client_raw_response.add_headers(header_dict)
            return client_raw_response

        return deserialized

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class ApplicationCreateParameters(Model):
    """
    Request parameters for create a new application

    :param available_to_other_tenants: Indicates if the application will be
     available to other tenants
    :type available_to_other_tenants: bool
    :param display_name: Application display name
    :type display_name: str
    :param homepage: Application homepage
    :type homepage: str
    :param identifier_uris: Application Uris
    :type identifier_uris: list of str
    :param reply_urls: Application reply Urls
    :type reply_urls: list of str
    :param key_credentials: Gets or sets the list of KeyCredential objects
    :type key_credentials: list of :class:`KeyCredential
     <azure.graphrbac.models.KeyCredential>`
    :param password_credentials: Gets or sets the list of PasswordCredential
     objects
    :type password_credentials: list of :class:`PasswordCredential
     <azure.graphrbac.models.PasswordCredential>`
    """ 

    _validation = {
        'available_to_other_tenants': {'required': True},
        'display_name': {'required': True},
        'homepage': {'required': True},
        'identifier_uris': {'required': True},
    }

    _attribute_map = {
        'available_to_other_tenants': {'key': 'availableToOtherTenants', 'type': 'bool'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'homepage': {'key': 'homepage', 'type': 'str'},
        'identifier_uris': {'key': 'identifierUris', 'type': '[str]'},
        'reply_urls': {'key': 'replyUrls', 'type': '[str]'},
        'key_credentials': {'key': 'keyCredentials', 'type': '[KeyCredential]'},
        'password_credentials': {'key': 'passwordCredentials', 'type': '[PasswordCredential]'},
    }

    def __init__(self, available_to_other_tenants, display_name, homepage, identifier_uris, reply_urls=None, key_credentials=None, password_credentials=None):
        self.available_to_other_tenants = available_to_other_tenants
        self.display_name = display_name
        self.homepage = homepage
        self.identifier_uris = identifier_uris
        self.reply_urls = reply_urls
        self.key_credentials = key_credentials
        self.password_credentials = password_credentials

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .authorization_management_client import AuthorizationManagementClient, AuthorizationManagementClientConfiguration
from .version import VERSION

__all__ = [
    'AuthorizationManagementClient',
    'AuthorizationManagementClientConfiguration'
]

__version__ = VERSION


# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

VERSION = "2015-07-01"


# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.pipeline import ClientRawResponse
from msrestazure.azure_exceptions import CloudError
import uuid

from .. import models


class ApplicationOperations(object):
    """ApplicationOperations operations.

    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An objec model deserializer.
    """

    def __init__(self, client, config, serializer, deserializer):

        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer

        self.config = config

    def activate_application_package(
            self, resource_group_name, account_name, id, version, format, custom_headers={}, raw=False, **operation_config):
        """
        Activates the specified application package.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param id: The id of the application.
        :type id: str
        :param version: The version of the application to activate.
        :type version: str
        :param format: The format of the application package binary file.
        :type format: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        parameters = models.ActivateApplicationPackageParameters(format=format)

        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{id}/versions/{version}/activate'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'id': self._serialize.url("id", id, 'str'),
            'version': self._serialize.url("version", version, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'ActivateApplicationPackageParameters')

        # Construct and send request
        request = self._client.post(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [204]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def add_application(
            self, resource_group_name, account_name, application_id, allow_updates=None, display_name=None, custom_headers={}, raw=False, **operation_config):
        """
        Adds an application to the specified Batch account.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param allow_updates: A value indicating whether packages within the
         application may be overwritten using the same version string.
        :type allow_updates: bool
        :param display_name: The display name for the application.
        :type display_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        parameters = None
        if allow_updates is not None or display_name is not None:
            parameters = models.AddApplicationParameters(allow_updates=allow_updates, display_name=display_name)

        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        if parameters is not None:
            body_content = self._serialize.body(parameters, 'AddApplicationParameters')
        else:
            body_content = None

        # Construct and send request
        request = self._client.put(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [201]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def delete_application(
            self, resource_group_name, account_name, application_id, custom_headers={}, raw=False, **operation_config):
        """
        Deletes an application.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.delete(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [204]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def get_application(
            self, resource_group_name, account_name, application_id, custom_headers={}, raw=False, **operation_config):
        """
        Gets information about the specified application.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`Application <azure.mgmt.batch.models.Application>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('Application', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def update_application(
            self, resource_group_name, account_name, application_id, parameters, custom_headers={}, raw=False, **operation_config):
        """
        Updates settings for the specified application.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param parameters: The parameters for the request.
        :type parameters: :class:`UpdateApplicationParameters
         <azure.mgmt.batch.models.UpdateApplicationParameters>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'UpdateApplicationParameters')

        # Construct and send request
        request = self._client.patch(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [204]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def add_application_package(
            self, resource_group_name, account_name, application_id, version, custom_headers={}, raw=False, **operation_config):
        """
        Creates an application package record.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param version: The version of the application.
        :type version: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`AddApplicationPackageResult
         <azure.mgmt.batch.models.AddApplicationPackageResult>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}/versions/{version}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'version': self._serialize.url("version", version, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.put(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [201]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 201:
            deserialized = self._deserialize('AddApplicationPackageResult', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def delete_application_package(
            self, resource_group_name, account_name, application_id, version, custom_headers={}, raw=False, **operation_config):
        """
        Deletes an application package record and the binary file.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param version: The version of the application to delete.
        :type version: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}/versions/{version}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'version': self._serialize.url("version", version, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.delete(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [204]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def get_application_package(
            self, resource_group_name, account_name, application_id, version, custom_headers={}, raw=False, **operation_config):
        """
        Gets information about the specified application package.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param application_id: The id of the application.
        :type application_id: str
        :param version: The version of the application.
        :type version: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`GetApplicationPackageResult
         <azure.mgmt.batch.models.GetApplicationPackageResult>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications/{applicationId}/versions/{version}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
            'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
            'applicationId': self._serialize.url("application_id", application_id, 'str'),
            'version': self._serialize.url("version", version, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('GetApplicationPackageResult', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def list(
            self, resource_group_name, account_name, maxresults=None, custom_headers={}, raw=False, **operation_config):
        """
        Lists all of the applications in the specified account.

        :param resource_group_name: The name of the resource group that
         contains the Batch account.
        :type resource_group_name: str
        :param account_name: The name of the Batch account.
        :type account_name: str
        :param maxresults: The maximum number of items to return in the
         response.
        :type maxresults: int
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`ApplicationPaged
         <azure.mgmt.batch.models.ApplicationPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Batch/batchAccounts/{accountName}/applications'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str', pattern='^[-\w\._]+$'),
                    'accountName': self._serialize.url("account_name", account_name, 'str', max_length=24, min_length=3, pattern='^[-\w\._]+$'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                if maxresults is not None:
                    query_parameters['maxresults'] = self._serialize.query("maxresults", maxresults, 'int')
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.ApplicationPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.ApplicationPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.paging import Paged


class ProfilePaged(Paged):
    """
    A paging container for iterating over a list of Profile object
    """

    _attribute_map = {
        'next_link': {'key': 'nextLink', 'type': 'str'},
        'current_page': {'key': 'value', 'type': '[Profile]'}
    }

    def __init__(self, *args, **kwargs):

        super(ProfilePaged, self).__init__(*args, **kwargs)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class BootDiagnosticsInstanceView(Model):
    """
    The instance view of a virtual machine boot diagnostics.

    :param console_screenshot_blob_uri: Gets or sets the console screenshot
     blob Uri.
    :type console_screenshot_blob_uri: str
    :param serial_console_log_blob_uri: Gets or sets the Linux serial console
     log blob Uri.
    :type serial_console_log_blob_uri: str
    """ 

    _attribute_map = {
        'console_screenshot_blob_uri': {'key': 'consoleScreenshotBlobUri', 'type': 'str'},
        'serial_console_log_blob_uri': {'key': 'serialConsoleLogBlobUri', 'type': 'str'},
    }

    def __init__(self, console_screenshot_blob_uri=None, serial_console_log_blob_uri=None):
        self.console_screenshot_blob_uri = console_screenshot_blob_uri
        self.serial_console_log_blob_uri = serial_console_log_blob_uri

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class UpgradePolicy(Model):
    """
    Describes an upgrade policy - automatic or manual.

    :param mode: Gets or sets the upgrade mode. Possible values include:
     'Automatic', 'Manual'
    :type mode: str
    """ 

    _attribute_map = {
        'mode': {'key': 'mode', 'type': 'UpgradeMode'},
    }

    def __init__(self, mode=None):
        self.mode = mode

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class VirtualMachineScaleSetOSProfile(Model):
    """
    Describes a virtual machine scale set OS profile.

    :param computer_name_prefix: Gets or sets the computer name prefix.
    :type computer_name_prefix: str
    :param admin_username: Gets or sets the admin user name.
    :type admin_username: str
    :param admin_password: Gets or sets the admin user password.
    :type admin_password: str
    :param custom_data: Gets or sets a base-64 encoded string of custom data.
    :type custom_data: str
    :param windows_configuration: Gets or sets the Windows Configuration of
     the OS profile.
    :type windows_configuration: :class:`WindowsConfiguration
     <azure.mgmt.compute.models.WindowsConfiguration>`
    :param linux_configuration: Gets or sets the Linux Configuration of the
     OS profile.
    :type linux_configuration: :class:`LinuxConfiguration
     <azure.mgmt.compute.models.LinuxConfiguration>`
    :param secrets: Gets or sets the List of certificates for addition to the
     VM.
    :type secrets: list of :class:`VaultSecretGroup
     <azure.mgmt.compute.models.VaultSecretGroup>`
    """ 

    _attribute_map = {
        'computer_name_prefix': {'key': 'computerNamePrefix', 'type': 'str'},
        'admin_username': {'key': 'adminUsername', 'type': 'str'},
        'admin_password': {'key': 'adminPassword', 'type': 'str'},
        'custom_data': {'key': 'customData', 'type': 'str'},
        'windows_configuration': {'key': 'windowsConfiguration', 'type': 'WindowsConfiguration'},
        'linux_configuration': {'key': 'linuxConfiguration', 'type': 'LinuxConfiguration'},
        'secrets': {'key': 'secrets', 'type': '[VaultSecretGroup]'},
    }

    def __init__(self, computer_name_prefix=None, admin_username=None, admin_password=None, custom_data=None, windows_configuration=None, linux_configuration=None, secrets=None):
        self.computer_name_prefix = computer_name_prefix
        self.admin_username = admin_username
        self.admin_password = admin_password
        self.custom_data = custom_data
        self.windows_configuration = windows_configuration
        self.linux_configuration = linux_configuration
        self.secrets = secrets

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.pipeline import ClientRawResponse
from msrestazure.azure_exceptions import CloudError
from msrestazure.azure_operation import AzureOperationPoller
import uuid

from .. import models


class VirtualMachinesOperations(object):
    """VirtualMachinesOperations operations.

    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An objec model deserializer.
    """

    def __init__(self, client, config, serializer, deserializer):

        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer

        self.config = config

    def capture(
            self, resource_group_name, vm_name, parameters, custom_headers={}, raw=False, **operation_config):
        """
        Captures the VM by copying virtual hard disks of the VM and outputs a
        template that can be used to create similar VMs.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param parameters: Parameters supplied to the Capture Virtual Machine
         operation.
        :type parameters: :class:`VirtualMachineCaptureParameters
         <azure.mgmt.compute.models.VirtualMachineCaptureParameters>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns :class:`VirtualMachineCaptureResult
         <azure.mgmt.compute.models.VirtualMachineCaptureResult>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/capture'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'VirtualMachineCaptureParameters')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(
                request, header_parameters, body_content, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [200, 202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            deserialized = None

            if response.status_code == 200:
                deserialized = self._deserialize('VirtualMachineCaptureResult', response)

            if raw:
                client_raw_response = ClientRawResponse(deserialized, response)
                return client_raw_response

            return deserialized

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def create_or_update(
            self, resource_group_name, vm_name, parameters, custom_headers={}, raw=False, **operation_config):
        """
        The operation to create or update a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param parameters: Parameters supplied to the Create Virtual Machine
         operation.
        :type parameters: :class:`VirtualMachine
         <azure.mgmt.compute.models.VirtualMachine>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns :class:`VirtualMachine
         <azure.mgmt.compute.models.VirtualMachine>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'VirtualMachine')

        # Construct and send request
        def long_running_send():

            request = self._client.put(url, query_parameters)
            return self._client.send(
                request, header_parameters, body_content, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [200, 201]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            deserialized = None

            if response.status_code == 200:
                deserialized = self._deserialize('VirtualMachine', response)
            if response.status_code == 201:
                deserialized = self._deserialize('VirtualMachine', response)

            if raw:
                client_raw_response = ClientRawResponse(deserialized, response)
                return client_raw_response

            return deserialized

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def delete(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to delete a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.delete(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202, 204]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def get(
            self, resource_group_name, vm_name, expand=None, custom_headers={}, raw=False, **operation_config):
        """
        The operation to get a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param expand: The expand expression to apply on the operation.
         Possible values include: 'instanceView'
        :type expand: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`VirtualMachine
         <azure.mgmt.compute.models.VirtualMachine>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        if expand is not None:
            query_parameters['$expand'] = self._serialize.query("expand", expand, 'InstanceViewTypes')
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('VirtualMachine', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def deallocate(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        Shuts down the Virtual Machine and releases the compute resources. You
        are not billed for the compute resources that this Virtual Machine
        uses.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/deallocate'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def generalize(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        Sets the state of the VM as Generalized.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/generalize'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.post(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def list(
            self, resource_group_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to list virtual machines under a resource group.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`VirtualMachinePaged
         <azure.mgmt.compute.models.VirtualMachinePaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.VirtualMachinePaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.VirtualMachinePaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list_all(
            self, custom_headers={}, raw=False, **operation_config):
        """
        Gets the list of Virtual Machines in the subscription. Use nextLink
        property in the response to get the next page of Virtual Machines. Do
        this till nextLink is not null to fetch all the Virtual Machines.

        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`VirtualMachinePaged
         <azure.mgmt.compute.models.VirtualMachinePaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/providers/Microsoft.Compute/virtualMachines'
                path_format_arguments = {
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.VirtualMachinePaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.VirtualMachinePaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list_available_sizes(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        Lists all available virtual machine sizes it can be resized to for a
        virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`VirtualMachineSizePaged
         <azure.mgmt.compute.models.VirtualMachineSizePaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/vmSizes'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'vmName': self._serialize.url("vm_name", vm_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.VirtualMachineSizePaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.VirtualMachineSizePaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def power_off(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to power off (stop) a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/powerOff'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def restart(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to restart a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/restart'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def start(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to start a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/start'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def redeploy(
            self, resource_group_name, vm_name, custom_headers={}, raw=False, **operation_config):
        """
        The operation to redeploy a virtual machine.

        :param resource_group_name: The name of the resource group.
        :type resource_group_name: str
        :param vm_name: The name of the virtual machine.
        :type vm_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Compute/virtualMachines/{vmName}/redeploy'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'vmName': self._serialize.url("vm_name", vm_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.post(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [202]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.pipeline import ClientRawResponse
from msrestazure.azure_exceptions import CloudError
from msrestazure.azure_operation import AzureOperationPoller
import uuid

from .. import models


class AccountOperations(object):
    """AccountOperations operations.

    :param client: Client for service requests.
    :param config: Configuration of service client.
    :param serializer: An object model serializer.
    :param deserializer: An objec model deserializer.
    """

    def __init__(self, client, config, serializer, deserializer):

        self._client = client
        self._serialize = serializer
        self._deserialize = deserializer

        self.config = config

    def get_storage_account(
            self, resource_group_name, account_name, storage_account_name, custom_headers={}, raw=False, **operation_config):
        """
        Gets the specified Azure Storage account linked to the given Data Lake
        Analytics account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account from
         which to retrieve Azure storage account details.
        :type account_name: str
        :param storage_account_name: The name of the Azure Storage account
         for which to retrieve the details.
        :type storage_account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`StorageAccountInfo
         <azure.mgmt.datalake.analytics.account.models.StorageAccountInfo>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('StorageAccountInfo', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def delete_storage_account(
            self, resource_group_name, account_name, storage_account_name, custom_headers={}, raw=False, **operation_config):
        """
        Updates the specified Data Lake Analytics account to remove an Azure
        Storage account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account from
         which to remove the Azure Storage account.
        :type account_name: str
        :param storage_account_name: The name of the Azure Storage account to
         remove
        :type storage_account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.delete(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def update_storage_account(
            self, resource_group_name, account_name, storage_account_name, properties, custom_headers={}, raw=False, **operation_config):
        """
        Updates the Data Lake Analytics account to replace Azure Storage blob
        account details, such as the access key and/or suffix.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account to
         modify storage accounts in
        :type account_name: str
        :param storage_account_name: The Azure Storage account to modify
        :type storage_account_name: str
        :param properties: Gets or sets the properties for the Azure Storage
         account being added.
        :type properties: :class:`StorageAccountProperties
         <azure.mgmt.datalake.analytics.account.models.StorageAccountProperties>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        parameters = models.AddStorageAccountParameters(properties=properties)

        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'AddStorageAccountParameters')

        # Construct and send request
        request = self._client.patch(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def add_storage_account(
            self, resource_group_name, account_name, storage_account_name, properties, custom_headers={}, raw=False, **operation_config):
        """
        Updates the specified Data Lake Analytics account to add an Azure
        Storage account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account to
         which to add the Azure Storage account.
        :type account_name: str
        :param storage_account_name: The name of the Azure Storage account to
         add
        :type storage_account_name: str
        :param properties: Gets or sets the properties for the Azure Storage
         account being added.
        :type properties: :class:`StorageAccountProperties
         <azure.mgmt.datalake.analytics.account.models.StorageAccountProperties>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        parameters = models.AddStorageAccountParameters(properties=properties)

        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'AddStorageAccountParameters')

        # Construct and send request
        request = self._client.put(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def get_storage_container(
            self, resource_group_name, account_name, storage_account_name, container_name, custom_headers={}, raw=False, **operation_config):
        """
        Gets the specified Azure Storage container associated with the given
        Data Lake Analytics and Azure Storage accounts.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account for
         which to retrieve blob container.
        :type account_name: str
        :param storage_account_name: The name of the Azure storage account
         from which to retrieve the blob container.
        :type storage_account_name: str
        :param container_name: The name of the Azure storage container to
         retrieve
        :type container_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`BlobContainer
         <azure.mgmt.datalake.analytics.account.models.BlobContainer>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}/Containers/{containerName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
            'containerName': self._serialize.url("container_name", container_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('BlobContainer', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def list_storage_containers(
            self, resource_group_name, account_name, storage_account_name, custom_headers={}, raw=False, **operation_config):
        """
        Lists the Azure Storage containers, if any, associated with the
        specified Data Lake Analytics and Azure Storage account combination.
        The response includes a link to the next page of results, if any.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account for
         which to list Azure Storage blob containers.
        :type account_name: str
        :param storage_account_name: The name of the Azure storage account
         from which to list blob containers.
        :type storage_account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`BlobContainerPaged
         <azure.mgmt.datalake.analytics.account.models.BlobContainerPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}/Containers'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'accountName': self._serialize.url("account_name", account_name, 'str'),
                    'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.BlobContainerPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.BlobContainerPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def storage_containers_list_next(
            self, next_link, custom_headers={}, raw=False, **operation_config):
        """
        Gets the next page of Azure Storage containers, if any, within the
        specified Azure Storage account. The response includes a link to the
        next page of results, if any.

        :param next_link: The URL to the next Azure Storage Container page.
        :type next_link: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`BlobContainerPaged
         <azure.mgmt.datalake.analytics.account.models.BlobContainerPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/{nextLink}'
                path_format_arguments = {
                    'nextLink': self._serialize.url("next_link", next_link, 'str', skip_quote=True)
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.BlobContainerPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.BlobContainerPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def sas_tokens_list_next(
            self, next_link, custom_headers={}, raw=False, **operation_config):
        """
        Gets the next page of the SAS token objects within the specified Azure
        Storage account and container, if any.

        :param next_link: The URL to the next Azure Storage Container SAS
         token page.
        :type next_link: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`SasTokenInfoPaged
         <azure.mgmt.datalake.analytics.account.models.SasTokenInfoPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/{nextLink}'
                path_format_arguments = {
                    'nextLink': self._serialize.url("next_link", next_link, 'str', skip_quote=True)
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.post(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.SasTokenInfoPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.SasTokenInfoPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list_sas_tokens(
            self, resource_group_name, account_name, storage_account_name, container_name, custom_headers={}, raw=False, **operation_config):
        """
        Gets the SAS token associated with the specified Data Lake Analytics
        and Azure Storage account and container combination.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account from
         which an Azure Storage account's SAS token is being requested.
        :type account_name: str
        :param storage_account_name: The name of the Azure storage account
         for which the SAS token is being requested.
        :type storage_account_name: str
        :param container_name: The name of the Azure storage container for
         which the SAS token is being requested.
        :type container_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`SasTokenInfoPaged
         <azure.mgmt.datalake.analytics.account.models.SasTokenInfoPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/{storageAccountName}/Containers/{containerName}/listSasTokens'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'accountName': self._serialize.url("account_name", account_name, 'str'),
                    'storageAccountName': self._serialize.url("storage_account_name", storage_account_name, 'str'),
                    'containerName': self._serialize.url("container_name", container_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.post(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.SasTokenInfoPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.SasTokenInfoPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def get_data_lake_store_account(
            self, resource_group_name, account_name, data_lake_store_account_name, custom_headers={}, raw=False, **operation_config):
        """
        Gets the specified Data Lake Store account details in the specified
        Data Lake Analytics account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account from
         which to retrieve the Data Lake Store account details.
        :type account_name: str
        :param data_lake_store_account_name: The name of the Data Lake Store
         account to retrieve
        :type data_lake_store_account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`DataLakeStoreAccountInfo
         <azure.mgmt.datalake.analytics.account.models.DataLakeStoreAccountInfo>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/DataLakeStoreAccounts/{dataLakeStoreAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'dataLakeStoreAccountName': self._serialize.url("data_lake_store_account_name", data_lake_store_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('DataLakeStoreAccountInfo', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def delete_data_lake_store_account(
            self, resource_group_name, account_name, data_lake_store_account_name, custom_headers={}, raw=False, **operation_config):
        """
        Updates the Data Lake Analytics account specified to remove the
        specified Data Lake Store account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account from
         which to remove the Data Lake Store account.
        :type account_name: str
        :param data_lake_store_account_name: The name of the Data Lake Store
         account to remove
        :type data_lake_store_account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/DataLakeStoreAccounts/{dataLakeStoreAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'dataLakeStoreAccountName': self._serialize.url("data_lake_store_account_name", data_lake_store_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.delete(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def add_data_lake_store_account(
            self, resource_group_name, account_name, data_lake_store_account_name, properties, custom_headers={}, raw=False, **operation_config):
        """
        Updates the specified Data Lake Analytics account to include the
        additional Data Lake Store account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account to
         which to add the Data Lake Store account.
        :type account_name: str
        :param data_lake_store_account_name: The name of the Data Lake Store
         account to add.
        :type data_lake_store_account_name: str
        :param properties: Gets or sets the properties for the Data Lake
         Store account being added.
        :type properties: :class:`DataLakeStoreAccountInfoProperties
         <azure.mgmt.datalake.analytics.account.models.DataLakeStoreAccountInfoProperties>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        parameters = models.AddDataLakeStoreParameters(properties=properties)

        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/DataLakeStoreAccounts/{dataLakeStoreAccountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'dataLakeStoreAccountName': self._serialize.url("data_lake_store_account_name", data_lake_store_account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'AddDataLakeStoreParameters')

        # Construct and send request
        request = self._client.put(url, query_parameters)
        response = self._client.send(
            request, header_parameters, body_content, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        if raw:
            client_raw_response = ClientRawResponse(None, response)
            return client_raw_response

    def list_storage_accounts(
            self, resource_group_name, account_name, filter=None, top=None, skip=None, expand=None, select=None, orderby=None, count=None, search=None, format=None, custom_headers={}, raw=False, **operation_config):
        """
        Gets the first page of Azure Storage accounts, if any, linked to the
        specified Data Lake Analytics account. The response includes a link
        to the next page, if any.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account for
         which to list Azure Storage accounts.
        :type account_name: str
        :param filter: The OData filter. Optional.
        :type filter: str
        :param top: The number of items to return. Optional.
        :type top: int
        :param skip: The number of items to skip over before returning
         elements. Optional.
        :type skip: int
        :param expand: OData expansion. Expand related resources in line with
         the retrieved resources, e.g. Categories/$expand=Products would
         expand Product data in line with each Category entry. Optional.
        :type expand: str
        :param select: OData Select statement. Limits the properties on each
         entry to just those requested, e.g.
         Categories?$select=CategoryName,Description. Optional.
        :type select: str
        :param orderby: OrderBy clause. One or more comma-separated
         expressions with an optional "asc" (the default) or "desc" depending
         on the order you'd like the values sorted, e.g.
         Categories?$orderby=CategoryName desc. Optional.
        :type orderby: str
        :param count: The Boolean value of true or false to request a count
         of the matching resources included with the resources in the
         response, e.g. Categories?$count=true. Optional.
        :type count: bool
        :param search: A free form search. A free-text search expression to
         match for whether a particular entry should be included in the feed,
         e.g. Categories?$search=blue OR green. Optional.
        :type search: str
        :param format: The desired return format. Return the response in
         particular formatxii without access to request headers for standard
         content-type negotiation (e.g Orders?$format=json). Optional.
        :type format: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`StorageAccountInfoPaged
         <azure.mgmt.datalake.analytics.account.models.StorageAccountInfoPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/StorageAccounts/'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'accountName': self._serialize.url("account_name", account_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                if filter is not None:
                    query_parameters['$filter'] = self._serialize.query("filter", filter, 'str')
                if top is not None:
                    query_parameters['$top'] = self._serialize.query("top", top, 'int')
                if skip is not None:
                    query_parameters['$skip'] = self._serialize.query("skip", skip, 'int')
                if expand is not None:
                    query_parameters['$expand'] = self._serialize.query("expand", expand, 'str')
                if select is not None:
                    query_parameters['$select'] = self._serialize.query("select", select, 'str')
                if orderby is not None:
                    query_parameters['$orderby'] = self._serialize.query("orderby", orderby, 'str')
                if count is not None:
                    query_parameters['$count'] = self._serialize.query("count", count, 'bool')
                if search is not None:
                    query_parameters['$search'] = self._serialize.query("search", search, 'str')
                if format is not None:
                    query_parameters['$format'] = self._serialize.query("format", format, 'str')
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.StorageAccountInfoPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.StorageAccountInfoPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list_data_lake_store_accounts(
            self, resource_group_name, account_name, filter=None, top=None, skip=None, expand=None, select=None, orderby=None, count=None, search=None, format=None, custom_headers={}, raw=False, **operation_config):
        """
        Gets the first page of Data Lake Store accounts linked to the
        specified Data Lake Analytics account. The response includes a link
        to the next page, if any.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account for
         which to list Data Lake Store accounts.
        :type account_name: str
        :param filter: OData filter. Optional.
        :type filter: str
        :param top: The number of items to return. Optional.
        :type top: int
        :param skip: The number of items to skip over before returning
         elements. Optional.
        :type skip: int
        :param expand: OData expansion. Expand related resources in line with
         the retrieved resources, e.g. Categories/$expand=Products would
         expand Product data in line with each Category entry. Optional.
        :type expand: str
        :param select: OData Select statement. Limits the properties on each
         entry to just those requested, e.g.
         Categories?$select=CategoryName,Description. Optional.
        :type select: str
        :param orderby: OrderBy clause. One or more comma-separated
         expressions with an optional "asc" (the default) or "desc" depending
         on the order you'd like the values sorted, e.g.
         Categories?$orderby=CategoryName desc. Optional.
        :type orderby: str
        :param count: The Boolean value of true or false to request a count
         of the matching resources included with the resources in the
         response, e.g. Categories?$count=true. Optional.
        :type count: bool
        :param search: A free form search. A free-text search expression to
         match for whether a particular entry should be included in the feed,
         e.g. Categories?$search=blue OR green. Optional.
        :type search: str
        :param format: The desired return format. Return the response in
         particular formatxii without access to request headers for standard
         content-type negotiation (e.g Orders?$format=json). Optional.
        :type format: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`DataLakeStoreAccountInfoPaged
         <azure.mgmt.datalake.analytics.account.models.DataLakeStoreAccountInfoPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}/DataLakeStoreAccounts/'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'accountName': self._serialize.url("account_name", account_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                if filter is not None:
                    query_parameters['$filter'] = self._serialize.query("filter", filter, 'str')
                if top is not None:
                    query_parameters['$top'] = self._serialize.query("top", top, 'int')
                if skip is not None:
                    query_parameters['$skip'] = self._serialize.query("skip", skip, 'int')
                if expand is not None:
                    query_parameters['$expand'] = self._serialize.query("expand", expand, 'str')
                if select is not None:
                    query_parameters['$select'] = self._serialize.query("select", select, 'str')
                if orderby is not None:
                    query_parameters['$orderby'] = self._serialize.query("orderby", orderby, 'str')
                if count is not None:
                    query_parameters['$count'] = self._serialize.query("count", count, 'bool')
                if search is not None:
                    query_parameters['$search'] = self._serialize.query("search", search, 'str')
                if format is not None:
                    query_parameters['$format'] = self._serialize.query("format", format, 'str')
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.DataLakeStoreAccountInfoPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.DataLakeStoreAccountInfoPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list_by_resource_group(
            self, resource_group_name, filter=None, top=None, skip=None, expand=None, select=None, orderby=None, count=None, search=None, format=None, custom_headers={}, raw=False, **operation_config):
        """
        Gets the first page of Data Lake Analytics accounts, if any, within a
        specific resource group. This includes a link to the next page, if
        any.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param filter: OData filter. Optional.
        :type filter: str
        :param top: The number of items to return. Optional.
        :type top: int
        :param skip: The number of items to skip over before returning
         elements. Optional.
        :type skip: int
        :param expand: OData expansion. Expand related resources in line with
         the retrieved resources, e.g. Categories/$expand=Products would
         expand Product data in line with each Category entry. Optional.
        :type expand: str
        :param select: OData Select statement. Limits the properties on each
         entry to just those requested, e.g.
         Categories?$select=CategoryName,Description. Optional.
        :type select: str
        :param orderby: OrderBy clause. One or more comma-separated
         expressions with an optional "asc" (the default) or "desc" depending
         on the order you'd like the values sorted, e.g.
         Categories?$orderby=CategoryName desc. Optional.
        :type orderby: str
        :param count: The Boolean value of true or false to request a count
         of the matching resources included with the resources in the
         response, e.g. Categories?$count=true. Optional.
        :type count: bool
        :param search: A free form search. A free-text search expression to
         match for whether a particular entry should be included in the feed,
         e.g. Categories?$search=blue OR green. Optional.
        :type search: str
        :param format: The return format. Return the response in particular
         formatxii without access to request headers for standard
         content-type negotiation (e.g Orders?$format=json). Optional.
        :type format: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`DataLakeAnalyticsAccountPaged
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccountPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts'
                path_format_arguments = {
                    'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                if filter is not None:
                    query_parameters['$filter'] = self._serialize.query("filter", filter, 'str')
                if top is not None:
                    query_parameters['$top'] = self._serialize.query("top", top, 'int')
                if skip is not None:
                    query_parameters['$skip'] = self._serialize.query("skip", skip, 'int')
                if expand is not None:
                    query_parameters['$expand'] = self._serialize.query("expand", expand, 'str')
                if select is not None:
                    query_parameters['$select'] = self._serialize.query("select", select, 'str')
                if orderby is not None:
                    query_parameters['$orderby'] = self._serialize.query("orderby", orderby, 'str')
                if count is not None:
                    query_parameters['$count'] = self._serialize.query("count", count, 'bool')
                if search is not None:
                    query_parameters['$search'] = self._serialize.query("search", search, 'str')
                if format is not None:
                    query_parameters['$format'] = self._serialize.query("format", format, 'str')
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.DataLakeAnalyticsAccountPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.DataLakeAnalyticsAccountPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def list(
            self, filter=None, top=None, skip=None, expand=None, select=None, orderby=None, count=None, search=None, format=None, custom_headers={}, raw=False, **operation_config):
        """
        Gets the first page of Data Lake Analytics accounts, if any, within
        the current subscription. This includes a link to the next page, if
        any.

        :param filter: OData filter. Optional.
        :type filter: str
        :param top: The number of items to return. Optional.
        :type top: int
        :param skip: The number of items to skip over before returning
         elements. Optional.
        :type skip: int
        :param expand: OData expansion. Expand related resources in line with
         the retrieved resources, e.g. Categories/$expand=Products would
         expand Product data in line with each Category entry. Optional.
        :type expand: str
        :param select: OData Select statement. Limits the properties on each
         entry to just those requested, e.g.
         Categories?$select=CategoryName,Description. Optional.
        :type select: str
        :param orderby: OrderBy clause. One or more comma-separated
         expressions with an optional "asc" (the default) or "desc" depending
         on the order you'd like the values sorted, e.g.
         Categories?$orderby=CategoryName desc. Optional.
        :type orderby: str
        :param count: The Boolean value of true or false to request a count
         of the matching resources included with the resources in the
         response, e.g. Categories?$count=true. Optional.
        :type count: bool
        :param search: A free form search. A free-text search expression to
         match for whether a particular entry should be included in the feed,
         e.g. Categories?$search=blue OR green. Optional.
        :type search: str
        :param format: The desired return format. Return the response in
         particular formatxii without access to request headers for standard
         content-type negotiation (e.g Orders?$format=json). Optional.
        :type format: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`DataLakeAnalyticsAccountPaged
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccountPaged>`
        """
        def internal_paging(next_link=None, raw=False):

            if not next_link:
                # Construct URL
                url = '/subscriptions/{subscriptionId}/providers/Microsoft.DataLakeAnalytics/accounts'
                path_format_arguments = {
                    'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
                }
                url = self._client.format_url(url, **path_format_arguments)

                # Construct parameters
                query_parameters = {}
                if filter is not None:
                    query_parameters['$filter'] = self._serialize.query("filter", filter, 'str')
                if top is not None:
                    query_parameters['$top'] = self._serialize.query("top", top, 'int')
                if skip is not None:
                    query_parameters['$skip'] = self._serialize.query("skip", skip, 'int')
                if expand is not None:
                    query_parameters['$expand'] = self._serialize.query("expand", expand, 'str')
                if select is not None:
                    query_parameters['$select'] = self._serialize.query("select", select, 'str')
                if orderby is not None:
                    query_parameters['$orderby'] = self._serialize.query("orderby", orderby, 'str')
                if count is not None:
                    query_parameters['$count'] = self._serialize.query("count", count, 'bool')
                if search is not None:
                    query_parameters['$search'] = self._serialize.query("search", search, 'str')
                if format is not None:
                    query_parameters['$format'] = self._serialize.query("format", format, 'str')
                query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

            else:
                url = next_link
                query_parameters = {}

            # Construct headers
            header_parameters = {}
            header_parameters['Content-Type'] = 'application/json; charset=utf-8'
            if self.config.generate_client_request_id:
                header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
            if custom_headers:
                header_parameters.update(custom_headers)
            if self.config.accept_language is not None:
                header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

            # Construct and send request
            request = self._client.get(url, query_parameters)
            response = self._client.send(
                request, header_parameters, **operation_config)

            if response.status_code not in [200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            return response

        # Deserialize response
        deserialized = models.DataLakeAnalyticsAccountPaged(internal_paging, self._deserialize.dependencies)

        if raw:
            header_dict = {}
            client_raw_response = models.DataLakeAnalyticsAccountPaged(internal_paging, self._deserialize.dependencies, header_dict)
            return client_raw_response

        return deserialized

    def get(
            self, resource_group_name, account_name, custom_headers={}, raw=False, **operation_config):
        """
        Gets details of the specified Data Lake Analytics account.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account to
         retrieve.
        :type account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :param operation_config: :ref:`Operation configuration
         overrides<msrest:optionsforoperations>`.
        :rtype: :class:`DataLakeAnalyticsAccount
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccount>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        request = self._client.get(url, query_parameters)
        response = self._client.send(request, header_parameters, **operation_config)

        if response.status_code not in [200]:
            exp = CloudError(response)
            exp.request_id = response.headers.get('x-ms-request-id')
            raise exp

        deserialized = None

        if response.status_code == 200:
            deserialized = self._deserialize('DataLakeAnalyticsAccount', response)

        if raw:
            client_raw_response = ClientRawResponse(deserialized, response)
            return client_raw_response

        return deserialized

    def delete(
            self, resource_group_name, account_name, custom_headers={}, raw=False, **operation_config):
        """
        Begins the delete delete process for the Data Lake Analytics account
        object specified by the account name.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param account_name: The name of the Data Lake Analytics account to
         delete
        :type account_name: str
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns None
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{accountName}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'accountName': self._serialize.url("account_name", account_name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct and send request
        def long_running_send():

            request = self._client.delete(url, query_parameters)
            return self._client.send(request, header_parameters, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [200, 202, 404, 204]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            if raw:
                client_raw_response = ClientRawResponse(None, response)
                return client_raw_response

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def create(
            self, resource_group_name, name, parameters, custom_headers={}, raw=False, **operation_config):
        """
        Creates the specified Data Lake Analytics account. This supplies the
        user with computation services for Data Lake Analytics workloads

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.the account will be
         associated with.
        :type resource_group_name: str
        :param name: The name of the Data Lake Analytics account to create.
        :type name: str
        :param parameters: Parameters supplied to the create Data Lake
         Analytics account operation.
        :type parameters: :class:`DataLakeAnalyticsAccount
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccount>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns :class:`DataLakeAnalyticsAccount
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccount>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{name}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'name': self._serialize.url("name", name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'DataLakeAnalyticsAccount')

        # Construct and send request
        def long_running_send():

            request = self._client.put(url, query_parameters)
            return self._client.send(
                request, header_parameters, body_content, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [201, 200]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            deserialized = None

            if response.status_code == 201:
                deserialized = self._deserialize('DataLakeAnalyticsAccount', response)
            if response.status_code == 200:
                deserialized = self._deserialize('DataLakeAnalyticsAccount', response)

            if raw:
                client_raw_response = ClientRawResponse(deserialized, response)
                return client_raw_response

            return deserialized

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

    def update(
            self, resource_group_name, name, parameters, custom_headers={}, raw=False, **operation_config):
        """
        Updates the Data Lake Analytics account object specified by the
        accountName with the contents of the account object.

        :param resource_group_name: The name of the Azure resource group that
         contains the Data Lake Analytics account.
        :type resource_group_name: str
        :param name: The name of the Data Lake Analytics account to update.
        :type name: str
        :param parameters: Parameters supplied to the update Data Lake
         Analytics account operation.
        :type parameters: :class:`DataLakeAnalyticsAccount
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccount>`
        :param dict custom_headers: headers that will be added to the request
        :param bool raw: returns the direct response alongside the
         deserialized response
        :rtype:
         :class:`AzureOperationPoller<msrestazure.azure_operation.AzureOperationPoller>`
         instance that returns :class:`DataLakeAnalyticsAccount
         <azure.mgmt.datalake.analytics.account.models.DataLakeAnalyticsAccount>`
        :rtype: :class:`ClientRawResponse<msrest.pipeline.ClientRawResponse>`
         if raw=true
        """
        # Construct URL
        url = '/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.DataLakeAnalytics/accounts/{name}'
        path_format_arguments = {
            'resourceGroupName': self._serialize.url("resource_group_name", resource_group_name, 'str'),
            'name': self._serialize.url("name", name, 'str'),
            'subscriptionId': self._serialize.url("self.config.subscription_id", self.config.subscription_id, 'str')
        }
        url = self._client.format_url(url, **path_format_arguments)

        # Construct parameters
        query_parameters = {}
        query_parameters['api-version'] = self._serialize.query("self.config.api_version", self.config.api_version, 'str')

        # Construct headers
        header_parameters = {}
        header_parameters['Content-Type'] = 'application/json; charset=utf-8'
        if self.config.generate_client_request_id:
            header_parameters['x-ms-client-request-id'] = str(uuid.uuid1())
        if custom_headers:
            header_parameters.update(custom_headers)
        if self.config.accept_language is not None:
            header_parameters['accept-language'] = self._serialize.header("self.config.accept_language", self.config.accept_language, 'str')

        # Construct body
        body_content = self._serialize.body(parameters, 'DataLakeAnalyticsAccount')

        # Construct and send request
        def long_running_send():

            request = self._client.patch(url, query_parameters)
            return self._client.send(
                request, header_parameters, body_content, **operation_config)

        def get_long_running_status(status_link, headers={}):

            request = self._client.get(status_link)
            request.headers.update(headers)
            return self._client.send(
                request, header_parameters, **operation_config)

        def get_long_running_output(response):

            if response.status_code not in [200, 201]:
                exp = CloudError(response)
                exp.request_id = response.headers.get('x-ms-request-id')
                raise exp

            deserialized = None

            if response.status_code == 200:
                deserialized = self._deserialize('DataLakeAnalyticsAccount', response)
            if response.status_code == 201:
                deserialized = self._deserialize('DataLakeAnalyticsAccount', response)

            if raw:
                client_raw_response = ClientRawResponse(deserialized, response)
                return client_raw_response

            return deserialized

        if raw:
            response = long_running_send()
            return get_long_running_output(response)

        long_running_operation_timeout = operation_config.get(
            'long_running_operation_timeout',
            self.config.long_running_operation_timeout)
        return AzureOperationPoller(
            long_running_send, get_long_running_output,
            get_long_running_status, long_running_operation_timeout)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .catalog_item import CatalogItem


class USqlSchema(CatalogItem):
    """
    A Data Lake Analytics catalog U-SQL schema item.

    :param compute_account_name: Gets or sets the name of the Data Lake
     Analytics account.
    :type compute_account_name: str
    :param version: Gets or sets the version of the catalog item.
    :type version: str
    :param database_name: Gets or sets the name of the database.
    :type database_name: str
    :param name: Gets or sets the name of the schema.
    :type name: str
    """ 

    _attribute_map = {
        'compute_account_name': {'key': 'computeAccountName', 'type': 'str'},
        'version': {'key': 'version', 'type': 'str'},
        'database_name': {'key': 'databaseName', 'type': 'str'},
        'name': {'key': 'schemaName', 'type': 'str'},
    }

    def __init__(self, compute_account_name=None, version=None, database_name=None, name=None):
        super(USqlSchema, self).__init__(compute_account_name=compute_account_name, version=version)
        self.database_name = database_name
        self.name = name

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class JobResource(Model):
    """
    The Data Lake Analytics U-SQL job resources.

    :param name: Gets or set the name of the resource.
    :type name: str
    :param resource_path: Gets or sets the path to the resource.
    :type resource_path: str
    :param type: Gets or sets the job resource type. Possible values include:
     'VertexResource', 'StatisticsResource'
    :type type: str
    """ 

    _attribute_map = {
        'name': {'key': 'name', 'type': 'str'},
        'resource_path': {'key': 'resourcePath', 'type': 'str'},
        'type': {'key': 'type', 'type': 'JobResourceType'},
    }

    def __init__(self, name=None, resource_path=None, type=None):
        self.name = name
        self.resource_path = resource_path
        self.type = type

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .file_operation_result import FileOperationResult
from .acl_status import AclStatus
from .acl_status_result import AclStatusResult
from .content_summary import ContentSummary
from .content_summary_result import ContentSummaryResult
from .file_status_properties import FileStatusProperties
from .file_statuses import FileStatuses
from .file_statuses_result import FileStatusesResult
from .file_status_result import FileStatusResult
from .data_lake_store_file_system_management_client_enums import (
    FileType,
    AppendModeType,
)

__all__ = [
    'FileOperationResult',
    'AclStatus',
    'AclStatusResult',
    'ContentSummary',
    'ContentSummaryResult',
    'FileStatusProperties',
    'FileStatuses',
    'FileStatusesResult',
    'FileStatusResult',
    'FileType',
    'AppendModeType',
]

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.paging import Paged


class WorkflowAccessKeyPaged(Paged):
    """
    A paging container for iterating over a list of WorkflowAccessKey object
    """

    _attribute_map = {
        'next_link': {'key': 'nextLink', 'type': 'str'},
        'current_page': {'key': 'value', 'type': '[WorkflowAccessKey]'}
    }

    def __init__(self, *args, **kwargs):

        super(WorkflowAccessKeyPaged, self).__init__(*args, **kwargs)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

VERSION = "2015-02-01-preview"


# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class Error(Model):
    """Error

    :param code:
    :type code: str
    :param message:
    :type message: str
    :param target:
    :type target: str
    :param details:
    :type details: list of :class:`ErrorDetails
     <azure.mgmt.network.models.ErrorDetails>`
    :param inner_error:
    :type inner_error: str
    """ 

    _attribute_map = {
        'code': {'key': 'code', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'target': {'key': 'target', 'type': 'str'},
        'details': {'key': 'details', 'type': '[ErrorDetails]'},
        'inner_error': {'key': 'innerError', 'type': 'str'},
    }

    def __init__(self, code=None, message=None, target=None, details=None, inner_error=None):
        self.code = code
        self.message = message
        self.target = target
        self.details = details
        self.inner_error = inner_error

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .resource import Resource


class LocalNetworkGateway(Resource):
    """
    A common class for general resource information

    Variables are only populated by the server, and will be ignored when
    sending a request.

    :param id: Resource Id
    :type id: str
    :ivar name: Resource name
    :vartype name: str
    :ivar type: Resource type
    :vartype type: str
    :param location: Resource location
    :type location: str
    :param tags: Resource tags
    :type tags: dict
    :param local_network_address_space: Local network site Address space
    :type local_network_address_space: :class:`AddressSpace
     <azure.mgmt.network.models.AddressSpace>`
    :param gateway_ip_address: IP address of local network gateway.
    :type gateway_ip_address: str
    :param bgp_settings: Local network gateway's BGP speaker settings
    :type bgp_settings: :class:`BgpSettings
     <azure.mgmt.network.models.BgpSettings>`
    :param resource_guid: Gets or sets resource guid property of the
     LocalNetworkGateway resource
    :type resource_guid: str
    :param provisioning_state: Gets or sets Provisioning state of the
     LocalNetworkGateway resource Updating/Deleting/Failed
    :type provisioning_state: str
    :param etag: Gets a unique read-only string that changes whenever the
     resource is updated
    :type etag: str
    """ 

    _validation = {
        'name': {'readonly': True},
        'type': {'readonly': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'location': {'key': 'location', 'type': 'str'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'local_network_address_space': {'key': 'properties.localNetworkAddressSpace', 'type': 'AddressSpace'},
        'gateway_ip_address': {'key': 'properties.gatewayIpAddress', 'type': 'str'},
        'bgp_settings': {'key': 'properties.bgpSettings', 'type': 'BgpSettings'},
        'resource_guid': {'key': 'properties.resourceGuid', 'type': 'str'},
        'provisioning_state': {'key': 'properties.provisioningState', 'type': 'str'},
        'etag': {'key': 'etag', 'type': 'str'},
    }

    def __init__(self, id=None, location=None, tags=None, local_network_address_space=None, gateway_ip_address=None, bgp_settings=None, resource_guid=None, provisioning_state=None, etag=None):
        super(LocalNetworkGateway, self).__init__(id=id, location=location, tags=tags)
        self.local_network_address_space = local_network_address_space
        self.gateway_ip_address = gateway_ip_address
        self.bgp_settings = bgp_settings
        self.resource_guid = resource_guid
        self.provisioning_state = provisioning_state
        self.etag = etag

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .sub_resource import SubResource


class VirtualNetworkGatewayIPConfiguration(SubResource):
    """
    IpConfiguration for Virtual network gateway

    :param id: Resource Id
    :type id: str
    :param private_ip_address: Gets or sets the privateIPAddress of the IP
     Configuration
    :type private_ip_address: str
    :param private_ip_allocation_method: Gets or sets PrivateIP allocation
     method (Static/Dynamic). Possible values include: 'Static', 'Dynamic'
    :type private_ip_allocation_method: str
    :param subnet: Gets or sets the reference of the subnet resource
    :type subnet: :class:`SubResource <azure.mgmt.network.models.SubResource>`
    :param public_ip_address: Gets or sets the reference of the PublicIP
     resource
    :type public_ip_address: :class:`SubResource
     <azure.mgmt.network.models.SubResource>`
    :param provisioning_state: Gets or sets Provisioning state of the
     PublicIP resource Updating/Deleting/Failed
    :type provisioning_state: str
    :param name: Gets name of the resource that is unique within a resource
     group. This name can be used to access the resource
    :type name: str
    :param etag: A unique read-only string that changes whenever the resource
     is updated
    :type etag: str
    """ 

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'private_ip_address': {'key': 'properties.privateIPAddress', 'type': 'str'},
        'private_ip_allocation_method': {'key': 'properties.privateIPAllocationMethod', 'type': 'IPAllocationMethod'},
        'subnet': {'key': 'properties.subnet', 'type': 'SubResource'},
        'public_ip_address': {'key': 'properties.publicIPAddress', 'type': 'SubResource'},
        'provisioning_state': {'key': 'properties.provisioningState', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'etag': {'key': 'etag', 'type': 'str'},
    }

    def __init__(self, id=None, private_ip_address=None, private_ip_allocation_method=None, subnet=None, public_ip_address=None, provisioning_state=None, name=None, etag=None):
        super(VirtualNetworkGatewayIPConfiguration, self).__init__(id=id)
        self.private_ip_address = private_ip_address
        self.private_ip_allocation_method = private_ip_allocation_method
        self.subnet = subnet
        self.public_ip_address = public_ip_address
        self.provisioning_state = provisioning_state
        self.name = name
        self.etag = etag

#!/usr/bin/env python

#-------------------------------------------------------------------------
# Copyright (c) Microsoft.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#--------------------------------------------------------------------------

from setuptools import setup

# azure v0.x is not compatible with this package
# azure v0.x used to have a __version__ attribute (newer versions don't)
try:
    import azure
    try:
        ver = azure.__version__
        raise Exception(
            'This package is incompatible with azure=={}. '.format(ver) +
            'Uninstall it with "pip uninstall azure".'
        )
    except AttributeError:
        pass
except ImportError:
    pass

setup(
    name='azure-mgmt-network',
    version='0.30.0rc3',
    description='Microsoft Azure Network Resource Management Client Library for Python',
    long_description=open('README.rst', 'r').read(),
    license='Apache License 2.0',
    author='Microsoft Corporation',
    author_email='ptvshelp@microsoft.com',
    url='https://github.com/Azure/azure-sdk-for-python',
    classifiers=[
        'Development Status :: 4 - Beta',
        'Programming Language :: Python',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'License :: OSI Approved :: Apache Software License',
    ],
    zip_safe=False,
    packages=[
        'azure',
        'azure.mgmt',
        'azure.mgmt.network',
        'azure.mgmt.network.models',
        'azure.mgmt.network.operations',
    ],
    install_requires=[
        'azure-mgmt-nspkg',
        'azure-common[autorest]==1.1.3',
    ],
)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.paging import Paged


class SharedAccessAuthorizationRuleResourcePaged(Paged):
    """
    A paging container for iterating over a list of SharedAccessAuthorizationRuleResource object
    """

    _attribute_map = {
        'next_link': {'key': 'nextLink', 'type': 'str'},
        'current_page': {'key': 'value', 'type': '[SharedAccessAuthorizationRuleResource]'}
    }

    def __init__(self, *args, **kwargs):

        super(SharedAccessAuthorizationRuleResourcePaged, self).__init__(*args, **kwargs)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .feature_client import FeatureClient, FeatureClientConfiguration
from .version import VERSION

__all__ = [
    'FeatureClient',
    'FeatureClientConfiguration'
]

__version__ = VERSION


# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class DeploymentExtended(Model):
    """
    Deployment information.

    :param id: Gets or sets the ID of the deployment.
    :type id: str
    :param name: Gets or sets the name of the deployment.
    :type name: str
    :param properties: Gets or sets deployment properties.
    :type properties: :class:`DeploymentPropertiesExtended
     <azure.mgmt.resource.resources.models.DeploymentPropertiesExtended>`
    """ 

    _validation = {
        'name': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'properties': {'key': 'properties', 'type': 'DeploymentPropertiesExtended'},
    }

    def __init__(self, name, id=None, properties=None):
        self.id = id
        self.name = name
        self.properties = properties

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.paging import Paged


class TagDetailsPaged(Paged):
    """
    A paging container for iterating over a list of TagDetails object
    """

    _attribute_map = {
        'next_link': {'key': 'nextLink', 'type': 'str'},
        'current_page': {'key': 'value', 'type': '[TagDetails]'}
    }

    def __init__(self, *args, **kwargs):

        super(TagDetailsPaged, self).__init__(*args, **kwargs)

#!/usr/bin/env python

#-------------------------------------------------------------------------
# Copyright (c) Microsoft.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#--------------------------------------------------------------------------

from setuptools import setup

# azure v0.x is not compatible with this package
# azure v0.x used to have a __version__ attribute (newer versions don't)
try:
    import azure
    try:
        ver = azure.__version__
        raise Exception(
            'This package is incompatible with azure=={}. '.format(ver) +
            'Uninstall it with "pip uninstall azure".'
        )
    except AttributeError:
        pass
except ImportError:
    pass

setup(
    name='azure-mgmt-resource',
    version='0.30.0rc3',
    description='Microsoft Azure Resource Management Client Library for Python',
    long_description=open('README.rst', 'r').read(),
    license='Apache License 2.0',
    author='Microsoft Corporation',
    author_email='ptvshelp@microsoft.com',
    url='https://github.com/Azure/azure-sdk-for-python',
    classifiers=[
        'Development Status :: 4 - Beta',
        'Programming Language :: Python',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'License :: OSI Approved :: Apache Software License',
    ],
    zip_safe=False,
    packages=[
        'azure',
        'azure.mgmt',
        'azure.mgmt.resource',
        'azure.mgmt.resource.resources',
        'azure.mgmt.resource.resources.models',
        'azure.mgmt.resource.resources.operations',
        'azure.mgmt.resource.features',
        'azure.mgmt.resource.features.models',
        'azure.mgmt.resource.features.operations',
        'azure.mgmt.resource.locks',
        'azure.mgmt.resource.locks.models',
        'azure.mgmt.resource.locks.operations',
        'azure.mgmt.resource.subscriptions',
        'azure.mgmt.resource.subscriptions.models',
        'azure.mgmt.resource.subscriptions.operations',
    ],
    install_requires=[
        'azure-mgmt-nspkg',
        'azure-common[autorest]==1.1.3',
    ],
)

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from enum import Enum


class SkuDefinition(Enum):

    standard = "Standard"
    free = "Free"
    premium = "Premium"


class JobCollectionState(Enum):

    enabled = "Enabled"
    disabled = "Disabled"
    suspended = "Suspended"
    deleted = "Deleted"


class RecurrenceFrequency(Enum):

    minute = "Minute"
    hour = "Hour"
    day = "Day"
    week = "Week"
    month = "Month"


class JobActionType(Enum):

    http = "Http"
    https = "Https"
    storage_queue = "StorageQueue"
    service_bus_queue = "ServiceBusQueue"
    service_bus_topic = "ServiceBusTopic"


class HttpAuthenticationType(Enum):

    not_specified = "NotSpecified"
    client_certificate = "ClientCertificate"
    active_directory_oauth = "ActiveDirectoryOAuth"
    basic = "Basic"


class RetryType(Enum):

    none = "None"
    fixed = "Fixed"


class DayOfWeek(Enum):

    sunday = "Sunday"
    monday = "Monday"
    tuesday = "Tuesday"
    wednesday = "Wednesday"
    thursday = "Thursday"
    friday = "Friday"
    saturday = "Saturday"


class JobScheduleDay(Enum):

    monday = "Monday"
    tuesday = "Tuesday"
    wednesday = "Wednesday"
    thursday = "Thursday"
    friday = "Friday"
    saturday = "Saturday"
    sunday = "Sunday"


class JobState(Enum):

    enabled = "Enabled"
    disabled = "Disabled"
    faulted = "Faulted"
    completed = "Completed"


class JobHistoryActionName(Enum):

    main_action = "MainAction"
    error_action = "ErrorAction"


class JobExecutionStatus(Enum):

    completed = "Completed"
    failed = "Failed"
    postponed = "Postponed"


class ServiceBusAuthenticationType(Enum):

    not_specified = "NotSpecified"
    shared_access_key = "SharedAccessKey"


class ServiceBusTransportType(Enum):

    not_specified = "NotSpecified"
    net_messaging = "NetMessaging"
    amqp = "AMQP"

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class UsageName(Model):
    """
    The Usage Names.

    :param value: Gets a string describing the resource name.
    :type value: str
    :param localized_value: Gets a localized string describing the resource
     name.
    :type localized_value: str
    """ 

    _attribute_map = {
        'value': {'key': 'value', 'type': 'str'},
        'localized_value': {'key': 'localizedValue', 'type': 'str'},
    }

    def __init__(self, value=None, localized_value=None):
        self.value = value
        self.localized_value = localized_value

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class ClassicMobileServiceCollection(Model):
    """
    Collection of Classic Mobile Services

    :param value: Collection of resources
    :type value: list of :class:`ClassicMobileService
     <azure.mgmt.web.models.ClassicMobileService>`
    :param next_link: Link to next page of resources
    :type next_link: str
    """ 

    _attribute_map = {
        'value': {'key': 'value', 'type': '[ClassicMobileService]'},
        'next_link': {'key': 'nextLink', 'type': 'str'},
    }

    def __init__(self, value=None, next_link=None):
        self.value = value
        self.next_link = next_link

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class FileSystemApplicationLogsConfig(Model):
    """
    Application logs to file system configuration

    :param level: Log level. Possible values include: 'Off', 'Verbose',
     'Information', 'Warning', 'Error'
    :type level: str
    """ 

    _attribute_map = {
        'level': {'key': 'level', 'type': 'LogLevel'},
    }

    def __init__(self, level=None):
        self.level = level

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from msrest.serialization import Model


class Recommendation(Model):
    """
    Represents a recommendation result generated by the recommendation engine

    :param creation_time: Timestamp when this instance was created.
    :type creation_time: datetime
    :param recommendation_id: A GUID value that each recommendation object is
     associated with.
    :type recommendation_id: str
    :param resource_id: Full ARM resource ID string that this recommendation
     object is associated with.
    :type resource_id: str
    :param resource_scope: Name of a resource type this recommendation
     applies, e.g. Subscription, ServerFarm, Site.
    :type resource_scope: str
    :param rule_name: Unique name of the rule
    :type rule_name: str
    :param display_name: UI friendly name of the rule (may not be unique)
    :type display_name: str
    :param message: Localized text of recommendation, good for UI.
    :type message: str
    :param level: Level indicating how critical this recommendation can
     impact. Possible values include: 'Critical', 'Warning', 'Information',
     'NonUrgentSuggestion'
    :type level: str
    :param channels: List of channels that this recommendation can apply.
     Possible values include: 'Notification', 'Api', 'Email', 'All'
    :type channels: str
    :param tags: The list of category tags that this recommendation belongs
     to.
    :type tags: list of str
    :param action_name: Name of action recommended by this object.
    :type action_name: str
    :param enabled: On/off flag indicating the rule is currently enabled or
     disabled.
    :type enabled: int
    :param start_time: The beginning time of a range that the recommendation
     refers to.
    :type start_time: datetime
    :param end_time: The end time of a range that the recommendation refers
     to.
    :type end_time: datetime
    :param next_notification_time: When to notify this recommendation next.
     Null means that this will never be notified anymore.
    :type next_notification_time: datetime
    :param notification_expiration_time: Date and time when this notification
     expires.
    :type notification_expiration_time: datetime
    :param notified_time: Last timestamp this instance was actually notified.
     Null means that this recommendation hasn't been notified yet.
    :type notified_time: datetime
    :param score: A metric value measured by the rule.
    :type score: float
    """ 

    _validation = {
        'level': {'required': True},
        'channels': {'required': True},
    }

    _attribute_map = {
        'creation_time': {'key': 'creationTime', 'type': 'iso-8601'},
        'recommendation_id': {'key': 'recommendationId', 'type': 'str'},
        'resource_id': {'key': 'resourceId', 'type': 'str'},
        'resource_scope': {'key': 'resourceScope', 'type': 'str'},
        'rule_name': {'key': 'ruleName', 'type': 'str'},
        'display_name': {'key': 'displayName', 'type': 'str'},
        'message': {'key': 'message', 'type': 'str'},
        'level': {'key': 'level', 'type': 'NotificationLevel'},
        'channels': {'key': 'channels', 'type': 'Channels'},
        'tags': {'key': 'tags', 'type': '[str]'},
        'action_name': {'key': 'actionName', 'type': 'str'},
        'enabled': {'key': 'enabled', 'type': 'int'},
        'start_time': {'key': 'startTime', 'type': 'iso-8601'},
        'end_time': {'key': 'endTime', 'type': 'iso-8601'},
        'next_notification_time': {'key': 'nextNotificationTime', 'type': 'iso-8601'},
        'notification_expiration_time': {'key': 'notificationExpirationTime', 'type': 'iso-8601'},
        'notified_time': {'key': 'notifiedTime', 'type': 'iso-8601'},
        'score': {'key': 'score', 'type': 'float'},
    }

    def __init__(self, level, channels, creation_time=None, recommendation_id=None, resource_id=None, resource_scope=None, rule_name=None, display_name=None, message=None, tags=None, action_name=None, enabled=None, start_time=None, end_time=None, next_notification_time=None, notification_expiration_time=None, notified_time=None, score=None):
        self.creation_time = creation_time
        self.recommendation_id = recommendation_id
        self.resource_id = resource_id
        self.resource_scope = resource_scope
        self.rule_name = rule_name
        self.display_name = display_name
        self.message = message
        self.level = level
        self.channels = channels
        self.tags = tags
        self.action_name = action_name
        self.enabled = enabled
        self.start_time = start_time
        self.end_time = end_time
        self.next_notification_time = next_notification_time
        self.notification_expiration_time = notification_expiration_time
        self.notified_time = notified_time
        self.score = score

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .resource import Resource


class SiteSourceControl(Resource):
    """
    Describes the source control configuration for web app

    :param id: Resource Id
    :type id: str
    :param name: Resource Name
    :type name: str
    :param kind: Kind of resource
    :type kind: str
    :param location: Resource Location
    :type location: str
    :param type: Resource type
    :type type: str
    :param tags: Resource tags
    :type tags: dict
    :param repo_url: Repository or source control url
    :type repo_url: str
    :param branch: Name of branch to use for deployment
    :type branch: str
    :param is_manual_integration: Whether to manual or continuous integration
    :type is_manual_integration: bool
    :param deployment_rollback_enabled: Whether to manual or continuous
     integration
    :type deployment_rollback_enabled: bool
    :param is_mercurial: Mercurial or Git repository type
    :type is_mercurial: bool
    """ 

    _validation = {
        'location': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'kind': {'key': 'kind', 'type': 'str'},
        'location': {'key': 'location', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'repo_url': {'key': 'properties.repoUrl', 'type': 'str'},
        'branch': {'key': 'properties.branch', 'type': 'str'},
        'is_manual_integration': {'key': 'properties.isManualIntegration', 'type': 'bool'},
        'deployment_rollback_enabled': {'key': 'properties.deploymentRollbackEnabled', 'type': 'bool'},
        'is_mercurial': {'key': 'properties.isMercurial', 'type': 'bool'},
    }

    def __init__(self, location, id=None, name=None, kind=None, type=None, tags=None, repo_url=None, branch=None, is_manual_integration=None, deployment_rollback_enabled=None, is_mercurial=None):
        super(SiteSourceControl, self).__init__(id=id, name=name, kind=kind, location=location, type=type, tags=tags)
        self.repo_url = repo_url
        self.branch = branch
        self.is_manual_integration = is_manual_integration
        self.deployment_rollback_enabled = deployment_rollback_enabled
        self.is_mercurial = is_mercurial

# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft and contributors.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#
# See the License for the specific language governing permissions and
# limitations under the License.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .resource import Resource


class VnetGateway(Resource):
    """
    The VnetGateway contract. This is used to give the vnet gateway access to
    the VPN package.

    :param id: Resource Id
    :type id: str
    :param name: Resource Name
    :type name: str
    :param kind: Kind of resource
    :type kind: str
    :param location: Resource Location
    :type location: str
    :param type: Resource type
    :type type: str
    :param tags: Resource tags
    :type tags: dict
    :param vnet_name: The VNET name.
    :type vnet_name: str
    :param vpn_package_uri: The URI where the Vpn package can be downloaded
    :type vpn_package_uri: str
    """ 

    _validation = {
        'location': {'required': True},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'kind': {'key': 'kind', 'type': 'str'},
        'location': {'key': 'location', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'vnet_name': {'key': 'properties.vnetName', 'type': 'str'},
        'vpn_package_uri': {'key': 'properties.vpnPackageUri', 'type': 'str'},
    }

    def __init__(self, location, id=None, name=None, kind=None, type=None, tags=None, vnet_name=None, vpn_package_uri=None):
        super(VnetGateway, self).__init__(id=id, name=name, kind=kind, location=location, type=type, tags=tags)
        self.vnet_name = vnet_name
        self.vpn_package_uri = vpn_package_uri

# coding: utf-8

#-------------------------------------------------------------------------
# Copyright (c) Microsoft.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#--------------------------------------------------------------------------

import unittest
from datetime import datetime

from azure.servicemanagement import (
    CloudServices,
    CloudService,
    SchedulerManagementService,
)
from testutils.common_recordingtestcase import (
    TestMode,
    record,
)
from tests.legacy_mgmt_testcase import LegacyMgmtTestCase


class LegacyMgmtSchedulerTest(LegacyMgmtTestCase):

    def setUp(self):
        super(LegacyMgmtSchedulerTest, self).setUp()

        self.ss = self.create_service_management(SchedulerManagementService)

        self.service_id = self.get_resource_name('cloud_service_')
        self.coll_id = self.get_resource_name('job_collection_')
        self.job_id = 'job_id'

    def tearDown(self):
        if not self.is_playback():
            try:
                self.ss.delete_cloud_service(self.service_id)
            except:
                pass

        return super(LegacyMgmtSchedulerTest, self).tearDown()

    def cleanup(self):
        self.ss.delete_cloud_service(self.service_id)
        pass

    def _create_cloud_service(self):
        result = self.ss.create_cloud_service(
            self.service_id,
            "label",
            "description",
            "West Europe",
        )
        self._wait_for_async(result.request_id)

    def _create_job_collection(self):
        result = self.ss.create_job_collection(self.service_id, self.coll_id)
        self._wait_for_async(result.request_id)

    def _create_job(self):
        result = self.ss.create_job(
            self.service_id,
            self.coll_id,
            self.job_id,
            self._create_job_dict(),
        )
        self._wait_for_async(result.request_id)

    def _create_job_dict(self):
        return {
            "startTime": datetime.utcnow(),
            "action":
            {
                "type": "http",
                "request":
                {
                    "uri": "http://bing.com/",
                    "method": "GET",
                    "headers":
                    {
                        "Content-Type": "text/plain"
                    }
                }
            },
            "recurrence":
            {
                "frequency": "minute",
                "interval": 30,
                "count": 10
            },
            "state": "enabled"
        }

    def _wait_for_async(self, request_id):
        # Note that we keep the same ratio of timeout/sleep_interval in
        # live and playback so we end up with same number of loops/requests
        if self.is_playback():
            self.ss.wait_for_operation_status(request_id, timeout=1.2, sleep_interval=0.2)
        else:
            self.ss.wait_for_operation_status(request_id, timeout=30, sleep_interval=5)

    #--Operations for scheduler ----------------------------------------
    @record
    def test_list_cloud_services(self):
        # Arrange
        self._create_cloud_service()

        # Act
        result = self.ss.list_cloud_services()

        # Assert
        self.assertIsNotNone(result)
        self.assertIsInstance(result, CloudServices)

        for cs in result:
            self.assertIsNotNone(cs)
            self.assertIsInstance(cs, CloudService)

    @record
    def test_get_cloud_service(self):
        # Arrange
        self._create_cloud_service()

        # Act
        result = self.ss.get_cloud_service(self.service_id)

        # Assert
        self.assertIsNotNone(result)
        self.assertEqual(result.name, self.service_id)
        self.assertEqual(result.label, "label")
        self.assertEqual(result.geo_region, "West Europe")

    @record
    def test_create_cloud_service(self):
        # Arrange

        # Act
        result = self.ss.create_cloud_service(
            self.service_id,
            "label",
            "description",
            "West Europe",
        )
        self._wait_for_async(result.request_id)

        # Assert
        self.assertIsNotNone(result)

    @unittest.skip("functionality not working, haven't had a chance to debug")
    @record
    def test_check_name_availability(self):
        # Arrange
        self._create_cloud_service()

        # Act
        result = self.ss.check_job_collection_name(self.service_id, "BOB")

        # Assert
        self.assertIsNotNone(result)

    @record
    def test_create_job_collection(self):
        # Arrange
        self._create_cloud_service()

        # Act
        result = self.ss.create_job_collection(self.service_id, self.coll_id)
        self._wait_for_async(result.request_id)

        # Assert
        self.assertIsNotNone(result)

    @record
    def test_delete_job_collection(self):
        # Arrange
        self._create_cloud_service()
        self._create_job_collection()

        # Act
        result = self.ss.delete_job_collection(self.service_id, self.coll_id)
        self._wait_for_async(result.request_id)

        # Assert
        self.assertIsNotNone(result)

    @record
    def test_get_job_collection(self):
        # Arrange
        self._create_cloud_service()
        self._create_job_collection()

        # Act
        result = self.ss.get_job_collection(self.service_id, self.coll_id)

        # Assert
        self.assertIsNotNone(result)
        self.assertEqual(result.name, self.coll_id)

    @record
    def test_create_job(self):
        # Arrange
        self._create_cloud_service()
        self._create_job_collection()

        # Act
        job = self._create_job_dict()
        result = self.ss.create_job(
            self.service_id,
            self.coll_id,
            self.job_id,
            job,
        )
        self._wait_for_async(result.request_id)

        # Assert
        self.assertIsNotNone(result)

    @record
    def test_delete_job(self):
        # Arrange
        self._create_cloud_service()
        self._create_job_collection()
        self._create_job()

        # Act
        result = self.ss.delete_job(self.service_id, self.coll_id, self.job_id)
        self._wait_for_async(result.request_id)

        # Assert
        self.assertIsNotNone(result)

    @record
    def test_get_job(self):
        self._create_cloud_service()
        self._create_job_collection()
        self._create_job()

        # Act
        result = self.ss.get_job(self.service_id, self.coll_id, self.job_id)

        # Assert
        self.assertIsNotNone(result)
        self.assertEqual(result["state"], "enabled")

    @record
    def test_get_all_jobs(self):
        self._create_cloud_service()
        self._create_job_collection()
        self._create_job()

        # Act
        result = self.ss.get_all_jobs(self.service_id, self.coll_id)

        # Assert
        self.assertIsNotNone(result)
        self.assertEqual(len(result), 1)

#------------------------------------------------------------------------------
if __name__ == '__main__':
    unittest.main()

# This should be deleted. In the autoscaling branch B extracted this from JobBatcher in leader.py
# but while reviving that branch I decided to undo the extraction. For one, JobBatcher and
# leader.py in general had changed too much (e.g. services) and two, the this would derail the
# caching branch as well. The main semantic changes to JobDispatcher were wall time and the
# IssuedJob stuff.

# Copyright (C) 2015 UCSC Computational Genomics Lab
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import absolute_import
import logging
from toil import resolveEntryPoint
from toil.lib.bioio import logStream
from collections import namedtuple
import time
from toil.toilState import ToilState

logger = logging.getLogger( __name__ )

# Represents a job and its requirements as issued to the batch system
IssuedJob = namedtuple("IssuedJob", "jobStoreID memory cores disk preemptable")

class JobDispatcher(object):
    """
    Class manages dispatching jobs to the batch system.
    """
    def __init__(self, config, batchSystem, jobStore, rootJobWrapper):
        """
        """
        self.config = config
        self.jobStore = jobStore
        self.batchSystem = batchSystem
        self.clusterScaler = None # This an optional parameter which may be set
        # if doing autoscaling
        self.toilState = ToilState(jobStore, rootJobWrapper)
        self.jobBatchSystemIDToIssuedJob = {} # Map of batch system IDs to IsseudJob tuples
        self.reissueMissingJobs_missingHash = {} #Hash to store number of observed misses
        
    def dispatch(self):
        """
        Starts a loop with the batch system to run the Toil workflow
        """

        # Kill any jobs on the batch system queue from the last time.
        assert len(self.batchSystem.getIssuedBatchJobIDs()) == 0 #Batch system must start with no active jobs!
        logger.info("Checked batch system has no running jobs and no updated jobs")
    
        logger.info("Found %s jobs to start and %i jobs with successors to run",
                    len(self.toilState.updatedJobs), len(self.toilState.successorCounts))
    
        # The main loop in which jobs are scheduled/processed
        
        # Sets up the timing of the jobWrapper rescuing method
        timeSinceJobsLastRescued = time.time()
        # Number of jobs that can not be completed successful after exhausting retries
        totalFailedJobs = 0
        logger.info("Starting the main loop")
        while True:
            # Process jobs that are ready to be scheduled/have successors to schedule
            
            if len(self.toilState.updatedJobs) > 0:
                logger.debug("Built the jobs list, currently have %i jobs to update and %i jobs issued",
                             len(self.toilState.updatedJobs), self.getNumberOfJobsIssued())
    
                for jobWrapper, resultStatus in self.toilState.updatedJobs:
                    #If the jobWrapper has a command it must be run before any successors
                    #Similarly, if the job previously failed we rerun it, even if it doesn't have a command to
                    #run, to eliminate any parts of the stack now completed.
                    if jobWrapper.command != None or resultStatus != 0:
                        if jobWrapper.remainingRetryCount > 0:
                            iJ = IssuedJob(jobWrapper.jobStoreID, jobWrapper.memory, 
                                           jobWrapper.cores, jobWrapper.disk, jobWrapper.preemptable)
                            self.issueJob(iJ)
                        else:
                            totalFailedJobs += 1
                            logger.warn("Job: %s is completely failed", jobWrapper.jobStoreID)
    
                    #There exist successors to run
                    elif len(jobWrapper.stack) > 0:
                        assert len(jobWrapper.stack[-1]) > 0
                        logger.debug("Job: %s has %i successors to schedule",
                                     jobWrapper.jobStoreID, len(jobWrapper.stack[-1]))
                        #Record the number of successors that must be completed before
                        #the jobWrapper can be considered again
                        assert jobWrapper not in self.toilState.successorCounts
                        self.toilState.successorCounts[jobWrapper] = len(jobWrapper.stack[-1])
                        #List of successors to schedule
                        successors = []
                        #For each successor schedule if all predecessors have been completed
                        for successorJobStoreID, memory, cores, disk, preemptable, predecessorID in jobWrapper.stack.pop():
                            #Build map from successor to predecessors.
                            if successorJobStoreID not in self.toilState.successorJobStoreIDToPredecessorJobs:
                                self.toilState.successorJobStoreIDToPredecessorJobs[successorJobStoreID] = []
                            self.toilState.successorJobStoreIDToPredecessorJobs[successorJobStoreID].append(jobWrapper)
                            #Case that the jobWrapper has multiple predecessors
                            if predecessorID != None:
                                #Load the wrapped jobWrapper
                                job2 = self.jobStore.load(successorJobStoreID)
                                #Remove the predecessor from the list of predecessors
                                job2.predecessorsFinished.add(predecessorID)
                                #Checkpoint
                                self.jobStore.update(job2)
                                #If the jobs predecessors have all not all completed then
                                #ignore the jobWrapper
                                assert len(job2.predecessorsFinished) >= 1
                                assert len(job2.predecessorsFinished) <= job2.predecessorNumber
                                if len(job2.predecessorsFinished) < job2.predecessorNumber:
                                    continue
                            successors.append(IssuedJob(successorJobStoreID, memory, cores, disk, preemptable))
                        map(self.issueJob, successors)
    
                    # There are no remaining tasks to schedule within the jobWrapper, but
                    # we schedule it anyway to allow it to be deleted.
    
                    # TODO: An alternative would be simple delete it here and add it to the
                    # list of jobs to process, or (better) to create an asynchronous
                    # process that deletes jobs and then feeds them back into the set
                    # of jobs to be processed
                    else:
                        if jobWrapper.remainingRetryCount > 0:
                            iJ = IssuedJob(jobWrapper.jobStoreID, 
                                           self.config.defaultMemory, self.config.defaultCores,
                                           self.config.defaultDisk, True)
                            self.issueJob(iJ) #We allow this cleanup to potentially occur on a preemptable instance
                            logger.debug("Job: %s is empty, we are scheduling to clean it up", jobWrapper.jobStoreID)
                        else:
                            totalFailedJobs += 1
                            logger.warn("Job: %s is empty but completely failed - something is very wrong", jobWrapper.jobStoreID)
    
                self.toilState.updatedJobs = set() #We've considered them all, so reset
    
            # The exit criterion
    
            if self.getNumberOfJobsIssued() == 0:
                logger.info("Only failed jobs and their dependents (%i total) are remaining, so exiting.", totalFailedJobs)
                return totalFailedJobs
    
            # Gather any new, updated jobs from the batch system
        
            if self.processAnyUpdatedJob(10) == None:  # Asks the batch system to 
                # process a job that has been completed,
                # In the case that there is nothing happening
                # (no updated jobWrapper to gather for 10 seconds)
                # check if their are any jobs that have run too long
                # (see JobBatcher.reissueOverLongJobs) or which
                # have gone missing from the batch system (see JobBatcher.reissueMissingJobs)
                if (time.time() - timeSinceJobsLastRescued >=
                    self.config.rescueJobsFrequency): #We only
                    #rescue jobs every N seconds, and when we have
                    #apparently exhausted the current jobWrapper supply
                    self.reissueOverLongJobs()
                    logger.info("Reissued any over long jobs")
    
                    hasNoMissingJobs = self.reissueMissingJobs()
                    if hasNoMissingJobs:
                        timeSinceJobsLastRescued = time.time()
                    else:
                        timeSinceJobsLastRescued += 60 #This means we'll try again
                        #in a minute, providing things are quiet
                    logger.info("Rescued any (long) missing jobs")


    def issueJob(self, issuedJob):
        """
        Add a job to the queue of jobs. 
        """
        jobCommand = ' '.join((resolveEntryPoint('_toil_worker'), 
                               self.config.jobStore, issuedJob.jobStoreID))
        jobBatchSystemID = self.batchSystem.issueBatchJob(jobCommand, issuedJob.memory, 
                                issuedJob.cores, issuedJob.disk, issuedJob.preemptable)
        self.jobBatchSystemIDToIssuedJob[jobBatchSystemID] = issuedJob
        logger.debug("Issued job with job store ID: %s and job batch system ID: "
                     "%s and cores: %i, disk: %i, and memory: %i",
                     issuedJob.jobStoreID, str(jobBatchSystemID), issuedJob.cores, 
                     issuedJob.disk, issuedJob.memory)
        
    def processAnyUpdatedJob(self, block=10):
        """
        Get an updated job from the batch system, blocking for up to block 
        seconds while waiting for the job. 
        """
        updatedJob = self.batchSystem.getUpdatedBatchJob(block)
        if updatedJob is not None:
            jobBatchSystemID, exitValue, wallTime = updatedJob
            if self.clusterScaler is not None:
                issuedJob = self.jobBatchSystemIDToIssuedJob[jobBatchSystemID]
                self.clusterScaler.addCompletedJob(issuedJob, wallTime)
            if self.hasJob(jobBatchSystemID):
                if exitValue == 0:
                    logger.debug("Batch system is reporting that the jobWrapper with "
                                 "batch system ID: %s and jobWrapper store ID: %s ended successfully",
                                 jobBatchSystemID, self.getJobStoreID(jobBatchSystemID))
                else:
                    logger.warn("Batch system is reporting that the jobWrapper with "
                                "batch system ID: %s and jobWrapper store ID: %s failed with exit value %i",
                                jobBatchSystemID, self.getJobStoreID(jobBatchSystemID), exitValue)
                self.processFinishedJob(jobBatchSystemID, exitValue)
            else:
                logger.warn("A result seems to already have been processed "
                            "for jobWrapper with batch system ID: %i", jobBatchSystemID)
        return updatedJob

    def getNumberOfJobsIssued(self):
        """
        Gets number of jobs that have been added by issueJob(s) and not
        removed by removeJobID
        """
        return len(self.jobBatchSystemIDToIssuedJob)

    def getJobStoreID(self, jobBatchSystemID):
        """
        Gets the jobStoreID associated the a given id
        """
        return self.jobBatchSystemIDToIssuedJob[jobBatchSystemID].jobStoreID

    def hasJob(self, jobBatchSystemID):
        """
        Returns true if the jobBatchSystemID is in the list of jobs.
        """
        return self.jobBatchSystemIDToIssuedJob.has_key(jobBatchSystemID)

    def getIssuedJobStoreIDs(self):
        """
        Gets the set of jobStoreIDs of jobs currently issued.
        """
        return self.jobBatchSystemIDToIssuedJob.keys()

    def removeJob(self, jobBatchSystemID):
        """
        Removes a job from the jobBatcher.
        """
        issuedJob = self.jobBatchSystemIDToIssuedJob.pop(jobBatchSystemID)
        return issuedJob

    def killJobs(self, jobsToKill):
        """
        Kills the given set of jobs and then sends them for processing
        """
        if len(jobsToKill) > 0:
            self.batchSystem.killBatchJobs(jobsToKill)
            for jobBatchSystemID in jobsToKill:
                self.processFinishedJob(jobBatchSystemID, 1)

    #Following functions handle error cases for when jobs have gone awry with the batch system.

    def reissueOverLongJobs(self):
        """
        Check each issued job - if it is running for longer than desirable
        issue a kill instruction.
        Wait for the job to die then we pass the job to processFinishedJob.
        """
        maxJobDuration = self.config.maxJobDuration
        jobsToKill = []
        if maxJobDuration < 10000000:  # We won't bother doing anything if the rescue
            # time is more than 16 weeks.
            runningJobs = self.batchSystem.getRunningBatchJobIDs()
            for jobBatchSystemID in runningJobs.keys():
                if runningJobs[jobBatchSystemID] > maxJobDuration:
                    logger.warn("The job: %s has been running for: %s seconds, more than the "
                                "max job duration: %s, we'll kill it",
                                str(self.getJobStoreID(jobBatchSystemID)),
                                str(runningJobs[jobBatchSystemID]),
                                str(maxJobDuration))
                    jobsToKill.append(jobBatchSystemID)
            self.killJobs(jobsToKill)

    def reissueMissingJobs(self, killAfterNTimesMissing=3):
        """
        Check all the current job ids are in the list of currently running batch system jobs.
        If a job is missing, we mark it as so, if it is missing for a number of runs of
        this function (say 10).. then we try deleting the job (though its probably lost), we wait
        then we pass the job to processFinishedJob.
        """
        runningJobs = set(self.batchSystem.getIssuedBatchJobIDs())
        jobBatchSystemIDsSet = set(self.getIssuedJobStoreIDs())
        #Clean up the reissueMissingJobs_missingHash hash, getting rid of jobs that have turned up
        missingJobIDsSet = set(self.reissueMissingJobs_missingHash.keys())
        for jobBatchSystemID in missingJobIDsSet.difference(jobBatchSystemIDsSet):
            self.reissueMissingJobs_missingHash.pop(jobBatchSystemID)
            logger.warn("Batch system id: %s is no longer missing", str(jobBatchSystemID))
        assert runningJobs.issubset(jobBatchSystemIDsSet) #Assert checks we have
        #no unexpected jobs running
        jobsToKill = []
        for jobBatchSystemID in set(jobBatchSystemIDsSet.difference(runningJobs)):
            jobStoreID = self.getJobStoreID(jobBatchSystemID)
            if self.reissueMissingJobs_missingHash.has_key(jobBatchSystemID):
                self.reissueMissingJobs_missingHash[jobBatchSystemID] = \
                self.reissueMissingJobs_missingHash[jobBatchSystemID]+1
            else:
                self.reissueMissingJobs_missingHash[jobBatchSystemID] = 1
            timesMissing = self.reissueMissingJobs_missingHash[jobBatchSystemID]
            logger.warn("Job store ID %s with batch system id %s is missing for the %i time",
                        jobStoreID, str(jobBatchSystemID), timesMissing)
            if timesMissing == killAfterNTimesMissing:
                self.reissueMissingJobs_missingHash.pop(jobBatchSystemID)
                jobsToKill.append(jobBatchSystemID)
        self.killJobs(jobsToKill)
        return len( self.reissueMissingJobs_missingHash ) == 0 #We use this to inform
        #if there are missing jobs

    def processFinishedJob(self, jobBatchSystemID, resultStatus):
        """
        Function reads a processed jobWrapper file and updates it state.
        """
        jobStoreID = self.removeJob(jobBatchSystemID).jobStoreID
        if self.jobStore.exists(jobStoreID):
            jobWrapper = self.jobStore.load(jobStoreID)
            if jobWrapper.logJobStoreFileID is not None:
                logger.warn("The jobWrapper seems to have left a log file, indicating failure: %s", jobStoreID)
                with jobWrapper.getLogFileHandle( self.jobStore ) as logFileStream:
                    logStream( logFileStream, jobStoreID, logger.warn )
            if resultStatus != 0:
                if jobWrapper.logJobStoreFileID is None:
                    logger.warn("No log file is present, despite jobWrapper failing: %s", jobStoreID)
                jobWrapper.setupJobAfterFailure(self.config)
            self.toilState.updatedJobs.add((jobWrapper, resultStatus)) #Now we know the
            #jobWrapper is done we can add it to the list of updated jobWrapper files
            logger.debug("Added jobWrapper: %s to active jobs", jobStoreID)
        else:  #The jobWrapper is done
            if resultStatus != 0:
                logger.warn("Despite the batch system claiming failure the "
                            "jobWrapper %s seems to have finished and been removed", jobStoreID)
            self._updatePredecessorStatus(jobStoreID)

    def _updatePredecessorStatus(self, jobStoreID):
        """
        Update status of a predecessor for finished successor job.
        """
        if jobStoreID not in self.toilState.successorJobStoreIDToPredecessorJobs:
            #We have reach the root job
            assert len(self.toilState.updatedJobs) == 0
            assert len(self.toilState.successorJobStoreIDToPredecessorJobs) == 0
            assert len(self.toilState.successorCounts) == 0
            return
        for predecessorJob in self.toilState.successorJobStoreIDToPredecessorJobs.pop(jobStoreID):
            self.toilState.successorCounts[predecessorJob] -= 1
            assert self.toilState.successorCounts[predecessorJob] >= 0
            if self.toilState.successorCounts[predecessorJob] == 0: #Job is done
                self.toilState.successorCounts.pop(predecessorJob)
                logger.debug("Job %s has all its successors run successfully", \
                             predecessorJob.jobStoreID)
                assert predecessorJob not in self.toilState.updatedJobs
                self.toilState.updatedJobs.add((predecessorJob, 0)) #Now we know
                #the job is done we can add it to the list of updated job files

# Copyright (C) 2015 UCSC Computational Genomics Lab
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import nacl
from nacl.secret import SecretBox


# 16-byte MAC plus a nonce is added to every message.
overhead = 16 + SecretBox.NONCE_SIZE

def encrypt(message, keyPath):
    """
    Encrypts a message given a path to a local file containing a key.

    :param message: The message to be encrypted.
    :param keyPath: A path to a file containing a 256-bit key (and nothing else).
    :type message: str
    :type keyPath: str
    :rtype: str

    A constant overhead is added to every encrypted message (for the nonce and MAC).
    >>> import tempfile
    >>> k = tempfile.mktemp()
    >>> with open(k, 'w') as f:
    ...     f.write(nacl.utils.random(SecretBox.KEY_SIZE))
    >>> message = 'test'
    >>> len(encrypt(message, k)) == overhead + len(message)
    True
    """
    with open(keyPath) as f:
        key = f.read()
    if len(key) != SecretBox.KEY_SIZE:
        raise ValueError("Key is %d bytes, but must be exactly %d bytes" % (len(key),
                                                                            SecretBox.KEY_SIZE))
    sb = SecretBox(key)
    # We generate the nonce using secure random bits. For long enough
    # nonce size, the chance of a random nonce collision becomes
    # *much* smaller than the chance of a subtle coding error causing
    # a nonce reuse. Currently the nonce size is 192 bits--the chance
    # of a collision is astronomically low. (This approach is
    # recommended in the libsodium documentation.)
    nonce = nacl.utils.random(SecretBox.NONCE_SIZE)
    assert len(nonce) == SecretBox.NONCE_SIZE
    return str(sb.encrypt(message, nonce))

def decrypt(ciphertext, keyPath):
    """
    Decrypts a given message that was encrypted with the encrypt() method.

    :param ciphertext: The encrypted message (as a string).
    :param keyPath: A path to a file containing a 256-bit key (and nothing else).
    :type keyPath: str
    :rtype: str

    Raises an error if ciphertext was modified
    >>> import tempfile
    >>> k = tempfile.mktemp()
    >>> with open(k, 'w') as f:
    ...     f.write(nacl.utils.random(SecretBox.KEY_SIZE))
    >>> ciphertext = encrypt("testMessage", k)
    >>> ciphertext = chr(ord(ciphertext[0]) ^ 1) + ciphertext[1:]
    >>> decrypt(ciphertext, k)
    Traceback (most recent call last):
    ...
    CryptoError: Decryption failed. Ciphertext failed verification

    Otherwise works correctly
    >>> decrypt(encrypt("testMessage", k), k)
    'testMessage'
    """
    with open(keyPath) as f:
        key = f.read()
    if len(key) != SecretBox.KEY_SIZE:
        raise ValueError("Key is %d bytes, but must be exactly %d bytes" % (len(key),
                                                                            SecretBox.KEY_SIZE))
    sb = SecretBox(key)
    # The nonce is kept with the message.
    return sb.decrypt(ciphertext)

# Copyright (C) 2015 UCSC Computational Genomics Lab
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from __future__ import absolute_import, print_function
import os
import shutil
from toil.job import Job
from toil.leader import FailedJobsException
from toil.test import ToilTest

class CleanWorkDirTest(ToilTest):
    """
    Tests testing the Job.FileStore class
    """
    def setUp(self):
        super(CleanWorkDirTest, self).setUp()
        self.testDir = self._createTempDir()

    def tearDown(self):
        super(CleanWorkDirTest, self).tearDown()
        shutil.rmtree(self.testDir)

    def testNever(self):
        retainedTempData = self._runAndReturnWorkDir("never", job=tempFileTestJob)
        self.assertNotEqual(retainedTempData, [], "The worker's temporary workspace was deleted despite "
                                                  "cleanWorkDir being set to 'never'")

    def testAlways(self):
        retainedTempData = self._runAndReturnWorkDir("always", job=tempFileTestJob)
        self.assertEqual(retainedTempData, [], "The worker's temporary workspace was not deleted despite "
                                               "cleanWorkDir being set to 'always'")

    def testOnErrorWithError(self):
        retainedTempData = self._runAndReturnWorkDir("onError", job=tempFileTestErrorJob, expectError=True)
        self.assertEqual(retainedTempData, [], "The worker's temporary workspace was not deleted despite "
                                               "an error occurring and cleanWorkDir being set to 'onError'")

    def testOnErrorWithNoError(self):
        retainedTempData = self._runAndReturnWorkDir("onError", job=tempFileTestJob)
        self.assertNotEqual(retainedTempData, [], "The worker's temporary workspace was deleted despite "
                                                  "no error occurring and cleanWorkDir being set to 'onError'")

    def testOnSuccessWithError(self):
        retainedTempData = self._runAndReturnWorkDir("onSuccess", job=tempFileTestErrorJob, expectError=True)
        self.assertNotEqual(retainedTempData, [], "The worker's temporary workspace was deleted despite "
                                                  "an error occurring and cleanWorkDir being set to 'onSuccesss'")

    def testOnSuccessWithSuccess(self):
        retainedTempData = self._runAndReturnWorkDir("onSuccess", job=tempFileTestJob)
        self.assertEqual(retainedTempData, [], "The worker's temporary workspace was not deleted despite "
                                               "a successful job execution and cleanWorkDir being set to 'onSuccesss'")

    def _runAndReturnWorkDir(self, cleanWorkDir, job, expectError=False):
        """
        Runs toil with the specified job and cleanWorkDir setting. expectError determines whether the test's toil
        run is expected to succeed, and the test will fail if that expectation is not met. returns the contents of
        the workDir after completion of the run
        """
        options = Job.Runner.getDefaultOptions(self._getTestJobStorePath())
        options.workDir = self.testDir
        options.clean = "always"
        options.cleanWorkDir = cleanWorkDir
        A = Job.wrapJobFn(job)
        if expectError:
            self._launchError(A, options)
        else:
            self._launchRegular(A, options)
        return os.listdir(self.testDir)

    def _launchRegular(self, A, options):
        Job.Runner.startToil(A, options)

    def _launchError(self, A, options):
        try:
            Job.Runner.startToil(A, options)
        except FailedJobsException:
            pass  # we expect a job to fail here
        else:
            self.fail("Toil run succeeded unexpectedly")

def tempFileTestJob(job):
    with open(job.fileStore.getLocalTempFile(), "w") as f:
        f.write("test file retention")

def tempFileTestErrorJob(job):
    with open(job.fileStore.getLocalTempFile(), "w") as f:
        f.write("test file retention")
    raise RuntimeError()  # test failure

# -*- coding: utf-8 -*-

"""Module that contains projection operators."""

from __future__ import absolute_import

import numpy as np

from .mathadapt import sqrt


def max_length_columns(arr, max_length):
    """Project the columns of an array below a certain length.

    Works in place.

    Parameters
    ----------

    arr : array_like
        2D array.

    max_length : int
        Maximum length of a column.
    """
    if arr.ndim != 2:
        raise ValueError('only 2d arrays allowed')

    max_length = float(max_length)

    lengths = sqrt((arr ** 2).sum(axis=0))
    too_big_by = lengths / max_length
    divisor = too_big_by
    non_violated = lengths < max_length

    if isinstance(arr, np.ndarray):
        divisor[np.where(non_violated)] = 1.
    else:
        # Gnumpy implementation.
        # TODO: can this be done more efficiently?
        for i, nv in enumerate(non_violated):
            if nv:
                divisor[i] = 1.

    arr /= divisor[np.newaxis]

# -*- coding: utf-8 -*-

from __future__ import absolute_import, print_function

import itertools

from climin import RmsProp

from .losses import LogisticRegression
from .common import continuation


def test_rmsprop_lr():
    obj = LogisticRegression()
    args = itertools.repeat(((obj.X, obj.Z), {}))
    opt = RmsProp(obj.pars, obj.fprime, 0.01, 0.9, args=args)
    for i, info in enumerate(opt):
        print(obj.f(opt.wrt, obj.X, obj.Z))
        if i > 3000:
            break
    assert obj.solved(0.15), 'did not find solution'


def test_rmsprop_continue():
    obj = LogisticRegression(n_inpt=2, n_classes=2)
    args = itertools.repeat(((obj.X, obj.Z), {}))
    opt = RmsProp(
        obj.pars, obj.fprime, step_rate=0.01, momentum=.9, decay=0.9,
        args=args)

    continuation(opt)

# Copyright 2014 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import re

from nova import exception
from nova.i18n import _

# Define the minimum and maximum version of the API across all of the
# REST API. The format of the version is:
# X.Y where:
#
# - X will only be changed if a significant backwards incompatible API
# change is made which affects the API as whole. That is, something
# that is only very very rarely incremented.
#
# - Y when you make any change to the API. Note that this includes
# semantic changes which may not affect the input or output formats or
# even originate in the API code layer. We are not distinguishing
# between backwards compatible and backwards incompatible changes in
# the versioning system. It must be made clear in the documentation as
# to what is a backwards compatible change and what is a backwards
# incompatible one.

#
# You must update the API version history string below with a one or
# two line description as well as update rest_api_version_history.rst
REST_API_VERSION_HISTORY = """REST API Version History:

    * 2.1 - Initial version. Equivalent to v2.0 code
    * 2.2 - Adds (keypair) type parameter for os-keypairs plugin
            Fixes success status code for create/delete a keypair method
    * 2.3 - Exposes additional os-extended-server-attributes
            Exposes delete_on_termination for os-extended-volumes
    * 2.4 - Exposes reserved field in os-fixed-ips.
    * 2.5 - Allow server search option ip6 for non-admin
    * 2.6 - Consolidate the APIs for getting remote consoles
    * 2.7 - Check flavor type before add tenant access.
    * 2.8 - Add new protocol for VM console (mks)
    * 2.9 - Exposes lock information in server details.
    * 2.10 - Allow admins to query, create and delete keypairs owned by any
             user.
    * 2.11 - Exposes forced_down attribute for os-services
    * 2.12 - Exposes VIF net_id in os-virtual-interfaces
    * 2.13 - Add project id and user id information for os-server-groups API
    * 2.14 - Remove onSharedStorage from evacuate request body and remove
             adminPass from the response body
    * 2.15 - Add soft-affinity and soft-anti-affinity policies
    * 2.16 - Exposes host_status for servers/detail and servers/{server_id}
    * 2.17 - Add trigger_crash_dump to server actions
    * 2.18 - Makes project_id optional in v2.1
    * 2.19 - Allow user to set and get the server description
    * 2.20 - Add attach and detach volume operations for instances in shelved
             and shelved_offloaded state
    * 2.21 - Make os-instance-actions read deleted instances
    * 2.22 - Add API to force live migration to complete
    * 2.23 - Add index/show API for server migrations.
             Also add migration_type for /os-migrations and add ref link for it
             when the migration is an in progress live migration.
    * 2.24 - Add API to cancel a running live migration
    * 2.25 - Make block_migration support 'auto' and remove
             disk_over_commit for os-migrateLive.

"""

# The minimum and maximum versions of the API supported
# The default api version request is defined to be the
# the minimum version of the API supported.
# Note(cyeoh): This only applies for the v2.1 API once microversions
# support is fully merged. It does not affect the V2 API.
_MIN_API_VERSION = "2.1"
_MAX_API_VERSION = "2.25"
DEFAULT_API_VERSION = _MIN_API_VERSION


# NOTE(cyeoh): min and max versions declared as functions so we can
# mock them for unittests. Do not use the constants directly anywhere
# else.
def min_api_version():
    return APIVersionRequest(_MIN_API_VERSION)


def max_api_version():
    return APIVersionRequest(_MAX_API_VERSION)


def is_supported(req, min_version=_MIN_API_VERSION,
                 max_version=_MAX_API_VERSION):
    """Check if API request version satisfies version restrictions.

    :param req: request object
    :param min_version: minimal version of API needed for correct
           request processing
    :param max_version: maximum version of API needed for correct
           request processing

    :returns True if request satisfies minimal and maximum API version
             requirements. False in other case.
    """

    return (APIVersionRequest(max_version) >= req.api_version_request >=
            APIVersionRequest(min_version))


class APIVersionRequest(object):
    """This class represents an API Version Request with convenience
    methods for manipulation and comparison of version
    numbers that we need to do to implement microversions.
    """

    def __init__(self, version_string=None):
        """Create an API version request object.

        :param version_string: String representation of APIVersionRequest.
            Correct format is 'X.Y', where 'X' and 'Y' are int values.
            None value should be used to create Null APIVersionRequest,
            which is equal to 0.0
        """
        self.ver_major = 0
        self.ver_minor = 0

        if version_string is not None:
            match = re.match(r"^([1-9]\d*)\.([1-9]\d*|0)$",
                             version_string)
            if match:
                self.ver_major = int(match.group(1))
                self.ver_minor = int(match.group(2))
            else:
                raise exception.InvalidAPIVersionString(version=version_string)

    def __str__(self):
        """Debug/Logging representation of object."""
        return ("API Version Request Major: %s, Minor: %s"
                % (self.ver_major, self.ver_minor))

    def is_null(self):
        return self.ver_major == 0 and self.ver_minor == 0

    def _format_type_error(self, other):
        return TypeError(_("'%(other)s' should be an instance of '%(cls)s'") %
                         {"other": other, "cls": self.__class__})

    def __lt__(self, other):
        if not isinstance(other, APIVersionRequest):
            raise self._format_type_error(other)

        return ((self.ver_major, self.ver_minor) <
                (other.ver_major, other.ver_minor))

    def __eq__(self, other):
        if not isinstance(other, APIVersionRequest):
            raise self._format_type_error(other)

        return ((self.ver_major, self.ver_minor) ==
                (other.ver_major, other.ver_minor))

    def __gt__(self, other):
        if not isinstance(other, APIVersionRequest):
            raise self._format_type_error(other)

        return ((self.ver_major, self.ver_minor) >
                (other.ver_major, other.ver_minor))

    def __le__(self, other):
        return self < other or self == other

    def __ne__(self, other):
        return not self.__eq__(other)

    def __ge__(self, other):
        return self > other or self == other

    def matches(self, min_version, max_version):
        """Returns whether the version object represents a version
        greater than or equal to the minimum version and less than
        or equal to the maximum version.

        @param min_version: Minimum acceptable version.
        @param max_version: Maximum acceptable version.
        @returns: boolean

        If min_version is null then there is no minimum limit.
        If max_version is null then there is no maximum limit.
        If self is null then raise ValueError
        """

        if self.is_null():
            raise ValueError
        if max_version.is_null() and min_version.is_null():
            return True
        elif max_version.is_null():
            return min_version <= self
        elif min_version.is_null():
            return self <= max_version
        else:
            return min_version <= self <= max_version

    def get_string(self):
        """Converts object to string representation which if used to create
        an APIVersionRequest object results in the same version request.
        """
        if self.is_null():
            raise ValueError
        return "%s.%s" % (self.ver_major, self.ver_minor)

#   Copyright 2011 OpenStack Foundation
#
#   Licensed under the Apache License, Version 2.0 (the "License"); you may
#   not use this file except in compliance with the License. You may obtain
#   a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#   WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#   License for the specific language governing permissions and limitations
#   under the License.

"""The Extended Status Admin API extension."""

from nova.api.openstack import extensions
from nova.api.openstack import wsgi

ALIAS = "os-extended-status"
authorize = extensions.os_compute_soft_authorizer(ALIAS)


class ExtendedStatusController(wsgi.Controller):
    def __init__(self, *args, **kwargs):
        super(ExtendedStatusController, self).__init__(*args, **kwargs)

    def _extend_server(self, server, instance):
        # Note(gmann): Removed 'locked_by' from extended status
        # to make it same as V2. If needed it can be added with
        # microversion.
        for state in ['task_state', 'vm_state', 'power_state']:
            # NOTE(mriedem): The OS-EXT-STS prefix should not be used for new
            # attributes after v2.1. They are only in v2.1 for backward compat
            # with v2.0.
            key = "%s:%s" % ('OS-EXT-STS', state)
            server[key] = instance[state]

    @wsgi.extends
    def show(self, req, resp_obj, id):
        context = req.environ['nova.context']
        if authorize(context):
            server = resp_obj.obj['server']
            db_instance = req.get_db_instance(server['id'])
            # server['id'] is guaranteed to be in the cache due to
            # the core API adding it in its 'show' method.
            self._extend_server(server, db_instance)

    @wsgi.extends
    def detail(self, req, resp_obj):
        context = req.environ['nova.context']
        if authorize(context):
            servers = list(resp_obj.obj['servers'])
            for server in servers:
                db_instance = req.get_db_instance(server['id'])
                # server['id'] is guaranteed to be in the cache due to
                # the core API adding it in its 'detail' method.
                self._extend_server(server, db_instance)


class ExtendedStatus(extensions.V21APIExtensionBase):
    """Extended Status support."""

    name = "ExtendedStatus"
    alias = ALIAS
    version = 1

    def get_controller_extensions(self):
        controller = ExtendedStatusController()
        extension = extensions.ControllerExtension(self, 'servers', controller)
        return [extension]

    def get_resources(self):
        return []

# Copyright (c) 2012 Citrix Systems, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""The Aggregate admin API extension."""

import datetime

import six
from webob import exc

from nova.api.openstack import extensions
from nova.compute import api as compute_api
from nova import context as nova_context
from nova import exception
from nova.i18n import _
from nova import utils

authorize = extensions.extension_authorizer('compute', 'aggregates')


def _get_context(req):
    return req.environ['nova.context']


def get_host_from_body(fn):
    """Makes sure that the host exists."""
    def wrapped(self, req, id, body, *args, **kwargs):
        if len(body) != 1:
            msg = _('Only host parameter can be specified')
            raise exc.HTTPBadRequest(explanation=msg)
        elif 'host' not in body:
            msg = _('Host parameter must be specified')
            raise exc.HTTPBadRequest(explanation=msg)
        try:
            utils.check_string_length(body['host'], 'host', 1, 255)
        except exception.InvalidInput as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        host = body['host']

        return fn(self, req, id, host, *args, **kwargs)
    return wrapped


class AggregateController(object):
    """The Host Aggregates API controller for the OpenStack API."""
    def __init__(self):
        self.api = compute_api.AggregateAPI()

    def index(self, req):
        """Returns a list a host aggregate's id, name, availability_zone."""
        context = _get_context(req)
        authorize(context)
        aggregates = self.api.get_aggregate_list(context)
        return {'aggregates': [self._marshall_aggregate(a)['aggregate']
                               for a in aggregates]}

    def create(self, req, body):
        """Creates an aggregate, given its name and
        optional availability zone.
        """
        context = _get_context(req)
        authorize(context)

        if len(body) != 1:
            raise exc.HTTPBadRequest()
        try:
            host_aggregate = body["aggregate"]
            name = host_aggregate["name"]
        except KeyError:
            raise exc.HTTPBadRequest()
        avail_zone = host_aggregate.get("availability_zone")
        try:
            utils.check_string_length(name, "Aggregate name", 1, 255)
            if avail_zone is not None:
                utils.check_string_length(avail_zone, "Availability_zone", 1,
                                          255)
        except exception.InvalidInput as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        try:
            aggregate = self.api.create_aggregate(context, name, avail_zone)
        except exception.AggregateNameExists as e:
            raise exc.HTTPConflict(explanation=e.format_message())
        except exception.InvalidAggregateAction as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        agg = self._marshall_aggregate(aggregate)

        # To maintain the same API result as before the changes for returning
        # nova objects were made.
        del agg['aggregate']['hosts']
        del agg['aggregate']['metadata']

        return agg

    def show(self, req, id):
        """Shows the details of an aggregate, hosts and metadata included."""
        context = _get_context(req)
        authorize(context)
        try:
            aggregate = self.api.get_aggregate(context, id)
        except exception.AggregateNotFound as e:
            raise exc.HTTPNotFound(explanation=e.format_message())
        return self._marshall_aggregate(aggregate)

    def update(self, req, id, body):
        """Updates the name and/or availability_zone of given aggregate."""
        context = _get_context(req)
        authorize(context)

        if len(body) != 1:
            raise exc.HTTPBadRequest()
        try:
            updates = body["aggregate"]
        except KeyError:
            raise exc.HTTPBadRequest()

        if len(updates) < 1:
            raise exc.HTTPBadRequest()

        for key in updates.keys():
            if key not in ["name", "availability_zone"]:
                raise exc.HTTPBadRequest()

        try:
            if 'name' in updates:
                utils.check_string_length(updates['name'], "Aggregate name", 1,
                                          255)
            if updates.get("availability_zone") is not None:
                utils.check_string_length(updates['availability_zone'],
                                          "Availability_zone", 1, 255)
        except exception.InvalidInput as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        try:
            aggregate = self.api.update_aggregate(context, id, updates)
        except exception.AggregateNameExists as e:
            raise exc.HTTPConflict(explanation=e.format_message())
        except exception.AggregateNotFound as e:
            raise exc.HTTPNotFound(explanation=e.format_message())
        except exception.InvalidAggregateAction as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        return self._marshall_aggregate(aggregate)

    def delete(self, req, id):
        """Removes an aggregate by id."""
        context = _get_context(req)
        authorize(context)
        try:
            self.api.delete_aggregate(context, id)
        except exception.AggregateNotFound as e:
            raise exc.HTTPNotFound(explanation=e.format_message())
        except exception.InvalidAggregateAction as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

    def action(self, req, id, body):
        _actions = {
            'add_host': self._add_host,
            'remove_host': self._remove_host,
            'set_metadata': self._set_metadata,
        }
        for action, data in six.iteritems(body):
            if action not in _actions.keys():
                msg = _('Aggregates does not have %s action') % action
                raise exc.HTTPBadRequest(explanation=msg)
            return _actions[action](req, id, data)

        raise exc.HTTPBadRequest(explanation=_("Invalid request body"))

    @get_host_from_body
    def _add_host(self, req, id, host):
        """Adds a host to the specified aggregate."""
        context = _get_context(req)
        authorize(context)

        # NOTE(alex_xu): back-compatible with db layer hard-code admin
        # permission checks. This has to be left only for API v2.0 because
        # this version has to be stable even if it means that only admins
        # can call this method while the policy could be changed.
        nova_context.require_admin_context(context)

        try:
            aggregate = self.api.add_host_to_aggregate(context, id, host)
        except (exception.AggregateNotFound, exception.ComputeHostNotFound):
            msg = _('Cannot add host %(host)s in aggregate'
                    ' %(id)s: not found') % {'host': host, 'id': id}
            raise exc.HTTPNotFound(explanation=msg)
        except (exception.AggregateHostExists,
                exception.InvalidAggregateAction):
            msg = _('Cannot add host %(host)s in aggregate'
                    ' %(id)s: host exists') % {'host': host, 'id': id}
            raise exc.HTTPConflict(explanation=msg)
        return self._marshall_aggregate(aggregate)

    @get_host_from_body
    def _remove_host(self, req, id, host):
        """Removes a host from the specified aggregate."""
        context = _get_context(req)
        authorize(context)

        # NOTE(alex_xu): back-compatible with db layer hard-code admin
        # permission checks. This has to be left only for API v2.0 because
        # this version has to be stable even if it means that only admins
        # can call this method while the policy could be changed.
        nova_context.require_admin_context(context)

        try:
            aggregate = self.api.remove_host_from_aggregate(context, id, host)
        except (exception.AggregateNotFound, exception.AggregateHostNotFound,
                exception.ComputeHostNotFound):
            msg = _('Cannot remove host %(host)s in aggregate'
                    ' %(id)s: not found') % {'host': host, 'id': id}
            raise exc.HTTPNotFound(explanation=msg)
        except exception.InvalidAggregateAction:
            msg = _('Cannot remove host %(host)s in aggregate'
                    ' %(id)s: invalid') % {'host': host, 'id': id}
            raise exc.HTTPConflict(explanation=msg)
        return self._marshall_aggregate(aggregate)

    def _set_metadata(self, req, id, body):
        """Replaces the aggregate's existing metadata with new metadata."""
        context = _get_context(req)
        authorize(context)

        if len(body) != 1:
            raise exc.HTTPBadRequest()
        try:
            metadata = body["metadata"]
        except KeyError:
            raise exc.HTTPBadRequest()

        # The metadata should be a dict
        if not isinstance(metadata, dict):
            msg = _('The value of metadata must be a dict')
            raise exc.HTTPBadRequest(explanation=msg)
        try:
            for key, value in metadata.items():
                utils.check_string_length(key, "metadata.key", 1, 255)
                if value is not None:
                    utils.check_string_length(value, "metadata.value", 0, 255)
        except exception.InvalidInput as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())
        try:
            aggregate = self.api.update_aggregate_metadata(context,
                                                           id, metadata)
        except exception.AggregateNotFound:
            msg = _('Cannot set metadata %(metadata)s in aggregate'
                    ' %(id)s') % {'metadata': metadata, 'id': id}
            raise exc.HTTPNotFound(explanation=msg)
        except exception.InvalidAggregateAction as e:
            raise exc.HTTPBadRequest(explanation=e.format_message())

        return self._marshall_aggregate(aggregate)

    def _marshall_aggregate(self, aggregate):
        _aggregate = {}
        for key, value in self._build_aggregate_items(aggregate):
            # NOTE(danms): The original API specified non-TZ-aware timestamps
            if isinstance(value, datetime.datetime):
                value = value.replace(tzinfo=None)
            _aggregate[key] = value
        return {"aggregate": _aggregate}

    def _build_aggregate_items(self, aggregate):
        # NOTE(rlrossit): Within the compute API, metadata will always be
        # set on the aggregate object (at a minimum to {}). Because of this,
        # we can freely use getattr() on keys in obj_extra_fields (in this
        # case it is only ['availability_zone']) without worrying about
        # lazy-loading an unset variable
        keys = aggregate.obj_fields
        for key in keys:
            # NOTE(danms): Skip the uuid field because we have no microversion
            # to expose it
            if ((aggregate.obj_attr_is_set(key)
                    or key in aggregate.obj_extra_fields) and
                  key != 'uuid'):
                yield key, getattr(aggregate, key)


class Aggregates(extensions.ExtensionDescriptor):
    """Admin-only aggregate administration."""

    name = "Aggregates"
    alias = "os-aggregates"
    namespace = "http://docs.openstack.org/compute/ext/aggregates/api/v1.1"
    updated = "2012-01-12T00:00:00Z"

    def get_resources(self):
        resources = []
        res = extensions.ResourceExtension('os-aggregates',
                AggregateController(),
                member_actions={"action": "POST", })
        resources.append(res)
        return resources

# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.api.openstack import extensions as exts


class Extended_rescue_with_image(exts.ExtensionDescriptor):
    """Allow the user to specify the image to use for rescue."""

    name = "ExtendedRescueWithImage"
    alias = "os-extended-rescue-with-image"
    namespace = ("http://docs.openstack.org/compute/ext/"
                 "extended_rescue_with_image/api/v2")
    updated = "2014-01-04T00:00:00Z"

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.api.openstack import extensions
from nova import compute
from nova import context as nova_context
from nova.objects import base as obj_base


XMLNS = "http://docs.openstack.org/compute/ext/migrations/api/v2.0"
ALIAS = "os-migrations"


def authorize(context, action_name):
    action = 'migrations:%s' % action_name
    extensions.extension_authorizer('compute', action)(context)


def output(migrations_obj):
    """Returns the desired output of the API from an object.

    From a MigrationsList's object this method returns a list of
    primitive objects with the only necessary fields.
    """
    detail_keys = ['memory_total', 'memory_processed', 'memory_remaining',
                   'disk_total', 'disk_processed', 'disk_remaining']
    # Note(Shaohe Feng): We need to leverage the oslo.versionedobjects.
    # Then we can pass the target version to it's obj_to_primitive.
    objects = obj_base.obj_to_primitive(migrations_obj)
    objects = [x for x in objects if not x['hidden']]
    for obj in objects:
        del obj['deleted']
        del obj['deleted_at']
        del obj['migration_type']
        del obj['hidden']
        if 'memory_total' in obj:
            for key in detail_keys:
                del obj[key]

    return objects


class MigrationsController(object):
    """Controller for accessing migrations in OpenStack API."""
    def __init__(self):
        self.compute_api = compute.API()

    def index(self, req):
        """Return all migrations in progress."""
        context = req.environ['nova.context']
        authorize(context, "index")
        # NOTE(alex_xu): back-compatible with db layer hard-code admin
        # permission checks.
        nova_context.require_admin_context(context)
        migrations = self.compute_api.get_migrations(context, req.GET)
        return {'migrations': output(migrations)}


class Migrations(extensions.ExtensionDescriptor):
    """Provide data on migrations."""
    name = "Migrations"
    alias = ALIAS
    namespace = XMLNS
    updated = "2013-05-30T00:00:00Z"

    def get_resources(self):
        resources = []
        resource = extensions.ResourceExtension('os-migrations',
                                                MigrationsController())
        resources.append(resource)
        return resources

# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.api.openstack import extensions


class User_quotas(extensions.ExtensionDescriptor):
    """Project user quota support."""

    name = "UserQuotas"
    alias = "os-user-quotas"
    namespace = ("http://docs.openstack.org/compute/ext/user_quotas"
                 "/api/v1.1")
    updated = "2013-07-18T00:00:00Z"

#   Copyright 2011 OpenStack Foundation
#
#   Licensed under the Apache License, Version 2.0 (the "License"); you may
#   not use this file except in compliance with the License. You may obtain
#   a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#   WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#   License for the specific language governing permissions and limitations
#   under the License.

"""The rescue mode extension."""

from oslo_config import cfg
from webob import exc

from nova.api.openstack import common
from nova.api.openstack.compute.schemas import rescue
from nova.api.openstack import extensions
from nova.api.openstack import wsgi
from nova.api import validation
from nova import compute
from nova import exception
from nova import utils


ALIAS = "os-rescue"
CONF = cfg.CONF
CONF.import_opt('enable_instance_password',
                'nova.api.openstack.compute.legacy_v2.servers')

authorize = extensions.os_compute_authorizer(ALIAS)


class RescueController(wsgi.Controller):
    def __init__(self, *args, **kwargs):
        super(RescueController, self).__init__(*args, **kwargs)
        self.compute_api = compute.API(skip_policy_check=True)

    # TODO(cyeoh): Should be responding here with 202 Accept
    # because rescue is an async call, but keep to 200
    # for backwards compatibility reasons.
    @extensions.expected_errors((400, 404, 409, 501))
    @wsgi.action('rescue')
    @validation.schema(rescue.rescue)
    def _rescue(self, req, id, body):
        """Rescue an instance."""
        context = req.environ["nova.context"]
        authorize(context)

        if body['rescue'] and 'adminPass' in body['rescue']:
            password = body['rescue']['adminPass']
        else:
            password = utils.generate_password()

        instance = common.get_instance(self.compute_api, context, id)
        rescue_image_ref = None
        if body['rescue'] and 'rescue_image_ref' in body['rescue']:
            rescue_image_ref = body['rescue']['rescue_image_ref']

        try:
            self.compute_api.rescue(context, instance,
                                    rescue_password=password,
                                    rescue_image_ref=rescue_image_ref)
        except exception.InstanceUnknownCell as e:
            raise exc.HTTPNotFound(explanation=e.format_message())
        except exception.InstanceIsLocked as e:
            raise exc.HTTPConflict(explanation=e.format_message())
        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                                                                  'rescue', id)
        except exception.InvalidVolume as volume_error:
            raise exc.HTTPConflict(explanation=volume_error.format_message())
        except exception.InstanceNotRescuable as non_rescuable:
            raise exc.HTTPBadRequest(
                explanation=non_rescuable.format_message())

        if CONF.enable_instance_password:
            return {'adminPass': password}
        else:
            return {}

    @wsgi.response(202)
    @extensions.expected_errors((404, 409, 501))
    @wsgi.action('unrescue')
    def _unrescue(self, req, id, body):
        """Unrescue an instance."""
        context = req.environ["nova.context"]
        authorize(context)
        instance = common.get_instance(self.compute_api, context, id)
        try:
            self.compute_api.unrescue(context, instance)
        except exception.InstanceUnknownCell as e:
            raise exc.HTTPNotFound(explanation=e.format_message())
        except exception.InstanceIsLocked as e:
            raise exc.HTTPConflict(explanation=e.format_message())
        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                                                                  'unrescue',
                                                                  id)


class Rescue(extensions.V21APIExtensionBase):
    """Instance rescue mode."""

    name = "Rescue"
    alias = ALIAS
    version = 1

    def get_resources(self):
        return []

    def get_controller_extensions(self):
        controller = RescueController()
        extension = extensions.ControllerExtension(self, 'servers', controller)
        return [extension]

#  Licensed under the Apache License, Version 2.0 (the "License"); you may
#  not use this file except in compliance with the License. You may obtain
#  a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#  WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#  License for the specific language governing permissions and limitations
#  under the License.

from nova.api.validation import parameter_types


add_fixed_ip = {
    'type': 'object',
    'properties': {
        'addFixedIp': {
            'type': 'object',
            'properties': {
                # The maxLength is from the column 'uuid' of the
                # table 'networks'
                'networkId': {
                    'type': ['string', 'number'],
                    'minLength': 1, 'maxLength': 36,
                },
            },
            'required': ['networkId'],
            'additionalProperties': False,
        },
    },
    'required': ['addFixedIp'],
    'additionalProperties': False,
}


remove_fixed_ip = {
    'type': 'object',
    'properties': {
        'removeFixedIp': {
            'type': 'object',
            'properties': {
                'address': parameter_types.ip_address
            },
            'required': ['address'],
            'additionalProperties': False,
        },
    },
    'required': ['removeFixedIp'],
    'additionalProperties': False,
}

# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import six
from webob import exc

from nova.api.openstack import common
from nova.api.openstack.compute.schemas import server_metadata
from nova.api.openstack import extensions
from nova.api.openstack import wsgi
from nova.api import validation
from nova import compute
from nova import exception
from nova.i18n import _

ALIAS = 'server-metadata'
authorize = extensions.os_compute_authorizer(ALIAS)


class ServerMetadataController(wsgi.Controller):
    """The server metadata API controller for the OpenStack API."""

    def __init__(self):
        self.compute_api = compute.API(skip_policy_check=True)
        super(ServerMetadataController, self).__init__()

    def _get_metadata(self, context, server_id):
        server = common.get_instance(self.compute_api, context, server_id)
        try:
            # NOTE(mikal): get_instanc_metadata sometimes returns
            # InstanceNotFound in unit tests, even though the instance is
            # fetched on the line above. I blame mocking.
            meta = self.compute_api.get_instance_metadata(context, server)
        except exception.InstanceNotFound:
            msg = _('Server does not exist')
            raise exc.HTTPNotFound(explanation=msg)
        meta_dict = {}
        for key, value in six.iteritems(meta):
            meta_dict[key] = value
        return meta_dict

    @extensions.expected_errors(404)
    def index(self, req, server_id):
        """Returns the list of metadata for a given instance."""
        context = req.environ['nova.context']
        authorize(context, action='index')
        return {'metadata': self._get_metadata(context, server_id)}

    @extensions.expected_errors((400, 403, 404, 409, 413))
    # NOTE(gmann): Returns 200 for backwards compatibility but should be 201
    # as this operation complete the creation of metadata.
    @validation.schema(server_metadata.create)
    def create(self, req, server_id, body):
        metadata = body['metadata']
        context = req.environ['nova.context']
        authorize(context, action='create')
        new_metadata = self._update_instance_metadata(context,
                                                      server_id,
                                                      metadata,
                                                      delete=False)

        return {'metadata': new_metadata}

    @extensions.expected_errors((400, 403, 404, 409, 413))
    @validation.schema(server_metadata.update)
    def update(self, req, server_id, id, body):
        context = req.environ['nova.context']
        authorize(context, action='update')
        meta_item = body['meta']
        if id not in meta_item:
            expl = _('Request body and URI mismatch')
            raise exc.HTTPBadRequest(explanation=expl)

        self._update_instance_metadata(context,
                                       server_id,
                                       meta_item,
                                       delete=False)

        return {'meta': meta_item}

    @extensions.expected_errors((400, 403, 404, 409, 413))
    @validation.schema(server_metadata.update_all)
    def update_all(self, req, server_id, body):
        context = req.environ['nova.context']
        authorize(context, action='update_all')
        metadata = body['metadata']
        new_metadata = self._update_instance_metadata(context,
                                                      server_id,
                                                      metadata,
                                                      delete=True)

        return {'metadata': new_metadata}

    def _update_instance_metadata(self, context, server_id, metadata,
                                  delete=False):
        try:
            server = common.get_instance(self.compute_api, context, server_id)
            return self.compute_api.update_instance_metadata(context,
                                                             server,
                                                             metadata,
                                                             delete)

        except exception.InstanceUnknownCell as e:
            raise exc.HTTPNotFound(explanation=e.format_message())

        except exception.QuotaError as error:
            raise exc.HTTPForbidden(explanation=error.format_message())

        except exception.InstanceIsLocked as e:
            raise exc.HTTPConflict(explanation=e.format_message())

        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                    'update metadata', server_id)

    @extensions.expected_errors(404)
    def show(self, req, server_id, id):
        """Return a single metadata item."""
        context = req.environ['nova.context']
        authorize(context, action='show')
        data = self._get_metadata(context, server_id)

        try:
            return {'meta': {id: data[id]}}
        except KeyError:
            msg = _("Metadata item was not found")
            raise exc.HTTPNotFound(explanation=msg)

    @extensions.expected_errors((404, 409))
    @wsgi.response(204)
    def delete(self, req, server_id, id):
        """Deletes an existing metadata."""
        context = req.environ['nova.context']
        authorize(context, action='delete')
        metadata = self._get_metadata(context, server_id)

        if id not in metadata:
            msg = _("Metadata item was not found")
            raise exc.HTTPNotFound(explanation=msg)

        server = common.get_instance(self.compute_api, context, server_id)
        try:
            self.compute_api.delete_instance_metadata(context, server, id)

        except exception.InstanceUnknownCell as e:
            raise exc.HTTPNotFound(explanation=e.format_message())

        except exception.InstanceIsLocked as e:
            raise exc.HTTPConflict(explanation=e.format_message())

        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                    'delete metadata', server_id)


class ServerMetadata(extensions.V21APIExtensionBase):
    """Server Metadata API."""
    name = "ServerMetadata"
    alias = ALIAS
    version = 1

    def get_resources(self):
        parent = {'member_name': 'server',
                  'collection_name': 'servers'}
        resources = [extensions.ResourceExtension('metadata',
                                                  ServerMetadataController(),
                                                  member_name='server_meta',
                                                  parent=parent,
                                                  custom_routes_fn=
                                                  self.server_metadata_map
                                                  )]
        return resources

    def get_controller_extensions(self):
        return []

    def server_metadata_map(self, mapper, wsgi_resource):
        mapper.connect("metadata",
                       "/{project_id}/servers/{server_id}/metadata",
                       controller=wsgi_resource,
                       action='update_all', conditions={"method": ['PUT']})
        # Also connect the non project_id routes
        mapper.connect("metadata",
                       "/servers/{server_id}/metadata",
                       controller=wsgi_resource,
                       action='update_all', conditions={"method": ['PUT']})

# Copyright 2013 NEC Corporation.  All rights reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Request Body validating middleware.

"""

import functools

from nova.api.openstack import api_version_request as api_version
from nova.api.validation import validators


def schema(request_body_schema, min_version=None, max_version=None):
    """Register a schema to validate request body.

    Registered schema will be used for validating request body just before
    API method executing.

    :argument dict request_body_schema: a schema to validate request body

    """

    def add_validator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            min_ver = api_version.APIVersionRequest(min_version)
            max_ver = api_version.APIVersionRequest(max_version)

            # The request object is always the second argument.
            # However numerous unittests pass in the request object
            # via kwargs instead so we handle that as well.
            # TODO(cyeoh): cleanup unittests so we don't have to
            # to do this
            if 'req' in kwargs:
                ver = kwargs['req'].api_version_request
                legacy_v2 = kwargs['req'].is_legacy_v2()
            else:
                ver = args[1].api_version_request
                legacy_v2 = args[1].is_legacy_v2()

            if legacy_v2:
                # NOTE: For v2.0 compatible API, here should work like
                #    client  | schema min_version | schema
                # -----------+--------------------+--------
                #  legacy_v2 | None               | work
                #  legacy_v2 | 2.0                | work
                #  legacy_v2 | 2.1+               | don't
                if min_version is None or min_version == '2.0':
                    schema_validator = validators._SchemaValidator(
                        request_body_schema, legacy_v2)
                    schema_validator.validate(kwargs['body'])
            elif ver.matches(min_ver, max_ver):
                # Only validate against the schema if it lies within
                # the version range specified. Note that if both min
                # and max are not specified the validator will always
                # be run.
                schema_validator = validators._SchemaValidator(
                    request_body_schema, legacy_v2)
                schema_validator.validate(kwargs['body'])

            return func(*args, **kwargs)
        return wrapper

    return add_validator

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
:mod:`nova.cloudpipe` -- VPN Server Management
=====================================================

.. automodule:: nova.cloudpipe
   :platform: Unix
   :synopsis: An OpenVPN server for every nova user.
"""

# Copyright 2014 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Possible results from instance build

Results represent the ultimate result of an attempt to build an instance.

Results describe whether an instance was actually built, failed to build, or
was rescheduled.
"""

ACTIVE = 'active'  # Instance is running
FAILED = 'failed'  # Instance failed to build and was not rescheduled
RESCHEDULED = 'rescheduled'  # Instance failed to build, but was rescheduled

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg
from oslo_log import log as logging
import oslo_messaging as messaging
import six

from nova.compute import power_state
from nova.conductor.tasks import base
from nova import exception
from nova.i18n import _
from nova import objects
from nova.scheduler import utils as scheduler_utils
from nova import utils

LOG = logging.getLogger(__name__)

migrate_opt = cfg.IntOpt('migrate_max_retries',
        default=-1,
        help='Number of times to retry live-migration before failing. '
             'If == -1, try until out of hosts. '
             'If == 0, only try once, no retries.')

CONF = cfg.CONF
CONF.register_opt(migrate_opt)


class LiveMigrationTask(base.TaskBase):
    def __init__(self, context, instance, destination,
                 block_migration, disk_over_commit, migration, compute_rpcapi,
                 servicegroup_api, scheduler_client, request_spec=None):
        super(LiveMigrationTask, self).__init__(context, instance)
        self.destination = destination
        self.block_migration = block_migration
        self.disk_over_commit = disk_over_commit
        self.migration = migration
        self.source = instance.host
        self.migrate_data = None

        self.compute_rpcapi = compute_rpcapi
        self.servicegroup_api = servicegroup_api
        self.scheduler_client = scheduler_client
        self.request_spec = request_spec

    def _execute(self):
        self._check_instance_is_active()
        self._check_host_is_up(self.source)

        if not self.destination:
            self.destination = self._find_destination()
            self.migration.dest_compute = self.destination
            self.migration.save()
        else:
            self._check_requested_destination()

        # TODO(johngarbutt) need to move complexity out of compute manager
        # TODO(johngarbutt) disk_over_commit?
        return self.compute_rpcapi.live_migration(self.context,
                host=self.source,
                instance=self.instance,
                dest=self.destination,
                block_migration=self.block_migration,
                migration=self.migration,
                migrate_data=self.migrate_data)

    def rollback(self):
        # TODO(johngarbutt) need to implement the clean up operation
        # but this will make sense only once we pull in the compute
        # calls, since this class currently makes no state changes,
        # except to call the compute method, that has no matching
        # rollback call right now.
        pass

    def _check_instance_is_active(self):
        if self.instance.power_state not in (power_state.RUNNING,
                                             power_state.PAUSED):
            raise exception.InstanceInvalidState(
                    instance_uuid = self.instance.uuid,
                    attr = 'power_state',
                    state = self.instance.power_state,
                    method = 'live migrate')

    def _check_host_is_up(self, host):
        try:
            service = objects.Service.get_by_compute_host(self.context, host)
        except exception.NotFound:
            raise exception.ComputeServiceUnavailable(host=host)

        if not self.servicegroup_api.service_is_up(service):
            raise exception.ComputeServiceUnavailable(host=host)

    def _check_requested_destination(self):
        self._check_destination_is_not_source()
        self._check_host_is_up(self.destination)
        self._check_destination_has_enough_memory()
        self._check_compatible_with_source_hypervisor(self.destination)
        self._call_livem_checks_on_host(self.destination)

    def _check_destination_is_not_source(self):
        if self.destination == self.source:
            raise exception.UnableToMigrateToSelf(
                    instance_id=self.instance.uuid, host=self.destination)

    def _check_destination_has_enough_memory(self):
        compute = self._get_compute_info(self.destination)
        free_ram_mb = compute.free_ram_mb
        total_ram_mb = compute.memory_mb
        mem_inst = self.instance.memory_mb
        # NOTE(sbauza): Now the ComputeNode object reports an allocation ratio
        # that can be provided by the compute_node if new or by the controller
        ram_ratio = compute.ram_allocation_ratio

        # NOTE(sbauza): Mimic the RAMFilter logic in order to have the same
        # ram validation
        avail = total_ram_mb * ram_ratio - (total_ram_mb - free_ram_mb)
        if not mem_inst or avail <= mem_inst:
            instance_uuid = self.instance.uuid
            dest = self.destination
            reason = _("Unable to migrate %(instance_uuid)s to %(dest)s: "
                       "Lack of memory(host:%(avail)s <= "
                       "instance:%(mem_inst)s)")
            raise exception.MigrationPreCheckError(reason=reason % dict(
                    instance_uuid=instance_uuid, dest=dest, avail=avail,
                    mem_inst=mem_inst))

    def _get_compute_info(self, host):
        return objects.ComputeNode.get_first_node_by_host_for_old_compat(
            self.context, host)

    def _check_compatible_with_source_hypervisor(self, destination):
        source_info = self._get_compute_info(self.source)
        destination_info = self._get_compute_info(destination)

        source_type = source_info.hypervisor_type
        destination_type = destination_info.hypervisor_type
        if source_type != destination_type:
            raise exception.InvalidHypervisorType()

        source_version = source_info.hypervisor_version
        destination_version = destination_info.hypervisor_version
        if source_version > destination_version:
            raise exception.DestinationHypervisorTooOld()

    def _call_livem_checks_on_host(self, destination):
        try:
            self.migrate_data = self.compute_rpcapi.\
                check_can_live_migrate_destination(self.context, self.instance,
                    destination, self.block_migration, self.disk_over_commit)
        except messaging.MessagingTimeout:
            msg = _("Timeout while checking if we can live migrate to host: "
                    "%s") % destination
            raise exception.MigrationPreCheckError(msg)

    def _find_destination(self):
        # TODO(johngarbutt) this retry loop should be shared
        attempted_hosts = [self.source]
        image = utils.get_image_from_system_metadata(
            self.instance.system_metadata)
        filter_properties = {'ignore_hosts': attempted_hosts}
        # TODO(sbauza): Remove that once setup_instance_group() accepts a
        # RequestSpec object
        request_spec = {'instance_properties': {'uuid': self.instance.uuid}}
        scheduler_utils.setup_instance_group(self.context, request_spec,
                                                 filter_properties)
        if not self.request_spec:
            # NOTE(sbauza): We were unable to find an original RequestSpec
            # object - probably because the instance is old.
            # We need to mock that the old way
            request_spec = objects.RequestSpec.from_components(
                self.context, self.instance.uuid, image,
                self.instance.flavor, self.instance.numa_topology,
                self.instance.pci_requests,
                filter_properties, None, self.instance.availability_zone
            )
        else:
            request_spec = self.request_spec
            # NOTE(sbauza): Force_hosts/nodes needs to be reset
            # if we want to make sure that the next destination
            # is not forced to be the original host
            request_spec.reset_forced_destinations()

        host = None
        while host is None:
            self._check_not_over_max_retries(attempted_hosts)
            request_spec.ignore_hosts = attempted_hosts
            try:
                host = self.scheduler_client.select_destinations(self.context,
                                request_spec)[0]['host']
            except messaging.RemoteError as ex:
                # TODO(ShaoHe Feng) There maybe multi-scheduler, and the
                # scheduling algorithm is R-R, we can let other scheduler try.
                # Note(ShaoHe Feng) There are types of RemoteError, such as
                # NoSuchMethod, UnsupportedVersion, we can distinguish it by
                # ex.exc_type.
                raise exception.MigrationSchedulerRPCError(
                    reason=six.text_type(ex))
            try:
                self._check_compatible_with_source_hypervisor(host)
                self._call_livem_checks_on_host(host)
            except (exception.Invalid, exception.MigrationPreCheckError) as e:
                LOG.debug("Skipping host: %(host)s because: %(e)s",
                    {"host": host, "e": e})
                attempted_hosts.append(host)
                host = None
        return host

    def _check_not_over_max_retries(self, attempted_hosts):
        if CONF.migrate_max_retries == -1:
            return

        retries = len(attempted_hosts) - 1
        if retries > CONF.migrate_max_retries:
            if self.migration:
                self.migration.status = 'failed'
                self.migration.save()
            msg = (_('Exceeded max scheduling retries %(max_retries)d for '
                     'instance %(instance_uuid)s during live migration')
                   % {'max_retries': retries,
                      'instance_uuid': self.instance.uuid})
            raise exception.MaxRetriesExceeded(reason=msg)

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
# Copyright 2012 Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_cache import core as cache
from oslo_db import options
from oslo_log import log

from nova.common import config
import nova.conf
from nova.db.sqlalchemy import api as sqlalchemy_api
from nova import paths
from nova import rpc
from nova import version


CONF = nova.conf.CONF

_DEFAULT_SQL_CONNECTION = 'sqlite:///' + paths.state_path_def('nova.sqlite')

_EXTRA_DEFAULT_LOG_LEVELS = ['glanceclient=WARN']


def parse_args(argv, default_config_files=None, configure_db=True,
               init_rpc=True):
    log.register_options(CONF)
    # We use the oslo.log default log levels which includes suds=INFO
    # and add only the extra levels that Nova needs
    log.set_defaults(default_log_levels=log.get_default_log_levels() +
                     _EXTRA_DEFAULT_LOG_LEVELS)
    options.set_defaults(CONF, connection=_DEFAULT_SQL_CONNECTION,
                         sqlite_db='nova.sqlite')
    rpc.set_defaults(control_exchange='nova')
    cache.configure(CONF)
    config.set_middleware_defaults()

    CONF(argv[1:],
         project='nova',
         version=version.version_string(),
         default_config_files=default_config_files)

    if init_rpc:
        rpc.init(CONF)

    if configure_db:
        sqlalchemy_api.configure(CONF)


#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

# This is a placeholder for backports.
# Do not use this number for new work.  New work starts after
# all the placeholders.
#
# See this for more information:
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html


def upgrade(migrate_engine):
    pass

# Copyright (c) 2014 Red Hat, Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


from migrate import UniqueConstraint
from sqlalchemy import MetaData, Table, Column, String


def upgrade(migrate_engine):
    meta = MetaData()
    meta.bind = migrate_engine

    # Add a new column host
    compute_nodes = Table('compute_nodes', meta, autoload=True)
    shadow_compute_nodes = Table('shadow_compute_nodes', meta, autoload=True)

    # NOTE(sbauza) : Old compute nodes can report stats without this field, we
    # need to set it as nullable
    host = Column('host', String(255), nullable=True)
    if not hasattr(compute_nodes.c, 'host'):
        compute_nodes.create_column(host)
    if not hasattr(shadow_compute_nodes.c, 'host'):
        shadow_compute_nodes.create_column(host.copy())

    # NOTE(sbauza) : Populate the host field with the value from the services
    # table will be done at the ComputeNode object level when save()

    ukey = UniqueConstraint('host', 'hypervisor_hostname', table=compute_nodes,
                            name="uniq_compute_nodes0host0hypervisor_hostname")
    ukey.create()

# Copyright 2015 Red Hat Inc
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

#
# See blueprint backportable-db-migrations-icehouse
# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html

from sqlalchemy import MetaData, Table, Column, String, Index


def upgrade(migrate_engine):
    meta = MetaData(bind=migrate_engine)

    # Add a new column to store PCI device parent address
    pci_devices = Table('pci_devices', meta, autoload=True)
    shadow_pci_devices = Table('shadow_pci_devices', meta, autoload=True)

    parent_addr = Column('parent_addr', String(12), nullable=True)

    if not hasattr(pci_devices.c, 'parent_addr'):
        pci_devices.create_column(parent_addr)
    if not hasattr(shadow_pci_devices.c, 'parent_addr'):
        shadow_pci_devices.create_column(parent_addr.copy())

    # Create index
    parent_index = Index('ix_pci_devices_compute_node_id_parent_addr_deleted',
                         pci_devices.c.compute_node_id,
                         pci_devices.c.parent_addr,
                         pci_devices.c.deleted)
    parent_index.create(migrate_engine)

# Copyright 2010 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# Copyright 2011 Justin Santa Barbara
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""RFC2462 style IPv6 address generation."""

import netaddr

from nova.i18n import _


def to_global(prefix, mac, project_id):
    try:
        mac64 = netaddr.EUI(mac).eui64().words
        int_addr = int(''.join(['%02x' % i for i in mac64]), 16)
        mac64_addr = netaddr.IPAddress(int_addr)
        maskIP = netaddr.IPNetwork(prefix).ip
        return (mac64_addr ^ netaddr.IPAddress('::0200:0:0:0') |
                maskIP).format()
    except netaddr.AddrFormatError:
        raise TypeError(_('Bad mac for to_global_ipv6: %s') % mac)
    except TypeError:
        raise TypeError(_('Bad prefix for to_global_ipv6: %s') % prefix)


def to_mac(ipv6_address):
    address = netaddr.IPAddress(ipv6_address)
    mask1 = netaddr.IPAddress('::ffff:ffff:ffff:ffff')
    mask2 = netaddr.IPAddress('::0200:0:0:0')
    mac64 = netaddr.EUI(int(address & mask1 ^ mask2)).words
    return ':'.join(['%02x' % i for i in mac64[0:3] + mac64[5:8]])

# Copyright 2013 UnitedStack Inc.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

QOS_QUEUE = 'QoS Queue'
NET_EXTERNAL = 'router:external'
PORTBINDING_EXT = 'Port Binding'
VNIC_INDEX_EXT = 'VNIC Index'
DNS_INTEGRATION = 'DNS Integration'

#    Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import contextlib

from oslo_config import cfg
from oslo_db import exception as db_exc
from oslo_log import log as logging
from oslo_serialization import jsonutils
from oslo_utils import timeutils
from oslo_utils import versionutils

from nova.cells import opts as cells_opts
from nova.cells import rpcapi as cells_rpcapi
from nova.cells import utils as cells_utils
from nova import db
from nova import exception
from nova.i18n import _LE
from nova import notifications
from nova import objects
from nova.objects import base
from nova.objects import fields
from nova import utils


CONF = cfg.CONF
LOG = logging.getLogger(__name__)


# List of fields that can be joined in DB layer.
_INSTANCE_OPTIONAL_JOINED_FIELDS = ['metadata', 'system_metadata',
                                    'info_cache', 'security_groups',
                                    'pci_devices', 'tags', 'services']
# These are fields that are optional but don't translate to db columns
_INSTANCE_OPTIONAL_NON_COLUMN_FIELDS = ['fault', 'flavor', 'old_flavor',
                                        'new_flavor', 'ec2_ids']
# These are fields that are optional and in instance_extra
_INSTANCE_EXTRA_FIELDS = ['numa_topology', 'pci_requests',
                          'flavor', 'vcpu_model', 'migration_context']

# These are fields that can be specified as expected_attrs
INSTANCE_OPTIONAL_ATTRS = (_INSTANCE_OPTIONAL_JOINED_FIELDS +
                           _INSTANCE_OPTIONAL_NON_COLUMN_FIELDS +
                           _INSTANCE_EXTRA_FIELDS)
# These are fields that most query calls load by default
INSTANCE_DEFAULT_FIELDS = ['metadata', 'system_metadata',
                           'info_cache', 'security_groups']


def _expected_cols(expected_attrs):
    """Return expected_attrs that are columns needing joining.

    NB: This function may modify expected_attrs if one
    requested attribute requires another.
    """
    if not expected_attrs:
        return expected_attrs

    simple_cols = [attr for attr in expected_attrs
                   if attr in _INSTANCE_OPTIONAL_JOINED_FIELDS]

    complex_cols = ['extra.%s' % field
                    for field in _INSTANCE_EXTRA_FIELDS
                    if field in expected_attrs]
    if complex_cols:
        simple_cols.append('extra')
    simple_cols = [x for x in simple_cols if x not in _INSTANCE_EXTRA_FIELDS]
    return simple_cols + complex_cols


_NO_DATA_SENTINEL = object()


# TODO(berrange): Remove NovaObjectDictCompat
@base.NovaObjectRegistry.register
class Instance(base.NovaPersistentObject, base.NovaObject,
               base.NovaObjectDictCompat):
    # Version 2.0: Initial version
    # Version 2.1: Added services
    VERSION = '2.1'

    fields = {
        'id': fields.IntegerField(),

        'user_id': fields.StringField(nullable=True),
        'project_id': fields.StringField(nullable=True),

        'image_ref': fields.StringField(nullable=True),
        'kernel_id': fields.StringField(nullable=True),
        'ramdisk_id': fields.StringField(nullable=True),
        'hostname': fields.StringField(nullable=True),

        'launch_index': fields.IntegerField(nullable=True),
        'key_name': fields.StringField(nullable=True),
        'key_data': fields.StringField(nullable=True),

        'power_state': fields.IntegerField(nullable=True),
        'vm_state': fields.StringField(nullable=True),
        'task_state': fields.StringField(nullable=True),

        'services': fields.ObjectField('ServiceList'),

        'memory_mb': fields.IntegerField(nullable=True),
        'vcpus': fields.IntegerField(nullable=True),
        'root_gb': fields.IntegerField(nullable=True),
        'ephemeral_gb': fields.IntegerField(nullable=True),
        'ephemeral_key_uuid': fields.UUIDField(nullable=True),

        'host': fields.StringField(nullable=True),
        'node': fields.StringField(nullable=True),

        'instance_type_id': fields.IntegerField(nullable=True),

        'user_data': fields.StringField(nullable=True),

        'reservation_id': fields.StringField(nullable=True),

        'launched_at': fields.DateTimeField(nullable=True),
        'terminated_at': fields.DateTimeField(nullable=True),

        'availability_zone': fields.StringField(nullable=True),

        'display_name': fields.StringField(nullable=True),
        'display_description': fields.StringField(nullable=True),

        'launched_on': fields.StringField(nullable=True),

        # NOTE(jdillaman): locked deprecated in favor of locked_by,
        # to be removed in Icehouse
        'locked': fields.BooleanField(default=False),
        'locked_by': fields.StringField(nullable=True),

        'os_type': fields.StringField(nullable=True),
        'architecture': fields.StringField(nullable=True),
        'vm_mode': fields.StringField(nullable=True),
        'uuid': fields.UUIDField(),

        'root_device_name': fields.StringField(nullable=True),
        'default_ephemeral_device': fields.StringField(nullable=True),
        'default_swap_device': fields.StringField(nullable=True),
        'config_drive': fields.StringField(nullable=True),

        'access_ip_v4': fields.IPV4AddressField(nullable=True),
        'access_ip_v6': fields.IPV6AddressField(nullable=True),

        'auto_disk_config': fields.BooleanField(default=False),
        'progress': fields.IntegerField(nullable=True),

        'shutdown_terminate': fields.BooleanField(default=False),
        'disable_terminate': fields.BooleanField(default=False),

        'cell_name': fields.StringField(nullable=True),

        'metadata': fields.DictOfStringsField(),
        'system_metadata': fields.DictOfNullableStringsField(),

        'info_cache': fields.ObjectField('InstanceInfoCache',
                                         nullable=True),

        'security_groups': fields.ObjectField('SecurityGroupList'),

        'fault': fields.ObjectField('InstanceFault', nullable=True),

        'cleaned': fields.BooleanField(default=False),

        'pci_devices': fields.ObjectField('PciDeviceList', nullable=True),
        'numa_topology': fields.ObjectField('InstanceNUMATopology',
                                            nullable=True),
        'pci_requests': fields.ObjectField('InstancePCIRequests',
                                           nullable=True),
        'tags': fields.ObjectField('TagList'),
        'flavor': fields.ObjectField('Flavor'),
        'old_flavor': fields.ObjectField('Flavor', nullable=True),
        'new_flavor': fields.ObjectField('Flavor', nullable=True),
        'vcpu_model': fields.ObjectField('VirtCPUModel', nullable=True),
        'ec2_ids': fields.ObjectField('EC2Ids'),
        'migration_context': fields.ObjectField('MigrationContext',
                                                nullable=True)
        }

    obj_extra_fields = ['name']

    def obj_make_compatible(self, primitive, target_version):
        super(Instance, self).obj_make_compatible(primitive, target_version)
        target_version = versionutils.convert_version_to_tuple(target_version)
        if target_version < (2, 1) and 'services' in primitive:
            del primitive['services']

    def __init__(self, *args, **kwargs):
        super(Instance, self).__init__(*args, **kwargs)
        self._reset_metadata_tracking()

    @property
    def image_meta(self):
        return objects.ImageMeta.from_instance(self)

    def _reset_metadata_tracking(self, fields=None):
        if fields is None or 'system_metadata' in fields:
            self._orig_system_metadata = (dict(self.system_metadata) if
                                          'system_metadata' in self else {})
        if fields is None or 'metadata' in fields:
            self._orig_metadata = (dict(self.metadata) if
                                   'metadata' in self else {})

    def obj_reset_changes(self, fields=None, recursive=False):
        super(Instance, self).obj_reset_changes(fields,
                                                recursive=recursive)
        self._reset_metadata_tracking(fields=fields)

    def obj_what_changed(self):
        changes = super(Instance, self).obj_what_changed()
        if 'metadata' in self and self.metadata != self._orig_metadata:
            changes.add('metadata')
        if 'system_metadata' in self and (self.system_metadata !=
                                          self._orig_system_metadata):
            changes.add('system_metadata')
        return changes

    @classmethod
    def _obj_from_primitive(cls, context, objver, primitive):
        self = super(Instance, cls)._obj_from_primitive(context, objver,
                                                        primitive)
        self._reset_metadata_tracking()
        return self

    @property
    def name(self):
        try:
            base_name = CONF.instance_name_template % self.id
        except TypeError:
            # Support templates like "uuid-%(uuid)s", etc.
            info = {}
            # NOTE(russellb): Don't use self.iteritems() here, as it will
            # result in infinite recursion on the name property.
            for key in self.fields:
                if key == 'name':
                    # NOTE(danms): prevent recursion
                    continue
                elif not self.obj_attr_is_set(key):
                    # NOTE(danms): Don't trigger lazy-loads
                    continue
                info[key] = self[key]
            try:
                base_name = CONF.instance_name_template % info
            except KeyError:
                base_name = self.uuid
        return base_name

    def _flavor_from_db(self, db_flavor):
        """Load instance flavor information from instance_extra."""

        flavor_info = jsonutils.loads(db_flavor)

        self.flavor = objects.Flavor.obj_from_primitive(flavor_info['cur'])
        if flavor_info['old']:
            self.old_flavor = objects.Flavor.obj_from_primitive(
                flavor_info['old'])
        else:
            self.old_flavor = None
        if flavor_info['new']:
            self.new_flavor = objects.Flavor.obj_from_primitive(
                flavor_info['new'])
        else:
            self.new_flavor = None
        self.obj_reset_changes(['flavor', 'old_flavor', 'new_flavor'])

    @staticmethod
    def _from_db_object(context, instance, db_inst, expected_attrs=None):
        """Method to help with migration to objects.

        Converts a database entity to a formal object.
        """
        instance._context = context
        if expected_attrs is None:
            expected_attrs = []
        # Most of the field names match right now, so be quick
        for field in instance.fields:
            if field in INSTANCE_OPTIONAL_ATTRS:
                continue
            elif field == 'deleted':
                instance.deleted = db_inst['deleted'] == db_inst['id']
            elif field == 'cleaned':
                instance.cleaned = db_inst['cleaned'] == 1
            else:
                instance[field] = db_inst[field]

        # NOTE(danms): We can be called with a dict instead of a
        # SQLAlchemy object, so we have to be careful here
        if hasattr(db_inst, '__dict__'):
            have_extra = 'extra' in db_inst.__dict__ and db_inst['extra']
        else:
            have_extra = 'extra' in db_inst and db_inst['extra']

        if 'metadata' in expected_attrs:
            instance['metadata'] = utils.instance_meta(db_inst)
        if 'system_metadata' in expected_attrs:
            instance['system_metadata'] = utils.instance_sys_meta(db_inst)
        if 'fault' in expected_attrs:
            instance['fault'] = (
                objects.InstanceFault.get_latest_for_instance(
                    context, instance.uuid))
        if 'numa_topology' in expected_attrs:
            if have_extra:
                instance._load_numa_topology(
                    db_inst['extra'].get('numa_topology'))
            else:
                instance.numa_topology = None
        if 'pci_requests' in expected_attrs:
            if have_extra:
                instance._load_pci_requests(
                    db_inst['extra'].get('pci_requests'))
            else:
                instance.pci_requests = None
        if 'vcpu_model' in expected_attrs:
            if have_extra:
                instance._load_vcpu_model(
                    db_inst['extra'].get('vcpu_model'))
            else:
                instance.vcpu_model = None
        if 'ec2_ids' in expected_attrs:
            instance._load_ec2_ids()
        if 'migration_context' in expected_attrs:
            if have_extra:
                instance._load_migration_context(
                    db_inst['extra'].get('migration_context'))
            else:
                instance.migration_context = None
        if 'info_cache' in expected_attrs:
            if db_inst.get('info_cache') is None:
                instance.info_cache = None
            elif not instance.obj_attr_is_set('info_cache'):
                # TODO(danms): If this ever happens on a backlevel instance
                # passed to us by a backlevel service, things will break
                instance.info_cache = objects.InstanceInfoCache(context)
            if instance.info_cache is not None:
                instance.info_cache._from_db_object(context,
                                                    instance.info_cache,
                                                    db_inst['info_cache'])

        if any([x in expected_attrs for x in ('flavor',
                                              'old_flavor',
                                              'new_flavor')]):
            if have_extra and db_inst['extra'].get('flavor'):
                instance._flavor_from_db(db_inst['extra']['flavor'])

        # TODO(danms): If we are updating these on a backlevel instance,
        # we'll end up sending back new versions of these objects (see
        # above note for new info_caches
        if 'pci_devices' in expected_attrs:
            pci_devices = base.obj_make_list(
                    context, objects.PciDeviceList(context),
                    objects.PciDevice, db_inst['pci_devices'])
            instance['pci_devices'] = pci_devices
        if 'security_groups' in expected_attrs:
            sec_groups = base.obj_make_list(
                    context, objects.SecurityGroupList(context),
                    objects.SecurityGroup, db_inst.get('security_groups', []))
            instance['security_groups'] = sec_groups

        if 'tags' in expected_attrs:
            tags = base.obj_make_list(
                context, objects.TagList(context),
                objects.Tag, db_inst['tags'])
            instance['tags'] = tags

        if 'services' in expected_attrs:
            services = base.obj_make_list(
                    context, objects.ServiceList(context),
                    objects.Service, db_inst['services'])
            instance['services'] = services

        instance.obj_reset_changes()
        return instance

    @staticmethod
    @db.select_db_reader_mode
    def _db_instance_get_by_uuid(context, uuid, columns_to_join,
                                 use_slave=False):
        return db.instance_get_by_uuid(context, uuid,
                                       columns_to_join=columns_to_join)

    @base.remotable_classmethod
    def get_by_uuid(cls, context, uuid, expected_attrs=None, use_slave=False):
        if expected_attrs is None:
            expected_attrs = ['info_cache', 'security_groups']
        columns_to_join = _expected_cols(expected_attrs)
        db_inst = cls._db_instance_get_by_uuid(context, uuid, columns_to_join,
                                               use_slave=use_slave)
        return cls._from_db_object(context, cls(), db_inst,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_by_id(cls, context, inst_id, expected_attrs=None):
        if expected_attrs is None:
            expected_attrs = ['info_cache', 'security_groups']
        columns_to_join = _expected_cols(expected_attrs)
        db_inst = db.instance_get(context, inst_id,
                                  columns_to_join=columns_to_join)
        return cls._from_db_object(context, cls(), db_inst,
                                   expected_attrs)

    @base.remotable
    def create(self):
        if self.obj_attr_is_set('id'):
            raise exception.ObjectActionError(action='create',
                                              reason='already created')
        updates = self.obj_get_changes()
        expected_attrs = [attr for attr in INSTANCE_DEFAULT_FIELDS
                          if attr in updates]
        if 'security_groups' in updates:
            updates['security_groups'] = [x.name for x in
                                          updates['security_groups']]
        if 'info_cache' in updates:
            updates['info_cache'] = {
                'network_info': updates['info_cache'].network_info.json()
                }
        updates['extra'] = {}
        numa_topology = updates.pop('numa_topology', None)
        expected_attrs.append('numa_topology')
        if numa_topology:
            updates['extra']['numa_topology'] = numa_topology._to_json()
        else:
            updates['extra']['numa_topology'] = None
        pci_requests = updates.pop('pci_requests', None)
        expected_attrs.append('pci_requests')
        if pci_requests:
            updates['extra']['pci_requests'] = (
                pci_requests.to_json())
        else:
            updates['extra']['pci_requests'] = None
        flavor = updates.pop('flavor', None)
        if flavor:
            expected_attrs.append('flavor')
            old = ((self.obj_attr_is_set('old_flavor') and
                    self.old_flavor) and
                   self.old_flavor.obj_to_primitive() or None)
            new = ((self.obj_attr_is_set('new_flavor') and
                    self.new_flavor) and
                   self.new_flavor.obj_to_primitive() or None)
            flavor_info = {
                'cur': self.flavor.obj_to_primitive(),
                'old': old,
                'new': new,
            }
            updates['extra']['flavor'] = jsonutils.dumps(flavor_info)
        vcpu_model = updates.pop('vcpu_model', None)
        expected_attrs.append('vcpu_model')
        if vcpu_model:
            updates['extra']['vcpu_model'] = (
                jsonutils.dumps(vcpu_model.obj_to_primitive()))
        else:
            updates['extra']['vcpu_model'] = None
        db_inst = db.instance_create(self._context, updates)
        self._from_db_object(self._context, self, db_inst, expected_attrs)

        # NOTE(danms): The EC2 ids are created on their first load. In order
        # to avoid them being missing and having to be loaded later, we
        # load them once here on create now that the instance record is
        # created.
        self._load_ec2_ids()
        self.obj_reset_changes(['ec2_ids'])

    @base.remotable
    def destroy(self):
        if not self.obj_attr_is_set('id'):
            raise exception.ObjectActionError(action='destroy',
                                              reason='already destroyed')
        if not self.obj_attr_is_set('uuid'):
            raise exception.ObjectActionError(action='destroy',
                                              reason='no uuid')
        if not self.obj_attr_is_set('host') or not self.host:
            # NOTE(danms): If our host is not set, avoid a race
            constraint = db.constraint(host=db.equal_any(None))
        else:
            constraint = None

        cell_type = cells_opts.get_cell_type()
        if cell_type is not None:
            stale_instance = self.obj_clone()

        try:
            db_inst = db.instance_destroy(self._context, self.uuid,
                                          constraint=constraint)
            self._from_db_object(self._context, self, db_inst)
        except exception.ConstraintNotMet:
            raise exception.ObjectActionError(action='destroy',
                                              reason='host changed')
        if cell_type == 'compute':
            cells_api = cells_rpcapi.CellsAPI()
            cells_api.instance_destroy_at_top(self._context, stale_instance)
        delattr(self, base.get_attrname('id'))

    def _save_info_cache(self, context):
        if self.info_cache:
            with self.info_cache.obj_alternate_context(context):
                self.info_cache.save()

    def _save_security_groups(self, context):
        security_groups = self.security_groups or []
        for secgroup in security_groups:
            with secgroup.obj_alternate_context(context):
                secgroup.save()
        self.security_groups.obj_reset_changes()

    def _save_fault(self, context):
        # NOTE(danms): I don't think we need to worry about this, do we?
        pass

    def _save_numa_topology(self, context):
        if self.numa_topology:
            self.numa_topology.instance_uuid = self.uuid
            with self.numa_topology.obj_alternate_context(context):
                self.numa_topology._save()
        else:
            objects.InstanceNUMATopology.delete_by_instance_uuid(
                    context, self.uuid)

    def _save_pci_requests(self, context):
        # NOTE(danms): No need for this yet.
        pass

    def _save_pci_devices(self, context):
        # NOTE(yjiang5): All devices held by PCI tracker, only PCI tracker
        # permitted to update the DB. all change to devices from here will
        # be dropped.
        pass

    def _save_flavor(self, context):
        if not any([x in self.obj_what_changed() for x in
                    ('flavor', 'old_flavor', 'new_flavor')]):
            return
        # FIXME(danms): We can do this smarterly by updating this
        # with all the other extra things at the same time
        flavor_info = {
            'cur': self.flavor.obj_to_primitive(),
            'old': (self.old_flavor and
                    self.old_flavor.obj_to_primitive() or None),
            'new': (self.new_flavor and
                    self.new_flavor.obj_to_primitive() or None),
        }
        db.instance_extra_update_by_uuid(
            context, self.uuid,
            {'flavor': jsonutils.dumps(flavor_info)})
        self.obj_reset_changes(['flavor', 'old_flavor', 'new_flavor'])

    def _save_old_flavor(self, context):
        if 'old_flavor' in self.obj_what_changed():
            self._save_flavor(context)

    def _save_new_flavor(self, context):
        if 'new_flavor' in self.obj_what_changed():
            self._save_flavor(context)

    def _save_vcpu_model(self, context):
        # TODO(yjiang5): should merge the db accesses for all the extra
        # fields
        if 'vcpu_model' in self.obj_what_changed():
            if self.vcpu_model:
                update = jsonutils.dumps(self.vcpu_model.obj_to_primitive())
            else:
                update = None
            db.instance_extra_update_by_uuid(
                context, self.uuid,
                {'vcpu_model': update})

    def _save_ec2_ids(self, context):
        # NOTE(hanlind): Read-only so no need to save this.
        pass

    def _save_migration_context(self, context):
        if self.migration_context:
            self.migration_context.instance_uuid = self.uuid
            with self.migration_context.obj_alternate_context(context):
                self.migration_context._save()
        else:
            objects.MigrationContext._destroy(context, self.uuid)

    @base.remotable
    def save(self, expected_vm_state=None,
             expected_task_state=None, admin_state_reset=False):
        """Save updates to this instance

        Column-wise updates will be made based on the result of
        self.what_changed(). If expected_task_state is provided,
        it will be checked against the in-database copy of the
        instance before updates are made.

        :param:context: Security context
        :param:expected_task_state: Optional tuple of valid task states
        for the instance to be in
        :param:expected_vm_state: Optional tuple of valid vm states
        for the instance to be in
        :param admin_state_reset: True if admin API is forcing setting
        of task_state/vm_state

        """
        # Store this on the class because _cell_name_blocks_sync is useless
        # after the db update call below.
        self._sync_cells = not self._cell_name_blocks_sync()

        context = self._context
        cell_type = cells_opts.get_cell_type()

        if cell_type is not None:
            # NOTE(comstud): We need to stash a copy of ourselves
            # before any updates are applied.  When we call the save
            # methods on nested objects, we will lose any changes to
            # them.  But we need to make sure child cells can tell
            # what is changed.
            #
            # We also need to nuke any updates to vm_state and task_state
            # unless admin_state_reset is True.  compute cells are
            # authoritative for their view of vm_state and task_state.
            stale_instance = self.obj_clone()

        cells_update_from_api = (cell_type == 'api' and self.cell_name and
                                 self._sync_cells)

        if cells_update_from_api:
            def _handle_cell_update_from_api():
                cells_api = cells_rpcapi.CellsAPI()
                cells_api.instance_update_from_api(context, stale_instance,
                            expected_vm_state,
                            expected_task_state,
                            admin_state_reset)

        updates = {}
        changes = self.obj_what_changed()

        for field in self.fields:
            # NOTE(danms): For object fields, we construct and call a
            # helper method like self._save_$attrname()
            if (self.obj_attr_is_set(field) and
                    isinstance(self.fields[field], fields.ObjectField)):
                try:
                    getattr(self, '_save_%s' % field)(context)
                except AttributeError:
                    LOG.exception(_LE('No save handler for %s'), field,
                                  instance=self)
                except db_exc.DBReferenceError as exp:
                    if exp.key != 'instance_uuid':
                        raise
                    # NOTE(melwitt): This will happen if we instance.save()
                    # before an instance.create() and FK constraint fails.
                    # In practice, this occurs in cells during a delete of
                    # an unscheduled instance. Otherwise, it could happen
                    # as a result of bug.
                    raise exception.InstanceNotFound(instance_id=self.uuid)
            elif field in changes:
                if (field == 'cell_name' and self[field] is not None and
                        self[field].startswith(cells_utils.BLOCK_SYNC_FLAG)):
                    updates[field] = self[field].replace(
                            cells_utils.BLOCK_SYNC_FLAG, '', 1)
                else:
                    updates[field] = self[field]

        if not updates:
            if cells_update_from_api:
                _handle_cell_update_from_api()
            return

        # Cleaned needs to be turned back into an int here
        if 'cleaned' in updates:
            if updates['cleaned']:
                updates['cleaned'] = 1
            else:
                updates['cleaned'] = 0

        if expected_task_state is not None:
            updates['expected_task_state'] = expected_task_state
        if expected_vm_state is not None:
            updates['expected_vm_state'] = expected_vm_state

        expected_attrs = [attr for attr in _INSTANCE_OPTIONAL_JOINED_FIELDS
                               if self.obj_attr_is_set(attr)]
        if 'pci_devices' in expected_attrs:
            # NOTE(danms): We don't refresh pci_devices on save right now
            expected_attrs.remove('pci_devices')

        # NOTE(alaski): We need to pull system_metadata for the
        # notification.send_update() below.  If we don't there's a KeyError
        # when it tries to extract the flavor.
        # NOTE(danms): If we have sysmeta, we need flavor since the caller
        # might be expecting flavor information as a result
        if 'system_metadata' not in expected_attrs:
            expected_attrs.append('system_metadata')
            expected_attrs.append('flavor')
        old_ref, inst_ref = db.instance_update_and_get_original(
                context, self.uuid, updates,
                columns_to_join=_expected_cols(expected_attrs))
        self._from_db_object(context, self, inst_ref,
                             expected_attrs=expected_attrs)

        if cells_update_from_api:
            _handle_cell_update_from_api()
        elif cell_type == 'compute':
            if self._sync_cells:
                cells_api = cells_rpcapi.CellsAPI()
                cells_api.instance_update_at_top(context, stale_instance)

        def _notify():
            # NOTE(danms): We have to be super careful here not to trigger
            # any lazy-loads that will unmigrate or unbackport something. So,
            # make a copy of the instance for notifications first.
            new_ref = self.obj_clone()

            notifications.send_update(context, old_ref, new_ref)

        # NOTE(alaski): If cell synchronization is blocked it means we have
        # already run this block of code in either the parent or child of this
        # cell.  Therefore this notification has already been sent.
        if not self._sync_cells:
            _notify = lambda: None  # noqa: F811

        _notify()

        self.obj_reset_changes()

    @base.remotable
    def refresh(self, use_slave=False):
        extra = [field for field in INSTANCE_OPTIONAL_ATTRS
                       if self.obj_attr_is_set(field)]
        current = self.__class__.get_by_uuid(self._context, uuid=self.uuid,
                                             expected_attrs=extra,
                                             use_slave=use_slave)
        # NOTE(danms): We orphan the instance copy so we do not unexpectedly
        # trigger a lazy-load (which would mean we failed to calculate the
        # expected_attrs properly)
        current._context = None

        for field in self.fields:
            if self.obj_attr_is_set(field):
                if field == 'info_cache':
                    self.info_cache.refresh()
                elif self[field] != current[field]:
                    self[field] = current[field]
        self.obj_reset_changes()

    def _load_generic(self, attrname):
        instance = self.__class__.get_by_uuid(self._context,
                                              uuid=self.uuid,
                                              expected_attrs=[attrname])

        # NOTE(danms): Never allow us to recursively-load
        if instance.obj_attr_is_set(attrname):
            self[attrname] = instance[attrname]
        else:
            raise exception.ObjectActionError(
                action='obj_load_attr',
                reason='loading %s requires recursion' % attrname)

    def _load_fault(self):
        self.fault = objects.InstanceFault.get_latest_for_instance(
            self._context, self.uuid)

    def _load_numa_topology(self, db_topology=None):
        if db_topology is not None:
            self.numa_topology = \
                objects.InstanceNUMATopology.obj_from_db_obj(self.uuid,
                                                             db_topology)
        else:
            try:
                self.numa_topology = \
                    objects.InstanceNUMATopology.get_by_instance_uuid(
                        self._context, self.uuid)
            except exception.NumaTopologyNotFound:
                self.numa_topology = None

    def _load_pci_requests(self, db_requests=None):
        # FIXME: also do this if none!
        if db_requests is not None:
            self.pci_requests = objects.InstancePCIRequests.obj_from_db(
                self._context, self.uuid, db_requests)
        else:
            self.pci_requests = \
                objects.InstancePCIRequests.get_by_instance_uuid(
                    self._context, self.uuid)

    def _load_flavor(self):
        instance = self.__class__.get_by_uuid(
            self._context, uuid=self.uuid,
            expected_attrs=['flavor', 'system_metadata'])

        # NOTE(danms): Orphan the instance to make sure we don't lazy-load
        # anything below
        instance._context = None
        self.flavor = instance.flavor
        self.old_flavor = instance.old_flavor
        self.new_flavor = instance.new_flavor

        # NOTE(danms): The query above may have migrated the flavor from
        # system_metadata. Since we have it anyway, go ahead and refresh
        # our system_metadata from it so that a save will be accurate.
        instance.system_metadata.update(self.get('system_metadata', {}))
        self.system_metadata = instance.system_metadata

    def _load_vcpu_model(self, db_vcpu_model=None):
        if db_vcpu_model is None:
            self.vcpu_model = objects.VirtCPUModel.get_by_instance_uuid(
                self._context, self.uuid)
        else:
            db_vcpu_model = jsonutils.loads(db_vcpu_model)
            self.vcpu_model = objects.VirtCPUModel.obj_from_primitive(
                db_vcpu_model)

    def _load_ec2_ids(self):
        self.ec2_ids = objects.EC2Ids.get_by_instance(self._context, self)

    def _load_security_groups(self):
        self.security_groups = objects.SecurityGroupList.get_by_instance(
            self._context, self)

    def _load_pci_devices(self):
        self.pci_devices = objects.PciDeviceList.get_by_instance_uuid(
            self._context, self.uuid)

    def _load_migration_context(self, db_context=_NO_DATA_SENTINEL):
        if db_context is _NO_DATA_SENTINEL:
            try:
                self.migration_context = (
                    objects.MigrationContext.get_by_instance_uuid(
                        self._context, self.uuid))
            except exception.MigrationContextNotFound:
                self.migration_context = None
        elif db_context is None:
            self.migration_context = None
        else:
            self.migration_context = objects.MigrationContext.obj_from_db_obj(
                db_context)

    def apply_migration_context(self):
        if self.migration_context:
            self.numa_topology = self.migration_context.new_numa_topology
        else:
            LOG.debug("Trying to apply a migration context that does not "
                      "seem to be set for this instance", instance=self)

    def revert_migration_context(self):
        if self.migration_context:
            self.numa_topology = self.migration_context.old_numa_topology
        else:
            LOG.debug("Trying to revert a migration context that does not "
                      "seem to be set for this instance", instance=self)

    @contextlib.contextmanager
    def mutated_migration_context(self):
        """Context manager to temporarily apply the migration context.

        Calling .save() from within the context manager means that the mutated
        context will be saved which can cause incorrect resource tracking, and
        should be avoided.
        """
        current_numa_topo = self.numa_topology
        self.apply_migration_context()
        try:
            yield
        finally:
            self.numa_topology = current_numa_topo

    @base.remotable
    def drop_migration_context(self):
        if self.migration_context:
            objects.MigrationContext._destroy(self._context, self.uuid)
            self.migration_context = None

    def clear_numa_topology(self):
        numa_topology = self.numa_topology
        if numa_topology is not None:
            self.numa_topology = numa_topology.clear_host_pinning()

    def obj_load_attr(self, attrname):
        if attrname not in INSTANCE_OPTIONAL_ATTRS:
            raise exception.ObjectActionError(
                action='obj_load_attr',
                reason='attribute %s not lazy-loadable' % attrname)

        if not self._context:
            raise exception.OrphanedObjectError(method='obj_load_attr',
                                                objtype=self.obj_name())

        LOG.debug("Lazy-loading '%(attr)s' on %(name)s uuid %(uuid)s",
                  {'attr': attrname,
                   'name': self.obj_name(),
                   'uuid': self.uuid,
                   })

        # NOTE(danms): We handle some fields differently here so that we
        # can be more efficient
        if attrname == 'fault':
            self._load_fault()
        elif attrname == 'numa_topology':
            self._load_numa_topology()
        elif attrname == 'pci_requests':
            self._load_pci_requests()
        elif attrname == 'vcpu_model':
            self._load_vcpu_model()
        elif attrname == 'ec2_ids':
            self._load_ec2_ids()
        elif attrname == 'migration_context':
            self._load_migration_context()
        elif attrname == 'security_groups':
            self._load_security_groups()
        elif attrname == 'pci_devices':
            self._load_pci_devices()
        elif 'flavor' in attrname:
            self._load_flavor()
        elif attrname == 'services' and self.deleted:
            # NOTE(mriedem): The join in the data model for instances.services
            # filters on instances.deleted == 0, so if the instance is deleted
            # don't attempt to even load services since we'll fail.
            self.services = objects.ServiceList(self._context)
        else:
            # FIXME(comstud): This should be optimized to only load the attr.
            self._load_generic(attrname)
        self.obj_reset_changes([attrname])

    def get_flavor(self, namespace=None):
        prefix = ('%s_' % namespace) if namespace is not None else ''
        attr = '%sflavor' % prefix
        try:
            return getattr(self, attr)
        except exception.FlavorNotFound:
            # NOTE(danms): This only happens in the case where we don't
            # have flavor information in sysmeta or extra, and doing
            # this triggers a lookup based on our instance_type_id for
            # (very) legacy instances. That legacy code expects a None here,
            # so emulate it for this helper, even though the actual attribute
            # is not nullable.
            return None

    @base.remotable
    def delete_metadata_key(self, key):
        """Optimized metadata delete method.

        This provides a more efficient way to delete a single metadata
        key, instead of just calling instance.save(). This should be called
        with the key still present in self.metadata, which it will update
        after completion.
        """
        db.instance_metadata_delete(self._context, self.uuid, key)
        md_was_changed = 'metadata' in self.obj_what_changed()
        del self.metadata[key]
        self._orig_metadata.pop(key, None)
        notifications.send_update(self._context, self, self)
        if not md_was_changed:
            self.obj_reset_changes(['metadata'])

    def _cell_name_blocks_sync(self):
        if (self.obj_attr_is_set('cell_name') and
                self.cell_name is not None and
                self.cell_name.startswith(cells_utils.BLOCK_SYNC_FLAG)):
            return True
        return False

    def _normalize_cell_name(self):
        """Undo skip_cell_sync()'s cell_name modification if applied"""

        if not self.obj_attr_is_set('cell_name') or self.cell_name is None:
            return
        cn_changed = 'cell_name' in self.obj_what_changed()
        if self.cell_name.startswith(cells_utils.BLOCK_SYNC_FLAG):
            self.cell_name = self.cell_name.replace(
                    cells_utils.BLOCK_SYNC_FLAG, '', 1)
            # cell_name is not normally an empty string, this means it was None
            # or unset before cells_utils.BLOCK_SYNC_FLAG was applied.
            if len(self.cell_name) == 0:
                self.cell_name = None
        if not cn_changed:
            self.obj_reset_changes(['cell_name'])

    @contextlib.contextmanager
    def skip_cells_sync(self):
        """Context manager to save an instance without syncing cells.

        Temporarily disables the cells syncing logic, if enabled.  This should
        only be used when saving an instance that has been passed down/up from
        another cell in order to avoid passing it back to the originator to be
        re-saved.
        """
        cn_changed = 'cell_name' in self.obj_what_changed()
        if not self.obj_attr_is_set('cell_name') or self.cell_name is None:
            self.cell_name = ''
        self.cell_name = '%s%s' % (cells_utils.BLOCK_SYNC_FLAG, self.cell_name)
        if not cn_changed:
            self.obj_reset_changes(['cell_name'])
        try:
            yield
        finally:
            self._normalize_cell_name()


def _make_instance_list(context, inst_list, db_inst_list, expected_attrs):
    get_fault = expected_attrs and 'fault' in expected_attrs
    inst_faults = {}
    if get_fault:
        # Build an instance_uuid:latest-fault mapping
        expected_attrs.remove('fault')
        instance_uuids = [inst['uuid'] for inst in db_inst_list]
        faults = objects.InstanceFaultList.get_by_instance_uuids(
            context, instance_uuids)
        for fault in faults:
            if fault.instance_uuid not in inst_faults:
                inst_faults[fault.instance_uuid] = fault

    inst_cls = objects.Instance

    inst_list.objects = []
    for db_inst in db_inst_list:
        inst_obj = inst_cls._from_db_object(
                context, inst_cls(context), db_inst,
                expected_attrs=expected_attrs)
        if get_fault:
            inst_obj.fault = inst_faults.get(inst_obj.uuid, None)
        inst_list.objects.append(inst_obj)
    inst_list.obj_reset_changes()
    return inst_list


@base.NovaObjectRegistry.register
class InstanceList(base.ObjectListBase, base.NovaObject):
    # Version 2.0: Initial Version
    VERSION = '2.0'

    fields = {
        'objects': fields.ListOfObjectsField('Instance'),
    }

    @classmethod
    @db.select_db_reader_mode
    def _get_by_filters_impl(cls, context, filters,
                       sort_key='created_at', sort_dir='desc', limit=None,
                       marker=None, expected_attrs=None, use_slave=False,
                       sort_keys=None, sort_dirs=None):
        if sort_keys or sort_dirs:
            db_inst_list = db.instance_get_all_by_filters_sort(
                context, filters, limit=limit, marker=marker,
                columns_to_join=_expected_cols(expected_attrs),
                sort_keys=sort_keys, sort_dirs=sort_dirs)
        else:
            db_inst_list = db.instance_get_all_by_filters(
                context, filters, sort_key, sort_dir, limit=limit,
                marker=marker, columns_to_join=_expected_cols(expected_attrs))
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_by_filters(cls, context, filters,
                       sort_key='created_at', sort_dir='desc', limit=None,
                       marker=None, expected_attrs=None, use_slave=False,
                       sort_keys=None, sort_dirs=None):
        return cls._get_by_filters_impl(
            context, filters, sort_key=sort_key, sort_dir=sort_dir,
            limit=limit, marker=marker, expected_attrs=expected_attrs,
            use_slave=use_slave, sort_keys=sort_keys, sort_dirs=sort_dirs)

    @staticmethod
    @db.select_db_reader_mode
    def _db_instance_get_all_by_host(context, host, columns_to_join,
                                     use_slave=False):
        return db.instance_get_all_by_host(context, host,
                                           columns_to_join=columns_to_join)

    @base.remotable_classmethod
    def get_by_host(cls, context, host, expected_attrs=None, use_slave=False):
        db_inst_list = cls._db_instance_get_all_by_host(
            context, host, columns_to_join=_expected_cols(expected_attrs),
            use_slave=use_slave)
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_by_host_and_node(cls, context, host, node, expected_attrs=None):
        db_inst_list = db.instance_get_all_by_host_and_node(
            context, host, node,
            columns_to_join=_expected_cols(expected_attrs))
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_by_host_and_not_type(cls, context, host, type_id=None,
                                 expected_attrs=None):
        db_inst_list = db.instance_get_all_by_host_and_not_type(
            context, host, type_id=type_id)
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_all(cls, context, expected_attrs=None):
        """Returns all instances on all nodes."""
        db_instances = db.instance_get_all(
                context, columns_to_join=_expected_cols(expected_attrs))
        return _make_instance_list(context, cls(), db_instances,
                                   expected_attrs)

    @base.remotable_classmethod
    def get_hung_in_rebooting(cls, context, reboot_window,
                              expected_attrs=None):
        db_inst_list = db.instance_get_all_hung_in_rebooting(context,
                                                             reboot_window)
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @staticmethod
    @db.select_db_reader_mode
    def _db_instance_get_active_by_window_joined(
            context, begin, end, project_id, host, columns_to_join,
            use_slave=False):
        return db.instance_get_active_by_window_joined(
            context, begin, end, project_id, host,
            columns_to_join=columns_to_join)

    @base.remotable_classmethod
    def _get_active_by_window_joined(cls, context, begin, end=None,
                                    project_id=None, host=None,
                                    expected_attrs=None,
                                    use_slave=False):
        # NOTE(mriedem): We need to convert the begin/end timestamp strings
        # to timezone-aware datetime objects for the DB API call.
        begin = timeutils.parse_isotime(begin)
        end = timeutils.parse_isotime(end) if end else None
        db_inst_list = cls._db_instance_get_active_by_window_joined(
            context, begin, end, project_id, host,
            columns_to_join=_expected_cols(expected_attrs),
            use_slave=use_slave)
        return _make_instance_list(context, cls(), db_inst_list,
                                   expected_attrs)

    @classmethod
    def get_active_by_window_joined(cls, context, begin, end=None,
                                    project_id=None, host=None,
                                    expected_attrs=None,
                                    use_slave=False):
        """Get instances and joins active during a certain time window.

        :param:context: nova request context
        :param:begin: datetime for the start of the time window
        :param:end: datetime for the end of the time window
        :param:project_id: used to filter instances by project
        :param:host: used to filter instances on a given compute host
        :param:expected_attrs: list of related fields that can be joined
        in the database layer when querying for instances
        :param use_slave if True, ship this query off to a DB slave
        :returns: InstanceList

        """
        # NOTE(mriedem): We have to convert the datetime objects to string
        # primitives for the remote call.
        begin = utils.isotime(begin)
        end = utils.isotime(end) if end else None
        return cls._get_active_by_window_joined(context, begin, end,
                                                project_id, host,
                                                expected_attrs,
                                                use_slave=use_slave)

    @base.remotable_classmethod
    def get_by_security_group_id(cls, context, security_group_id):
        db_secgroup = db.security_group_get(
            context, security_group_id,
            columns_to_join=['instances.info_cache',
                             'instances.system_metadata'])
        return _make_instance_list(context, cls(), db_secgroup['instances'],
                                   ['info_cache', 'system_metadata'])

    @classmethod
    def get_by_security_group(cls, context, security_group):
        return cls.get_by_security_group_id(context, security_group.id)

    @base.remotable_classmethod
    def get_by_grantee_security_group_ids(cls, context, security_group_ids):
        db_instances = db.instance_get_all_by_grantee_security_groups(
            context, security_group_ids)
        return _make_instance_list(context, cls(), db_instances, [])

    def fill_faults(self):
        """Batch query the database for our instances' faults.

        :returns: A list of instance uuids for which faults were found.
        """
        uuids = [inst.uuid for inst in self]
        faults = objects.InstanceFaultList.get_by_instance_uuids(
            self._context, uuids)
        faults_by_uuid = {}
        for fault in faults:
            if fault.instance_uuid not in faults_by_uuid:
                faults_by_uuid[fault.instance_uuid] = fault

        for instance in self:
            if instance.uuid in faults_by_uuid:
                instance.fault = faults_by_uuid[instance.uuid]
            else:
                # NOTE(danms): Otherwise the caller will cause a lazy-load
                # when checking it, and we know there are none
                instance.fault = None
            instance.obj_reset_changes(['fault'])

        return faults_by_uuid.keys()

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.objects import base
from nova.objects import fields


@base.NovaObjectRegistry.register
class VirtCPUTopology(base.NovaObject):
    # Version 1.0: Initial version
    VERSION = '1.0'

    fields = {
        'sockets': fields.IntegerField(nullable=True, default=1),
        'cores': fields.IntegerField(nullable=True, default=1),
        'threads': fields.IntegerField(nullable=True, default=1),
        }

    # NOTE(jaypipes): for backward compatibility, the virt CPU topology
    # data is stored in the database as a nested dict.
    @classmethod
    def from_dict(cls, data):
        return cls(sockets=data.get('sockets'),
                   cores=data.get('cores'),
                   threads=data.get('threads'))

    def to_dict(self):
        return {
            'sockets': self.sockets,
            'cores': self.cores,
            'threads': self.threads
        }

# Copyright (c) 2012 OpenStack Foundation
# Copyright (c) 2012 Cloudscaling
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_log import log as logging
import six

from nova.scheduler import filters
from nova.scheduler.filters import extra_specs_ops
from nova.scheduler.filters import utils


LOG = logging.getLogger(__name__)

_SCOPE = 'aggregate_instance_extra_specs'


class AggregateInstanceExtraSpecsFilter(filters.BaseHostFilter):
    """AggregateInstanceExtraSpecsFilter works with InstanceType records."""

    # Aggregate data and instance type does not change within a request
    run_filter_once_per_request = True

    def host_passes(self, host_state, spec_obj):
        """Return a list of hosts that can create instance_type

        Check that the extra specs associated with the instance type match
        the metadata provided by aggregates.  If not present return False.
        """
        instance_type = spec_obj.flavor
        # If 'extra_specs' is not present or extra_specs are empty then we
        # need not proceed further
        if (not instance_type.obj_attr_is_set('extra_specs')
                or not instance_type.extra_specs):
            return True

        metadata = utils.aggregate_metadata_get_by_host(host_state)

        for key, req in six.iteritems(instance_type.extra_specs):
            # Either not scope format, or aggregate_instance_extra_specs scope
            scope = key.split(':', 1)
            if len(scope) > 1:
                if scope[0] != _SCOPE:
                    continue
                else:
                    del scope[0]
            key = scope[0]
            aggregate_vals = metadata.get(key, None)
            if not aggregate_vals:
                LOG.debug("%(host_state)s fails instance_type extra_specs "
                    "requirements. Extra_spec %(key)s is not in aggregate.",
                    {'host_state': host_state, 'key': key})
                return False
            for aggregate_val in aggregate_vals:
                if extra_specs_ops.match(aggregate_val, req):
                    break
            else:
                LOG.debug("%(host_state)s fails instance_type extra_specs "
                            "requirements. '%(aggregate_vals)s' do not "
                            "match '%(req)s'",
                          {'host_state': host_state, 'req': req,
                           'aggregate_vals': aggregate_vals})
                return False
        return True

# Copyright 2013, Red Hat, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Client side of the scheduler manager RPC API.
"""

import oslo_messaging as messaging

import nova.conf
from nova.objects import base as objects_base
from nova import rpc

CONF = nova.conf.CONF


class SchedulerAPI(object):
    '''Client side of the scheduler rpc API.

    API version history:

        * 1.0 - Initial version.
        * 1.1 - Changes to prep_resize():
            * remove instance_uuid, add instance
            * remove instance_type_id, add instance_type
            * remove topic, it was unused
        * 1.2 - Remove topic from run_instance, it was unused
        * 1.3 - Remove instance_id, add instance to live_migration
        * 1.4 - Remove update_db from prep_resize
        * 1.5 - Add reservations argument to prep_resize()
        * 1.6 - Remove reservations argument to run_instance()
        * 1.7 - Add create_volume() method, remove topic from live_migration()

        * 2.0 - Remove 1.x backwards compat
        * 2.1 - Add image_id to create_volume()
        * 2.2 - Remove reservations argument to create_volume()
        * 2.3 - Remove create_volume()
        * 2.4 - Change update_service_capabilities()
            * accepts a list of capabilities
        * 2.5 - Add get_backdoor_port()
        * 2.6 - Add select_hosts()

        ... Grizzly supports message version 2.6.  So, any changes to existing
        methods in 2.x after that point should be done such that they can
        handle the version_cap being set to 2.6.

        * 2.7 - Add select_destinations()
        * 2.8 - Deprecate prep_resize() -- JUST KIDDING.  It is still used
                by the compute manager for retries.
        * 2.9 - Added the legacy_bdm_in_spec parameter to run_instance()

        ... Havana supports message version 2.9.  So, any changes to existing
        methods in 2.x after that point should be done such that they can
        handle the version_cap being set to 2.9.

        * Deprecated live_migration() call, moved to conductor
        * Deprecated select_hosts()

        3.0 - Removed backwards compat

        ... Icehouse and Juno support message version 3.0.  So, any changes to
        existing methods in 3.x after that point should be done such that they
        can handle the version_cap being set to 3.0.

        * 3.1 - Made select_destinations() send flavor object

        * 4.0 - Removed backwards compat for Icehouse
        * 4.1 - Add update_aggregates() and delete_aggregate()
        * 4.2 - Added update_instance_info(), delete_instance_info(), and
                sync_instance_info()  methods

        ... Kilo and Liberty support message version 4.2. So, any
        changes to existing methods in 4.x after that point should be
        done such that they can handle the version_cap being set to
        4.2.

        * 4.3 - Modify select_destinations() signature by providing a
                RequestSpec obj

        ... Mitaka supports message version 4.3. So, any changes to
        existing methods in 4.x after that point should be done such
        that they can handle the version_cap being set to 4.3.

    '''

    VERSION_ALIASES = {
        'grizzly': '2.6',
        'havana': '2.9',
        'icehouse': '3.0',
        'juno': '3.0',
        'kilo': '4.2',
        'liberty': '4.2',
        'mitaka': '4.3',
    }

    def __init__(self):
        super(SchedulerAPI, self).__init__()
        target = messaging.Target(topic=CONF.scheduler_topic, version='4.0')
        version_cap = self.VERSION_ALIASES.get(CONF.upgrade_levels.scheduler,
                                               CONF.upgrade_levels.scheduler)
        serializer = objects_base.NovaObjectSerializer()
        self.client = rpc.get_client(target, version_cap=version_cap,
                                     serializer=serializer)

    def select_destinations(self, ctxt, spec_obj):
        version = '4.3'
        msg_args = {'spec_obj': spec_obj}
        if not self.client.can_send_version(version):
            del msg_args['spec_obj']
            msg_args['request_spec'] = spec_obj.to_legacy_request_spec_dict()
            msg_args['filter_properties'
                     ] = spec_obj.to_legacy_filter_properties_dict()
            version = '4.0'
        cctxt = self.client.prepare(version=version)
        return cctxt.call(ctxt, 'select_destinations', **msg_args)

    def update_aggregates(self, ctxt, aggregates):
        # NOTE(sbauza): Yes, it's a fanout, we need to update all schedulers
        cctxt = self.client.prepare(fanout=True, version='4.1')
        cctxt.cast(ctxt, 'update_aggregates', aggregates=aggregates)

    def delete_aggregate(self, ctxt, aggregate):
        # NOTE(sbauza): Yes, it's a fanout, we need to update all schedulers
        cctxt = self.client.prepare(fanout=True, version='4.1')
        cctxt.cast(ctxt, 'delete_aggregate', aggregate=aggregate)

    def update_instance_info(self, ctxt, host_name, instance_info):
        cctxt = self.client.prepare(version='4.2', fanout=True)
        return cctxt.cast(ctxt, 'update_instance_info', host_name=host_name,
                          instance_info=instance_info)

    def delete_instance_info(self, ctxt, host_name, instance_uuid):
        cctxt = self.client.prepare(version='4.2', fanout=True)
        return cctxt.cast(ctxt, 'delete_instance_info', host_name=host_name,
                          instance_uuid=instance_uuid)

    def sync_instance_info(self, ctxt, host_name, instance_uuids):
        cctxt = self.client.prepare(version='4.2', fanout=True)
        return cctxt.cast(ctxt, 'sync_instance_info', host_name=host_name,
                          instance_uuids=instance_uuids)

# Copyright 2012 Nebula, Inc.
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg

from nova.tests.functional.api_sample_tests import api_sample_base

CONF = cfg.CONF
CONF.import_opt('osapi_compute_extension',
                'nova.api.openstack.compute.legacy_v2.extensions')


class AggregatesSampleJsonTest(api_sample_base.ApiSampleTestBaseV21):
    ADMIN_API = True
    extension_name = "os-aggregates"

    def _get_flags(self):
        f = super(AggregatesSampleJsonTest, self)._get_flags()
        f['osapi_compute_extension'] = CONF.osapi_compute_extension[:]
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.aggregates.Aggregates')
        return f

    def test_aggregate_create(self):
        subs = {
            "aggregate_id": '(?P<id>\d+)'
        }
        response = self._do_post('os-aggregates', 'aggregate-post-req', subs)
        return self._verify_response('aggregate-post-resp',
                                     subs, response, 200)

    def test_list_aggregates(self):
        self.test_aggregate_create()
        response = self._do_get('os-aggregates')
        self._verify_response('aggregates-list-get-resp', {}, response, 200)

    def test_aggregate_get(self):
        agg_id = self.test_aggregate_create()
        response = self._do_get('os-aggregates/%s' % agg_id)
        self._verify_response('aggregates-get-resp', {}, response, 200)

    def test_add_metadata(self):
        agg_id = self.test_aggregate_create()
        response = self._do_post('os-aggregates/%s/action' % agg_id,
                                 'aggregate-metadata-post-req',
                                 {'action': 'set_metadata'})
        self._verify_response('aggregates-metadata-post-resp', {},
                              response, 200)

    def test_add_host(self):
        aggregate_id = self.test_aggregate_create()
        subs = {
            "host_name": self.compute.host,
        }
        response = self._do_post('os-aggregates/%s/action' % aggregate_id,
                                 'aggregate-add-host-post-req', subs)
        self._verify_response('aggregates-add-host-post-resp', subs,
                              response, 200)

    def test_remove_host(self):
        self.test_add_host()
        subs = {
            "host_name": self.compute.host,
        }
        response = self._do_post('os-aggregates/1/action',
                                 'aggregate-remove-host-post-req', subs)
        self._verify_response('aggregates-remove-host-post-resp',
                              subs, response, 200)

    def test_update_aggregate(self):
        aggregate_id = self.test_aggregate_create()
        response = self._do_put('os-aggregates/%s' % aggregate_id,
                                  'aggregate-update-post-req', {})
        self._verify_response('aggregate-update-post-resp',
                              {}, response, 200)

# Copyright 2012 Nebula, Inc.
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg

from nova.tests.functional.api_sample_tests import api_sample_base

CONF = cfg.CONF
CONF.import_opt('osapi_compute_extension',
                'nova.api.openstack.compute.legacy_v2.extensions')


class FlavorsSampleJsonTest(api_sample_base.ApiSampleTestBaseV21):
    sample_dir = 'flavors'

    def _get_flags(self):
        f = super(FlavorsSampleJsonTest, self)._get_flags()
        f['osapi_compute_extension'] = CONF.osapi_compute_extension[:]
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.flavor_swap.Flavor_swap')
        f['osapi_compute_extension'].append('nova.api.openstack.compute.'
                      'contrib.flavor_disabled.Flavor_disabled')
        f['osapi_compute_extension'].append('nova.api.openstack.compute.'
                      'contrib.flavor_access.Flavor_access')
        f['osapi_compute_extension'].append('nova.api.openstack.compute.'
                       'contrib.flavorextradata.Flavorextradata')
        return f

    def test_flavors_get(self):
        response = self._do_get('flavors/1')
        self._verify_response('flavor-get-resp', {}, response, 200)

    def test_flavors_list(self):
        response = self._do_get('flavors')
        self._verify_response('flavors-list-resp', {}, response, 200)

    def test_flavors_detail(self):
        response = self._do_get('flavors/detail')
        self._verify_response('flavors-detail-resp', {}, response, 200)


class FlavorsSampleAllExtensionJsonTest(FlavorsSampleJsonTest):
    all_extensions = True
    sample_dir = None

    def _get_flags(self):
        f = super(FlavorsSampleJsonTest, self)._get_flags()
        f['osapi_compute_extension'] = CONF.osapi_compute_extension[:]
        return f

# Copyright 2012 Nebula, Inc.
# Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from oslo_config import cfg

from nova.tests.functional.api_sample_tests import test_servers


CONF = cfg.CONF
CONF.import_opt('osapi_compute_extension',
                'nova.api.openstack.compute.legacy_v2.extensions')


class RescueJsonTest(test_servers.ServersSampleBase):
    extension_name = "os-rescue"

    def _get_flags(self):
        f = super(RescueJsonTest, self)._get_flags()
        f['osapi_compute_extension'] = CONF.osapi_compute_extension[:]
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.rescue.Rescue')
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.extended_rescue_with_image.'
            'Extended_rescue_with_image')
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.keypairs.Keypairs')
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.extended_ips.Extended_ips')
        f['osapi_compute_extension'].append(
            'nova.api.openstack.compute.contrib.extended_ips_mac.'
            'Extended_ips_mac')
        return f

    def _rescue(self, uuid):
        req_subs = {
            'password': 'MySecretPass'
        }
        response = self._do_post('servers/%s/action' % uuid,
                                 'server-rescue-req', req_subs)
        self._verify_response('server-rescue', req_subs, response, 200)

    def _unrescue(self, uuid):
        response = self._do_post('servers/%s/action' % uuid,
                                 'server-unrescue-req', {})
        self.assertEqual(202, response.status_code)

    def test_server_rescue(self):
        uuid = self._post_server()

        self._rescue(uuid)

        # Do a server get to make sure that the 'RESCUE' state is set
        response = self._do_get('servers/%s' % uuid)
        subs = {}
        subs['hostid'] = '[a-f0-9]+'
        subs['id'] = uuid
        subs['status'] = 'RESCUE'
        subs['access_ip_v4'] = '1.2.3.4'
        subs['access_ip_v6'] = '80fe::'
        self._verify_response('server-get-resp-rescue', subs, response, 200)

    def test_server_rescue_with_image_ref_specified(self):
        uuid = self._post_server()

        req_subs = {
            'password': 'MySecretPass',
            'image_ref': '2341-Abc'
        }
        response = self._do_post('servers/%s/action' % uuid,
                                 'server-rescue-req-with-image-ref', req_subs)
        self._verify_response('server-rescue', req_subs, response, 200)

        # Do a server get to make sure that the 'RESCUE' state is set
        response = self._do_get('servers/%s' % uuid)
        subs = {}
        subs['hostid'] = '[a-f0-9]+'
        subs['id'] = uuid
        subs['status'] = 'RESCUE'
        subs['access_ip_v4'] = '1.2.3.4'
        subs['access_ip_v6'] = '80fe::'
        self._verify_response('server-get-resp-rescue', subs, response, 200)

    def test_server_unrescue(self):
        uuid = self._post_server()

        self._rescue(uuid)
        self._unrescue(uuid)

        # Do a server get to make sure that the 'ACTIVE' state is back
        response = self._do_get('servers/%s' % uuid)
        subs = {}
        subs['hostid'] = '[a-f0-9]+'
        subs['id'] = uuid
        subs['status'] = 'ACTIVE'
        subs['access_ip_v4'] = '1.2.3.4'
        subs['access_ip_v6'] = '80fe::'
        self._verify_response('server-get-resp-unrescue', subs, response, 200)

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import os

from oslo_utils import uuidutils

from nova import context
from nova import exception
from nova import objects
from nova import test
from nova.tests import fixtures


class ConnectionSwitchTestCase(test.TestCase):
    test_filename = 'foo.db'
    fake_conn = 'sqlite:///' + test_filename

    def setUp(self):
        super(ConnectionSwitchTestCase, self).setUp()
        self.addCleanup(self.cleanup)
        # Use a file-based sqlite database so data will persist across new
        # connections
        # The 'main' database connection will stay open, so in-memory is fine
        self.useFixture(fixtures.Database(connection=self.fake_conn))

    def cleanup(self):
        try:
            os.remove(self.test_filename)
        except OSError:
            pass

    def test_connection_switch(self):
        # Make a request context with a cell mapping
        mapping = objects.CellMapping(database_connection=self.fake_conn)
        ctxt = context.RequestContext('fake-user', 'fake-project')
        # Create an instance in the cell database
        uuid = uuidutils.generate_uuid()
        with context.target_cell(ctxt, mapping):
            # Must set project_id because instance get specifies
            # project_only=True to model_query, which means non-admin
            # users can only read instances for their project
            instance = objects.Instance(context=ctxt, uuid=uuid,
                                        project_id='fake-project')
            instance.create()

            # Verify the instance is found in the cell database
            inst = objects.Instance.get_by_uuid(ctxt, uuid)
            self.assertEqual(uuid, inst.uuid)

        # Verify the instance isn't found in the main database
        self.assertRaises(exception.InstanceNotFound,
                          objects.Instance.get_by_uuid, ctxt, uuid)

# Copyright 2013 IBM Corp.
#
#   Licensed under the Apache License, Version 2.0 (the "License"); you may
#   not use this file except in compliance with the License. You may obtain
#   a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#   Unless required by applicable law or agreed to in writing, software
#   distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#   WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#   License for the specific language governing permissions and limitations
#   under the License.

from oslo_utils import timeutils
from oslo_utils import uuidutils
import webob

from nova.compute import vm_states
from nova import exception
from nova import test
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import fake_instance


class CommonMixin(object):
    def setUp(self):
        super(CommonMixin, self).setUp()
        self.compute_api = None
        self.req = fakes.HTTPRequest.blank('')
        self.context = self.req.environ['nova.context']

    def _stub_instance_get(self, uuid=None):
        if uuid is None:
            uuid = uuidutils.generate_uuid()
        instance = fake_instance.fake_instance_obj(self.context,
                id=1, uuid=uuid, vm_state=vm_states.ACTIVE,
                task_state=None, launched_at=timeutils.utcnow())
        self.compute_api.get(self.context, uuid, expected_attrs=None,
                             want_objects=True).AndReturn(instance)
        return instance

    def _stub_instance_get_failure(self, exc_info, uuid=None):
        if uuid is None:
            uuid = uuidutils.generate_uuid()
        self.compute_api.get(self.context, uuid, expected_attrs=None,
                             want_objects=True).AndRaise(exc_info)
        return uuid

    def _test_non_existing_instance(self, action, body_map=None):
        uuid = uuidutils.generate_uuid()
        self._stub_instance_get_failure(
                exception.InstanceNotFound(instance_id=uuid), uuid=uuid)

        self.mox.ReplayAll()
        controller_function = getattr(self.controller, action)
        self.assertRaises(webob.exc.HTTPNotFound,
                          controller_function,
                          self.req, uuid, body=body_map)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()

    def _test_action(self, action, body=None, method=None,
                     compute_api_args_map=None):
        if method is None:
            method = action.replace('_', '')
        compute_api_args_map = compute_api_args_map or {}

        instance = self._stub_instance_get()
        args, kwargs = compute_api_args_map.get(action, ((), {}))
        getattr(self.compute_api, method)(self.context, instance, *args,
                                          **kwargs)

        self.mox.ReplayAll()
        controller_function = getattr(self.controller, action)
        res = controller_function(self.req, instance.uuid, body=body)
        # NOTE: on v2.1, http status code is set as wsgi_code of API
        # method instead of status_int in a response object.
        if self._api_version == '2.1':
            status_int = controller_function.wsgi_code
        else:
            status_int = res.status_int
        self.assertEqual(202, status_int)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()

    def _test_not_implemented_state(self, action, method=None):
        if method is None:
            method = action.replace('_', '')

        instance = self._stub_instance_get()
        body = {}
        compute_api_args_map = {}
        args, kwargs = compute_api_args_map.get(action, ((), {}))
        getattr(self.compute_api, method)(self.context, instance,
                                          *args, **kwargs).AndRaise(
                NotImplementedError())

        self.mox.ReplayAll()
        controller_function = getattr(self.controller, action)
        self.assertRaises(webob.exc.HTTPNotImplemented,
                          controller_function,
                          self.req, instance.uuid, body=body)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()

    def _test_invalid_state(self, action, method=None, body_map=None,
                            compute_api_args_map=None,
                            exception_arg=None):
        if method is None:
            method = action.replace('_', '')
        if body_map is None:
            body_map = {}
        if compute_api_args_map is None:
            compute_api_args_map = {}

        instance = self._stub_instance_get()

        args, kwargs = compute_api_args_map.get(action, ((), {}))

        getattr(self.compute_api, method)(self.context, instance,
                                          *args, **kwargs).AndRaise(
                exception.InstanceInvalidState(
                    attr='vm_state', instance_uuid=instance.uuid,
                    state='foo', method=method))

        self.mox.ReplayAll()
        controller_function = getattr(self.controller, action)
        ex = self.assertRaises(webob.exc.HTTPConflict,
                               controller_function,
                               self.req, instance.uuid,
                               body=body_map)
        self.assertIn("Cannot \'%(action)s\' instance %(id)s"
                      % {'action': exception_arg or method,
                         'id': instance.uuid}, ex.explanation)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()

    def _test_locked_instance(self, action, method=None, body=None,
                              compute_api_args_map=None):
        if method is None:
            method = action.replace('_', '')

        compute_api_args_map = compute_api_args_map or {}
        instance = self._stub_instance_get()

        args, kwargs = compute_api_args_map.get(action, ((), {}))
        getattr(self.compute_api, method)(self.context, instance, *args,
                                          **kwargs).AndRaise(
                exception.InstanceIsLocked(instance_uuid=instance.uuid))

        self.mox.ReplayAll()

        controller_function = getattr(self.controller, action)
        self.assertRaises(webob.exc.HTTPConflict,
                          controller_function,
                          self.req, instance.uuid, body=body)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()

    def _test_instance_not_found_in_compute_api(self, action,
                         method=None, body=None, compute_api_args_map=None):
        if method is None:
            method = action.replace('_', '')
        compute_api_args_map = compute_api_args_map or {}

        instance = self._stub_instance_get()

        args, kwargs = compute_api_args_map.get(action, ((), {}))
        getattr(self.compute_api, method)(self.context, instance, *args,
                                          **kwargs).AndRaise(
                exception.InstanceNotFound(instance_id=instance.uuid))

        self.mox.ReplayAll()

        controller_function = getattr(self.controller, action)
        self.assertRaises(webob.exc.HTTPNotFound,
                          controller_function,
                          self.req, instance.uuid, body=body)
        # Do these here instead of tearDown because this method is called
        # more than once for the same test case
        self.mox.VerifyAll()
        self.mox.UnsetStubs()


class CommonTests(CommonMixin, test.NoDBTestCase):
    def _test_actions(self, actions, method_translations=None, body_map=None,
                      args_map=None):
        method_translations = method_translations or {}
        body_map = body_map or {}
        args_map = args_map or {}
        for action in actions:
            method = method_translations.get(action)
            body = body_map.get(action)
            self.mox.StubOutWithMock(self.compute_api,
                                     method or action.replace('_', ''))
            self._test_action(action, method=method, body=body,
                              compute_api_args_map=args_map)
            # Re-mock this.
            self.mox.StubOutWithMock(self.compute_api, 'get')

    def _test_actions_instance_not_found_in_compute_api(self,
                  actions, method_translations=None, body_map=None,
                  args_map=None):
        method_translations = method_translations or {}
        body_map = body_map or {}
        args_map = args_map or {}
        for action in actions:
            method = method_translations.get(action)
            body = body_map.get(action)
            self.mox.StubOutWithMock(self.compute_api,
                                     method or action.replace('_', ''))
            self._test_instance_not_found_in_compute_api(
                action, method=method, body=body,
                compute_api_args_map=args_map)
            # Re-mock this.
            self.mox.StubOutWithMock(self.compute_api, 'get')

    def _test_actions_with_non_existed_instance(self, actions, body_map=None):
        body_map = body_map or {}
        for action in actions:
            self._test_non_existing_instance(action,
                                             body_map=body_map)
            # Re-mock this.
            self.mox.StubOutWithMock(self.compute_api, 'get')

    def _test_actions_raise_conflict_on_invalid_state(
            self, actions, method_translations=None, body_map=None,
            args_map=None, exception_args=None):
        method_translations = method_translations or {}
        body_map = body_map or {}
        args_map = args_map or {}
        exception_args = exception_args or {}
        for action in actions:
            method = method_translations.get(action)
            exception_arg = exception_args.get(action)
            self.mox.StubOutWithMock(self.compute_api,
                                     method or action.replace('_', ''))
            self._test_invalid_state(action, method=method,
                                     body_map=body_map,
                                     compute_api_args_map=args_map,
                                     exception_arg=exception_arg)
            # Re-mock this.
            self.mox.StubOutWithMock(self.compute_api, 'get')

    def _test_actions_with_locked_instance(self, actions,
                                           method_translations=None,
                                           body_map=None, args_map=None):
        method_translations = method_translations or {}
        body_map = body_map or {}
        args_map = args_map or {}
        for action in actions:
            method = method_translations.get(action)
            body = body_map.get(action)
            self.mox.StubOutWithMock(self.compute_api,
                                     method or action.replace('_', ''))
            self._test_locked_instance(action, method=method, body=body,
                                       compute_api_args_map=args_map)
            # Re-mock this.
            self.mox.StubOutWithMock(self.compute_api, 'get')

# Copyright 2010-2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import base64

from oslo_serialization import jsonutils
import webob

from nova.compute import api as compute_api
from nova import exception
from nova import test
from nova.tests.unit.api.openstack import fakes
from nova.tests import uuidsentinel as uuids

FAKE_UUID = fakes.FAKE_UUID

FAKE_NETWORKS = [('aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa', '10.0.1.12'),
                 ('bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb', '10.0.2.12')]

DUPLICATE_NETWORKS = [('aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa', '10.0.1.12'),
                      ('aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa', '10.0.1.12')]

INVALID_NETWORKS = [('invalid', 'invalid-ip-address')]


def return_security_group_non_existing(context, project_id, group_name):
    raise exception.SecurityGroupNotFoundForProject(project_id=project_id,
                                                 security_group_id=group_name)


def return_security_group_get_by_name(context, project_id, group_name):
    return {'id': 1, 'name': group_name}


def return_security_group_get(context, security_group_id, session):
    return {'id': security_group_id}


def return_instance_add_security_group(context, instance_id,
                                       security_group_id):
    pass


class CreateserverextTest(test.TestCase):
    def setUp(self):
        super(CreateserverextTest, self).setUp()

        self.security_group = None
        self.injected_files = None
        self.networks = None
        self.user_data = None

        def create(*args, **kwargs):
            if 'security_group' in kwargs:
                self.security_group = kwargs['security_group']
            else:
                self.security_group = None
            if 'injected_files' in kwargs:
                self.injected_files = kwargs['injected_files']
            else:
                self.injected_files = None

            if 'requested_networks' in kwargs:
                self.networks = kwargs['requested_networks']
            else:
                self.networks = None

            if 'user_data' in kwargs:
                self.user_data = kwargs['user_data']

            resv_id = None

            return ([{'id': '1234', 'display_name': 'fakeinstance',
                     'uuid': FAKE_UUID,
                     'user_id': 'fake',
                     'project_id': 'fake',
                     'created_at': "",
                     'updated_at': "",
                     'fixed_ips': [],
                     'progress': 0}], resv_id)

        self.stubs.Set(compute_api.API, 'create', create)
        self.flags(
            osapi_compute_extension=[
                'nova.api.openstack.compute.contrib.select_extensions'],
            osapi_compute_ext_list=['Createserverext', 'User_data',
                'Security_groups', 'Os_networks'])

    def _create_security_group_request_dict(self, security_groups):
        server = {}
        server['name'] = 'new-server-test'
        server['imageRef'] = 'cedef40a-ed67-4d10-800e-17455edce175'
        server['flavorRef'] = 1
        if security_groups is not None:
            sg_list = []
            for name in security_groups:
                sg_list.append({'name': name})
            server['security_groups'] = sg_list
        return {'server': server}

    def _create_networks_request_dict(self, networks):
        server = {}
        server['name'] = 'new-server-test'
        server['imageRef'] = 'cedef40a-ed67-4d10-800e-17455edce175'
        server['flavorRef'] = 1
        if networks is not None:
            network_list = []
            for uuid, fixed_ip in networks:
                network_list.append({'uuid': uuid, 'fixed_ip': fixed_ip})
            server['networks'] = network_list
        return {'server': server}

    def _create_user_data_request_dict(self, user_data):
        server = {}
        server['name'] = 'new-server-test'
        server['imageRef'] = 'cedef40a-ed67-4d10-800e-17455edce175'
        server['flavorRef'] = 1
        server['user_data'] = user_data
        return {'server': server}

    def _get_create_request_json(self, body_dict):
        req = webob.Request.blank('/v2/fake/os-create-server-ext')
        req.headers['Content-Type'] = 'application/json'
        req.method = 'POST'
        req.body = jsonutils.dump_as_bytes(body_dict)
        return req

    def _create_instance_with_networks_json(self, networks):
        body_dict = self._create_networks_request_dict(networks)
        request = self._get_create_request_json(body_dict)
        response = request.get_response(fakes.wsgi_app(
            init_only=('servers', 'os-create-server-ext')))
        return request, response, self.networks

    def _create_instance_with_user_data_json(self, networks):
        body_dict = self._create_user_data_request_dict(networks)
        request = self._get_create_request_json(body_dict)
        response = request.get_response(fakes.wsgi_app(
            init_only=('servers', 'os-create-server-ext')))
        return request, response, self.user_data

    def test_create_instance_with_no_networks(self):
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(networks=None)
        self.assertEqual(response.status_int, 202)
        self.assertIsNone(networks)

    def test_create_instance_with_one_network(self):
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst([FAKE_NETWORKS[0]])
        self.assertEqual(response.status_int, 202)
        self.assertEqual([FAKE_NETWORKS[0]], networks.as_tuples())

    def test_create_instance_with_two_networks(self):
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(FAKE_NETWORKS)
        self.assertEqual(response.status_int, 202)
        self.assertEqual(FAKE_NETWORKS, networks.as_tuples())

    def test_create_instance_with_duplicate_networks(self):
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(DUPLICATE_NETWORKS)
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(networks)

    def test_create_instance_with_network_no_id(self):
        body_dict = self._create_networks_request_dict([FAKE_NETWORKS[0]])
        del body_dict['server']['networks'][0]['uuid']
        request = self._get_create_request_json(body_dict)
        response = request.get_response(fakes.wsgi_app(
            init_only=('servers', 'os-create-server-ext')))
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(self.networks)

    def test_create_instance_with_network_invalid_id(self):
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(INVALID_NETWORKS)
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(networks)

    def test_create_instance_with_network_empty_fixed_ip(self):
        networks = [('1', '')]
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(networks)
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(networks)

    def test_create_instance_with_network_non_string_fixed_ip(self):
        networks = [('1', 12345)]
        _create_inst = self._create_instance_with_networks_json
        request, response, networks = _create_inst(networks)
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(networks)

    def test_create_instance_with_network_no_fixed_ip(self):
        body_dict = self._create_networks_request_dict([FAKE_NETWORKS[0]])
        del body_dict['server']['networks'][0]['fixed_ip']
        request = self._get_create_request_json(body_dict)
        response = request.get_response(fakes.wsgi_app(
            init_only=('servers', 'os-create-server-ext')))
        self.assertEqual(response.status_int, 202)
        self.assertEqual([('aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa', None)],
                         self.networks.as_tuples())

    def test_create_instance_with_userdata(self):
        user_data_contents = '#!/bin/bash\necho "Oh no!"\n'
        user_data_contents = base64.b64encode(user_data_contents)
        _create_inst = self._create_instance_with_user_data_json
        request, response, user_data = _create_inst(user_data_contents)
        self.assertEqual(response.status_int, 202)
        self.assertEqual(user_data, user_data_contents)

    def test_create_instance_with_userdata_none(self):
        user_data_contents = None
        _create_inst = self._create_instance_with_user_data_json
        request, response, user_data = _create_inst(user_data_contents)
        self.assertEqual(response.status_int, 202)
        self.assertEqual(user_data, user_data_contents)

    def test_create_instance_with_userdata_with_non_b64_content(self):
        user_data_contents = '#!/bin/bash\necho "Oh no!"\n'
        _create_inst = self._create_instance_with_user_data_json
        request, response, user_data = _create_inst(user_data_contents)
        self.assertEqual(response.status_int, 400)
        self.assertIsNone(user_data)

    def test_create_instance_with_security_group_json(self):
        security_groups = ['test', 'test1']
        self.stub_out('nova.db.security_group_get_by_name',
                      return_security_group_get_by_name)
        self.stub_out('nova.db.instance_add_security_group',
                      return_instance_add_security_group)
        body_dict = self._create_security_group_request_dict(security_groups)
        request = self._get_create_request_json(body_dict)
        response = request.get_response(fakes.wsgi_app(
            init_only=('servers', 'os-create-server-ext')))
        self.assertEqual(response.status_int, 202)
        self.assertJsonEqual(self.security_group, security_groups)

    def test_get_server_by_id_verify_security_groups_json(self):
        self.stub_out('nova.db.instance_get', fakes.fake_instance_get())
        self.stub_out('nova.db.instance_get_by_uuid',
                      fakes.fake_instance_get())
        req = webob.Request.blank('/v2/fake/os-create-server-ext/' +
                                  uuids.server)
        req.headers['Content-Type'] = 'application/json'
        response = req.get_response(fakes.wsgi_app(
            init_only=('os-create-server-ext', 'servers')))
        self.assertEqual(response.status_int, 200)
        res_dict = jsonutils.loads(response.body)
        expected_security_group = [{"name": "test"}]
        self.assertEqual(res_dict['server'].get('security_groups'),
                         expected_security_group)

# Copyright 2011 Grid Dynamics
# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
import webob

from nova.api.openstack.compute import fping as fping_v21
from nova.api.openstack.compute.legacy_v2.contrib import fping
from nova import exception
from nova import test
from nova.tests.unit.api.openstack import fakes
import nova.utils


FAKE_UUID = fakes.FAKE_UUID


def execute(*cmd, **args):
    return "".join(["%s is alive" % ip for ip in cmd[1:]])


class FpingTestV21(test.TestCase):
    controller_cls = fping_v21.FpingController

    def setUp(self):
        super(FpingTestV21, self).setUp()
        self.flags(verbose=True, use_ipv6=False)
        return_server = fakes.fake_instance_get()
        return_servers = fakes.fake_instance_get_all_by_filters()
        self.stub_out("nova.db.instance_get_all_by_filters",
                      return_servers)
        self.stub_out("nova.db.instance_get_by_uuid",
                      return_server)
        self.stubs.Set(nova.utils, "execute",
                       execute)
        self.stubs.Set(self.controller_cls, "check_fping",
                       lambda self: None)
        self.controller = self.controller_cls()

    def _get_url(self):
        return "/v2/1234"

    def test_fping_index(self):
        req = fakes.HTTPRequest.blank(self._get_url() + "/os-fping")
        res_dict = self.controller.index(req)
        self.assertIn("servers", res_dict)
        for srv in res_dict["servers"]:
            for key in "project_id", "id", "alive":
                self.assertIn(key, srv)

    def test_fping_index_policy(self):
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "os-fping?all_tenants=1")
        self.assertRaises(exception.Forbidden, self.controller.index, req)
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "/os-fping?all_tenants=1")
        req.environ["nova.context"].is_admin = True
        res_dict = self.controller.index(req)
        self.assertIn("servers", res_dict)

    def test_fping_index_include(self):
        req = fakes.HTTPRequest.blank(self._get_url() + "/os-fping")
        res_dict = self.controller.index(req)
        ids = [srv["id"] for srv in res_dict["servers"]]
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "/os-fping?include=%s" % ids[0])
        res_dict = self.controller.index(req)
        self.assertEqual(len(res_dict["servers"]), 1)
        self.assertEqual(res_dict["servers"][0]["id"], ids[0])

    def test_fping_index_exclude(self):
        req = fakes.HTTPRequest.blank(self._get_url() + "/os-fping")
        res_dict = self.controller.index(req)
        ids = [srv["id"] for srv in res_dict["servers"]]
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "/os-fping?exclude=%s" %
                                      ",".join(ids[1:]))
        res_dict = self.controller.index(req)
        self.assertEqual(len(res_dict["servers"]), 1)
        self.assertEqual(res_dict["servers"][0]["id"], ids[0])

    def test_fping_show(self):
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "os-fping/%s" % FAKE_UUID)
        res_dict = self.controller.show(req, FAKE_UUID)
        self.assertIn("server", res_dict)
        srv = res_dict["server"]
        for key in "project_id", "id", "alive":
            self.assertIn(key, srv)

    @mock.patch('nova.db.instance_get_by_uuid')
    def test_fping_show_with_not_found(self, mock_get_instance):
        mock_get_instance.side_effect = exception.InstanceNotFound(
            instance_id='')
        req = fakes.HTTPRequest.blank(self._get_url() +
                                      "os-fping/%s" % FAKE_UUID)
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.controller.show, req, FAKE_UUID)


class FpingTestV2(FpingTestV21):
    controller_cls = fping.FpingController


class FpingPolicyEnforcementV21(test.NoDBTestCase):

    def setUp(self):
        super(FpingPolicyEnforcementV21, self).setUp()
        self.controller = fping_v21.FpingController()
        self.req = fakes.HTTPRequest.blank('')

    def common_policy_check(self, rule, func, *arg, **kwarg):
        self.policy.set_rules(rule)
        exc = self.assertRaises(
            exception.PolicyNotAuthorized, func, *arg, **kwarg)
        self.assertEqual(
            "Policy doesn't allow %s to be performed." %
            rule.popitem()[0], exc.format_message())

    def test_list_policy_failed(self):
        rule = {"os_compute_api:os-fping": "project:non_fake"}
        self.common_policy_check(rule, self.controller.index, self.req)

        self.req.GET.update({"all_tenants": "True"})
        rule = {"os_compute_api:os-fping:all_tenants":
                "project:non_fake"}
        self.common_policy_check(rule, self.controller.index, self.req)

    def test_show_policy_failed(self):
        rule = {"os_compute_api:os-fping": "project:non_fake"}
        self.common_policy_check(
            rule, self.controller.show, self.req, FAKE_UUID)

# Copyright 2013 Metacloud, Inc
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
import webob

from nova.api.openstack.compute.legacy_v2.contrib import \
    security_group_default_rules as security_group_default_rules_v2
from nova.api.openstack.compute import \
    security_group_default_rules as security_group_default_rules_v21
from nova import context
import nova.db
from nova import exception
from nova import test
from nova.tests.unit.api.openstack import fakes


class AttrDict(dict):
    def __getattr__(self, k):
        return self[k]


def security_group_default_rule_template(**kwargs):
    rule = kwargs.copy()
    rule.setdefault('ip_protocol', 'TCP')
    rule.setdefault('from_port', 22)
    rule.setdefault('to_port', 22)
    rule.setdefault('cidr', '10.10.10.0/24')
    return rule


def security_group_default_rule_db(security_group_default_rule, id=None):
    attrs = security_group_default_rule.copy()
    if id is not None:
        attrs['id'] = id
    return AttrDict(attrs)


class TestSecurityGroupDefaultRulesNeutronV21(test.TestCase):
    controller_cls = (security_group_default_rules_v21.
                      SecurityGroupDefaultRulesController)

    def setUp(self):
        self.flags(security_group_api='neutron')
        super(TestSecurityGroupDefaultRulesNeutronV21, self).setUp()
        self.controller = self.controller_cls()

    def test_create_security_group_default_rule_not_implemented_neutron(self):
        sgr = security_group_default_rule_template()
        req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules', use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotImplemented, self.controller.create,
                          req, {'security_group_default_rule': sgr})

    def test_security_group_default_rules_list_not_implemented_neutron(self):
        req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules', use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotImplemented, self.controller.index,
                          req)

    def test_security_group_default_rules_show_not_implemented_neutron(self):
        req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules', use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotImplemented, self.controller.show,
                          req, '602ed77c-a076-4f9b-a617-f93b847b62c5')

    def test_security_group_default_rules_delete_not_implemented_neutron(self):
        req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules', use_admin_context=True)
        self.assertRaises(webob.exc.HTTPNotImplemented, self.controller.delete,
                          req, '602ed77c-a076-4f9b-a617-f93b847b62c5')


class TestSecurityGroupDefaultRulesNeutronV2(test.TestCase):
    controller_cls = (security_group_default_rules_v2.
                      SecurityGroupDefaultRulesController)


class TestSecurityGroupDefaultRulesV21(test.TestCase):
    controller_cls = (security_group_default_rules_v21.
                      SecurityGroupDefaultRulesController)

    def setUp(self):
        super(TestSecurityGroupDefaultRulesV21, self).setUp()
        self.controller = self.controller_cls()
        self.req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules')

    def test_create_security_group_default_rule(self):
        sgr = security_group_default_rule_template()

        sgr_dict = dict(security_group_default_rule=sgr)
        res_dict = self.controller.create(self.req, sgr_dict)
        security_group_default_rule = res_dict['security_group_default_rule']
        self.assertEqual(security_group_default_rule['ip_protocol'],
                         sgr['ip_protocol'])
        self.assertEqual(security_group_default_rule['from_port'],
                         sgr['from_port'])
        self.assertEqual(security_group_default_rule['to_port'],
                         sgr['to_port'])
        self.assertEqual(security_group_default_rule['ip_range']['cidr'],
                         sgr['cidr'])

    def test_create_security_group_default_rule_with_no_to_port(self):
        sgr = security_group_default_rule_template()
        del sgr['to_port']

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_no_from_port(self):
        sgr = security_group_default_rule_template()
        del sgr['from_port']

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_no_ip_protocol(self):
        sgr = security_group_default_rule_template()
        del sgr['ip_protocol']

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_no_cidr(self):
        sgr = security_group_default_rule_template()
        del sgr['cidr']

        res_dict = self.controller.create(self.req,
                                          {'security_group_default_rule': sgr})
        security_group_default_rule = res_dict['security_group_default_rule']
        self.assertNotEqual(security_group_default_rule['id'], 0)
        self.assertEqual(security_group_default_rule['ip_range']['cidr'],
                         '0.0.0.0/0')

    def test_create_security_group_default_rule_with_blank_to_port(self):
        sgr = security_group_default_rule_template(to_port='')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_blank_from_port(self):
        sgr = security_group_default_rule_template(from_port='')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_blank_ip_protocol(self):
        sgr = security_group_default_rule_template(ip_protocol='')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_blank_cidr(self):
        sgr = security_group_default_rule_template(cidr='')

        res_dict = self.controller.create(self.req,
                                          {'security_group_default_rule': sgr})
        security_group_default_rule = res_dict['security_group_default_rule']
        self.assertNotEqual(security_group_default_rule['id'], 0)
        self.assertEqual(security_group_default_rule['ip_range']['cidr'],
                         '0.0.0.0/0')

    def test_create_security_group_default_rule_non_numerical_to_port(self):
        sgr = security_group_default_rule_template(to_port='invalid')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_non_numerical_from_port(self):
        sgr = security_group_default_rule_template(from_port='invalid')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_invalid_ip_protocol(self):
        sgr = security_group_default_rule_template(ip_protocol='invalid')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_invalid_cidr(self):
        sgr = security_group_default_rule_template(cidr='10.10.2222.0/24')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_invalid_to_port(self):
        sgr = security_group_default_rule_template(to_port='666666')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_invalid_from_port(self):
        sgr = security_group_default_rule_template(from_port='666666')

        self.assertRaises(webob.exc.HTTPBadRequest, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_create_security_group_default_rule_with_no_body(self):
        self.assertRaises(webob.exc.HTTPBadRequest,
                          self.controller.create, self.req, None)

    def test_create_duplicate_security_group_default_rule(self):
        sgr = security_group_default_rule_template()

        self.controller.create(self.req, {'security_group_default_rule': sgr})

        self.assertRaises(webob.exc.HTTPConflict, self.controller.create,
                          self.req, {'security_group_default_rule': sgr})

    def test_security_group_default_rules_list(self):
        self.test_create_security_group_default_rule()
        rules = [dict(id=1,
        ip_protocol='TCP',
        from_port=22,
        to_port=22,
        ip_range=dict(cidr='10.10.10.0/24'))]
        expected = {'security_group_default_rules': rules}

        res_dict = self.controller.index(self.req)
        self.assertEqual(res_dict, expected)

    @mock.patch('nova.db.security_group_default_rule_list',
                side_effect=(exception.
                    SecurityGroupDefaultRuleNotFound("Rule Not Found")))
    def test_non_existing_security_group_default_rules_list(self,
                                                            mock_sec_grp_rule):
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.controller.index, self.req)

    def test_default_security_group_default_rule_show(self):
        sgr = security_group_default_rule_template(id=1)

        self.test_create_security_group_default_rule()

        res_dict = self.controller.show(self.req, '1')

        security_group_default_rule = res_dict['security_group_default_rule']

        self.assertEqual(security_group_default_rule['ip_protocol'],
                         sgr['ip_protocol'])
        self.assertEqual(security_group_default_rule['to_port'],
                         sgr['to_port'])
        self.assertEqual(security_group_default_rule['from_port'],
                         sgr['from_port'])
        self.assertEqual(security_group_default_rule['ip_range']['cidr'],
                         sgr['cidr'])

    @mock.patch('nova.db.security_group_default_rule_get',
                side_effect=(exception.
                    SecurityGroupDefaultRuleNotFound("Rule Not Found")))
    def test_non_existing_security_group_default_rule_show(self,
                                                           mock_sec_grp_rule):
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.controller.show, self.req, '1')

    def test_delete_security_group_default_rule(self):
        sgr = security_group_default_rule_template(id=1)

        self.test_create_security_group_default_rule()

        self.called = False

        def security_group_default_rule_destroy(context, id):
            self.called = True

        def return_security_group_default_rule(context, id):
            self.assertEqual(sgr['id'], id)
            return security_group_default_rule_db(sgr)

        self.stub_out('nova.db.security_group_default_rule_destroy',
                      security_group_default_rule_destroy)
        self.stub_out('nova.db.security_group_default_rule_get',
                      return_security_group_default_rule)

        self.controller.delete(self.req, '1')

        self.assertTrue(self.called)

    @mock.patch('nova.db.security_group_default_rule_destroy',
                side_effect=(exception.
                    SecurityGroupDefaultRuleNotFound("Rule Not Found")))
    def test_non_existing_security_group_default_rule_delete(
            self, mock_sec_grp_rule):
        self.assertRaises(webob.exc.HTTPNotFound,
                          self.controller.delete, self.req, '1')

    def test_security_group_ensure_default(self):
        sgr = security_group_default_rule_template(id=1)
        self.test_create_security_group_default_rule()

        ctxt = context.get_admin_context()

        setattr(ctxt, 'project_id', 'new_project_id')

        sg = nova.db.security_group_ensure_default(ctxt)
        rules = nova.db.security_group_rule_get_by_security_group(ctxt, sg.id)
        security_group_rule = rules[0]
        self.assertEqual(sgr['id'], security_group_rule.id)
        self.assertEqual(sgr['ip_protocol'], security_group_rule.protocol)
        self.assertEqual(sgr['from_port'], security_group_rule.from_port)
        self.assertEqual(sgr['to_port'], security_group_rule.to_port)
        self.assertEqual(sgr['cidr'], security_group_rule.cidr)


class TestSecurityGroupDefaultRulesV2(test.TestCase):
    controller_cls = (security_group_default_rules_v2.
                      SecurityGroupDefaultRulesController)

    def setUp(self):
        super(TestSecurityGroupDefaultRulesV2, self).setUp()
        self.req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules', use_admin_context=True)
        self.non_admin_req = fakes.HTTPRequest.blank(
            '/v2/fake/os-security-group-default-rules')

    def test_create_security_group_default_rules_with_non_admin(self):
        self.controller = self.controller_cls()
        sgr = security_group_default_rule_template()
        sgr_dict = dict(security_group_default_rule=sgr)
        self.assertRaises(exception.AdminRequired, self.controller.create,
                          self.non_admin_req, sgr_dict)

    def test_delete_security_group_default_rules_with_non_admin(self):
        self.controller = self.controller_cls()
        self.assertRaises(exception.AdminRequired,
                          self.controller.delete, self.non_admin_req, 1)


class SecurityGroupDefaultRulesPolicyEnforcementV21(test.NoDBTestCase):

    def setUp(self):
        super(SecurityGroupDefaultRulesPolicyEnforcementV21, self).setUp()
        self.controller = (security_group_default_rules_v21.
                           SecurityGroupDefaultRulesController())
        self.req = fakes.HTTPRequest.blank('')

    def _common_policy_check(self, func, *arg, **kwarg):
        rule_name = "os_compute_api:os-security-group-default-rules"
        rule = {rule_name: "project:non_fake"}
        self.policy.set_rules(rule)
        exc = self.assertRaises(
            exception.PolicyNotAuthorized, func, *arg, **kwarg)
        self.assertEqual(
            "Policy doesn't allow %s to be performed." %
            rule_name, exc.format_message())

    def test_create_policy_failed(self):
        self._common_policy_check(self.controller.create, self.req, {})

    def test_show_policy_failed(self):
        self._common_policy_check(
            self.controller.show, self.req, fakes.FAKE_UUID)

    def test_delete_policy_failed(self):
        self._common_policy_check(
            self.controller.delete, self.req, fakes.FAKE_UUID)

    def test_index_policy_failed(self):
        self._common_policy_check(self.controller.index, self.req)

# Copyright 2010 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Test suites for 'common' code used throughout the OpenStack HTTP API.
"""

import mock
import six
from testtools import matchers
import webob
import webob.exc
import webob.multidict

from nova.api.openstack import common
from nova.compute import task_states
from nova.compute import vm_states
from nova import exception
from nova import test
from nova.tests.unit.api.openstack import fakes
from nova.tests.unit import utils


NS = "{http://docs.openstack.org/compute/api/v1.1}"
ATOMNS = "{http://www.w3.org/2005/Atom}"


class LimiterTest(test.NoDBTestCase):
    """Unit tests for the `nova.api.openstack.common.limited` method which
    takes in a list of items and, depending on the 'offset' and 'limit' GET
    params, returns a subset or complete set of the given items.
    """

    def setUp(self):
        """Run before each test."""
        super(LimiterTest, self).setUp()
        self.tiny = range(1)
        self.small = range(10)
        self.medium = range(1000)
        self.large = range(10000)

    def test_limiter_offset_zero(self):
        # Test offset key works with 0.
        req = webob.Request.blank('/?offset=0')
        self.assertEqual(common.limited(self.tiny, req), self.tiny)
        self.assertEqual(common.limited(self.small, req), self.small)
        self.assertEqual(common.limited(self.medium, req), self.medium)
        self.assertEqual(common.limited(self.large, req), self.large[:1000])

    def test_limiter_offset_medium(self):
        # Test offset key works with a medium sized number.
        req = webob.Request.blank('/?offset=10')
        self.assertEqual(0, len(common.limited(self.tiny, req)))
        self.assertEqual(common.limited(self.small, req), self.small[10:])
        self.assertEqual(common.limited(self.medium, req), self.medium[10:])
        self.assertEqual(common.limited(self.large, req), self.large[10:1010])

    def test_limiter_offset_over_max(self):
        # Test offset key works with a number over 1000 (max_limit).
        req = webob.Request.blank('/?offset=1001')
        self.assertEqual(0, len(common.limited(self.tiny, req)))
        self.assertEqual(0, len(common.limited(self.small, req)))
        self.assertEqual(0, len(common.limited(self.medium, req)))
        self.assertEqual(
            common.limited(self.large, req), self.large[1001:2001])

    def test_limiter_offset_blank(self):
        # Test offset key works with a blank offset.
        req = webob.Request.blank('/?offset=')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.limited, self.tiny, req)

    def test_limiter_offset_bad(self):
        # Test offset key works with a BAD offset.
        req = webob.Request.blank(u'/?offset=\u0020aa')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.limited, self.tiny, req)

    def test_limiter_nothing(self):
        # Test request with no offset or limit.
        req = webob.Request.blank('/')
        self.assertEqual(common.limited(self.tiny, req), self.tiny)
        self.assertEqual(common.limited(self.small, req), self.small)
        self.assertEqual(common.limited(self.medium, req), self.medium)
        self.assertEqual(common.limited(self.large, req), self.large[:1000])

    def test_limiter_limit_zero(self):
        # Test limit of zero.
        req = webob.Request.blank('/?limit=0')
        self.assertEqual(common.limited(self.tiny, req), self.tiny)
        self.assertEqual(common.limited(self.small, req), self.small)
        self.assertEqual(common.limited(self.medium, req), self.medium)
        self.assertEqual(common.limited(self.large, req), self.large[:1000])

    def test_limiter_limit_medium(self):
        # Test limit of 10.
        req = webob.Request.blank('/?limit=10')
        self.assertEqual(common.limited(self.tiny, req), self.tiny)
        self.assertEqual(common.limited(self.small, req), self.small)
        self.assertEqual(common.limited(self.medium, req), self.medium[:10])
        self.assertEqual(common.limited(self.large, req), self.large[:10])

    def test_limiter_limit_over_max(self):
        # Test limit of 3000.
        req = webob.Request.blank('/?limit=3000')
        self.assertEqual(common.limited(self.tiny, req), self.tiny)
        self.assertEqual(common.limited(self.small, req), self.small)
        self.assertEqual(common.limited(self.medium, req), self.medium)
        self.assertEqual(common.limited(self.large, req), self.large[:1000])

    def test_limiter_limit_and_offset(self):
        # Test request with both limit and offset.
        items = range(2000)
        req = webob.Request.blank('/?offset=1&limit=3')
        self.assertEqual(common.limited(items, req), items[1:4])
        req = webob.Request.blank('/?offset=3&limit=0')
        self.assertEqual(common.limited(items, req), items[3:1003])
        req = webob.Request.blank('/?offset=3&limit=1500')
        self.assertEqual(common.limited(items, req), items[3:1003])
        req = webob.Request.blank('/?offset=3000&limit=10')
        self.assertEqual(0, len(common.limited(items, req)))

    def test_limiter_custom_max_limit(self):
        # Test a max_limit other than 1000.
        items = range(2000)
        req = webob.Request.blank('/?offset=1&limit=3')
        self.assertEqual(
            common.limited(items, req, max_limit=2000), items[1:4])
        req = webob.Request.blank('/?offset=3&limit=0')
        self.assertEqual(
            common.limited(items, req, max_limit=2000), items[3:])
        req = webob.Request.blank('/?offset=3&limit=2500')
        self.assertEqual(
            common.limited(items, req, max_limit=2000), items[3:])
        req = webob.Request.blank('/?offset=3000&limit=10')
        self.assertEqual(0, len(common.limited(items, req, max_limit=2000)))

    def test_limiter_negative_limit(self):
        # Test a negative limit.
        req = webob.Request.blank('/?limit=-3000')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.limited, self.tiny, req)

    def test_limiter_negative_offset(self):
        # Test a negative offset.
        req = webob.Request.blank('/?offset=-30')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.limited, self.tiny, req)


class SortParamUtilsTest(test.NoDBTestCase):

    def test_get_sort_params_defaults(self):
        '''Verifies the default sort key and direction.'''
        sort_keys, sort_dirs = common.get_sort_params({})
        self.assertEqual(['created_at'], sort_keys)
        self.assertEqual(['desc'], sort_dirs)

    def test_get_sort_params_override_defaults(self):
        '''Verifies that the defaults can be overriden.'''
        sort_keys, sort_dirs = common.get_sort_params({}, default_key='key1',
                                                      default_dir='dir1')
        self.assertEqual(['key1'], sort_keys)
        self.assertEqual(['dir1'], sort_dirs)

        sort_keys, sort_dirs = common.get_sort_params({}, default_key=None,
                                                      default_dir=None)
        self.assertEqual([], sort_keys)
        self.assertEqual([], sort_dirs)

    def test_get_sort_params_single_value(self):
        '''Verifies a single sort key and direction.'''
        params = webob.multidict.MultiDict()
        params.add('sort_key', 'key1')
        params.add('sort_dir', 'dir1')
        sort_keys, sort_dirs = common.get_sort_params(params)
        self.assertEqual(['key1'], sort_keys)
        self.assertEqual(['dir1'], sort_dirs)

    def test_get_sort_params_single_with_default(self):
        '''Verifies a single sort value with a default.'''
        params = webob.multidict.MultiDict()
        params.add('sort_key', 'key1')
        sort_keys, sort_dirs = common.get_sort_params(params)
        self.assertEqual(['key1'], sort_keys)
        # sort_key was supplied, sort_dir should be defaulted
        self.assertEqual(['desc'], sort_dirs)

        params = webob.multidict.MultiDict()
        params.add('sort_dir', 'dir1')
        sort_keys, sort_dirs = common.get_sort_params(params)
        self.assertEqual(['created_at'], sort_keys)
        # sort_dir was supplied, sort_key should be defaulted
        self.assertEqual(['dir1'], sort_dirs)

    def test_get_sort_params_multiple_values(self):
        '''Verifies multiple sort parameter values.'''
        params = webob.multidict.MultiDict()
        params.add('sort_key', 'key1')
        params.add('sort_key', 'key2')
        params.add('sort_key', 'key3')
        params.add('sort_dir', 'dir1')
        params.add('sort_dir', 'dir2')
        params.add('sort_dir', 'dir3')
        sort_keys, sort_dirs = common.get_sort_params(params)
        self.assertEqual(['key1', 'key2', 'key3'], sort_keys)
        self.assertEqual(['dir1', 'dir2', 'dir3'], sort_dirs)
        # Also ensure that the input parameters are not modified
        sort_key_vals = []
        sort_dir_vals = []
        while 'sort_key' in params:
            sort_key_vals.append(params.pop('sort_key'))
        while 'sort_dir' in params:
            sort_dir_vals.append(params.pop('sort_dir'))
        self.assertEqual(['key1', 'key2', 'key3'], sort_key_vals)
        self.assertEqual(['dir1', 'dir2', 'dir3'], sort_dir_vals)
        self.assertEqual(0, len(params))


class PaginationParamsTest(test.NoDBTestCase):
    """Unit tests for the `nova.api.openstack.common.get_pagination_params`
    method which takes in a request object and returns 'marker' and 'limit'
    GET params.
    """

    def test_no_params(self):
        # Test no params.
        req = webob.Request.blank('/')
        self.assertEqual(common.get_pagination_params(req), {})

    def test_valid_marker(self):
        # Test valid marker param.
        req = webob.Request.blank(
                '/?marker=263abb28-1de6-412f-b00b-f0ee0c4333c2')
        self.assertEqual(common.get_pagination_params(req),
                         {'marker': '263abb28-1de6-412f-b00b-f0ee0c4333c2'})

    def test_valid_limit(self):
        # Test valid limit param.
        req = webob.Request.blank('/?limit=10')
        self.assertEqual(common.get_pagination_params(req), {'limit': 10})

    def test_invalid_limit(self):
        # Test invalid limit param.
        req = webob.Request.blank('/?limit=-2')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.get_pagination_params, req)

    def test_valid_limit_and_marker(self):
        # Test valid limit and marker parameters.
        marker = '263abb28-1de6-412f-b00b-f0ee0c4333c2'
        req = webob.Request.blank('/?limit=20&marker=%s' % marker)
        self.assertEqual(common.get_pagination_params(req),
                         {'marker': marker, 'limit': 20})

    def test_valid_page_size(self):
        # Test valid page_size param.
        req = webob.Request.blank('/?page_size=10')
        self.assertEqual(common.get_pagination_params(req),
                         {'page_size': 10})

    def test_invalid_page_size(self):
        # Test invalid page_size param.
        req = webob.Request.blank('/?page_size=-2')
        self.assertRaises(
            webob.exc.HTTPBadRequest, common.get_pagination_params, req)

    def test_valid_limit_and_page_size(self):
        # Test valid limit and page_size parameters.
        req = webob.Request.blank('/?limit=20&page_size=5')
        self.assertEqual(common.get_pagination_params(req),
                         {'page_size': 5, 'limit': 20})


class MiscFunctionsTest(test.TestCase):

    def test_remove_trailing_version_from_href(self):
        fixture = 'http://www.testsite.com/v1.1'
        expected = 'http://www.testsite.com'
        actual = common.remove_trailing_version_from_href(fixture)
        self.assertEqual(actual, expected)

    def test_remove_trailing_version_from_href_2(self):
        fixture = 'http://www.testsite.com/compute/v1.1'
        expected = 'http://www.testsite.com/compute'
        actual = common.remove_trailing_version_from_href(fixture)
        self.assertEqual(actual, expected)

    def test_remove_trailing_version_from_href_3(self):
        fixture = 'http://www.testsite.com/v1.1/images/v10.5'
        expected = 'http://www.testsite.com/v1.1/images'
        actual = common.remove_trailing_version_from_href(fixture)
        self.assertEqual(actual, expected)

    def test_remove_trailing_version_from_href_bad_request(self):
        fixture = 'http://www.testsite.com/v1.1/images'
        self.assertRaises(ValueError,
                          common.remove_trailing_version_from_href,
                          fixture)

    def test_remove_trailing_version_from_href_bad_request_2(self):
        fixture = 'http://www.testsite.com/images/v'
        self.assertRaises(ValueError,
                          common.remove_trailing_version_from_href,
                          fixture)

    def test_remove_trailing_version_from_href_bad_request_3(self):
        fixture = 'http://www.testsite.com/v1.1images'
        self.assertRaises(ValueError,
                          common.remove_trailing_version_from_href,
                          fixture)

    def test_get_id_from_href_with_int_url(self):
        fixture = 'http://www.testsite.com/dir/45'
        actual = common.get_id_from_href(fixture)
        expected = '45'
        self.assertEqual(actual, expected)

    def test_get_id_from_href_with_int(self):
        fixture = '45'
        actual = common.get_id_from_href(fixture)
        expected = '45'
        self.assertEqual(actual, expected)

    def test_get_id_from_href_with_int_url_query(self):
        fixture = 'http://www.testsite.com/dir/45?asdf=jkl'
        actual = common.get_id_from_href(fixture)
        expected = '45'
        self.assertEqual(actual, expected)

    def test_get_id_from_href_with_uuid_url(self):
        fixture = 'http://www.testsite.com/dir/abc123'
        actual = common.get_id_from_href(fixture)
        expected = "abc123"
        self.assertEqual(actual, expected)

    def test_get_id_from_href_with_uuid_url_query(self):
        fixture = 'http://www.testsite.com/dir/abc123?asdf=jkl'
        actual = common.get_id_from_href(fixture)
        expected = "abc123"
        self.assertEqual(actual, expected)

    def test_get_id_from_href_with_uuid(self):
        fixture = 'abc123'
        actual = common.get_id_from_href(fixture)
        expected = 'abc123'
        self.assertEqual(actual, expected)

    def test_raise_http_conflict_for_instance_invalid_state(self):
        exc = exception.InstanceInvalidState(attr='fake_attr',
                state='fake_state', method='fake_method',
                instance_uuid='fake')
        try:
            common.raise_http_conflict_for_instance_invalid_state(exc,
                    'meow', 'fake_server_id')
        except webob.exc.HTTPConflict as e:
            self.assertEqual(six.text_type(e),
                "Cannot 'meow' instance fake_server_id while it is in "
                "fake_attr fake_state")
        else:
            self.fail("webob.exc.HTTPConflict was not raised")

    def test_check_img_metadata_properties_quota_valid_metadata(self):
        ctxt = utils.get_test_admin_context()
        metadata1 = {"key": "value"}
        actual = common.check_img_metadata_properties_quota(ctxt, metadata1)
        self.assertIsNone(actual)

        metadata2 = {"key": "v" * 260}
        actual = common.check_img_metadata_properties_quota(ctxt, metadata2)
        self.assertIsNone(actual)

        metadata3 = {"key": ""}
        actual = common.check_img_metadata_properties_quota(ctxt, metadata3)
        self.assertIsNone(actual)

    def test_check_img_metadata_properties_quota_inv_metadata(self):
        ctxt = utils.get_test_admin_context()
        metadata1 = {"a" * 260: "value"}
        self.assertRaises(webob.exc.HTTPBadRequest,
                common.check_img_metadata_properties_quota, ctxt, metadata1)

        metadata2 = {"": "value"}
        self.assertRaises(webob.exc.HTTPBadRequest,
                common.check_img_metadata_properties_quota, ctxt, metadata2)

        metadata3 = "invalid metadata"
        self.assertRaises(webob.exc.HTTPBadRequest,
                common.check_img_metadata_properties_quota, ctxt, metadata3)

        metadata4 = None
        self.assertIsNone(common.check_img_metadata_properties_quota(ctxt,
                                                        metadata4))
        metadata5 = {}
        self.assertIsNone(common.check_img_metadata_properties_quota(ctxt,
                                                        metadata5))

    def test_status_from_state(self):
        for vm_state in (vm_states.ACTIVE, vm_states.STOPPED):
            for task_state in (task_states.RESIZE_PREP,
                               task_states.RESIZE_MIGRATING,
                               task_states.RESIZE_MIGRATED,
                               task_states.RESIZE_FINISH):
                actual = common.status_from_state(vm_state, task_state)
                expected = 'RESIZE'
                self.assertEqual(expected, actual)

    def test_status_rebuild_from_state(self):
        for vm_state in (vm_states.ACTIVE, vm_states.STOPPED,
                         vm_states.ERROR):
            for task_state in (task_states.REBUILDING,
                               task_states.REBUILD_BLOCK_DEVICE_MAPPING,
                               task_states.REBUILD_SPAWNING):
                actual = common.status_from_state(vm_state, task_state)
                expected = 'REBUILD'
                self.assertEqual(expected, actual)

    def test_status_migrating_from_state(self):
        for vm_state in (vm_states.ACTIVE, vm_states.PAUSED):
            task_state = task_states.MIGRATING
            actual = common.status_from_state(vm_state, task_state)
            expected = 'MIGRATING'
            self.assertEqual(expected, actual)

    def test_task_and_vm_state_from_status(self):
        fixture1 = ['reboot']
        actual = common.task_and_vm_state_from_status(fixture1)
        expected = [vm_states.ACTIVE], [task_states.REBOOT_PENDING,
                                        task_states.REBOOT_STARTED,
                                        task_states.REBOOTING]
        self.assertEqual(expected, actual)

        fixture2 = ['resize']
        actual = common.task_and_vm_state_from_status(fixture2)
        expected = ([vm_states.ACTIVE, vm_states.STOPPED],
                    [task_states.RESIZE_FINISH,
                     task_states.RESIZE_MIGRATED,
                     task_states.RESIZE_MIGRATING,
                     task_states.RESIZE_PREP])
        self.assertEqual(expected, actual)

        fixture3 = ['resize', 'reboot']
        actual = common.task_and_vm_state_from_status(fixture3)
        expected = ([vm_states.ACTIVE, vm_states.STOPPED],
                    [task_states.REBOOT_PENDING,
                     task_states.REBOOT_STARTED,
                     task_states.REBOOTING,
                     task_states.RESIZE_FINISH,
                     task_states.RESIZE_MIGRATED,
                     task_states.RESIZE_MIGRATING,
                     task_states.RESIZE_PREP])
        self.assertEqual(expected, actual)

    def test_is_all_tenants_true(self):
        for value in ('', '1', 'true', 'True'):
            search_opts = {'all_tenants': value}
            self.assertTrue(common.is_all_tenants(search_opts))
            self.assertIn('all_tenants', search_opts)

    def test_is_all_tenants_false(self):
        for value in ('0', 'false', 'False'):
            search_opts = {'all_tenants': value}
            self.assertFalse(common.is_all_tenants(search_opts))
            self.assertIn('all_tenants', search_opts)

    def test_is_all_tenants_missing(self):
        self.assertFalse(common.is_all_tenants({}))

    def test_is_all_tenants_invalid(self):
        search_opts = {'all_tenants': 'wonk'}
        self.assertRaises(exception.InvalidInput, common.is_all_tenants,
                          search_opts)


class TestCollectionLinks(test.NoDBTestCase):
    """Tests the _get_collection_links method."""

    @mock.patch('nova.api.openstack.common.ViewBuilder._get_next_link')
    def test_items_less_than_limit(self, href_link_mock):
        items = [
            {"uuid": "123"}
        ]
        req = mock.MagicMock()
        params = mock.PropertyMock(return_value=dict(limit=10))
        type(req).params = params

        builder = common.ViewBuilder()
        results = builder._get_collection_links(req, items, "ignored", "uuid")

        self.assertFalse(href_link_mock.called)
        self.assertThat(results, matchers.HasLength(0))

    @mock.patch('nova.api.openstack.common.ViewBuilder._get_next_link')
    def test_items_equals_given_limit(self, href_link_mock):
        items = [
            {"uuid": "123"}
        ]
        req = mock.MagicMock()
        params = mock.PropertyMock(return_value=dict(limit=1))
        type(req).params = params

        builder = common.ViewBuilder()
        results = builder._get_collection_links(req, items,
                                                mock.sentinel.coll_key,
                                                "uuid")

        href_link_mock.assert_called_once_with(req, "123",
                                               mock.sentinel.coll_key)
        self.assertThat(results, matchers.HasLength(1))

    @mock.patch('nova.api.openstack.common.ViewBuilder._get_next_link')
    def test_items_equals_default_limit(self, href_link_mock):
        items = [
            {"uuid": "123"}
        ]
        req = mock.MagicMock()
        params = mock.PropertyMock(return_value=dict())
        type(req).params = params
        self.flags(osapi_max_limit=1)

        builder = common.ViewBuilder()
        results = builder._get_collection_links(req, items,
                                                mock.sentinel.coll_key,
                                                "uuid")

        href_link_mock.assert_called_once_with(req, "123",
                                               mock.sentinel.coll_key)
        self.assertThat(results, matchers.HasLength(1))

    @mock.patch('nova.api.openstack.common.ViewBuilder._get_next_link')
    def test_items_equals_default_limit_with_given(self, href_link_mock):
        items = [
            {"uuid": "123"}
        ]
        req = mock.MagicMock()
        # Given limit is greater than default max, only return default max
        params = mock.PropertyMock(return_value=dict(limit=2))
        type(req).params = params
        self.flags(osapi_max_limit=1)

        builder = common.ViewBuilder()
        results = builder._get_collection_links(req, items,
                                                mock.sentinel.coll_key,
                                                "uuid")

        href_link_mock.assert_called_once_with(req, "123",
                                               mock.sentinel.coll_key)
        self.assertThat(results, matchers.HasLength(1))


class LinkPrefixTest(test.NoDBTestCase):

    def test_update_link_prefix(self):
        vb = common.ViewBuilder()
        result = vb._update_link_prefix("http://192.168.0.243:24/",
                                        "http://127.0.0.1/compute")
        self.assertEqual("http://127.0.0.1/compute", result)

        result = vb._update_link_prefix("http://foo.x.com/v1",
                                        "http://new.prefix.com")
        self.assertEqual("http://new.prefix.com/v1", result)

        result = vb._update_link_prefix(
                "http://foo.x.com/v1",
                "http://new.prefix.com:20455/new_extra_prefix")
        self.assertEqual("http://new.prefix.com:20455/new_extra_prefix/v1",
                         result)


class UrlJoinTest(test.NoDBTestCase):
    def test_url_join(self):
        pieces = ["one", "two", "three"]
        joined = common.url_join(*pieces)
        self.assertEqual("one/two/three", joined)

    def test_url_join_extra_slashes(self):
        pieces = ["one/", "/two//", "/three/"]
        joined = common.url_join(*pieces)
        self.assertEqual("one/two/three", joined)

    def test_url_join_trailing_slash(self):
        pieces = ["one", "two", "three", ""]
        joined = common.url_join(*pieces)
        self.assertEqual("one/two/three/", joined)

    def test_url_join_empty_list(self):
        pieces = []
        joined = common.url_join(*pieces)
        self.assertEqual("", joined)

    def test_url_join_single_empty_string(self):
        pieces = [""]
        joined = common.url_join(*pieces)
        self.assertEqual("", joined)

    def test_url_join_single_slash(self):
        pieces = ["/"]
        joined = common.url_join(*pieces)
        self.assertEqual("", joined)


class ViewBuilderLinkTest(test.NoDBTestCase):
    project_id = "fake"
    api_version = "2.1"

    def setUp(self):
        super(ViewBuilderLinkTest, self).setUp()
        self.request = self.req("/%s" % self.project_id)
        self.vb = common.ViewBuilder()

    def req(self, url, use_admin_context=False):
        return fakes.HTTPRequest.blank(url,
                use_admin_context=use_admin_context, version=self.api_version)

    def test_get_project_id(self):
        proj_id = self.vb._get_project_id(self.request)
        self.assertEqual(self.project_id, proj_id)

    def test_get_next_link(self):
        identifier = "identifier"
        collection = "collection"
        next_link = self.vb._get_next_link(self.request, identifier,
                                           collection)
        expected = "/".join((self.request.url,
                             "%s?marker=%s" % (collection, identifier)))
        self.assertEqual(expected, next_link)

    def test_get_href_link(self):
        identifier = "identifier"
        collection = "collection"
        href_link = self.vb._get_href_link(self.request, identifier,
                                           collection)
        expected = "/".join((self.request.url, collection, identifier))
        self.assertEqual(expected, href_link)

    def test_get_bookmark_link(self):
        identifier = "identifier"
        collection = "collection"
        bookmark_link = self.vb._get_bookmark_link(self.request, identifier,
                                                   collection)
        bmk_url = common.remove_trailing_version_from_href(
                self.request.application_url)
        expected = "/".join((bmk_url, self.project_id, collection, identifier))
        self.assertEqual(expected, bookmark_link)

# Copyright 2013 Intel Corporation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Tests for resource monitors."""

import mock

from nova.compute import monitors
from nova import test


class MonitorsTestCase(test.NoDBTestCase):
    """Test case for monitors."""

    @mock.patch('stevedore.enabled.EnabledExtensionManager')
    def test_check_enabled_monitor(self, _mock_ext_manager):
        class FakeExt(object):
            def __init__(self, ept, name):
                self.entry_point_target = ept
                self.name = name

        # We check to ensure only one CPU monitor is loaded...
        self.flags(compute_monitors=['mon1', 'mon2'])
        handler = monitors.MonitorHandler(None)
        ext_cpu_mon1 = FakeExt('nova.compute.monitors.cpu.virt_driver:Monitor',
                               'mon1')
        ext_cpu_mon2 = FakeExt('nova.compute.monitors.cpu.virt_driver:Monitor',
                               'mon2')
        self.assertTrue(handler.check_enabled_monitor(ext_cpu_mon1))
        self.assertFalse(handler.check_enabled_monitor(ext_cpu_mon2))

        # We check to ensure that the auto-prefixing of the CPU
        # namespace is handled properly...
        self.flags(compute_monitors=['cpu.mon1', 'mon2'])
        handler = monitors.MonitorHandler(None)
        ext_cpu_mon1 = FakeExt('nova.compute.monitors.cpu.virt_driver:Monitor',
                               'mon1')
        ext_cpu_mon2 = FakeExt('nova.compute.monitors.cpu.virt_driver:Monitor',
                               'mon2')
        self.assertTrue(handler.check_enabled_monitor(ext_cpu_mon1))
        self.assertFalse(handler.check_enabled_monitor(ext_cpu_mon2))

# Copyright (c) 2010 OpenStack Foundation
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Tests For Console proxy."""

import mock
from oslo_config import cfg
from oslo_utils import importutils

from nova.compute import rpcapi as compute_rpcapi
from nova.console import api as console_api
from nova import context
from nova import db
from nova import exception
from nova import objects
from nova import test

CONF = cfg.CONF
CONF.import_opt('console_manager', 'nova.service')
CONF.import_opt('console_driver', 'nova.console.manager')


class ConsoleTestCase(test.TestCase):
    """Test case for console proxy manager."""
    def setUp(self):
        super(ConsoleTestCase, self).setUp()
        self.flags(console_driver='nova.console.fake.FakeConsoleProxy',
                   stub_compute=True)
        self.console = importutils.import_object(CONF.console_manager)
        self.user_id = 'fake'
        self.project_id = 'fake'
        self.context = context.RequestContext(self.user_id, self.project_id)
        self.host = 'test_compute_host'

    def test_reset(self):
        with mock.patch('nova.compute.rpcapi.ComputeAPI') as mock_rpc:
            old_rpcapi = self.console.compute_rpcapi
            self.console.reset()
            mock_rpc.assert_called_once_with()
            self.assertNotEqual(old_rpcapi,
                                self.console.compute_rpcapi)

    def _create_instance(self):
        """Create a test instance."""
        inst = {}
        inst['image_id'] = 1
        inst['reservation_id'] = 'r-fakeres'
        inst['user_id'] = self.user_id
        inst['project_id'] = self.project_id
        inst['instance_type_id'] = 1
        inst['ami_launch_index'] = 0
        return db.instance_create(self.context, inst)

    def test_get_pool_for_instance_host(self):
        pool = self.console._get_pool_for_instance_host(self.context,
                self.host)
        self.assertEqual(pool['compute_host'], self.host)

    def test_get_pool_creates_new_pool_if_needed(self):
        self.assertRaises(exception.NotFound,
                          db.console_pool_get_by_host_type,
                          self.context,
                          self.host,
                          self.console.host,
                          self.console.driver.console_type)
        pool = self.console._get_pool_for_instance_host(self.context,
                                                           self.host)
        pool2 = db.console_pool_get_by_host_type(self.context,
                              self.host,
                              self.console.host,
                              self.console.driver.console_type)
        self.assertEqual(pool['id'], pool2['id'])

    def test_get_pool_does_not_create_new_pool_if_exists(self):
        pool_info = {'address': '127.0.0.1',
                     'username': 'test',
                     'password': '1234pass',
                     'host': self.console.host,
                     'console_type': self.console.driver.console_type,
                     'compute_host': 'sometesthostname'}
        new_pool = db.console_pool_create(self.context, pool_info)
        pool = self.console._get_pool_for_instance_host(self.context,
                                                       'sometesthostname')
        self.assertEqual(pool['id'], new_pool['id'])

    def test_add_console(self):
        instance = self._create_instance()
        self.console.add_console(self.context, instance['id'])
        instance = db.instance_get(self.context, instance['id'])
        pool = db.console_pool_get_by_host_type(self.context,
                instance['host'], self.console.host,
                self.console.driver.console_type)

        console_instances = [con['instance_uuid'] for con in pool['consoles']]
        self.assertIn(instance['uuid'], console_instances)
        db.instance_destroy(self.context, instance['uuid'])

    def test_add_console_does_not_duplicate(self):
        instance = self._create_instance()
        cons1 = self.console.add_console(self.context, instance['id'])
        cons2 = self.console.add_console(self.context, instance['id'])
        self.assertEqual(cons1, cons2)
        db.instance_destroy(self.context, instance['uuid'])

    def test_remove_console(self):
        instance = self._create_instance()
        console_id = self.console.add_console(self.context, instance['id'])
        self.console.remove_console(self.context, console_id)

        self.assertRaises(exception.NotFound,
                          db.console_get,
                          self.context,
                          console_id)
        db.instance_destroy(self.context, instance['uuid'])


class ConsoleAPITestCase(test.NoDBTestCase):
    """Test case for console API."""
    def setUp(self):
        super(ConsoleAPITestCase, self).setUp()

        self.context = context.RequestContext('fake', 'fake')
        self.console_api = console_api.API()
        self.fake_uuid = '00000000-aaaa-bbbb-cccc-000000000000'
        self.fake_instance = {
            'id': 1,
            'uuid': self.fake_uuid,
            'host': 'fake_host'
        }
        self.fake_console = {
            'pool': {'host': 'fake_host'},
            'id': 'fake_id'
        }

        def _fake_db_console_get(_ctxt, _console_uuid, _instance_uuid):
            return self.fake_console
        self.stub_out('nova.db.console_get', _fake_db_console_get)

        def _fake_db_console_get_all_by_instance(_ctxt, _instance_uuid,
                                                 columns_to_join):
            return [self.fake_console]
        self.stub_out('nova.db.console_get_all_by_instance',
                       _fake_db_console_get_all_by_instance)

    def test_get_consoles(self):
        console = self.console_api.get_consoles(self.context, self.fake_uuid)
        self.assertEqual(console, [self.fake_console])

    def test_get_console(self):
        console = self.console_api.get_console(self.context, self.fake_uuid,
                                               'fake_id')
        self.assertEqual(console, self.fake_console)

    @mock.patch('nova.console.rpcapi.ConsoleAPI.remove_console')
    def test_delete_console(self, mock_remove):
        self.console_api.delete_console(self.context, self.fake_uuid,
                                        'fake_id')
        mock_remove.assert_called_once_with(self.context, 'fake_id')

    @mock.patch.object(compute_rpcapi.ComputeAPI, 'get_console_topic',
                       return_value='compute.fake_host')
    @mock.patch.object(objects.Instance, 'get_by_uuid')
    def test_create_console(self, mock_get_instance_by_uuid,
                            mock_get_console_topic):
        mock_get_instance_by_uuid.return_value = objects.Instance(
            **self.fake_instance)
        self.console_api.create_console(self.context, self.fake_uuid)
        mock_get_console_topic.assert_called_once_with(self.context,
                                                       'fake_host')

# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.network import model


def new_ip(ip_dict=None, version=4):
    if version == 6:
        new_ip = dict(address='fd00::1:100', version=6)
    elif version == 4:
        new_ip = dict(address='192.168.1.100')
    ip_dict = ip_dict or {}
    new_ip.update(ip_dict)
    return model.IP(**new_ip)


def new_fixed_ip(ip_dict=None, version=4):
    if version == 6:
        new_fixed_ip = dict(address='fd00::1:100', version=6)
    elif version == 4:
        new_fixed_ip = dict(address='192.168.1.100')
    ip_dict = ip_dict or {}
    new_fixed_ip.update(ip_dict)
    return model.FixedIP(**new_fixed_ip)


def new_route(route_dict=None, version=4):
    if version == 6:
        new_route = dict(
            cidr='::/48',
            gateway=new_ip(dict(address='fd00::1:1'), version=6),
            interface='eth0')
    elif version == 4:
        new_route = dict(
            cidr='0.0.0.0/24',
            gateway=new_ip(dict(address='192.168.1.1')),
            interface='eth0')

    route_dict = route_dict or {}
    new_route.update(route_dict)
    return model.Route(**new_route)


def new_subnet(subnet_dict=None, version=4):
    if version == 6:
        new_subnet = dict(
            cidr='fd00::/48',
            dns=[new_ip(dict(address='1:2:3:4::'), version=6),
                    new_ip(dict(address='2:3:4:5::'), version=6)],
            gateway=new_ip(dict(address='fd00::1'), version=6),
            ips=[new_fixed_ip(dict(address='fd00::2'), version=6),
                    new_fixed_ip(dict(address='fd00::3'), version=6)],
            routes=[new_route(version=6)],
            version=6)
    elif version == 4:
        new_subnet = dict(
            cidr='10.10.0.0/24',
            dns=[new_ip(dict(address='1.2.3.4')),
                    new_ip(dict(address='2.3.4.5'))],
            gateway=new_ip(dict(address='10.10.0.1')),
            ips=[new_fixed_ip(dict(address='10.10.0.2')),
                    new_fixed_ip(dict(address='10.10.0.3'))],
            routes=[new_route()])
    subnet_dict = subnet_dict or {}
    new_subnet.update(subnet_dict)
    return model.Subnet(**new_subnet)


def new_network(network_dict=None, version=4):
    if version == 6:
        new_net = dict(
            id=1,
            bridge='br0',
            label='public',
            subnets=[new_subnet(version=6),
                     new_subnet(dict(cidr='ffff:ffff:ffff:ffff::'),
                                version=6)])
    elif version == 4:
        new_net = dict(
            id=1,
            bridge='br0',
            label='public',
            subnets=[new_subnet(), new_subnet(dict(cidr='255.255.255.255'))])
    network_dict = network_dict or {}
    new_net.update(network_dict)
    return model.Network(**new_net)


def new_vif(vif_dict=None, version=4):
    vif = dict(
        id=1,
        address='aa:aa:aa:aa:aa:aa',
        type='bridge',
        network=new_network(version=version))
    vif_dict = vif_dict or {}
    vif.update(vif_dict)
    return model.VIF(**vif)

# Copyright 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""Example Module B for testing utils.monkey_patch()."""


def example_function_b():
    return 'Example function'


class ExampleClassB(object):
    def example_method(self):
        return 'Example method'

    def example_method_add(self, arg1, arg2):
        return arg1 + arg2

#    Copyright 2013 IBM Corp.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import datetime

import mock
from mox3 import mox
import netaddr
from oslo_db import exception as db_exc
from oslo_serialization import jsonutils
from oslo_utils import timeutils

from nova.cells import rpcapi as cells_rpcapi
from nova.compute import flavors
from nova import db
from nova import exception
from nova.network import model as network_model
from nova import notifications
from nova import objects
from nova.objects import base
from nova.objects import fields
from nova.objects import instance
from nova.objects import instance_info_cache
from nova.objects import pci_device
from nova.objects import security_group
from nova import test
from nova.tests.unit import fake_instance
from nova.tests.unit.objects import test_instance_fault
from nova.tests.unit.objects import test_instance_info_cache
from nova.tests.unit.objects import test_instance_numa_topology
from nova.tests.unit.objects import test_instance_pci_requests
from nova.tests.unit.objects import test_migration_context as test_mig_ctxt
from nova.tests.unit.objects import test_objects
from nova.tests.unit.objects import test_security_group
from nova.tests.unit.objects import test_vcpu_model
from nova.tests import uuidsentinel as uuids
from nova import utils


class _TestInstanceObject(object):
    @property
    def fake_instance(self):
        db_inst = fake_instance.fake_db_instance(id=2,
                                                 access_ip_v4='1.2.3.4',
                                                 access_ip_v6='::1')
        db_inst['uuid'] = uuids.db_instance
        db_inst['cell_name'] = 'api!child'
        db_inst['terminated_at'] = None
        db_inst['deleted_at'] = None
        db_inst['created_at'] = None
        db_inst['updated_at'] = None
        db_inst['launched_at'] = datetime.datetime(1955, 11, 12,
                                                   22, 4, 0)
        db_inst['deleted'] = False
        db_inst['security_groups'] = []
        db_inst['pci_devices'] = []
        db_inst['user_id'] = self.context.user_id
        db_inst['project_id'] = self.context.project_id
        db_inst['tags'] = []

        db_inst['info_cache'] = dict(test_instance_info_cache.fake_info_cache,
                                     instance_uuid=db_inst['uuid'])

        db_inst['system_metadata'] = {
            'image_name': 'os2-warp',
            'image_min_ram': 100,
            'image_hw_disk_bus': 'ide',
            'image_hw_vif_model': 'ne2k_pci',
        }
        return db_inst

    def test_datetime_deserialization(self):
        red_letter_date = timeutils.parse_isotime(
            utils.isotime(datetime.datetime(1955, 11, 5)))
        inst = objects.Instance(uuid=uuids.instance,
                                launched_at=red_letter_date)
        primitive = inst.obj_to_primitive()
        expected = {'nova_object.name': 'Instance',
                    'nova_object.namespace': 'nova',
                    'nova_object.version': inst.VERSION,
                    'nova_object.data':
                        {'uuid': uuids.instance,
                         'launched_at': '1955-11-05T00:00:00Z'},
                    'nova_object.changes': ['launched_at', 'uuid']}
        self.assertJsonEqual(primitive, expected)
        inst2 = objects.Instance.obj_from_primitive(primitive)
        self.assertIsInstance(inst2.launched_at, datetime.datetime)
        self.assertEqual(red_letter_date, inst2.launched_at)

    def test_ip_deserialization(self):
        inst = objects.Instance(uuid=uuids.instance, access_ip_v4='1.2.3.4',
                                access_ip_v6='::1')
        primitive = inst.obj_to_primitive()
        expected = {'nova_object.name': 'Instance',
                    'nova_object.namespace': 'nova',
                    'nova_object.version': inst.VERSION,
                    'nova_object.data':
                        {'uuid': uuids.instance,
                         'access_ip_v4': '1.2.3.4',
                         'access_ip_v6': '::1'},
                    'nova_object.changes': ['uuid', 'access_ip_v6',
                                            'access_ip_v4']}
        self.assertJsonEqual(primitive, expected)
        inst2 = objects.Instance.obj_from_primitive(primitive)
        self.assertIsInstance(inst2.access_ip_v4, netaddr.IPAddress)
        self.assertIsInstance(inst2.access_ip_v6, netaddr.IPAddress)
        self.assertEqual(netaddr.IPAddress('1.2.3.4'), inst2.access_ip_v4)
        self.assertEqual(netaddr.IPAddress('::1'), inst2.access_ip_v6)

    def test_get_without_expected(self):
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, 'uuid',
                                columns_to_join=[]
                                ).AndReturn(self.fake_instance)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, 'uuid',
                                             expected_attrs=[])
        for attr in instance.INSTANCE_OPTIONAL_ATTRS:
            self.assertFalse(inst.obj_attr_is_set(attr))

    def test_get_with_expected(self):
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_fault_get_by_instance_uuids')
        self.mox.StubOutWithMock(
                db, 'instance_extra_get_by_instance_uuid')

        exp_cols = instance.INSTANCE_OPTIONAL_ATTRS[:]
        exp_cols.remove('fault')
        exp_cols.remove('numa_topology')
        exp_cols.remove('pci_requests')
        exp_cols.remove('vcpu_model')
        exp_cols.remove('ec2_ids')
        exp_cols.remove('migration_context')
        exp_cols = list(filter(lambda x: 'flavor' not in x, exp_cols))
        exp_cols.extend(['extra', 'extra.numa_topology', 'extra.pci_requests',
                         'extra.flavor', 'extra.vcpu_model',
                         'extra.migration_context'])

        fake_topology = (test_instance_numa_topology.
                         fake_db_topology['numa_topology'])
        fake_requests = jsonutils.dumps(test_instance_pci_requests.
                                        fake_pci_requests)
        fake_flavor = jsonutils.dumps(
            {'cur': objects.Flavor().obj_to_primitive(),
             'old': None, 'new': None})
        fake_vcpu_model = jsonutils.dumps(
            test_vcpu_model.fake_vcpumodel.obj_to_primitive())
        fake_mig_context = jsonutils.dumps(
            test_mig_ctxt.fake_migration_context_obj.obj_to_primitive())
        fake_service = {'created_at': None, 'updated_at': None,
                        'deleted_at': None, 'deleted': False, 'id': 123,
                        'host': 'fake-host', 'binary': 'nova-fake',
                        'topic': 'fake-service-topic', 'report_count': 1,
                        'forced_down': False, 'disabled': False,
                        'disabled_reason': None, 'last_seen_up': None,
                        'version': 1,
                    }
        fake_instance = dict(self.fake_instance,
                             services=[fake_service],
                             extra={
                                 'numa_topology': fake_topology,
                                 'pci_requests': fake_requests,
                                 'flavor': fake_flavor,
                                 'vcpu_model': fake_vcpu_model,
                                 'migration_context': fake_mig_context,
                                 })
        db.instance_get_by_uuid(
            self.context, 'uuid',
            columns_to_join=exp_cols).AndReturn(fake_instance)
        fake_faults = test_instance_fault.fake_faults
        db.instance_fault_get_by_instance_uuids(
                self.context, [fake_instance['uuid']]
                ).AndReturn(fake_faults)

        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(
            self.context, 'uuid',
            expected_attrs=instance.INSTANCE_OPTIONAL_ATTRS)
        for attr in instance.INSTANCE_OPTIONAL_ATTRS:
            self.assertTrue(inst.obj_attr_is_set(attr))
        self.assertEqual(123, inst.services[0].id)

    def test_lazy_load_services_on_deleted_instance(self):
        # We should avoid trying to hit the database to reload the instance
        # and just set the services attribute to an empty list.
        instance = objects.Instance(self.context, uuid=uuids.instance,
                                    deleted=True)
        self.assertEqual(0, len(instance.services))

    def test_get_by_id(self):
        self.mox.StubOutWithMock(db, 'instance_get')
        db.instance_get(self.context, 'instid',
                        columns_to_join=['info_cache',
                                         'security_groups']
                        ).AndReturn(self.fake_instance)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_id(self.context, 'instid')
        self.assertEqual(self.fake_instance['uuid'], inst.uuid)

    def test_load(self):
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        fake_uuid = self.fake_instance['uuid']
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(self.fake_instance)
        fake_inst2 = dict(self.fake_instance,
                          metadata=[{'key': 'foo', 'value': 'bar'}])
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['metadata']
                                ).AndReturn(fake_inst2)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        self.assertFalse(hasattr(inst, '_obj_metadata'))
        meta = inst.metadata
        self.assertEqual({'foo': 'bar'}, meta)
        self.assertTrue(hasattr(inst, '_obj_metadata'))
        # Make sure we don't run load again
        meta2 = inst.metadata
        self.assertEqual({'foo': 'bar'}, meta2)

    def test_load_invalid(self):
        inst = objects.Instance(context=self.context, uuid=uuids.instance)
        self.assertRaises(exception.ObjectActionError,
                          inst.obj_load_attr, 'foo')

    def test_get_remote(self):
        # isotime doesn't have microseconds and is always UTC
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        fake_instance = self.fake_instance
        db.instance_get_by_uuid(self.context, uuids.instance,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_instance)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, uuids.instance)
        self.assertEqual(fake_instance['id'], inst.id)
        self.assertEqual(fake_instance['launched_at'],
                         inst.launched_at.replace(tzinfo=None))
        self.assertEqual(fake_instance['access_ip_v4'],
                         str(inst.access_ip_v4))
        self.assertEqual(fake_instance['access_ip_v6'],
                         str(inst.access_ip_v6))

    def test_refresh(self):
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        fake_uuid = self.fake_instance['uuid']
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(dict(self.fake_instance,
                                                 host='orig-host'))
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(dict(self.fake_instance,
                                                 host='new-host'))
        self.mox.StubOutWithMock(instance_info_cache.InstanceInfoCache,
                                 'refresh')
        instance_info_cache.InstanceInfoCache.refresh()
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        self.assertEqual('orig-host', inst.host)
        inst.refresh()
        self.assertEqual('new-host', inst.host)
        self.assertEqual(set([]), inst.obj_what_changed())

    def test_refresh_does_not_recurse(self):
        inst = objects.Instance(context=self.context, uuid=uuids.instance,
                                metadata={})
        inst_copy = objects.Instance()
        inst_copy.uuid = inst.uuid
        self.mox.StubOutWithMock(objects.Instance, 'get_by_uuid')
        objects.Instance.get_by_uuid(self.context, uuid=inst.uuid,
                                     expected_attrs=['metadata'],
                                     use_slave=False
                                     ).AndReturn(inst_copy)
        self.mox.ReplayAll()
        self.assertRaises(exception.OrphanedObjectError, inst.refresh)

    def _save_test_helper(self, cell_type, save_kwargs):
        """Common code for testing save() for cells/non-cells."""
        if cell_type:
            self.flags(enable=True, cell_type=cell_type, group='cells')
        else:
            self.flags(enable=False, group='cells')

        old_ref = dict(self.fake_instance, host='oldhost', user_data='old',
                       vm_state='old', task_state='old')
        fake_uuid = old_ref['uuid']

        expected_updates = dict(vm_state='meow', task_state='wuff',
                                user_data='new')

        new_ref = dict(old_ref, host='newhost', **expected_updates)
        exp_vm_state = save_kwargs.get('expected_vm_state')
        exp_task_state = save_kwargs.get('expected_task_state')
        admin_reset = save_kwargs.get('admin_state_reset', False)
        if exp_vm_state:
            expected_updates['expected_vm_state'] = exp_vm_state
        if exp_task_state:
            if (exp_task_state == 'image_snapshot' and
                    'instance_version' in save_kwargs and
                    save_kwargs['instance_version'] == '1.9'):
                expected_updates['expected_task_state'] = [
                    'image_snapshot', 'image_snapshot_pending']
            else:
                expected_updates['expected_task_state'] = exp_task_state
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_update_and_get_original')
        self.mox.StubOutWithMock(db, 'instance_info_cache_update')
        cells_api_mock = self.mox.CreateMock(cells_rpcapi.CellsAPI)
        self.mox.StubOutWithMock(cells_api_mock,
                                 'instance_update_at_top')
        self.mox.StubOutWithMock(cells_api_mock,
                                 'instance_update_from_api')
        self.mox.StubOutWithMock(cells_rpcapi, 'CellsAPI',
                                 use_mock_anything=True)
        self.mox.StubOutWithMock(notifications, 'send_update')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(old_ref)
        db.instance_update_and_get_original(
                self.context, fake_uuid, expected_updates,
                columns_to_join=['info_cache', 'security_groups',
                                 'system_metadata', 'extra', 'extra.flavor']
                ).AndReturn((old_ref, new_ref))
        if cell_type == 'api':
            cells_rpcapi.CellsAPI().AndReturn(cells_api_mock)
            cells_api_mock.instance_update_from_api(
                    self.context, mox.IsA(objects.Instance),
                    exp_vm_state, exp_task_state, admin_reset)
        elif cell_type == 'compute':
            cells_rpcapi.CellsAPI().AndReturn(cells_api_mock)
            cells_api_mock.instance_update_at_top(self.context,
                                                  mox.IsA(objects.Instance))
        notifications.send_update(self.context, mox.IgnoreArg(),
                                  mox.IgnoreArg())

        self.mox.ReplayAll()

        inst = objects.Instance.get_by_uuid(self.context, old_ref['uuid'])
        if 'instance_version' in save_kwargs:
            inst.VERSION = save_kwargs.pop('instance_version')
        self.assertEqual('old', inst.task_state)
        self.assertEqual('old', inst.vm_state)
        self.assertEqual('old', inst.user_data)
        inst.vm_state = 'meow'
        inst.task_state = 'wuff'
        inst.user_data = 'new'
        save_kwargs.pop('context', None)
        inst.save(**save_kwargs)
        self.assertEqual('newhost', inst.host)
        self.assertEqual('meow', inst.vm_state)
        self.assertEqual('wuff', inst.task_state)
        self.assertEqual('new', inst.user_data)
        # NOTE(danms): Ignore flavor migrations for the moment
        self.assertEqual(set([]), inst.obj_what_changed() - set(['flavor']))

    def test_save(self):
        self._save_test_helper(None, {})

    def test_save_in_api_cell(self):
        self._save_test_helper('api', {})

    def test_save_in_compute_cell(self):
        self._save_test_helper('compute', {})

    def test_save_exp_vm_state(self):
        self._save_test_helper(None, {'expected_vm_state': ['meow']})

    def test_save_exp_task_state(self):
        self._save_test_helper(None, {'expected_task_state': ['meow']})

    def test_save_exp_vm_state_api_cell(self):
        self._save_test_helper('api', {'expected_vm_state': ['meow']})

    def test_save_exp_task_state_api_cell(self):
        self._save_test_helper('api', {'expected_task_state': ['meow']})

    def test_save_exp_task_state_api_cell_admin_reset(self):
        self._save_test_helper('api', {'admin_state_reset': True})

    def test_save_rename_sends_notification(self):
        # Tests that simply changing the 'display_name' on the instance
        # will send a notification.
        self.flags(enable=False, group='cells')
        old_ref = dict(self.fake_instance, display_name='hello')
        fake_uuid = old_ref['uuid']
        expected_updates = dict(display_name='goodbye')
        new_ref = dict(old_ref, **expected_updates)
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_update_and_get_original')
        self.mox.StubOutWithMock(notifications, 'send_update')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(old_ref)
        db.instance_update_and_get_original(
                self.context, fake_uuid, expected_updates,
                columns_to_join=['info_cache', 'security_groups',
                                 'system_metadata', 'extra', 'extra.flavor']
                ).AndReturn((old_ref, new_ref))
        notifications.send_update(self.context, mox.IgnoreArg(),
                                  mox.IgnoreArg())

        self.mox.ReplayAll()

        inst = objects.Instance.get_by_uuid(self.context, old_ref['uuid'],
                                            use_slave=False)
        self.assertEqual('hello', inst.display_name)
        inst.display_name = 'goodbye'
        inst.save()
        self.assertEqual('goodbye', inst.display_name)
        # NOTE(danms): Ignore flavor migrations for the moment
        self.assertEqual(set([]), inst.obj_what_changed() - set(['flavor']))

    def test_save_related_object_if_none(self):
        with mock.patch.object(objects.Instance, '_save_pci_requests'
                ) as save_mock:
            inst = objects.Instance()
            inst = objects.Instance._from_db_object(self.context, inst,
                    self.fake_instance)
            inst.pci_requests = None
            inst.save()
            self.assertTrue(save_mock.called)

    @mock.patch('nova.db.instance_update_and_get_original')
    @mock.patch.object(instance.Instance, '_from_db_object')
    def test_save_does_not_refresh_pci_devices(self, mock_fdo, mock_update):
        # NOTE(danms): This tests that we don't update the pci_devices
        # field from the contents of the database. This is not because we
        # don't necessarily want to, but because the way pci_devices is
        # currently implemented it causes versioning issues. When that is
        # resolved, this test should go away.
        mock_update.return_value = None, None
        inst = objects.Instance(context=self.context, id=123)
        inst.uuid = uuids.test_instance_not_refresh
        inst.pci_devices = pci_device.PciDeviceList()
        inst.save()
        self.assertNotIn('pci_devices',
                         mock_fdo.call_args_list[0][1]['expected_attrs'])

    @mock.patch('nova.db.instance_extra_update_by_uuid')
    @mock.patch('nova.db.instance_update_and_get_original')
    @mock.patch.object(instance.Instance, '_from_db_object')
    def test_save_updates_numa_topology(self, mock_fdo, mock_update,
            mock_extra_update):
        fake_obj_numa_topology = objects.InstanceNUMATopology(cells=[
            objects.InstanceNUMACell(id=0, cpuset=set([0]), memory=128),
            objects.InstanceNUMACell(id=1, cpuset=set([1]), memory=128)])
        fake_obj_numa_topology.instance_uuid = uuids.instance
        jsonified = fake_obj_numa_topology._to_json()

        mock_update.return_value = None, None
        inst = objects.Instance(
            context=self.context, id=123, uuid=uuids.instance)
        inst.numa_topology = fake_obj_numa_topology
        inst.save()

        # NOTE(sdague): the json representation of nova object for
        # NUMA isn't stable from a string comparison
        # perspective. There are sets which get converted to lists,
        # and based on platform differences may show up in different
        # orders. So we can't have mock do the comparison. Instead
        # manually compare the final parameter using our json equality
        # operator which does the right thing here.
        mock_extra_update.assert_called_once_with(
            self.context, inst.uuid, mock.ANY)
        called_arg = mock_extra_update.call_args_list[0][0][2]['numa_topology']
        self.assertJsonEqual(called_arg, jsonified)

        mock_extra_update.reset_mock()
        inst.numa_topology = None
        inst.save()
        mock_extra_update.assert_called_once_with(
                self.context, inst.uuid, {'numa_topology': None})

    @mock.patch('nova.db.instance_extra_update_by_uuid')
    def test_save_vcpu_model(self, mock_update):
        inst = fake_instance.fake_instance_obj(self.context)
        inst.vcpu_model = test_vcpu_model.fake_vcpumodel
        inst.save()
        self.assertTrue(mock_update.called)
        self.assertEqual(1, mock_update.call_count)
        actual_args = mock_update.call_args
        self.assertEqual(self.context, actual_args[0][0])
        self.assertEqual(inst.uuid, actual_args[0][1])
        self.assertEqual(['vcpu_model'], list(actual_args[0][2].keys()))
        self.assertJsonEqual(jsonutils.dumps(
                test_vcpu_model.fake_vcpumodel.obj_to_primitive()),
                             actual_args[0][2]['vcpu_model'])
        mock_update.reset_mock()
        inst.vcpu_model = None
        inst.save()
        mock_update.assert_called_once_with(
            self.context, inst.uuid, {'vcpu_model': None})

    @mock.patch('nova.db.instance_extra_update_by_uuid')
    def test_save_migration_context_model(self, mock_update):
        inst = fake_instance.fake_instance_obj(self.context)
        inst.migration_context = test_mig_ctxt.get_fake_migration_context_obj(
            self.context)
        inst.save()
        self.assertTrue(mock_update.called)
        self.assertEqual(1, mock_update.call_count)
        actual_args = mock_update.call_args
        self.assertEqual(self.context, actual_args[0][0])
        self.assertEqual(inst.uuid, actual_args[0][1])
        self.assertEqual(['migration_context'], list(actual_args[0][2].keys()))
        self.assertIsInstance(
            objects.MigrationContext.obj_from_db_obj(
                actual_args[0][2]['migration_context']),
            objects.MigrationContext)
        mock_update.reset_mock()
        inst.migration_context = None
        inst.save()
        mock_update.assert_called_once_with(
            self.context, inst.uuid, {'migration_context': None})

    def test_save_flavor_skips_unchanged_flavors(self):
        inst = objects.Instance(context=self.context,
                                flavor=objects.Flavor())
        inst.obj_reset_changes()
        with mock.patch('nova.db.instance_extra_update_by_uuid') as mock_upd:
            inst.save()
            self.assertFalse(mock_upd.called)

    @mock.patch.object(notifications, 'send_update')
    @mock.patch.object(cells_rpcapi.CellsAPI, 'instance_update_from_api')
    @mock.patch.object(cells_rpcapi.CellsAPI, 'instance_update_at_top')
    @mock.patch.object(db, 'instance_update_and_get_original')
    def _test_skip_cells_sync_helper(self, mock_db_update, mock_update_at_top,
            mock_update_from_api, mock_notif_update, cell_type):
        self.flags(enable=True, cell_type=cell_type, group='cells')
        inst = fake_instance.fake_instance_obj(self.context, cell_name='fake')
        inst.vm_state = 'foo'
        inst.task_state = 'bar'
        inst.cell_name = 'foo!bar@baz'

        old_ref = dict(base.obj_to_primitive(inst), vm_state='old',
                task_state='old')
        new_ref = dict(old_ref, vm_state='foo', task_state='bar')
        newer_ref = dict(new_ref, vm_state='bar', task_state='foo')
        mock_db_update.side_effect = [(old_ref, new_ref), (new_ref, newer_ref)]

        with inst.skip_cells_sync():
            inst.save()

        mock_update_at_top.assert_has_calls([])
        mock_update_from_api.assert_has_calls([])
        self.assertFalse(mock_notif_update.called)

        inst.vm_state = 'bar'
        inst.task_state = 'foo'

        def fake_update_from_api(context, instance, expected_vm_state,
                expected_task_state, admin_state_reset):
            self.assertEqual('foo!bar@baz', instance.cell_name)

        # This is re-mocked so that cell_name can be checked above.  Since
        # instance objects have no equality testing assert_called_once_with
        # doesn't work.
        with mock.patch.object(cells_rpcapi.CellsAPI,
                'instance_update_from_api',
                side_effect=fake_update_from_api) as fake_update_from_api:
            inst.save()

        self.assertEqual('foo!bar@baz', inst.cell_name)
        self.assertTrue(mock_notif_update.called)
        if cell_type == 'compute':
            mock_update_at_top.assert_called_once_with(self.context, mock.ANY)
            # Compare primitives since we can't check instance object equality
            expected_inst_p = base.obj_to_primitive(inst)
            actual_inst = mock_update_at_top.call_args[0][1]
            actual_inst_p = base.obj_to_primitive(actual_inst)
            self.assertEqual(expected_inst_p, actual_inst_p)
            self.assertFalse(fake_update_from_api.called)
        elif cell_type == 'api':
            self.assertFalse(mock_update_at_top.called)
            fake_update_from_api.assert_called_once_with(self.context,
                    mock.ANY, None, None, False)

        expected_calls = [
                mock.call(self.context, inst.uuid,
                    {'vm_state': 'foo', 'task_state': 'bar',
                     'cell_name': 'foo!bar@baz'},
                    columns_to_join=['system_metadata', 'extra',
                        'extra.flavor']),
                mock.call(self.context, inst.uuid,
                    {'vm_state': 'bar', 'task_state': 'foo'},
                    columns_to_join=['system_metadata'])]
        mock_db_update.assert_has_calls(expected_calls)

    def test_skip_cells_api(self):
        self._test_skip_cells_sync_helper(cell_type='api')

    def test_skip_cells_compute(self):
        self._test_skip_cells_sync_helper(cell_type='compute')

    def test_get_deleted(self):
        fake_inst = dict(self.fake_instance, id=123, deleted=123)
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        # NOTE(danms): Make sure it's actually a bool
        self.assertTrue(inst.deleted)

    def test_get_not_cleaned(self):
        fake_inst = dict(self.fake_instance, id=123, cleaned=None)
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        # NOTE(mikal): Make sure it's actually a bool
        self.assertFalse(inst.cleaned)

    def test_get_cleaned(self):
        fake_inst = dict(self.fake_instance, id=123, cleaned=1)
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        # NOTE(mikal): Make sure it's actually a bool
        self.assertTrue(inst.cleaned)

    def test_with_info_cache(self):
        fake_inst = dict(self.fake_instance)
        fake_uuid = fake_inst['uuid']
        nwinfo1 = network_model.NetworkInfo.hydrate([{'address': 'foo'}])
        nwinfo2 = network_model.NetworkInfo.hydrate([{'address': 'bar'}])
        nwinfo1_json = nwinfo1.json()
        nwinfo2_json = nwinfo2.json()
        fake_info_cache = test_instance_info_cache.fake_info_cache
        fake_inst['info_cache'] = dict(
            fake_info_cache,
            network_info=nwinfo1_json,
            instance_uuid=fake_uuid)
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_update_and_get_original')
        self.mox.StubOutWithMock(db, 'instance_info_cache_update')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        db.instance_info_cache_update(self.context, fake_uuid,
                {'network_info': nwinfo2_json}).AndReturn(fake_info_cache)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        self.assertEqual(nwinfo1, inst.info_cache.network_info)
        self.assertEqual(fake_uuid, inst.info_cache.instance_uuid)
        inst.info_cache.network_info = nwinfo2
        inst.save()

    def test_with_info_cache_none(self):
        fake_inst = dict(self.fake_instance, info_cache=None)
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid,
                                             ['info_cache'])
        self.assertIsNone(inst.info_cache)

    def test_with_security_groups(self):
        fake_inst = dict(self.fake_instance)
        fake_uuid = fake_inst['uuid']
        fake_inst['security_groups'] = [
            {'id': 1, 'name': 'secgroup1', 'description': 'fake-desc',
             'user_id': 'fake-user', 'project_id': 'fake_project',
             'created_at': None, 'updated_at': None, 'deleted_at': None,
             'deleted': False},
            {'id': 2, 'name': 'secgroup2', 'description': 'fake-desc',
             'user_id': 'fake-user', 'project_id': 'fake_project',
             'created_at': None, 'updated_at': None, 'deleted_at': None,
             'deleted': False},
            ]
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_update_and_get_original')
        self.mox.StubOutWithMock(db, 'security_group_update')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        db.security_group_update(self.context, 1, {'description': 'changed'}
                                 ).AndReturn(fake_inst['security_groups'][0])
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        self.assertEqual(2, len(inst.security_groups))
        for index, group in enumerate(fake_inst['security_groups']):
            for key in group:
                self.assertEqual(group[key],
                                 inst.security_groups[index][key])
                self.assertIsInstance(inst.security_groups[index],
                                      security_group.SecurityGroup)
        self.assertEqual(set(), inst.security_groups.obj_what_changed())
        inst.security_groups[0].description = 'changed'
        inst.save()
        self.assertEqual(set(), inst.security_groups.obj_what_changed())

    def test_with_empty_security_groups(self):
        fake_inst = dict(self.fake_instance, security_groups=[])
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['info_cache',
                                                 'security_groups']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid)
        self.assertEqual(0, len(inst.security_groups))

    def test_with_empty_pci_devices(self):
        fake_inst = dict(self.fake_instance, pci_devices=[])
        fake_uuid = fake_inst['uuid']
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['pci_devices']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid,
            ['pci_devices'])
        self.assertEqual(0, len(inst.pci_devices))

    def test_with_pci_devices(self):
        fake_inst = dict(self.fake_instance)
        fake_uuid = fake_inst['uuid']
        fake_inst['pci_devices'] = [
            {'created_at': None,
             'updated_at': None,
             'deleted_at': None,
             'deleted': None,
             'id': 2,
             'compute_node_id': 1,
             'address': 'a1',
             'vendor_id': 'v1',
             'numa_node': 0,
             'product_id': 'p1',
             'dev_type': fields.PciDeviceType.STANDARD,
             'status': fields.PciDeviceStatus.ALLOCATED,
             'dev_id': 'i',
             'label': 'l',
             'instance_uuid': fake_uuid,
             'request_id': None,
             'parent_addr': None,
             'extra_info': '{}'},
            {
             'created_at': None,
             'updated_at': None,
             'deleted_at': None,
             'deleted': None,
             'id': 1,
             'compute_node_id': 1,
             'address': 'a',
             'vendor_id': 'v',
             'numa_node': 1,
             'product_id': 'p',
             'dev_type': fields.PciDeviceType.STANDARD,
             'status': fields.PciDeviceStatus.ALLOCATED,
             'dev_id': 'i',
             'label': 'l',
             'instance_uuid': fake_uuid,
             'request_id': None,
             'parent_addr': 'a1',
             'extra_info': '{}'},
            ]
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=['pci_devices']
                                ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid,
            ['pci_devices'])
        self.assertEqual(2, len(inst.pci_devices))
        self.assertEqual(fake_uuid, inst.pci_devices[0].instance_uuid)
        self.assertEqual(fake_uuid, inst.pci_devices[1].instance_uuid)

    def test_with_fault(self):
        fake_inst = dict(self.fake_instance)
        fake_uuid = fake_inst['uuid']
        fake_faults = [dict(x, instance_uuid=fake_uuid)
                       for x in test_instance_fault.fake_faults['fake-uuid']]
        self.mox.StubOutWithMock(db, 'instance_get_by_uuid')
        self.mox.StubOutWithMock(db, 'instance_fault_get_by_instance_uuids')
        db.instance_get_by_uuid(self.context, fake_uuid,
                                columns_to_join=[]
                                ).AndReturn(self.fake_instance)
        db.instance_fault_get_by_instance_uuids(
            self.context, [fake_uuid]).AndReturn({fake_uuid: fake_faults})
        self.mox.ReplayAll()
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid,
                                            expected_attrs=['fault'])
        self.assertEqual(fake_faults[0], dict(inst.fault.items()))

    @mock.patch('nova.objects.EC2Ids.get_by_instance')
    @mock.patch('nova.db.instance_get_by_uuid')
    def test_with_ec2_ids(self, mock_get, mock_ec2):
        fake_inst = dict(self.fake_instance)
        fake_uuid = fake_inst['uuid']
        mock_get.return_value = fake_inst
        fake_ec2_ids = objects.EC2Ids(instance_id='fake-inst',
                                      ami_id='fake-ami')
        mock_ec2.return_value = fake_ec2_ids
        inst = objects.Instance.get_by_uuid(self.context, fake_uuid,
                                            expected_attrs=['ec2_ids'])
        mock_ec2.assert_called_once_with(self.context, mock.ANY)

        self.assertEqual(fake_ec2_ids.instance_id, inst.ec2_ids.instance_id)

    @mock.patch('nova.db.instance_get_by_uuid')
    def test_with_image_meta(self, mock_get):
        fake_inst = dict(self.fake_instance)
        mock_get.return_value = fake_inst

        inst = instance.Instance.get_by_uuid(self.context,
                                             fake_inst['uuid'],
                                             expected_attrs=['image_meta'])

        image_meta = inst.image_meta
        self.assertIsInstance(image_meta, objects.ImageMeta)
        self.assertEqual(100, image_meta.min_ram)
        self.assertEqual('ide', image_meta.properties.hw_disk_bus)
        self.assertEqual('ne2k_pci', image_meta.properties.hw_vif_model)

    def test_iteritems_with_extra_attrs(self):
        self.stubs.Set(objects.Instance, 'name', 'foo')
        inst = objects.Instance(uuid=uuids.instance)
        self.assertEqual(sorted({'uuid': uuids.instance,
                                 'name': 'foo',
                                }.items()), sorted(inst.items()))

    def _test_metadata_change_tracking(self, which):
        inst = objects.Instance(uuid=uuids.instance)
        setattr(inst, which, {})
        inst.obj_reset_changes()
        getattr(inst, which)['foo'] = 'bar'
        self.assertEqual(set([which]), inst.obj_what_changed())
        inst.obj_reset_changes()
        self.assertEqual(set(), inst.obj_what_changed())

    def test_create_skip_scheduled_at(self):
        self.mox.StubOutWithMock(db, 'instance_create')
        vals = {'host': 'foo-host',
                'memory_mb': 128,
                'system_metadata': {'foo': 'bar'},
                'extra': {
                    'vcpu_model': None,
                    'numa_topology': None,
                    'pci_requests': None,
                }}
        fake_inst = fake_instance.fake_db_instance(**vals)
        db.instance_create(self.context, vals).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance(context=self.context,
                                host='foo-host', memory_mb=128,
                                scheduled_at=None,
                                system_metadata={'foo': 'bar'})
        inst.create()
        self.assertEqual('foo-host', inst.host)

    def test_metadata_change_tracking(self):
        self._test_metadata_change_tracking('metadata')

    def test_system_metadata_change_tracking(self):
        self._test_metadata_change_tracking('system_metadata')

    def test_create_stubbed(self):
        self.mox.StubOutWithMock(db, 'instance_create')
        vals = {'host': 'foo-host',
                'memory_mb': 128,
                'system_metadata': {'foo': 'bar'},
                'extra': {
                    'vcpu_model': None,
                    'numa_topology': None,
                    'pci_requests': None,
                }}
        fake_inst = fake_instance.fake_db_instance(**vals)
        db.instance_create(self.context, vals).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance(context=self.context,
                                host='foo-host', memory_mb=128,
                                system_metadata={'foo': 'bar'})
        inst.create()

    def test_create(self):
        self.mox.StubOutWithMock(db, 'instance_create')
        extras = {'vcpu_model': None,
                  'numa_topology': None,
                  'pci_requests': None}
        db.instance_create(self.context, {'extra': extras}).AndReturn(
            self.fake_instance)
        self.mox.ReplayAll()
        inst = objects.Instance(context=self.context)
        inst.create()
        self.assertEqual(self.fake_instance['id'], inst.id)
        self.assertIsNotNone(inst.ec2_ids)

    def test_create_with_values(self):
        inst1 = objects.Instance(context=self.context,
                                 user_id=self.context.user_id,
                                 project_id=self.context.project_id,
                                 host='foo-host')
        inst1.create()
        self.assertEqual('foo-host', inst1.host)
        inst2 = objects.Instance.get_by_uuid(self.context, inst1.uuid)
        self.assertEqual('foo-host', inst2.host)

    def test_create_with_extras(self):
        inst = objects.Instance(context=self.context,
            uuid=self.fake_instance['uuid'],
            numa_topology=test_instance_numa_topology.fake_obj_numa_topology,
            pci_requests=objects.InstancePCIRequests(
                requests=[
                    objects.InstancePCIRequest(count=123,
                                               spec=[])]),
            vcpu_model=test_vcpu_model.fake_vcpumodel,
            )
        inst.create()
        self.assertIsNotNone(inst.numa_topology)
        self.assertIsNotNone(inst.pci_requests)
        self.assertEqual(1, len(inst.pci_requests.requests))
        self.assertIsNotNone(inst.vcpu_model)
        got_numa_topo = objects.InstanceNUMATopology.get_by_instance_uuid(
            self.context, inst.uuid)
        self.assertEqual(inst.numa_topology.instance_uuid,
                         got_numa_topo.instance_uuid)
        got_pci_requests = objects.InstancePCIRequests.get_by_instance_uuid(
            self.context, inst.uuid)
        self.assertEqual(123, got_pci_requests.requests[0].count)
        vcpu_model = objects.VirtCPUModel.get_by_instance_uuid(
            self.context, inst.uuid)
        self.assertEqual('fake-model', vcpu_model.model)

    def test_recreate_fails(self):
        inst = objects.Instance(context=self.context,
                                user_id=self.context.user_id,
                                project_id=self.context.project_id,
                                host='foo-host')
        inst.create()
        self.assertRaises(exception.ObjectActionError, inst.create)

    def test_create_with_special_things(self):
        self.mox.StubOutWithMock(db, 'instance_create')
        fake_inst = fake_instance.fake_db_instance()
        db.instance_create(self.context,
                           {'host': 'foo-host',
                            'security_groups': ['foo', 'bar'],
                            'info_cache': {'network_info': '[]'},
                            'extra': {
                                'vcpu_model': None,
                                'numa_topology': None,
                                'pci_requests': None,
                            },
                            }
                           ).AndReturn(fake_inst)
        self.mox.ReplayAll()
        secgroups = security_group.SecurityGroupList()
        secgroups.objects = []
        for name in ('foo', 'bar'):
            secgroup = security_group.SecurityGroup()
            secgroup.name = name
            secgroups.objects.append(secgroup)
        info_cache = instance_info_cache.InstanceInfoCache()
        info_cache.network_info = network_model.NetworkInfo()
        inst = objects.Instance(context=self.context,
                                host='foo-host', security_groups=secgroups,
                                info_cache=info_cache)
        inst.create()

    def test_destroy_stubbed(self):
        self.mox.StubOutWithMock(db, 'instance_destroy')
        deleted_at = datetime.datetime(1955, 11, 6)
        fake_inst = fake_instance.fake_db_instance(deleted_at=deleted_at,
                                                   deleted=True)
        db.instance_destroy(self.context, uuids.instance,
                            constraint=None).AndReturn(fake_inst)
        self.mox.ReplayAll()
        inst = objects.Instance(context=self.context, id=1,
                                uuid=uuids.instance, host='foo')
        inst.destroy()
        self.assertEqual(timeutils.normalize_time(deleted_at),
                         timeutils.normalize_time(inst.deleted_at))
        self.assertTrue(inst.deleted)

    def test_destroy(self):
        values = {'user_id': self.context.user_id,
                  'project_id': self.context.project_id}
        db_inst = db.instance_create(self.context, values)
        inst = objects.Instance(context=self.context, id=db_inst['id'],
                                 uuid=db_inst['uuid'])
        inst.destroy()
        self.assertRaises(exception.InstanceNotFound,
                          db.instance_get_by_uuid, self.context,
                          db_inst['uuid'])

    def test_destroy_host_constraint(self):
        values = {'user_id': self.context.user_id,
                  'project_id': self.context.project_id,
                  'host': 'foo'}
        db_inst = db.instance_create(self.context, values)
        inst = objects.Instance.get_by_uuid(self.context, db_inst['uuid'])
        inst.host = None
        self.assertRaises(exception.ObjectActionError,
                          inst.destroy)

    @mock.patch.object(cells_rpcapi.CellsAPI, 'instance_destroy_at_top')
    @mock.patch.object(db, 'instance_destroy')
    def test_destroy_cell_sync_to_top(self, mock_destroy, mock_destroy_at_top):
        self.flags(enable=True, cell_type='compute', group='cells')
        fake_inst = fake_instance.fake_db_instance(deleted=True)
        mock_destroy.return_value = fake_inst
        inst = objects.Instance(context=self.context, id=1,
                                uuid=uuids.instance)
        inst.destroy()
        mock_destroy_at_top.assert_called_once_with(self.context, mock.ANY)
        actual_inst = mock_destroy_at_top.call_args[0][1]
        self.assertIsInstance(actual_inst, instance.Instance)

    @mock.patch.object(cells_rpcapi.CellsAPI, 'instance_destroy_at_top')
    @mock.patch.object(db, 'instance_destroy')
    def test_destroy_no_cell_sync_to_top(self, mock_destroy,
                                         mock_destroy_at_top):
        fake_inst = fake_instance.fake_db_instance(deleted=True)
        mock_destroy.return_value = fake_inst
        inst = objects.Instance(context=self.context, id=1,
                                uuid=uuids.instance)
        inst.destroy()
        self.assertFalse(mock_destroy_at_top.called)

    def test_name_does_not_trigger_lazy_loads(self):
        values = {'user_id': self.context.user_id,
                  'project_id': self.context.project_id,
                  'host': 'foo'}
        db_inst = db.instance_create(self.context, values)
        inst = objects.Instance.get_by_uuid(self.context, db_inst['uuid'])
        self.assertFalse(inst.obj_attr_is_set('fault'))
        self.flags(instance_name_template='foo-%(uuid)s')
        self.assertEqual('foo-%s' % db_inst['uuid'], inst.name)
        self.assertFalse(inst.obj_attr_is_set('fault'))

    def test_from_db_object_not_overwrite_info_cache(self):
        info_cache = instance_info_cache.InstanceInfoCache()
        inst = objects.Instance(context=self.context,
                                info_cache=info_cache)
        db_inst = fake_instance.fake_db_instance()
        db_inst['info_cache'] = dict(
            test_instance_info_cache.fake_info_cache)
        inst._from_db_object(self.context, inst, db_inst,
                             expected_attrs=['info_cache'])
        self.assertIs(info_cache, inst.info_cache)

    def test_from_db_object_info_cache_not_set(self):
        inst = instance.Instance(context=self.context,
                                 info_cache=None)
        db_inst = fake_instance.fake_db_instance()
        db_inst.pop('info_cache')
        inst._from_db_object(self.context, inst, db_inst,
                             expected_attrs=['info_cache'])
        self.assertIsNone(inst.info_cache)

    def test_from_db_object_security_groups_net_set(self):
        inst = instance.Instance(context=self.context,
                                 info_cache=None)
        db_inst = fake_instance.fake_db_instance()
        db_inst.pop('security_groups')
        inst._from_db_object(self.context, inst, db_inst,
                             expected_attrs=['security_groups'])
        self.assertEqual([], inst.security_groups.objects)

    @mock.patch('nova.objects.InstancePCIRequests.get_by_instance_uuid')
    def test_get_with_pci_requests(self, mock_get):
        mock_get.return_value = objects.InstancePCIRequests()
        db_instance = db.instance_create(self.context, {
            'user_id': self.context.user_id,
            'project_id': self.context.project_id})
        instance = objects.Instance.get_by_uuid(
            self.context, db_instance['uuid'],
            expected_attrs=['pci_requests'])
        self.assertTrue(instance.obj_attr_is_set('pci_requests'))
        self.assertIsNotNone(instance.pci_requests)

    def test_get_flavor(self):
        db_flavor = flavors.get_default_flavor()
        inst = objects.Instance(flavor=db_flavor)
        self.assertEqual(db_flavor['flavorid'],
                         inst.get_flavor().flavorid)

    def test_get_flavor_namespace(self):
        db_flavor = flavors.get_default_flavor()
        inst = objects.Instance(old_flavor=db_flavor)
        self.assertEqual(db_flavor['flavorid'],
                         inst.get_flavor('old').flavorid)

    @mock.patch.object(db, 'instance_metadata_delete')
    def test_delete_metadata_key(self, db_delete):
        inst = objects.Instance(context=self.context,
                                id=1, uuid=uuids.instance)
        inst.metadata = {'foo': '1', 'bar': '2'}
        inst.obj_reset_changes()
        inst.delete_metadata_key('foo')
        self.assertEqual({'bar': '2'}, inst.metadata)
        self.assertEqual({}, inst.obj_get_changes())
        db_delete.assert_called_once_with(self.context, inst.uuid, 'foo')

    def test_reset_changes(self):
        inst = objects.Instance()
        inst.metadata = {'1985': 'present'}
        inst.system_metadata = {'1955': 'past'}
        self.assertEqual({}, inst._orig_metadata)
        inst.obj_reset_changes(['metadata'])
        self.assertEqual({'1985': 'present'}, inst._orig_metadata)
        self.assertEqual({}, inst._orig_system_metadata)

    def test_load_generic_calls_handler(self):
        inst = objects.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(inst, '_load_generic') as mock_load:
            def fake_load(name):
                inst.system_metadata = {}

            mock_load.side_effect = fake_load
            inst.system_metadata
            mock_load.assert_called_once_with('system_metadata')

    def test_load_fault_calls_handler(self):
        inst = objects.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(inst, '_load_fault') as mock_load:
            def fake_load():
                inst.fault = None

            mock_load.side_effect = fake_load
            inst.fault
            mock_load.assert_called_once_with()

    def test_load_ec2_ids_calls_handler(self):
        inst = objects.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(inst, '_load_ec2_ids') as mock_load:
            def fake_load():
                inst.ec2_ids = objects.EC2Ids(instance_id='fake-inst',
                                              ami_id='fake-ami')

            mock_load.side_effect = fake_load
            inst.ec2_ids
            mock_load.assert_called_once_with()

    def test_load_migration_context(self):
        inst = instance.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(
                objects.MigrationContext, 'get_by_instance_uuid',
                return_value=test_mig_ctxt.fake_migration_context_obj
        ) as mock_get:
            inst.migration_context
            mock_get.assert_called_once_with(self.context, inst.uuid)

    def test_load_migration_context_no_context(self):
        inst = instance.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(
                objects.MigrationContext, 'get_by_instance_uuid',
            side_effect=exception.MigrationContextNotFound(
                instance_uuid=inst.uuid)
        ) as mock_get:
            mig_ctxt = inst.migration_context
            mock_get.assert_called_once_with(self.context, inst.uuid)
            self.assertIsNone(mig_ctxt)

    def test_load_migration_context_no_data(self):
        inst = instance.Instance(context=self.context, uuid=uuids.instance)
        with mock.patch.object(
                objects.MigrationContext, 'get_by_instance_uuid') as mock_get:
            loaded_ctxt = inst._load_migration_context(db_context=None)
            self.assertFalse(mock_get.called)
            self.assertIsNone(loaded_ctxt)

    def test_apply_revert_migration_context(self):
        inst = instance.Instance(context=self.context, uuid=uuids.instance,
                                 numa_topology=None)
        inst.migration_context = test_mig_ctxt.get_fake_migration_context_obj(
            self.context)
        inst.apply_migration_context()
        self.assertIsInstance(inst.numa_topology, objects.InstanceNUMATopology)
        inst.revert_migration_context()
        self.assertIsNone(inst.numa_topology)

    def test_drop_migration_context(self):
        inst = instance.Instance(context=self.context, uuid=uuids.instance)
        inst.migration_context = test_mig_ctxt.get_fake_migration_context_obj(
            self.context)
        inst.migration_context.instance_uuid = inst.uuid
        inst.migration_context.id = 7
        with mock.patch(
                'nova.db.instance_extra_update_by_uuid') as update_extra:
            inst.drop_migration_context()
            self.assertIsNone(inst.migration_context)
            update_extra.assert_called_once_with(self.context, inst.uuid,
                                                 {"migration_context": None})

    def test_mutated_migration_context(self):
        numa_topology = (test_instance_numa_topology.
                            fake_obj_numa_topology.obj_clone())
        numa_topology.cells[0].memory = 1024
        numa_topology.cells[1].memory = 1024

        inst = instance.Instance(context=self.context, uuid=uuids.instance,
                                 numa_topology=numa_topology)
        inst.migration_context = test_mig_ctxt.get_fake_migration_context_obj(
            self.context)
        with inst.mutated_migration_context():
            self.assertIs(inst.numa_topology,
                          inst.migration_context.new_numa_topology)
        self.assertIs(numa_topology, inst.numa_topology)

    def test_clear_numa_topology(self):
        numa_topology = (test_instance_numa_topology.
                            fake_obj_numa_topology.obj_clone())
        numa_topology.cells[0].id = 42
        numa_topology.cells[1].id = 43

        inst = instance.Instance(context=self.context, uuid=uuids.instance,
                                 numa_topology=numa_topology)
        inst.obj_reset_changes()
        inst.clear_numa_topology()
        self.assertIn('numa_topology', inst.obj_what_changed())
        self.assertEqual(-1, numa_topology.cells[0].id)
        self.assertEqual(-1, numa_topology.cells[1].id)

    @mock.patch.object(objects.Instance, 'get_by_uuid')
    def test_load_generic(self, mock_get):
        inst2 = instance.Instance(metadata={'foo': 'bar'})
        mock_get.return_value = inst2
        inst = instance.Instance(context=self.context, uuid=uuids.instance)
        inst.metadata

    @mock.patch('nova.db.instance_fault_get_by_instance_uuids')
    def test_load_fault(self, mock_get):
        fake_fault = test_instance_fault.fake_faults['fake-uuid'][0]
        mock_get.return_value = {uuids.load_fault_instance: [fake_fault]}
        inst = objects.Instance(context=self.context,
                                uuid=uuids.load_fault_instance)
        fault = inst.fault
        mock_get.assert_called_once_with(self.context,
                                         [uuids.load_fault_instance])
        self.assertEqual(fake_fault['id'], fault.id)
        self.assertNotIn('metadata', inst.obj_what_changed())

    @mock.patch('nova.objects.EC2Ids.get_by_instance')
    def test_load_ec2_ids(self, mock_get):
        fake_ec2_ids = objects.EC2Ids(instance_id='fake-inst',
                                      ami_id='fake-ami')
        mock_get.return_value = fake_ec2_ids
        inst = objects.Instance(context=self.context, uuid=uuids.instance)
        ec2_ids = inst.ec2_ids
        mock_get.assert_called_once_with(self.context, inst)
        self.assertEqual(fake_ec2_ids, ec2_ids)

    @mock.patch('nova.objects.SecurityGroupList.get_by_instance')
    def test_load_security_groups(self, mock_get):
        secgroups = []
        for name in ('foo', 'bar'):
            secgroup = security_group.SecurityGroup()
            secgroup.name = name
            secgroups.append(secgroup)
        fake_secgroups = security_group.SecurityGroupList(objects=secgroups)
        mock_get.return_value = fake_secgroups
        inst = objects.Instance(context=self.context, uuid='fake')
        secgroups = inst.security_groups
        mock_get.assert_called_once_with(self.context, inst)
        self.assertEqual(fake_secgroups, secgroups)

    @mock.patch('nova.objects.PciDeviceList.get_by_instance_uuid')
    def test_load_pci_devices(self, mock_get):
        fake_pci_devices = pci_device.PciDeviceList()
        mock_get.return_value = fake_pci_devices
        inst = objects.Instance(context=self.context, uuid=uuids.pci_devices)
        pci_devices = inst.pci_devices
        mock_get.assert_called_once_with(self.context, uuids.pci_devices)
        self.assertEqual(fake_pci_devices, pci_devices)

    def test_get_with_extras(self):
        pci_requests = objects.InstancePCIRequests(requests=[
            objects.InstancePCIRequest(count=123, spec=[])])
        inst = objects.Instance(context=self.context,
                                user_id=self.context.user_id,
                                project_id=self.context.project_id,
                                pci_requests=pci_requests)
        inst.create()
        uuid = inst.uuid
        inst = objects.Instance.get_by_uuid(self.context, uuid)
        self.assertFalse(inst.obj_attr_is_set('pci_requests'))
        inst = objects.Instance.get_by_uuid(
            self.context, uuid, expected_attrs=['pci_requests'])
        self.assertTrue(inst.obj_attr_is_set('pci_requests'))


class TestInstanceObject(test_objects._LocalTest,
                         _TestInstanceObject):
    def _test_save_objectfield_fk_constraint_fails(self, foreign_key,
                                                   expected_exception):
        # NOTE(danms): Do this here and not in the remote test because
        # we're mocking out obj_attr_is_set() without the thing actually
        # being set, which confuses the heck out of the serialization
        # stuff.
        error = db_exc.DBReferenceError('table', 'constraint', foreign_key,
                                        'key_table')
        # Prevent lazy-loading any fields, results in InstanceNotFound
        attrs = objects.instance.INSTANCE_OPTIONAL_ATTRS
        instance = fake_instance.fake_instance_obj(self.context,
                                                   expected_attrs=attrs)
        fields_with_save_methods = [field for field in instance.fields
                                    if hasattr(instance, '_save_%s' % field)]
        for field in fields_with_save_methods:
            @mock.patch.object(instance, '_save_%s' % field)
            @mock.patch.object(instance, 'obj_attr_is_set')
            def _test(mock_is_set, mock_save_field):
                mock_is_set.return_value = True
                mock_save_field.side_effect = error
                instance.obj_reset_changes(fields=[field])
                instance._changed_fields.add(field)
                self.assertRaises(expected_exception, instance.save)
                instance.obj_reset_changes(fields=[field])
            _test()

    def test_save_objectfield_missing_instance_row(self):
        self._test_save_objectfield_fk_constraint_fails(
                'instance_uuid', exception.InstanceNotFound)

    def test_save_objectfield_reraises_if_not_instance_related(self):
        self._test_save_objectfield_fk_constraint_fails(
                'other_foreign_key', db_exc.DBReferenceError)


class TestRemoteInstanceObject(test_objects._RemoteTest,
                               _TestInstanceObject):
    pass


class _TestInstanceListObject(object):
    def fake_instance(self, id, updates=None):
        db_inst = fake_instance.fake_db_instance(id=2,
                                                 access_ip_v4='1.2.3.4',
                                                 access_ip_v6='::1')
        db_inst['terminated_at'] = None
        db_inst['deleted_at'] = None
        db_inst['created_at'] = None
        db_inst['updated_at'] = None
        db_inst['launched_at'] = datetime.datetime(1955, 11, 12,
                                                   22, 4, 0)
        db_inst['security_groups'] = []
        db_inst['deleted'] = 0

        db_inst['info_cache'] = dict(test_instance_info_cache.fake_info_cache,
                                     instance_uuid=db_inst['uuid'])

        if updates:
            db_inst.update(updates)
        return db_inst

    def test_get_all_by_filters(self):
        fakes = [self.fake_instance(1), self.fake_instance(2)]
        self.mox.StubOutWithMock(db, 'instance_get_all_by_filters')
        db.instance_get_all_by_filters(self.context, {'foo': 'bar'}, 'uuid',
                                       'asc', limit=None, marker=None,
                                       columns_to_join=['metadata']
                                       ).AndReturn(fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_filters(
            self.context, {'foo': 'bar'}, 'uuid', 'asc',
            expected_attrs=['metadata'], use_slave=False)

        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    def test_get_all_by_filters_sorted(self):
        fakes = [self.fake_instance(1), self.fake_instance(2)]
        self.mox.StubOutWithMock(db, 'instance_get_all_by_filters_sort')
        db.instance_get_all_by_filters_sort(self.context, {'foo': 'bar'},
                                            limit=None, marker=None,
                                            columns_to_join=['metadata'],
                                            sort_keys=['uuid'],
                                            sort_dirs=['asc']).AndReturn(fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_filters(
            self.context, {'foo': 'bar'}, expected_attrs=['metadata'],
            use_slave=False, sort_keys=['uuid'], sort_dirs=['asc'])

        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    @mock.patch.object(db, 'instance_get_all_by_filters_sort')
    @mock.patch.object(db, 'instance_get_all_by_filters')
    def test_get_all_by_filters_calls_non_sort(self,
                                               mock_get_by_filters,
                                               mock_get_by_filters_sort):
        '''Verifies InstanceList.get_by_filters calls correct DB function.'''
        # Single sort key/direction is set, call non-sorted DB function
        objects.InstanceList.get_by_filters(
            self.context, {'foo': 'bar'}, sort_key='key', sort_dir='dir',
            limit=100, marker='uuid', use_slave=True)
        mock_get_by_filters.assert_called_once_with(
            self.context, {'foo': 'bar'}, 'key', 'dir', limit=100,
            marker='uuid', columns_to_join=None)
        self.assertEqual(0, mock_get_by_filters_sort.call_count)

    @mock.patch.object(db, 'instance_get_all_by_filters_sort')
    @mock.patch.object(db, 'instance_get_all_by_filters')
    def test_get_all_by_filters_calls_sort(self,
                                           mock_get_by_filters,
                                           mock_get_by_filters_sort):
        '''Verifies InstanceList.get_by_filters calls correct DB function.'''
        # Multiple sort keys/directions are set, call sorted DB function
        objects.InstanceList.get_by_filters(
            self.context, {'foo': 'bar'}, limit=100, marker='uuid',
            use_slave=True, sort_keys=['key1', 'key2'],
            sort_dirs=['dir1', 'dir2'])
        mock_get_by_filters_sort.assert_called_once_with(
            self.context, {'foo': 'bar'}, limit=100,
            marker='uuid', columns_to_join=None,
            sort_keys=['key1', 'key2'], sort_dirs=['dir1', 'dir2'])
        self.assertEqual(0, mock_get_by_filters.call_count)

    def test_get_all_by_filters_works_for_cleaned(self):
        fakes = [self.fake_instance(1),
                 self.fake_instance(2, updates={'deleted': 2,
                                                'cleaned': None})]
        self.context.read_deleted = 'yes'
        self.mox.StubOutWithMock(db, 'instance_get_all_by_filters')
        db.instance_get_all_by_filters(self.context,
                                       {'deleted': True, 'cleaned': False},
                                       'uuid', 'asc', limit=None, marker=None,
                                       columns_to_join=['metadata']).AndReturn(
                                           [fakes[1]])
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_filters(
            self.context, {'deleted': True, 'cleaned': False}, 'uuid', 'asc',
            expected_attrs=['metadata'], use_slave=False)

        self.assertEqual(1, len(inst_list))
        self.assertIsInstance(inst_list.objects[0], instance.Instance)
        self.assertEqual(fakes[1]['uuid'], inst_list.objects[0].uuid)

    def test_get_by_host(self):
        fakes = [self.fake_instance(1),
                 self.fake_instance(2)]
        self.mox.StubOutWithMock(db, 'instance_get_all_by_host')
        db.instance_get_all_by_host(self.context, 'foo',
                                    columns_to_join=None).AndReturn(fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_host(self.context, 'foo')
        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)
            self.assertEqual(self.context, inst_list.objects[i]._context)
        self.assertEqual(set(), inst_list.obj_what_changed())

    def test_get_by_host_and_node(self):
        fakes = [self.fake_instance(1),
                 self.fake_instance(2)]
        self.mox.StubOutWithMock(db, 'instance_get_all_by_host_and_node')
        db.instance_get_all_by_host_and_node(self.context, 'foo', 'bar',
                                             columns_to_join=None).AndReturn(
                                                 fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_host_and_node(self.context,
                                                              'foo', 'bar')
        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    def test_get_by_host_and_not_type(self):
        fakes = [self.fake_instance(1),
                 self.fake_instance(2)]
        self.mox.StubOutWithMock(db, 'instance_get_all_by_host_and_not_type')
        db.instance_get_all_by_host_and_not_type(self.context, 'foo',
                                                 type_id='bar').AndReturn(
                                                     fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_by_host_and_not_type(
            self.context, 'foo', 'bar')
        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    @mock.patch('nova.objects.instance._expected_cols')
    @mock.patch('nova.db.instance_get_all')
    def test_get_all(self, mock_get_all, mock_exp):
        fakes = [self.fake_instance(1), self.fake_instance(2)]
        mock_get_all.return_value = fakes
        mock_exp.return_value = mock.sentinel.exp_att
        inst_list = objects.InstanceList.get_all(
                self.context, expected_attrs='fake')
        mock_get_all.assert_called_once_with(
                self.context, columns_to_join=mock.sentinel.exp_att)
        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    def test_get_hung_in_rebooting(self):
        fakes = [self.fake_instance(1),
                 self.fake_instance(2)]
        dt = utils.isotime()
        self.mox.StubOutWithMock(db, 'instance_get_all_hung_in_rebooting')
        db.instance_get_all_hung_in_rebooting(self.context, dt).AndReturn(
            fakes)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList.get_hung_in_rebooting(self.context,
                                                               dt)
        for i in range(0, len(fakes)):
            self.assertIsInstance(inst_list.objects[i], instance.Instance)
            self.assertEqual(fakes[i]['uuid'], inst_list.objects[i].uuid)

    def test_get_active_by_window_joined(self):
        fakes = [self.fake_instance(1), self.fake_instance(2)]
        # NOTE(mriedem): Send in a timezone-naive datetime since the
        # InstanceList.get_active_by_window_joined method should convert it
        # to tz-aware for the DB API call, which we'll assert with our stub.
        dt = timeutils.utcnow()

        def fake_instance_get_active_by_window_joined(context, begin, end,
                                                      project_id, host,
                                                      columns_to_join):
            # make sure begin is tz-aware
            self.assertIsNotNone(begin.utcoffset())
            self.assertIsNone(end)
            self.assertEqual(['metadata'], columns_to_join)
            return fakes

        with mock.patch.object(db, 'instance_get_active_by_window_joined',
                               fake_instance_get_active_by_window_joined):
            inst_list = objects.InstanceList.get_active_by_window_joined(
                            self.context, dt, expected_attrs=['metadata'])

        for fake, obj in zip(fakes, inst_list.objects):
            self.assertIsInstance(obj, instance.Instance)
            self.assertEqual(fake['uuid'], obj.uuid)

    def test_with_fault(self):
        fake_insts = [
            fake_instance.fake_db_instance(uuid=uuids.faults_instance,
                                           host='host'),
            fake_instance.fake_db_instance(uuid=uuids.faults_instance_nonexist,
                                           host='host'),
            ]
        fake_faults = test_instance_fault.fake_faults
        self.mox.StubOutWithMock(db, 'instance_get_all_by_host')
        self.mox.StubOutWithMock(db, 'instance_fault_get_by_instance_uuids')
        db.instance_get_all_by_host(self.context, 'host',
                                    columns_to_join=[]).AndReturn(fake_insts)
        db.instance_fault_get_by_instance_uuids(
            self.context, [x['uuid'] for x in fake_insts]
            ).AndReturn(fake_faults)
        self.mox.ReplayAll()
        instances = objects.InstanceList.get_by_host(self.context, 'host',
                                                     expected_attrs=['fault'],
                                                     use_slave=False)
        self.assertEqual(2, len(instances))
        self.assertEqual(fake_faults['fake-uuid'][0],
                         dict(instances[0].fault))
        self.assertIsNone(instances[1].fault)

    def test_fill_faults(self):
        self.mox.StubOutWithMock(db, 'instance_fault_get_by_instance_uuids')

        inst1 = objects.Instance(uuid=uuids.db_fault_1)
        inst2 = objects.Instance(uuid=uuids.db_fault_2)
        insts = [inst1, inst2]
        for inst in insts:
            inst.obj_reset_changes()
        db_faults = {
            'uuid1': [{'id': 123,
                       'instance_uuid': uuids.db_fault_1,
                       'code': 456,
                       'message': 'Fake message',
                       'details': 'No details',
                       'host': 'foo',
                       'deleted': False,
                       'deleted_at': None,
                       'updated_at': None,
                       'created_at': None,
                       }
                      ]}

        db.instance_fault_get_by_instance_uuids(self.context,
                                                [x.uuid for x in insts],
                                                ).AndReturn(db_faults)
        self.mox.ReplayAll()
        inst_list = objects.InstanceList()
        inst_list._context = self.context
        inst_list.objects = insts
        faulty = inst_list.fill_faults()
        self.assertEqual([uuids.db_fault_1], list(faulty))
        self.assertEqual(db_faults['uuid1'][0]['message'],
                         inst_list[0].fault.message)
        self.assertIsNone(inst_list[1].fault)
        for inst in inst_list:
            self.assertEqual(set(), inst.obj_what_changed())

    @mock.patch('nova.objects.instance.Instance.obj_make_compatible')
    def test_get_by_security_group(self, mock_compat):
        fake_secgroup = dict(test_security_group.fake_secgroup)
        fake_secgroup['instances'] = [
            fake_instance.fake_db_instance(id=1,
                                           system_metadata={'foo': 'bar'}),
            fake_instance.fake_db_instance(id=2),
            ]

        with mock.patch.object(db, 'security_group_get') as sgg:
            sgg.return_value = fake_secgroup
            secgroup = security_group.SecurityGroup()
            secgroup.id = fake_secgroup['id']
            instances = instance.InstanceList.get_by_security_group(
                self.context, secgroup)

        self.assertEqual(2, len(instances))
        self.assertEqual([1, 2], [x.id for x in instances])
        self.assertTrue(instances[0].obj_attr_is_set('system_metadata'))
        self.assertEqual({'foo': 'bar'}, instances[0].system_metadata)

    def test_get_by_grantee_security_group_ids(self):
        fake_instances = [
            fake_instance.fake_db_instance(id=1),
            fake_instance.fake_db_instance(id=2)
            ]

        with mock.patch.object(
            db, 'instance_get_all_by_grantee_security_groups') as igabgsg:
            igabgsg.return_value = fake_instances
            secgroup_ids = [1]
            instances = objects.InstanceList.get_by_grantee_security_group_ids(
                self.context, secgroup_ids)
            igabgsg.assert_called_once_with(self.context, secgroup_ids)

        self.assertEqual(2, len(instances))
        self.assertEqual([1, 2], [x.id for x in instances])


class TestInstanceListObject(test_objects._LocalTest,
                             _TestInstanceListObject):
    pass


class TestRemoteInstanceListObject(test_objects._RemoteTest,
                                   _TestInstanceListObject):
    pass


class TestInstanceObjectMisc(test.TestCase):
    def test_expected_cols(self):
        self.stubs.Set(instance, '_INSTANCE_OPTIONAL_JOINED_FIELDS', ['bar'])
        self.assertEqual(['bar'], instance._expected_cols(['foo', 'bar']))
        self.assertIsNone(instance._expected_cols(None))

    def test_expected_cols_extra(self):
        self.assertEqual(['metadata', 'extra', 'extra.numa_topology'],
                         instance._expected_cols(['metadata',
                                                  'numa_topology']))

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock

from nova import objects
from nova.scheduler.filters import core_filter
from nova import test
from nova.tests.unit.scheduler import fakes


class TestCoreFilter(test.NoDBTestCase):

    def test_core_filter_passes(self):
        self.filt_cls = core_filter.CoreFilter()
        spec_obj = objects.RequestSpec(flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 4, 'vcpus_used': 7,
                 'cpu_allocation_ratio': 2})
        self.assertTrue(self.filt_cls.host_passes(host, spec_obj))

    def test_core_filter_fails_safe(self):
        self.filt_cls = core_filter.CoreFilter()
        spec_obj = objects.RequestSpec(flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1', {})
        self.assertTrue(self.filt_cls.host_passes(host, spec_obj))

    def test_core_filter_fails(self):
        self.filt_cls = core_filter.CoreFilter()
        spec_obj = objects.RequestSpec(flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 4, 'vcpus_used': 8,
                 'cpu_allocation_ratio': 2})
        self.assertFalse(self.filt_cls.host_passes(host, spec_obj))

    def test_core_filter_single_instance_overcommit_fails(self):
        self.filt_cls = core_filter.CoreFilter()
        spec_obj = objects.RequestSpec(flavor=objects.Flavor(vcpus=2))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 1, 'vcpus_used': 0,
                 'cpu_allocation_ratio': 2})
        self.assertFalse(self.filt_cls.host_passes(host, spec_obj))

    @mock.patch('nova.scheduler.filters.utils.aggregate_values_from_key')
    def test_aggregate_core_filter_value_error(self, agg_mock):
        self.filt_cls = core_filter.AggregateCoreFilter()
        spec_obj = objects.RequestSpec(
            context=mock.sentinel.ctx, flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 4, 'vcpus_used': 7,
                 'cpu_allocation_ratio': 2})
        agg_mock.return_value = set(['XXX'])
        self.assertTrue(self.filt_cls.host_passes(host, spec_obj))
        agg_mock.assert_called_once_with(host, 'cpu_allocation_ratio')
        self.assertEqual(4 * 2, host.limits['vcpu'])

    @mock.patch('nova.scheduler.filters.utils.aggregate_values_from_key')
    def test_aggregate_core_filter_default_value(self, agg_mock):
        self.filt_cls = core_filter.AggregateCoreFilter()
        spec_obj = objects.RequestSpec(
            context=mock.sentinel.ctx, flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 4, 'vcpus_used': 8,
                 'cpu_allocation_ratio': 2})
        agg_mock.return_value = set([])
        # False: fallback to default flag w/o aggregates
        self.assertFalse(self.filt_cls.host_passes(host, spec_obj))
        agg_mock.assert_called_once_with(host, 'cpu_allocation_ratio')
        # True: use ratio from aggregates
        agg_mock.return_value = set(['3'])
        self.assertTrue(self.filt_cls.host_passes(host, spec_obj))
        self.assertEqual(4 * 3, host.limits['vcpu'])

    @mock.patch('nova.scheduler.filters.utils.aggregate_values_from_key')
    def test_aggregate_core_filter_conflict_values(self, agg_mock):
        self.filt_cls = core_filter.AggregateCoreFilter()
        spec_obj = objects.RequestSpec(
            context=mock.sentinel.ctx, flavor=objects.Flavor(vcpus=1))
        host = fakes.FakeHostState('host1', 'node1',
                {'vcpus_total': 4, 'vcpus_used': 8,
                 'cpu_allocation_ratio': 1})
        agg_mock.return_value = set(['2', '3'])
        # use the minimum ratio from aggregates
        self.assertFalse(self.filt_cls.host_passes(host, spec_obj))
        self.assertEqual(4 * 2, host.limits['vcpu'])

# Copyright (c) 2014 OpenStack Foundation
# Copyright (c) 2011 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Tests For IronicHostManager
"""

import mock

from nova import exception
from nova import objects
from nova.objects import base as obj_base
from nova.scheduler import filters
from nova.scheduler import host_manager
from nova.scheduler import ironic_host_manager
from nova import test
from nova.tests.unit.scheduler import ironic_fakes


class FakeFilterClass1(filters.BaseHostFilter):
    def host_passes(self, host_state, filter_properties):
        pass


class FakeFilterClass2(filters.BaseHostFilter):
    def host_passes(self, host_state, filter_properties):
        pass


class IronicHostManagerTestCase(test.NoDBTestCase):
    """Test case for IronicHostManager class."""

    @mock.patch.object(host_manager.HostManager, '_init_instance_info')
    @mock.patch.object(host_manager.HostManager, '_init_aggregates')
    def setUp(self, mock_init_agg, mock_init_inst):
        super(IronicHostManagerTestCase, self).setUp()
        self.host_manager = ironic_host_manager.IronicHostManager()

    @mock.patch.object(host_manager.HostManager, '_init_instance_info')
    @mock.patch.object(host_manager.HostManager, '_init_aggregates')
    def test_manager_public_api_signatures(self, mock_init_aggs,
                                           mock_init_inst):
        self.assertPublicAPISignatures(host_manager.HostManager(),
                                       self.host_manager)

    def test_state_public_api_signatures(self):
        self.assertPublicAPISignatures(
            host_manager.HostState("dummy",
                                   "dummy"),
            ironic_host_manager.IronicNodeState("dummy",
                                                "dummy")
        )

    @mock.patch('nova.objects.ServiceList.get_by_binary')
    @mock.patch('nova.objects.ComputeNodeList.get_all')
    @mock.patch('nova.objects.InstanceList.get_by_host')
    def test_get_all_host_states(self, mock_get_by_host, mock_get_all,
                                 mock_get_by_binary):
        mock_get_all.return_value = ironic_fakes.COMPUTE_NODES
        mock_get_by_binary.return_value = ironic_fakes.SERVICES
        context = 'fake_context'

        self.host_manager.get_all_host_states(context)
        self.assertEqual(0, mock_get_by_host.call_count)
        host_states_map = self.host_manager.host_state_map
        self.assertEqual(len(host_states_map), 4)

        for i in range(4):
            compute_node = ironic_fakes.COMPUTE_NODES[i]
            host = compute_node.host
            node = compute_node.hypervisor_hostname
            state_key = (host, node)
            self.assertEqual(host_states_map[state_key].service,
                             obj_base.obj_to_primitive(
                                 ironic_fakes.get_service_by_host(host)))
            self.assertEqual(compute_node.stats,
                             host_states_map[state_key].stats)
            self.assertEqual(compute_node.free_ram_mb,
                             host_states_map[state_key].free_ram_mb)
            self.assertEqual(compute_node.free_disk_gb * 1024,
                             host_states_map[state_key].free_disk_mb)


class IronicHostManagerChangedNodesTestCase(test.NoDBTestCase):
    """Test case for IronicHostManager class."""

    @mock.patch.object(host_manager.HostManager, '_init_instance_info')
    @mock.patch.object(host_manager.HostManager, '_init_aggregates')
    def setUp(self, mock_init_agg, mock_init_inst):
        super(IronicHostManagerChangedNodesTestCase, self).setUp()
        self.host_manager = ironic_host_manager.IronicHostManager()
        ironic_driver = "nova.virt.ironic.driver.IronicDriver"
        supported_instances = [
            objects.HVSpec.from_list(["i386", "baremetal", "baremetal"])]
        self.compute_node = objects.ComputeNode(
            id=1, local_gb=10, memory_mb=1024, vcpus=1,
            vcpus_used=0, local_gb_used=0, memory_mb_used=0,
            updated_at=None, cpu_info='baremetal cpu',
            stats=dict(
                ironic_driver=ironic_driver,
                cpu_arch='i386'),
            supported_hv_specs=supported_instances,
            free_disk_gb=10, free_ram_mb=1024,
            hypervisor_type='ironic',
            hypervisor_version=1,
            hypervisor_hostname='fake_host',
            cpu_allocation_ratio=16.0, ram_allocation_ratio=1.5,
            disk_allocation_ratio=1.0)

    @mock.patch.object(ironic_host_manager.IronicNodeState, '__init__')
    def test_create_ironic_node_state(self, init_mock):
        init_mock.return_value = None
        compute = objects.ComputeNode(**{'hypervisor_type': 'ironic'})
        host_state = self.host_manager.host_state_cls('fake-host', 'fake-node',
                                                      compute=compute)
        self.assertIs(ironic_host_manager.IronicNodeState, type(host_state))

    @mock.patch.object(host_manager.HostState, '__init__')
    def test_create_non_ironic_host_state(self, init_mock):
        init_mock.return_value = None
        compute = objects.ComputeNode(**{'cpu_info': 'other cpu'})
        host_state = self.host_manager.host_state_cls('fake-host', 'fake-node',
                                                      compute=compute)
        self.assertIs(host_manager.HostState, type(host_state))

    @mock.patch.object(host_manager.HostState, '__init__')
    def test_create_host_state_null_compute(self, init_mock):
        init_mock.return_value = None
        host_state = self.host_manager.host_state_cls('fake-host', 'fake-node')
        self.assertIs(host_manager.HostState, type(host_state))

    @mock.patch('nova.objects.ServiceList.get_by_binary')
    @mock.patch('nova.objects.ComputeNodeList.get_all')
    def test_get_all_host_states_after_delete_one(self, mock_get_all,
                                                  mock_get_by_binary):
        getter = (lambda n: n.hypervisor_hostname
                  if 'hypervisor_hostname' in n else None)
        running_nodes = [n for n in ironic_fakes.COMPUTE_NODES
                         if getter(n) != 'node4uuid']

        mock_get_all.side_effect = [
            ironic_fakes.COMPUTE_NODES, running_nodes]
        mock_get_by_binary.side_effect = [
            ironic_fakes.SERVICES, ironic_fakes.SERVICES]
        context = 'fake_context'

        # first call: all nodes
        self.host_manager.get_all_host_states(context)
        host_states_map = self.host_manager.host_state_map
        self.assertEqual(4, len(host_states_map))

        # second call: just running nodes
        self.host_manager.get_all_host_states(context)
        host_states_map = self.host_manager.host_state_map
        self.assertEqual(3, len(host_states_map))

    @mock.patch('nova.objects.ServiceList.get_by_binary')
    @mock.patch('nova.objects.ComputeNodeList.get_all')
    def test_get_all_host_states_after_delete_all(self, mock_get_all,
                                                  mock_get_by_binary):
        mock_get_all.side_effect = [
            ironic_fakes.COMPUTE_NODES, []]
        mock_get_by_binary.side_effect = [
            ironic_fakes.SERVICES, ironic_fakes.SERVICES]
        context = 'fake_context'

        # first call: all nodes
        self.host_manager.get_all_host_states(context)
        host_states_map = self.host_manager.host_state_map
        self.assertEqual(len(host_states_map), 4)

        # second call: no nodes
        self.host_manager.get_all_host_states(context)
        host_states_map = self.host_manager.host_state_map
        self.assertEqual(len(host_states_map), 0)

    def test_update_from_compute_node(self):
        host = ironic_host_manager.IronicNodeState("fakehost", "fakenode")
        host.update(compute=self.compute_node)

        self.assertEqual(1024, host.free_ram_mb)
        self.assertEqual(1024, host.total_usable_ram_mb)
        self.assertEqual(10240, host.free_disk_mb)
        self.assertEqual(1, host.vcpus_total)
        self.assertEqual(0, host.vcpus_used)
        self.assertEqual(self.compute_node.stats, host.stats)
        self.assertEqual('ironic', host.hypervisor_type)
        self.assertEqual(1, host.hypervisor_version)
        self.assertEqual('fake_host', host.hypervisor_hostname)

    def test_consume_identical_instance_from_compute(self):
        host = ironic_host_manager.IronicNodeState("fakehost", "fakenode")
        host.update(compute=self.compute_node)

        self.assertIsNone(host.updated)
        spec_obj = objects.RequestSpec(
            flavor=objects.Flavor(root_gb=10, ephemeral_gb=0, memory_mb=1024,
                                  vcpus=1),
            uuid='fake-uuid')
        host.consume_from_request(spec_obj)

        self.assertEqual(1, host.vcpus_used)
        self.assertEqual(0, host.free_ram_mb)
        self.assertEqual(0, host.free_disk_mb)
        self.assertIsNotNone(host.updated)

    def test_consume_larger_instance_from_compute(self):
        host = ironic_host_manager.IronicNodeState("fakehost", "fakenode")
        host.update(compute=self.compute_node)

        self.assertIsNone(host.updated)
        spec_obj = objects.RequestSpec(
            flavor=objects.Flavor(root_gb=20, ephemeral_gb=0, memory_mb=2048,
                                  vcpus=2))
        host.consume_from_request(spec_obj)

        self.assertEqual(1, host.vcpus_used)
        self.assertEqual(0, host.free_ram_mb)
        self.assertEqual(0, host.free_disk_mb)
        self.assertIsNotNone(host.updated)

    def test_consume_smaller_instance_from_compute(self):
        host = ironic_host_manager.IronicNodeState("fakehost", "fakenode")
        host.update(compute=self.compute_node)

        self.assertIsNone(host.updated)
        spec_obj = objects.RequestSpec(
            flavor=objects.Flavor(root_gb=5, ephemeral_gb=0, memory_mb=512,
                                  vcpus=1))
        host.consume_from_request(spec_obj)

        self.assertEqual(1, host.vcpus_used)
        self.assertEqual(0, host.free_ram_mb)
        self.assertEqual(0, host.free_disk_mb)
        self.assertIsNotNone(host.updated)


class IronicHostManagerTestFilters(test.NoDBTestCase):
    """Test filters work for IronicHostManager."""

    @mock.patch.object(host_manager.HostManager, '_init_instance_info')
    @mock.patch.object(host_manager.HostManager, '_init_aggregates')
    def setUp(self, mock_init_agg, mock_init_inst):
        super(IronicHostManagerTestFilters, self).setUp()
        self.flags(scheduler_available_filters=['%s.%s' % (__name__, cls) for
                                                cls in ['FakeFilterClass1',
                                                        'FakeFilterClass2']])
        self.flags(scheduler_default_filters=['FakeFilterClass1'])
        self.flags(baremetal_scheduler_default_filters=['FakeFilterClass2'])
        self.host_manager = ironic_host_manager.IronicHostManager()
        self.fake_hosts = [ironic_host_manager.IronicNodeState(
                'fake_host%s' % x, 'fake-node') for x in range(1, 5)]
        self.fake_hosts += [ironic_host_manager.IronicNodeState(
                'fake_multihost', 'fake-node%s' % x) for x in range(1, 5)]

    def test_default_filters(self):
        default_filters = self.host_manager.default_filters
        self.assertEqual(1, len(default_filters))
        self.assertIsInstance(default_filters[0], FakeFilterClass1)

    def test_choose_host_filters_not_found(self):
        self.assertRaises(exception.SchedulerHostFilterNotFound,
                          self.host_manager._choose_host_filters,
                          'FakeFilterClass3')

    def test_choose_host_filters(self):
        # Test we return 1 correct filter object
        host_filters = self.host_manager._choose_host_filters(
                ['FakeFilterClass2'])
        self.assertEqual(1, len(host_filters))
        self.assertIsInstance(host_filters[0], FakeFilterClass2)

    def test_host_manager_default_filters(self):
        default_filters = self.host_manager.default_filters
        self.assertEqual(1, len(default_filters))
        self.assertIsInstance(default_filters[0], FakeFilterClass1)

    @mock.patch.object(host_manager.HostManager, '_init_instance_info')
    @mock.patch.object(host_manager.HostManager, '_init_aggregates')
    def test_host_manager_default_filters_uses_baremetal(self, mock_init_agg,
                                                         mock_init_inst):
        self.flags(scheduler_use_baremetal_filters=True)
        host_manager = ironic_host_manager.IronicHostManager()

        # ensure the defaults come from baremetal_scheduler_default_filters
        # and not scheduler_default_filters
        default_filters = host_manager.default_filters
        self.assertEqual(1, len(default_filters))
        self.assertIsInstance(default_filters[0], FakeFilterClass2)

    def test_load_filters(self):
        # without scheduler_use_baremetal_filters
        filters = self.host_manager._load_filters()
        self.assertEqual(['FakeFilterClass1'], filters)

    def test_load_filters_baremetal(self):
        # with scheduler_use_baremetal_filters
        self.flags(scheduler_use_baremetal_filters=True)
        filters = self.host_manager._load_filters()
        self.assertEqual(['FakeFilterClass2'], filters)

    def _mock_get_filtered_hosts(self, info):
        info['got_objs'] = []
        info['got_fprops'] = []

        def fake_filter_one(_self, obj, filter_props):
            info['got_objs'].append(obj)
            info['got_fprops'].append(filter_props)
            return True

        self.stub_out(__name__ + '.FakeFilterClass1._filter_one',
                      fake_filter_one)

    def _verify_result(self, info, result, filters=True):
        for x in info['got_fprops']:
            self.assertEqual(x, info['expected_fprops'])
        if filters:
            self.assertEqual(set(info['expected_objs']), set(info['got_objs']))
        self.assertEqual(set(info['expected_objs']), set(result))

    def test_get_filtered_hosts(self):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=[],
            force_nodes=[])

        info = {'expected_objs': self.fake_hosts,
                'expected_fprops': fake_properties}

        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result)

    @mock.patch.object(FakeFilterClass2, '_filter_one', return_value=True)
    def test_get_filtered_hosts_with_specified_filters(self, mock_filter_one):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=[],
            force_nodes=[])

        specified_filters = ['FakeFilterClass1', 'FakeFilterClass2']
        info = {'expected_objs': self.fake_hosts,
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties, filter_class_names=specified_filters)
        self._verify_result(info, result)

    def test_get_filtered_hosts_with_ignore(self):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=['fake_host1', 'fake_host3',
                          'fake_host5', 'fake_multihost'],
            force_hosts=[],
            force_nodes=[])

        # [1] and [3] are host2 and host4
        info = {'expected_objs': [self.fake_hosts[1], self.fake_hosts[3]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result)

    def test_get_filtered_hosts_with_force_hosts(self):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=['fake_host1', 'fake_host3', 'fake_host5'],
            force_nodes=[])

        # [0] and [2] are host1 and host3
        info = {'expected_objs': [self.fake_hosts[0], self.fake_hosts[2]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_no_matching_force_hosts(self):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=['fake_host5', 'fake_host6'],
            force_nodes=[])

        info = {'expected_objs': [],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_ignore_and_force_hosts(self):
        # Ensure ignore_hosts processed before force_hosts in host filters.
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=['fake_host1'],
            force_hosts=['fake_host3', 'fake_host1'],
            force_nodes=[])

        # only fake_host3 should be left.
        info = {'expected_objs': [self.fake_hosts[2]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_force_host_and_many_nodes(self):
        # Ensure all nodes returned for a host with many nodes
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=['fake_multihost'],
            force_nodes=[])

        info = {'expected_objs': [self.fake_hosts[4], self.fake_hosts[5],
                                  self.fake_hosts[6], self.fake_hosts[7]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_force_nodes(self):
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=[],
            force_nodes=['fake-node2', 'fake-node4', 'fake-node9'])

        # [5] is fake-node2, [7] is fake-node4
        info = {'expected_objs': [self.fake_hosts[5], self.fake_hosts[7]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_force_hosts_and_nodes(self):
        # Ensure only overlapping results if both force host and node
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=['fake_host1', 'fake_multihost'],
            force_nodes=['fake-node2', 'fake-node9'])

        # [5] is fake-node2
        info = {'expected_objs': [self.fake_hosts[5]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_force_hosts_and_wrong_nodes(self):
        # Ensure non-overlapping force_node and force_host yield no result
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=[],
            force_hosts=['fake_multihost'],
            force_nodes=['fake-node'])

        info = {'expected_objs': [],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_ignore_hosts_and_force_nodes(self):
        # Ensure ignore_hosts can coexist with force_nodes
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=['fake_host1', 'fake_host2'],
            force_hosts=[],
            force_nodes=['fake-node4', 'fake-node2'])

        info = {'expected_objs': [self.fake_hosts[5], self.fake_hosts[7]],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

    def test_get_filtered_hosts_with_ignore_hosts_and_force_same_nodes(self):
        # Ensure ignore_hosts is processed before force_nodes
        fake_properties = objects.RequestSpec(
            instance_uuid='fake-uuid',
            ignore_hosts=['fake_multihost'],
            force_hosts=[],
            force_nodes=['fake_node4', 'fake_node2'])

        info = {'expected_objs': [],
                'expected_fprops': fake_properties}
        self._mock_get_filtered_hosts(info)

        result = self.host_manager.get_filtered_hosts(self.fake_hosts,
                fake_properties)
        self._verify_result(info, result, False)

# Copyright 2011 University of Southern California
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Unit Tests for instance types extra specs code
"""

from nova.compute import arch
from nova import context
from nova import objects
from nova import test


class InstanceTypeExtraSpecsTestCase(test.TestCase):

    def setUp(self):
        super(InstanceTypeExtraSpecsTestCase, self).setUp()
        self.context = context.get_admin_context()
        flavor = objects.Flavor(context=self.context,
                                name="cg1.4xlarge",
                                memory_mb=22000,
                                vcpus=8,
                                root_gb=1690,
                                ephemeral_gb=2000,
                                flavorid=105)
        self.specs = dict(cpu_arch=arch.X86_64,
                          cpu_model="Nehalem",
                          xpu_arch="fermi",
                          xpus="2",
                          xpu_model="Tesla 2050")
        flavor.extra_specs = self.specs
        flavor.create()
        self.flavor = flavor
        self.instance_type_id = flavor.id
        self.flavorid = flavor.flavorid

    def tearDown(self):
        # Remove the instance type from the database
        self.flavor.destroy()
        super(InstanceTypeExtraSpecsTestCase, self).tearDown()

    def test_instance_type_specs_get(self):
        flavor = objects.Flavor.get_by_flavor_id(self.context,
                                                 self.flavorid)
        self.assertEqual(self.specs, flavor.extra_specs)

    def test_flavor_extra_specs_delete(self):
        del self.specs["xpu_model"]
        del self.flavor.extra_specs['xpu_model']
        self.flavor.save()
        flavor = objects.Flavor.get_by_flavor_id(self.context,
                                                 self.flavorid)
        self.assertEqual(self.specs, flavor.extra_specs)

    def test_instance_type_extra_specs_update(self):
        self.specs["cpu_model"] = "Sandy Bridge"
        self.flavor.extra_specs["cpu_model"] = "Sandy Bridge"
        self.flavor.save()
        flavor = objects.Flavor.get_by_flavor_id(self.context,
                                                 self.flavorid)
        self.assertEqual(self.specs, flavor.extra_specs)

    def test_instance_type_extra_specs_create(self):
        net_attrs = {
            "net_arch": "ethernet",
            "net_mbps": "10000"
        }
        self.specs.update(net_attrs)
        self.flavor.extra_specs.update(net_attrs)
        self.flavor.save()
        flavor = objects.Flavor.get_by_flavor_id(self.context,
                                                 self.flavorid)
        self.assertEqual(self.specs, flavor.extra_specs)

    def test_instance_type_get_with_extra_specs(self):
        flavor = objects.Flavor.get_by_id(self.context, 5)
        self.assertEqual(flavor.extra_specs, {})

    def test_instance_type_get_by_name_with_extra_specs(self):
        flavor = objects.Flavor.get_by_name(self.context,
                                            "cg1.4xlarge")
        self.assertEqual(flavor.extra_specs, self.specs)
        flavor = objects.Flavor.get_by_name(self.context,
                                            "m1.small")
        self.assertEqual(flavor.extra_specs, {})

    def test_instance_type_get_by_flavor_id_with_extra_specs(self):
        flavor = objects.Flavor.get_by_flavor_id(self.context, 105)
        self.assertEqual(flavor.extra_specs, self.specs)
        flavor = objects.Flavor.get_by_flavor_id(self.context, 2)
        self.assertEqual(flavor.extra_specs, {})

    def test_instance_type_get_all(self):
        flavors = objects.FlavorList.get_all(self.context)

        name2specs = {flavor.name: flavor.extra_specs
                      for flavor in flavors}

        self.assertEqual(name2specs['cg1.4xlarge'], self.specs)
        self.assertEqual(name2specs['m1.small'], {})

# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import tempfile

import fixtures
import mock
from oslo_concurrency import processutils

from nova import test
from nova import utils
from nova.virt.disk import api
from nova.virt.disk.mount import api as mount
from nova.virt.image import model as imgmodel


class FakeMount(object):
    device = None

    @staticmethod
    def instance_for_format(image, mountdir, partition):
        return FakeMount()

    def get_dev(self):
        pass

    def unget_dev(self):
        pass


class APITestCase(test.NoDBTestCase):
    def test_can_resize_need_fs_type_specified(self):
        # NOTE(mikal): Bug 1094373 saw a regression where we failed to
        # treat a failure to mount as a failure to be able to resize the
        # filesystem
        def _fake_get_disk_size(path):
            return 10
        self.useFixture(fixtures.MonkeyPatch(
                'nova.virt.disk.api.get_disk_size', _fake_get_disk_size))

        def fake_trycmd(*args, **kwargs):
            return '', 'broken'
        self.useFixture(fixtures.MonkeyPatch('nova.utils.trycmd', fake_trycmd))

        def fake_returns_true(*args, **kwargs):
            return True

        def fake_returns_nothing(*args, **kwargs):
            return ''
        self.useFixture(fixtures.MonkeyPatch(
                'nova.virt.disk.mount.nbd.NbdMount.get_dev',
                fake_returns_true))
        self.useFixture(fixtures.MonkeyPatch(
                'nova.virt.disk.mount.nbd.NbdMount.map_dev',
                fake_returns_true))
        self.useFixture(fixtures.MonkeyPatch(
                'nova.virt.disk.vfs.localfs.VFSLocalFS.get_image_fs',
                fake_returns_nothing))

        # Force the use of localfs, which is what was used during the failure
        # reported in the bug
        def fake_import_fails(*args, **kwargs):
            raise Exception('Failed')
        self.useFixture(fixtures.MonkeyPatch(
                'oslo_utils.import_module',
                fake_import_fails))

        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)
        image = imgmodel.LocalFileImage(imgfile.name, imgmodel.FORMAT_QCOW2)
        self.assertFalse(api.is_image_extendable(image))

    def test_is_image_extendable_raw(self):
        imgfile = tempfile.NamedTemporaryFile()

        self.mox.StubOutWithMock(utils, 'execute')
        utils.execute('e2label', imgfile)
        self.mox.ReplayAll()

        image = imgmodel.LocalFileImage(imgfile, imgmodel.FORMAT_RAW)
        self.addCleanup(imgfile.close)
        self.assertTrue(api.is_image_extendable(image))

    def test_resize2fs_success(self):
        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)

        self.mox.StubOutWithMock(utils, 'execute')
        utils.execute('e2fsck',
                      '-fp',
                      imgfile,
                      check_exit_code=[0, 1, 2],
                      run_as_root=False)
        utils.execute('resize2fs',
                      imgfile,
                      check_exit_code=False,
                      run_as_root=False)

        self.mox.ReplayAll()
        api.resize2fs(imgfile)

    def test_resize2fs_e2fsck_fails(self):
        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)

        self.mox.StubOutWithMock(utils, 'execute')
        utils.execute('e2fsck',
                      '-fp',
                      imgfile,
                      check_exit_code=[0, 1, 2],
                      run_as_root=False).AndRaise(
                          processutils.ProcessExecutionError("fs error"))
        self.mox.ReplayAll()
        api.resize2fs(imgfile)

    def test_extend_qcow_success(self):
        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)
        imgsize = 10
        device = "/dev/sdh"
        image = imgmodel.LocalFileImage(imgfile, imgmodel.FORMAT_QCOW2)

        self.flags(resize_fs_using_block_device=True)
        mounter = FakeMount.instance_for_format(
            image, None, None)
        mounter.device = device

        self.mox.StubOutWithMock(api, 'can_resize_image')
        self.mox.StubOutWithMock(utils, 'execute')
        self.mox.StubOutWithMock(api, 'is_image_extendable')
        self.mox.StubOutWithMock(mounter, 'get_dev')
        self.mox.StubOutWithMock(mounter, 'unget_dev')
        self.mox.StubOutWithMock(api, 'resize2fs')
        self.mox.StubOutWithMock(mount.Mount, 'instance_for_format',
                                 use_mock_anything=True)

        api.can_resize_image(imgfile, imgsize).AndReturn(True)
        utils.execute('qemu-img', 'resize', imgfile, imgsize)
        api.is_image_extendable(image).AndReturn(True)
        mount.Mount.instance_for_format(image, None, None).AndReturn(mounter)
        mounter.get_dev().AndReturn(True)
        api.resize2fs(mounter.device, run_as_root=True, check_exit_code=[0])
        mounter.unget_dev()

        self.mox.ReplayAll()
        api.extend(image, imgsize)

    @mock.patch.object(api, 'can_resize_image', return_value=True)
    @mock.patch.object(api, 'is_image_extendable')
    @mock.patch.object(utils, 'execute')
    def test_extend_qcow_no_resize(self, mock_execute, mock_extendable,
                                   mock_can_resize_image):
        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)
        imgsize = 10
        image = imgmodel.LocalFileImage(imgfile, imgmodel.FORMAT_QCOW2)

        self.flags(resize_fs_using_block_device=False)

        api.extend(image, imgsize)

        mock_can_resize_image.assert_called_once_with(imgfile, imgsize)
        mock_execute.assert_called_once_with('qemu-img', 'resize', imgfile,
                                             imgsize)
        self.assertFalse(mock_extendable.called)

    def test_extend_raw_success(self):
        imgfile = tempfile.NamedTemporaryFile()
        self.addCleanup(imgfile.close)
        imgsize = 10
        image = imgmodel.LocalFileImage(imgfile, imgmodel.FORMAT_RAW)

        self.mox.StubOutWithMock(api, 'can_resize_image')
        self.mox.StubOutWithMock(utils, 'execute')
        self.mox.StubOutWithMock(api, 'resize2fs')

        api.can_resize_image(imgfile, imgsize).AndReturn(True)
        utils.execute('qemu-img', 'resize', imgfile, imgsize)
        utils.execute('e2label', image.path)
        api.resize2fs(imgfile, run_as_root=False, check_exit_code=[0])

        self.mox.ReplayAll()
        api.extend(image, imgsize)

    HASH_VFAT = utils.get_hash_str(api.FS_FORMAT_VFAT)[:7]
    HASH_EXT4 = utils.get_hash_str(api.FS_FORMAT_EXT4)[:7]
    HASH_NTFS = utils.get_hash_str(api.FS_FORMAT_NTFS)[:7]

    def test_get_file_extension_for_os_type(self):
        self.assertEqual(self.HASH_VFAT,
                         api.get_file_extension_for_os_type(None, None))
        self.assertEqual(self.HASH_EXT4,
                         api.get_file_extension_for_os_type('linux', None))
        self.assertEqual(self.HASH_NTFS,
                         api.get_file_extension_for_os_type(
                             'windows', None))

    def test_get_file_extension_for_os_type_with_overrides(self):
        with mock.patch('nova.virt.disk.api._DEFAULT_MKFS_COMMAND',
                        'custom mkfs command'):
            self.assertEqual("a74d253",
                             api.get_file_extension_for_os_type(
                                 'linux', None))
            self.assertEqual("a74d253",
                             api.get_file_extension_for_os_type(
                                 'windows', None))
            self.assertEqual("a74d253",
                             api.get_file_extension_for_os_type('osx', None))

        with mock.patch.dict(api._MKFS_COMMAND,
                             {'osx': 'custom mkfs command'}, clear=True):
            self.assertEqual(self.HASH_VFAT,
                             api.get_file_extension_for_os_type(None, None))
            self.assertEqual(self.HASH_EXT4,
                             api.get_file_extension_for_os_type('linux', None))
            self.assertEqual(self.HASH_NTFS,
                             api.get_file_extension_for_os_type(
                                 'windows', None))
            self.assertEqual("a74d253",
                             api.get_file_extension_for_os_type(
                                 'osx', None))

#    Copyright 2010 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time
import uuid

import fixtures
from lxml import etree
import six

from nova.compute import arch
from nova.virt.libvirt import config as vconfig

# Allow passing None to the various connect methods
# (i.e. allow the client to rely on default URLs)
allow_default_uri_connection = True

# Has libvirt connection been used at least once
connection_used = False


def _reset():
    global allow_default_uri_connection
    allow_default_uri_connection = True

# virDomainState
VIR_DOMAIN_NOSTATE = 0
VIR_DOMAIN_RUNNING = 1
VIR_DOMAIN_BLOCKED = 2
VIR_DOMAIN_PAUSED = 3
VIR_DOMAIN_SHUTDOWN = 4
VIR_DOMAIN_SHUTOFF = 5
VIR_DOMAIN_CRASHED = 6

# NOTE(mriedem): These values come from include/libvirt/libvirt-domain.h
VIR_DOMAIN_XML_SECURE = 1
VIR_DOMAIN_XML_INACTIVE = 2
VIR_DOMAIN_XML_UPDATE_CPU = 4
VIR_DOMAIN_XML_MIGRATABLE = 8

VIR_DOMAIN_BLOCK_REBASE_SHALLOW = 1
VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT = 2
VIR_DOMAIN_BLOCK_REBASE_COPY = 8

VIR_DOMAIN_BLOCK_JOB_ABORT_ASYNC = 1
VIR_DOMAIN_BLOCK_JOB_ABORT_PIVOT = 2

VIR_DOMAIN_EVENT_ID_LIFECYCLE = 0

VIR_DOMAIN_EVENT_DEFINED = 0
VIR_DOMAIN_EVENT_UNDEFINED = 1
VIR_DOMAIN_EVENT_STARTED = 2
VIR_DOMAIN_EVENT_SUSPENDED = 3
VIR_DOMAIN_EVENT_RESUMED = 4
VIR_DOMAIN_EVENT_STOPPED = 5
VIR_DOMAIN_EVENT_SHUTDOWN = 6
VIR_DOMAIN_EVENT_PMSUSPENDED = 7

VIR_DOMAIN_UNDEFINE_MANAGED_SAVE = 1

VIR_DOMAIN_AFFECT_CURRENT = 0
VIR_DOMAIN_AFFECT_LIVE = 1
VIR_DOMAIN_AFFECT_CONFIG = 2

VIR_CPU_COMPARE_ERROR = -1
VIR_CPU_COMPARE_INCOMPATIBLE = 0
VIR_CPU_COMPARE_IDENTICAL = 1
VIR_CPU_COMPARE_SUPERSET = 2

VIR_CRED_USERNAME = 1
VIR_CRED_AUTHNAME = 2
VIR_CRED_LANGUAGE = 3
VIR_CRED_CNONCE = 4
VIR_CRED_PASSPHRASE = 5
VIR_CRED_ECHOPROMPT = 6
VIR_CRED_NOECHOPROMPT = 7
VIR_CRED_REALM = 8
VIR_CRED_EXTERNAL = 9

VIR_MIGRATE_LIVE = 1
VIR_MIGRATE_PEER2PEER = 2
VIR_MIGRATE_TUNNELLED = 4
VIR_MIGRATE_PERSIST_DEST = 8
VIR_MIGRATE_UNDEFINE_SOURCE = 16
VIR_MIGRATE_NON_SHARED_INC = 128

VIR_NODE_CPU_STATS_ALL_CPUS = -1

VIR_DOMAIN_START_PAUSED = 1

# libvirtError enums
# (Intentionally different from what's in libvirt. We do this to check,
#  that consumers of the library are using the symbolic names rather than
#  hardcoding the numerical values)
VIR_FROM_QEMU = 100
VIR_FROM_DOMAIN = 200
VIR_FROM_NWFILTER = 330
VIR_FROM_REMOTE = 340
VIR_FROM_RPC = 345
VIR_FROM_NODEDEV = 666
VIR_ERR_INVALID_ARG = 8
VIR_ERR_NO_SUPPORT = 3
VIR_ERR_XML_DETAIL = 350
VIR_ERR_NO_DOMAIN = 420
VIR_ERR_OPERATION_FAILED = 510
VIR_ERR_OPERATION_INVALID = 55
VIR_ERR_OPERATION_TIMEOUT = 68
VIR_ERR_NO_NWFILTER = 620
VIR_ERR_SYSTEM_ERROR = 900
VIR_ERR_INTERNAL_ERROR = 950
VIR_ERR_CONFIG_UNSUPPORTED = 951
VIR_ERR_NO_NODE_DEVICE = 667
VIR_ERR_NO_SECRET = 66

# Readonly
VIR_CONNECT_RO = 1

# virConnectBaselineCPU flags
VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES = 1

# snapshotCreateXML flags
VIR_DOMAIN_SNAPSHOT_CREATE_NO_METADATA = 4
VIR_DOMAIN_SNAPSHOT_CREATE_DISK_ONLY = 16
VIR_DOMAIN_SNAPSHOT_CREATE_REUSE_EXT = 32
VIR_DOMAIN_SNAPSHOT_CREATE_QUIESCE = 64

# blockCommit flags
VIR_DOMAIN_BLOCK_COMMIT_RELATIVE = 4
# blockRebase flags
VIR_DOMAIN_BLOCK_REBASE_RELATIVE = 8


VIR_CONNECT_LIST_DOMAINS_ACTIVE = 1
VIR_CONNECT_LIST_DOMAINS_INACTIVE = 2

# secret type
VIR_SECRET_USAGE_TYPE_NONE = 0
VIR_SECRET_USAGE_TYPE_VOLUME = 1
VIR_SECRET_USAGE_TYPE_CEPH = 2
VIR_SECRET_USAGE_TYPE_ISCSI = 3

# Libvirt version
FAKE_LIBVIRT_VERSION = 10002


class HostInfo(object):
    def __init__(self, arch=arch.X86_64, kB_mem=4096,
                 cpus=2, cpu_mhz=800, cpu_nodes=1,
                 cpu_sockets=1, cpu_cores=2,
                 cpu_threads=1, cpu_model="Penryn",
                 cpu_vendor="Intel", numa_topology='',
                 cpu_disabled=None):
        """Create a new Host Info object

        :param arch: (string) indicating the CPU arch
                     (eg 'i686' or whatever else uname -m might return)
        :param kB_mem: (int) memory size in KBytes
        :param cpus: (int) the number of active CPUs
        :param cpu_mhz: (int) expected CPU frequency
        :param cpu_nodes: (int) the number of NUMA cell, 1 for unusual
                          NUMA topologies or uniform
        :param cpu_sockets: (int) number of CPU sockets per node if nodes > 1,
                            total number of CPU sockets otherwise
        :param cpu_cores: (int) number of cores per socket
        :param cpu_threads: (int) number of threads per core
        :param cpu_model: CPU model
        :param cpu_vendor: CPU vendor
        :param numa_topology: Numa topology
        :param cpu_disabled: List of disabled cpus
        """

        self.arch = arch
        self.kB_mem = kB_mem
        self.cpus = cpus
        self.cpu_mhz = cpu_mhz
        self.cpu_nodes = cpu_nodes
        self.cpu_cores = cpu_cores
        self.cpu_threads = cpu_threads
        self.cpu_sockets = cpu_sockets
        self.cpu_model = cpu_model
        self.cpu_vendor = cpu_vendor
        self.numa_topology = numa_topology
        self.disabled_cpus_list = cpu_disabled or []

    @classmethod
    def _gen_numa_topology(self, cpu_nodes, cpu_sockets, cpu_cores,
                           cpu_threads, kb_mem, numa_mempages_list=None):

        topology = vconfig.LibvirtConfigCapsNUMATopology()

        cpu_count = 0
        for cell_count in range(cpu_nodes):
            cell = vconfig.LibvirtConfigCapsNUMACell()
            cell.id = cell_count
            cell.memory = kb_mem / cpu_nodes
            for socket_count in range(cpu_sockets):
                for cpu_num in range(cpu_cores * cpu_threads):
                    cpu = vconfig.LibvirtConfigCapsNUMACPU()
                    cpu.id = cpu_count
                    cpu.socket_id = cell_count
                    cpu.core_id = cpu_num // cpu_threads
                    cpu.siblings = set([cpu_threads *
                                       (cpu_count // cpu_threads) + thread
                                        for thread in range(cpu_threads)])
                    cell.cpus.append(cpu)

                    cpu_count += 1
            # Set mempages per numa cell. if numa_mempages_list is empty
            # we will set only the default 4K pages.
            if numa_mempages_list:
                mempages = numa_mempages_list[cell_count]
            else:
                mempages = vconfig.LibvirtConfigCapsNUMAPages()
                mempages.size = 4
                mempages.total = cell.memory / mempages.size
                mempages = [mempages]
            cell.mempages = mempages
            topology.cells.append(cell)

        return topology

    def get_numa_topology(self):
        return self.numa_topology


VIR_DOMAIN_JOB_NONE = 0
VIR_DOMAIN_JOB_BOUNDED = 1
VIR_DOMAIN_JOB_UNBOUNDED = 2
VIR_DOMAIN_JOB_COMPLETED = 3
VIR_DOMAIN_JOB_FAILED = 4
VIR_DOMAIN_JOB_CANCELLED = 5


def _parse_disk_info(element):
    disk_info = {}
    disk_info['type'] = element.get('type', 'file')
    disk_info['device'] = element.get('device', 'disk')

    driver = element.find('./driver')
    if driver is not None:
        disk_info['driver_name'] = driver.get('name')
        disk_info['driver_type'] = driver.get('type')

    source = element.find('./source')
    if source is not None:
        disk_info['source'] = source.get('file')
        if not disk_info['source']:
            disk_info['source'] = source.get('dev')

        if not disk_info['source']:
            disk_info['source'] = source.get('path')

    target = element.find('./target')
    if target is not None:
        disk_info['target_dev'] = target.get('dev')
        disk_info['target_bus'] = target.get('bus')

    return disk_info


def disable_event_thread(self):
    """Disable nova libvirt driver event thread.

    The Nova libvirt driver includes a native thread which monitors
    the libvirt event channel. In a testing environment this becomes
    problematic because it means we've got a floating thread calling
    sleep(1) over the life of the unit test. Seems harmless? It's not,
    because we sometimes want to test things like retry loops that
    should have specific sleep paterns. An unlucky firing of the
    libvirt thread will cause a test failure.

    """
    # because we are patching a method in a class MonkeyPatch doesn't
    # auto import correctly. Import explicitly otherwise the patching
    # may silently fail.
    import nova.virt.libvirt.host  # noqa

    def evloop(*args, **kwargs):
        pass

    self.useFixture(fixtures.MonkeyPatch(
        'nova.virt.libvirt.host.Host._init_events',
        evloop))


class libvirtError(Exception):
    """This class was copied and slightly modified from
    `libvirt-python:libvirt-override.py`.

    Since a test environment will use the real `libvirt-python` version of
    `libvirtError` if it's installed and not this fake, we need to maintain
    strict compatibility with the original class, including `__init__` args
    and instance-attributes.

    To create a libvirtError instance you should:

        # Create an unsupported error exception
        exc = libvirtError('my message')
        exc.err = (libvirt.VIR_ERR_NO_SUPPORT,)

    self.err is a tuple of form:
        (error_code, error_domain, error_message, error_level, str1, str2,
         str3, int1, int2)

    Alternatively, you can use the `make_libvirtError` convenience function to
    allow you to specify these attributes in one shot.
    """
    def __init__(self, defmsg, conn=None, dom=None, net=None, pool=None,
                 vol=None):
        Exception.__init__(self, defmsg)
        self.err = None

    def get_error_code(self):
        if self.err is None:
            return None
        return self.err[0]

    def get_error_domain(self):
        if self.err is None:
            return None
        return self.err[1]

    def get_error_message(self):
        if self.err is None:
            return None
        return self.err[2]

    def get_error_level(self):
        if self.err is None:
            return None
        return self.err[3]

    def get_str1(self):
        if self.err is None:
            return None
        return self.err[4]

    def get_str2(self):
        if self.err is None:
            return None
        return self.err[5]

    def get_str3(self):
        if self.err is None:
            return None
        return self.err[6]

    def get_int1(self):
        if self.err is None:
            return None
        return self.err[7]

    def get_int2(self):
        if self.err is None:
            return None
        return self.err[8]


class NWFilter(object):
    def __init__(self, connection, xml):
        self._connection = connection

        self._xml = xml
        self._parse_xml(xml)

    def _parse_xml(self, xml):
        tree = etree.fromstring(xml)
        root = tree.find('.')
        self._name = root.get('name')

    def undefine(self):
        self._connection._remove_filter(self)


class NodeDevice(object):

    def __init__(self, connection, xml=None):
        self._connection = connection

        self._xml = xml
        if xml is not None:
            self._parse_xml(xml)

    def _parse_xml(self, xml):
        tree = etree.fromstring(xml)
        root = tree.find('.')
        self._name = root.get('name')

    def attach(self):
        pass

    def dettach(self):
        pass

    def reset(self):
        pass


class Domain(object):
    def __init__(self, connection, xml, running=False, transient=False):
        self._connection = connection
        if running:
            connection._mark_running(self)

        self._state = running and VIR_DOMAIN_RUNNING or VIR_DOMAIN_SHUTOFF
        self._transient = transient
        self._def = self._parse_definition(xml)
        self._has_saved_state = False
        self._snapshots = {}
        self._id = self._connection._id_counter

    def _parse_definition(self, xml):
        try:
            tree = etree.fromstring(xml)
        except etree.ParseError:
            raise make_libvirtError(
                    libvirtError, "Invalid XML.",
                    error_code=VIR_ERR_XML_DETAIL,
                    error_domain=VIR_FROM_DOMAIN)

        definition = {}

        name = tree.find('./name')
        if name is not None:
            definition['name'] = name.text

        uuid_elem = tree.find('./uuid')
        if uuid_elem is not None:
            definition['uuid'] = uuid_elem.text
        else:
            definition['uuid'] = str(uuid.uuid4())

        vcpu = tree.find('./vcpu')
        if vcpu is not None:
            definition['vcpu'] = int(vcpu.text)

        memory = tree.find('./memory')
        if memory is not None:
            definition['memory'] = int(memory.text)

        os = {}
        os_type = tree.find('./os/type')
        if os_type is not None:
            os['type'] = os_type.text
            os['arch'] = os_type.get('arch', self._connection.host_info.arch)

        os_kernel = tree.find('./os/kernel')
        if os_kernel is not None:
            os['kernel'] = os_kernel.text

        os_initrd = tree.find('./os/initrd')
        if os_initrd is not None:
            os['initrd'] = os_initrd.text

        os_cmdline = tree.find('./os/cmdline')
        if os_cmdline is not None:
            os['cmdline'] = os_cmdline.text

        os_boot = tree.find('./os/boot')
        if os_boot is not None:
            os['boot_dev'] = os_boot.get('dev')

        definition['os'] = os

        features = {}

        acpi = tree.find('./features/acpi')
        if acpi is not None:
            features['acpi'] = True

        definition['features'] = features

        devices = {}

        device_nodes = tree.find('./devices')
        if device_nodes is not None:
            disks_info = []
            disks = device_nodes.findall('./disk')
            for disk in disks:
                disks_info += [_parse_disk_info(disk)]
            devices['disks'] = disks_info

            nics_info = []
            nics = device_nodes.findall('./interface')
            for nic in nics:
                nic_info = {}
                nic_info['type'] = nic.get('type')

                mac = nic.find('./mac')
                if mac is not None:
                    nic_info['mac'] = mac.get('address')

                source = nic.find('./source')
                if source is not None:
                    if nic_info['type'] == 'network':
                        nic_info['source'] = source.get('network')
                    elif nic_info['type'] == 'bridge':
                        nic_info['source'] = source.get('bridge')

                nics_info += [nic_info]

            devices['nics'] = nics_info

        definition['devices'] = devices

        return definition

    def create(self):
        self.createWithFlags(0)

    def createWithFlags(self, flags):
        # FIXME: Not handling flags at the moment
        self._state = VIR_DOMAIN_RUNNING
        self._connection._mark_running(self)
        self._has_saved_state = False

    def isActive(self):
        return int(self._state == VIR_DOMAIN_RUNNING)

    def undefine(self):
        self._connection._undefine(self)

    def isPersistent(self):
        return True

    def undefineFlags(self, flags):
        self.undefine()
        if flags & VIR_DOMAIN_UNDEFINE_MANAGED_SAVE:
            if self.hasManagedSaveImage(0):
                self.managedSaveRemove()

    def destroy(self):
        self._state = VIR_DOMAIN_SHUTOFF
        self._connection._mark_not_running(self)

    def ID(self):
        return self._id

    def name(self):
        return self._def['name']

    def UUIDString(self):
        return self._def['uuid']

    def interfaceStats(self, device):
        return [10000242400, 1234, 0, 2, 213412343233, 34214234, 23, 3]

    def blockStats(self, device):
        return [2, 10000242400, 234, 2343424234, 34]

    def suspend(self):
        self._state = VIR_DOMAIN_PAUSED

    def shutdown(self):
        self._state = VIR_DOMAIN_SHUTDOWN
        self._connection._mark_not_running(self)

    def reset(self, flags):
        # FIXME: Not handling flags at the moment
        self._state = VIR_DOMAIN_RUNNING
        self._connection._mark_running(self)

    def info(self):
        return [self._state,
                int(self._def['memory']),
                int(self._def['memory']),
                self._def['vcpu'],
                123456789]

    def migrateToURI(self, desturi, flags, dname, bandwidth):
        raise make_libvirtError(
                libvirtError,
                "Migration always fails for fake libvirt!",
                error_code=VIR_ERR_INTERNAL_ERROR,
                error_domain=VIR_FROM_QEMU)

    def migrateToURI2(self, dconnuri, miguri, dxml, flags, dname, bandwidth):
        raise make_libvirtError(
                libvirtError,
                "Migration always fails for fake libvirt!",
                error_code=VIR_ERR_INTERNAL_ERROR,
                error_domain=VIR_FROM_QEMU)

    def migrateToURI3(self, dconnuri, params, logical_sum):
        raise make_libvirtError(
                libvirtError,
                "Migration always fails for fake libvirt!",
                error_code=VIR_ERR_INTERNAL_ERROR,
                error_domain=VIR_FROM_QEMU)

    def migrateSetMaxDowntime(self, downtime):
        pass

    def attachDevice(self, xml):
        disk_info = _parse_disk_info(etree.fromstring(xml))
        disk_info['_attached'] = True
        self._def['devices']['disks'] += [disk_info]
        return True

    def attachDeviceFlags(self, xml, flags):
        if (flags & VIR_DOMAIN_AFFECT_LIVE and
                self._state != VIR_DOMAIN_RUNNING):
            raise make_libvirtError(
                libvirtError,
                "AFFECT_LIVE only allowed for running domains!",
                error_code=VIR_ERR_INTERNAL_ERROR,
                error_domain=VIR_FROM_QEMU)
        self.attachDevice(xml)

    def detachDevice(self, xml):
        disk_info = _parse_disk_info(etree.fromstring(xml))
        disk_info['_attached'] = True
        return disk_info in self._def['devices']['disks']

    def detachDeviceFlags(self, xml, flags):
        self.detachDevice(xml)

    def setUserPassword(self, user, password, flags=0):
        pass

    def XMLDesc(self, flags):
        disks = ''
        for disk in self._def['devices']['disks']:
            disks += '''<disk type='%(type)s' device='%(device)s'>
      <driver name='%(driver_name)s' type='%(driver_type)s'/>
      <source file='%(source)s'/>
      <target dev='%(target_dev)s' bus='%(target_bus)s'/>
      <address type='drive' controller='0' bus='0' unit='0'/>
    </disk>''' % disk

        nics = ''
        for nic in self._def['devices']['nics']:
            nics += '''<interface type='%(type)s'>
      <mac address='%(mac)s'/>
      <source %(type)s='%(source)s'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x03'
               function='0x0'/>
    </interface>''' % nic

        return '''<domain type='kvm'>
  <name>%(name)s</name>
  <uuid>%(uuid)s</uuid>
  <memory>%(memory)s</memory>
  <currentMemory>%(memory)s</currentMemory>
  <vcpu>%(vcpu)s</vcpu>
  <os>
    <type arch='%(arch)s' machine='pc-0.12'>hvm</type>
    <boot dev='hd'/>
  </os>
  <features>
    <acpi/>
    <apic/>
    <pae/>
  </features>
  <clock offset='localtime'/>
  <on_poweroff>destroy</on_poweroff>
  <on_reboot>restart</on_reboot>
  <on_crash>restart</on_crash>
  <devices>
    <emulator>/usr/bin/kvm</emulator>
    %(disks)s
    <controller type='ide' index='0'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x01'
               function='0x1'/>
    </controller>
    %(nics)s
    <serial type='file'>
      <source path='dummy.log'/>
      <target port='0'/>
    </serial>
    <serial type='pty'>
      <source pty='/dev/pts/27'/>
      <target port='1'/>
    </serial>
    <serial type='tcp'>
      <source host="-1" service="-1" mode="bind"/>
    </serial>
    <console type='file'>
      <source path='dummy.log'/>
      <target port='0'/>
    </console>
    <input type='tablet' bus='usb'/>
    <input type='mouse' bus='ps2'/>
    <graphics type='vnc' port='-1' autoport='yes'/>
    <graphics type='spice' port='-1' autoport='yes'/>
    <video>
      <model type='cirrus' vram='9216' heads='1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x02'
               function='0x0'/>
    </video>
    <memballoon model='virtio'>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x04'
               function='0x0'/>
    </memballoon>
  </devices>
</domain>''' % {'name': self._def['name'],
                'uuid': self._def['uuid'],
                'memory': self._def['memory'],
                'vcpu': self._def['vcpu'],
                'arch': self._def['os']['arch'],
                'disks': disks,
                'nics': nics}

    def managedSave(self, flags):
        self._connection._mark_not_running(self)
        self._has_saved_state = True

    def managedSaveRemove(self, flags):
        self._has_saved_state = False

    def hasManagedSaveImage(self, flags):
        return int(self._has_saved_state)

    def resume(self):
        self._state = VIR_DOMAIN_RUNNING

    def snapshotCreateXML(self, xml, flags):
        tree = etree.fromstring(xml)
        name = tree.find('./name').text
        snapshot = DomainSnapshot(name, self)
        self._snapshots[name] = snapshot
        return snapshot

    def vcpus(self):
        vcpus = ([], [])
        for i in range(0, self._def['vcpu']):
            vcpus[0].append((i, 1, 120405, i))
            vcpus[1].append((True, True, True, True))
        return vcpus

    def memoryStats(self):
        return {}

    def maxMemory(self):
        return self._def['memory']

    def blockJobInfo(self, disk, flags):
        return {}

    def blockJobAbort(self, disk, flags):
        pass

    def blockResize(self, disk, size):
        pass

    def blockRebase(self, disk, base, bandwidth=0, flags=0):
        if (not base) and (flags and VIR_DOMAIN_BLOCK_REBASE_RELATIVE):
            raise make_libvirtError(
                    libvirtError,
                    'flag VIR_DOMAIN_BLOCK_REBASE_RELATIVE is '
                    'valid only with non-null base',
                    error_code=VIR_ERR_INVALID_ARG,
                    error_domain=VIR_FROM_QEMU)
        return 0

    def blockCommit(self, disk, base, top, flags):
        return 0

    def jobInfo(self):
        # NOTE(danms): This is an array of 12 integers, so just report
        # something to avoid an IndexError if we look at this
        return [0] * 12

    def jobStats(self, flags=0):
        return {}

    def injectNMI(self, flags=0):
        return 0

    def abortJob(self):
        pass

    def fsFreeze(self):
        pass

    def fsThaw(self):
        pass


class DomainSnapshot(object):
    def __init__(self, name, domain):
        self._name = name
        self._domain = domain

    def delete(self, flags):
        del self._domain._snapshots[self._name]


class Connection(object):
    def __init__(self, uri=None, readonly=False, version=FAKE_LIBVIRT_VERSION,
                 hv_version=1001000, host_info=None):
        if not uri or uri == '':
            if allow_default_uri_connection:
                uri = 'qemu:///session'
            else:
                raise ValueError("URI was None, but fake libvirt is "
                                 "configured to not accept this.")

        uri_whitelist = ['qemu:///system',
                         'qemu:///session',
                         'lxc:///',     # from LibvirtDriver._uri()
                         'xen:///',     # from LibvirtDriver._uri()
                         'uml:///system',
                         'test:///default',
                         'parallels:///system']

        if uri not in uri_whitelist:
            raise make_libvirtError(
                    libvirtError,
                   "libvirt error: no connection driver "
                   "available for No connection for URI %s" % uri,
                   error_code=5, error_domain=0)

        self.readonly = readonly
        self._uri = uri
        self._vms = {}
        self._running_vms = {}
        self._id_counter = 1  # libvirt reserves 0 for the hypervisor.
        self._nwfilters = {}
        self._nodedevs = {}
        self._event_callbacks = {}
        self.fakeLibVersion = version
        self.fakeVersion = hv_version
        self.host_info = host_info or HostInfo()

    def _add_filter(self, nwfilter):
        self._nwfilters[nwfilter._name] = nwfilter

    def _remove_filter(self, nwfilter):
        del self._nwfilters[nwfilter._name]

    def _add_nodedev(self, nodedev):
        self._nodedevs[nodedev._name] = nodedev

    def _remove_nodedev(self, nodedev):
        del self._nodedevs[nodedev._name]

    def _mark_running(self, dom):
        self._running_vms[self._id_counter] = dom
        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)
        self._id_counter += 1

    def _mark_not_running(self, dom):
        if dom._transient:
            self._undefine(dom)

        dom._id = -1

        for (k, v) in six.iteritems(self._running_vms):
            if v == dom:
                del self._running_vms[k]
                self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STOPPED, 0)
                return

    def _undefine(self, dom):
        del self._vms[dom.name()]
        if not dom._transient:
            self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_UNDEFINED, 0)

    def getInfo(self):
        return [self.host_info.arch,
                self.host_info.kB_mem,
                self.host_info.cpus,
                self.host_info.cpu_mhz,
                self.host_info.cpu_nodes,
                self.host_info.cpu_sockets,
                self.host_info.cpu_cores,
                self.host_info.cpu_threads]

    def numOfDomains(self):
        return len(self._running_vms)

    def listDomainsID(self):
        return list(self._running_vms.keys())

    def lookupByID(self, id):
        if id in self._running_vms:
            return self._running_vms[id]
        raise make_libvirtError(
                libvirtError,
                'Domain not found: no domain with matching id %d' % id,
                error_code=VIR_ERR_NO_DOMAIN,
                error_domain=VIR_FROM_QEMU)

    def lookupByName(self, name):
        if name in self._vms:
            return self._vms[name]
        raise make_libvirtError(
                libvirtError,
                'Domain not found: no domain with matching name "%s"' % name,
                error_code=VIR_ERR_NO_DOMAIN,
                error_domain=VIR_FROM_QEMU)

    def listAllDomains(self, flags):
        vms = []
        for vm in self._vms:
            if flags & VIR_CONNECT_LIST_DOMAINS_ACTIVE:
                if vm.state != VIR_DOMAIN_SHUTOFF:
                    vms.append(vm)
            if flags & VIR_CONNECT_LIST_DOMAINS_INACTIVE:
                if vm.state == VIR_DOMAIN_SHUTOFF:
                    vms.append(vm)
        return vms

    def _emit_lifecycle(self, dom, event, detail):
        if VIR_DOMAIN_EVENT_ID_LIFECYCLE not in self._event_callbacks:
            return

        cbinfo = self._event_callbacks[VIR_DOMAIN_EVENT_ID_LIFECYCLE]
        callback = cbinfo[0]
        opaque = cbinfo[1]
        callback(self, dom, event, detail, opaque)

    def defineXML(self, xml):
        dom = Domain(connection=self, running=False, transient=False, xml=xml)
        self._vms[dom.name()] = dom
        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_DEFINED, 0)
        return dom

    def createXML(self, xml, flags):
        dom = Domain(connection=self, running=True, transient=True, xml=xml)
        self._vms[dom.name()] = dom
        self._emit_lifecycle(dom, VIR_DOMAIN_EVENT_STARTED, 0)
        return dom

    def getType(self):
        if self._uri == 'qemu:///system':
            return 'QEMU'

    def getLibVersion(self):
        return self.fakeLibVersion

    def getVersion(self):
        return self.fakeVersion

    def getHostname(self):
        return 'compute1'

    def domainEventRegisterAny(self, dom, eventid, callback, opaque):
        self._event_callbacks[eventid] = [callback, opaque]

    def registerCloseCallback(self, cb, opaque):
        pass

    def getCPUMap(self):
        """Return calculated CPU map from HostInfo, by default showing 2
           online CPUs.
        """
        active_cpus = self.host_info.cpus
        total_cpus = active_cpus + len(self.host_info.disabled_cpus_list)
        cpu_map = [True if cpu_num not in self.host_info.disabled_cpus_list
                   else False for cpu_num in range(total_cpus)]
        return (total_cpus, cpu_map, active_cpus)

    def getCapabilities(self):
        """Return spoofed capabilities."""
        numa_topology = self.host_info.get_numa_topology()
        if isinstance(numa_topology, vconfig.LibvirtConfigCapsNUMATopology):
            numa_topology = numa_topology.to_xml()

        return '''<capabilities>
  <host>
    <uuid>cef19ce0-0ca2-11df-855d-b19fbce37686</uuid>
    <cpu>
      <arch>x86_64</arch>
      <model>Penryn</model>
      <vendor>Intel</vendor>
      <topology sockets='%(sockets)s' cores='%(cores)s' threads='%(threads)s'/>
      <feature name='xtpr'/>
      <feature name='tm2'/>
      <feature name='est'/>
      <feature name='vmx'/>
      <feature name='ds_cpl'/>
      <feature name='monitor'/>
      <feature name='pbe'/>
      <feature name='tm'/>
      <feature name='ht'/>
      <feature name='ss'/>
      <feature name='acpi'/>
      <feature name='ds'/>
      <feature name='vme'/>
    </cpu>
    <migration_features>
      <live/>
      <uri_transports>
        <uri_transport>tcp</uri_transport>
      </uri_transports>
    </migration_features>
    %(topology)s
    <secmodel>
      <model>apparmor</model>
      <doi>0</doi>
    </secmodel>
  </host>

  <guest>
    <os_type>hvm</os_type>
    <arch name='i686'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu</emulator>
      <machine>pc-0.14</machine>
      <machine canonical='pc-0.14'>pc</machine>
      <machine>pc-0.13</machine>
      <machine>pc-0.12</machine>
      <machine>pc-0.11</machine>
      <machine>pc-0.10</machine>
      <machine>isapc</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/bin/kvm</emulator>
        <machine>pc-0.14</machine>
        <machine canonical='pc-0.14'>pc</machine>
        <machine>pc-0.13</machine>
        <machine>pc-0.12</machine>
        <machine>pc-0.11</machine>
        <machine>pc-0.10</machine>
        <machine>isapc</machine>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <pae/>
      <nonpae/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='x86_64'>
      <wordsize>64</wordsize>
      <emulator>/usr/bin/qemu-system-x86_64</emulator>
      <machine>pc-0.14</machine>
      <machine canonical='pc-0.14'>pc</machine>
      <machine>pc-0.13</machine>
      <machine>pc-0.12</machine>
      <machine>pc-0.11</machine>
      <machine>pc-0.10</machine>
      <machine>isapc</machine>
      <domain type='qemu'>
      </domain>
      <domain type='kvm'>
        <emulator>/usr/bin/kvm</emulator>
        <machine>pc-0.14</machine>
        <machine canonical='pc-0.14'>pc</machine>
        <machine>pc-0.13</machine>
        <machine>pc-0.12</machine>
        <machine>pc-0.11</machine>
        <machine>pc-0.10</machine>
        <machine>isapc</machine>
      </domain>
    </arch>
    <features>
      <cpuselection/>
      <deviceboot/>
      <acpi default='on' toggle='yes'/>
      <apic default='on' toggle='no'/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='armv7l'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-arm</emulator>
      <machine>integratorcp</machine>
      <machine>vexpress-a9</machine>
      <machine>syborg</machine>
      <machine>musicpal</machine>
      <machine>mainstone</machine>
      <machine>n800</machine>
      <machine>n810</machine>
      <machine>n900</machine>
      <machine>cheetah</machine>
      <machine>sx1</machine>
      <machine>sx1-v1</machine>
      <machine>beagle</machine>
      <machine>beaglexm</machine>
      <machine>tosa</machine>
      <machine>akita</machine>
      <machine>spitz</machine>
      <machine>borzoi</machine>
      <machine>terrier</machine>
      <machine>connex</machine>
      <machine>verdex</machine>
      <machine>lm3s811evb</machine>
      <machine>lm3s6965evb</machine>
      <machine>realview-eb</machine>
      <machine>realview-eb-mpcore</machine>
      <machine>realview-pb-a8</machine>
      <machine>realview-pbx-a9</machine>
      <machine>versatilepb</machine>
      <machine>versatileab</machine>
      <domain type='qemu'>
      </domain>
    </arch>
    <features>
      <deviceboot/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mips'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-mips</emulator>
      <machine>malta</machine>
      <machine>mipssim</machine>
      <machine>magnum</machine>
      <machine>pica61</machine>
      <machine>mips</machine>
      <domain type='qemu'>
      </domain>
    </arch>
    <features>
      <deviceboot/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='mipsel'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-mipsel</emulator>
      <machine>malta</machine>
      <machine>mipssim</machine>
      <machine>magnum</machine>
      <machine>pica61</machine>
      <machine>mips</machine>
      <domain type='qemu'>
      </domain>
    </arch>
    <features>
      <deviceboot/>
    </features>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='sparc'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-sparc</emulator>
      <machine>SS-5</machine>
      <machine>leon3_generic</machine>
      <machine>SS-10</machine>
      <machine>SS-600MP</machine>
      <machine>SS-20</machine>
      <machine>Voyager</machine>
      <machine>LX</machine>
      <machine>SS-4</machine>
      <machine>SPARCClassic</machine>
      <machine>SPARCbook</machine>
      <machine>SS-1000</machine>
      <machine>SS-2000</machine>
      <machine>SS-2</machine>
      <domain type='qemu'>
      </domain>
    </arch>
  </guest>

  <guest>
    <os_type>hvm</os_type>
    <arch name='ppc'>
      <wordsize>32</wordsize>
      <emulator>/usr/bin/qemu-system-ppc</emulator>
      <machine>g3beige</machine>
      <machine>virtex-ml507</machine>
      <machine>mpc8544ds</machine>
      <machine canonical='bamboo-0.13'>bamboo</machine>
      <machine>bamboo-0.13</machine>
      <machine>bamboo-0.12</machine>
      <machine>ref405ep</machine>
      <machine>taihu</machine>
      <machine>mac99</machine>
      <machine>prep</machine>
      <domain type='qemu'>
      </domain>
    </arch>
    <features>
      <deviceboot/>
    </features>
  </guest>

</capabilities>''' % {'sockets': self.host_info.cpu_sockets,
                      'cores': self.host_info.cpu_cores,
                      'threads': self.host_info.cpu_threads,
                      'topology': numa_topology}

    def compareCPU(self, xml, flags):
        tree = etree.fromstring(xml)

        arch_node = tree.find('./arch')
        if arch_node is not None:
            if arch_node.text not in [arch.X86_64,
                                      arch.I686]:
                return VIR_CPU_COMPARE_INCOMPATIBLE

        model_node = tree.find('./model')
        if model_node is not None:
            if model_node.text != self.host_info.cpu_model:
                return VIR_CPU_COMPARE_INCOMPATIBLE

        vendor_node = tree.find('./vendor')
        if vendor_node is not None:
            if vendor_node.text != self.host_info.cpu_vendor:
                return VIR_CPU_COMPARE_INCOMPATIBLE

        # The rest of the stuff libvirt implements is rather complicated
        # and I don't think it adds much value to replicate it here.

        return VIR_CPU_COMPARE_IDENTICAL

    def getCPUStats(self, cpuNum, flag):
        if cpuNum < 2:
            return {'kernel': 5664160000000,
                    'idle': 1592705190000000,
                    'user': 26728850000000,
                    'iowait': 6121490000000}
        else:
            raise make_libvirtError(
                    libvirtError,
                    "invalid argument: Invalid cpu number",
                    error_code=VIR_ERR_INTERNAL_ERROR,
                    error_domain=VIR_FROM_QEMU)

    def nwfilterLookupByName(self, name):
        try:
            return self._nwfilters[name]
        except KeyError:
            raise make_libvirtError(
                    libvirtError,
                    "no nwfilter with matching name %s" % name,
                    error_code=VIR_ERR_NO_NWFILTER,
                    error_domain=VIR_FROM_NWFILTER)

    def nwfilterDefineXML(self, xml):
        nwfilter = NWFilter(self, xml)
        self._add_filter(nwfilter)

    def nodeDeviceLookupByName(self, name):
        try:
            return self._nodedevs[name]
        except KeyError:
            raise make_libvirtError(
                    libvirtError,
                    "no nodedev with matching name %s" % name,
                    error_code=VIR_ERR_NO_NODE_DEVICE,
                    error_domain=VIR_FROM_NODEDEV)

    def listDefinedDomains(self):
        return []

    def listDevices(self, cap, flags):
        return []

    def baselineCPU(self, cpu, flag):
        """Add new libvirt API."""
        return """<cpu mode='custom' match='exact'>
                    <model>Penryn</model>
                    <vendor>Intel</vendor>
                    <feature name='xtpr'/>
                    <feature name='tm2'/>
                    <feature name='est'/>
                    <feature name='vmx'/>
                    <feature name='ds_cpl'/>
                    <feature name='monitor'/>
                    <feature name='pbe'/>
                    <feature name='tm'/>
                    <feature name='ht'/>
                    <feature name='ss'/>
                    <feature name='acpi'/>
                    <feature name='ds'/>
                    <feature name='vme'/>
                    <feature policy='require' name='aes'/>
                  </cpu>"""

    def secretLookupByUsage(self, usage_type_obj, usage_id):
        pass

    def secretDefineXML(self, xml):
        pass


def openAuth(uri, auth, flags=0):

    if type(auth) != list:
        raise Exception("Expected a list for 'auth' parameter")

    if type(auth[0]) != list:
        raise Exception("Expected a function in 'auth[0]' parameter")

    if not callable(auth[1]):
        raise Exception("Expected a function in 'auth[1]' parameter")

    return Connection(uri, (flags == VIR_CONNECT_RO))


def virEventRunDefaultImpl():
    time.sleep(1)


def virEventRegisterDefaultImpl():
    if connection_used:
        raise Exception("virEventRegisterDefaultImpl() must be "
                        "called before connection is used.")


def registerErrorHandler(handler, ctxt):
    pass


def make_libvirtError(error_class, msg, error_code=None,
                       error_domain=None, error_message=None,
                       error_level=None, str1=None, str2=None, str3=None,
                       int1=None, int2=None):
    """Convenience function for creating `libvirtError` exceptions which
    allow you to specify arguments in constructor without having to manipulate
    the `err` tuple directly.

    We need to pass in `error_class` to this function because it may be
    `libvirt.libvirtError` or `fakelibvirt.libvirtError` depending on whether
    `libvirt-python` is installed.
    """
    exc = error_class(msg)
    exc.err = (error_code, error_domain, error_message, error_level,
               str1, str2, str3, int1, int2)
    return exc


virDomain = Domain
virNodeDevice = NodeDevice

virConnect = Connection


class FakeLibvirtFixture(fixtures.Fixture):
    """Performs global setup/stubbing for all libvirt tests.
    """

    def setUp(self):
        super(FakeLibvirtFixture, self).setUp()

        disable_event_thread(self)

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import mock
from oslo_config import cfg

from nova.tests.unit.virt.libvirt.volume import test_volume
from nova.virt.libvirt import host
from nova.virt.libvirt.volume import net

CONF = cfg.CONF
CONF.import_opt('rbd_user', 'nova.virt.libvirt.volume.net', group='libvirt')
CONF.import_opt('rbd_secret_uuid', 'nova.virt.libvirt.volume.net',
                group='libvirt')


class LibvirtNetVolumeDriverTestCase(
    test_volume.LibvirtISCSIVolumeBaseTestCase):
    """Tests the libvirt network volume driver."""

    def _assertNetworkAndProtocolEquals(self, tree):
        self.assertEqual('network', tree.get('type'))
        self.assertEqual('rbd', tree.find('./source').get('protocol'))
        rbd_name = '%s/%s' % ('rbd', self.name)
        self.assertEqual(rbd_name, tree.find('./source').get('name'))

    def _assertISCSINetworkAndProtocolEquals(self, tree):
        self.assertEqual('network', tree.get('type'))
        self.assertEqual('iscsi', tree.find('./source').get('protocol'))
        iscsi_name = '%s/%s' % (self.iqn, self.vol['id'])
        self.assertEqual(iscsi_name, tree.find('./source').get('name'))

    def sheepdog_connection(self, volume):
        return {
            'driver_volume_type': 'sheepdog',
            'data': {
                'name': volume['name']
            }
        }

    def test_libvirt_sheepdog_driver(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.sheepdog_connection(self.vol)
        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self.assertEqual('network', tree.get('type'))
        self.assertEqual('sheepdog', tree.find('./source').get('protocol'))
        self.assertEqual(self.name, tree.find('./source').get('name'))
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def rbd_connection(self, volume):
        return {
            'driver_volume_type': 'rbd',
            'data': {
                'name': '%s/%s' % ('rbd', volume['name']),
                'auth_enabled': CONF.libvirt.rbd_secret_uuid is not None,
                'auth_username': CONF.libvirt.rbd_user,
                'secret_type': 'ceph',
                'secret_uuid': CONF.libvirt.rbd_secret_uuid,
                'qos_specs': {
                    'total_bytes_sec': '1048576',
                    'read_iops_sec': '500',
                    }
            }
        }

    def test_libvirt_rbd_driver(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertIsNone(tree.find('./source/auth'))
        self.assertEqual('1048576', tree.find('./iotune/total_bytes_sec').text)
        self.assertEqual('500', tree.find('./iotune/read_iops_sec').text)
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def test_libvirt_rbd_driver_hosts(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        hosts = ['example.com', '1.2.3.4', '::1']
        ports = [None, '6790', '6791']
        connection_info['data']['hosts'] = hosts
        connection_info['data']['ports'] = ports
        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertIsNone(tree.find('./source/auth'))
        found_hosts = tree.findall('./source/host')
        self.assertEqual(hosts, [host.get('name') for host in found_hosts])
        self.assertEqual(ports, [host.get('port') for host in found_hosts])
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def test_libvirt_rbd_driver_auth_enabled(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        secret_type = 'ceph'
        connection_info['data']['auth_enabled'] = True
        connection_info['data']['auth_username'] = self.user
        connection_info['data']['secret_type'] = secret_type
        connection_info['data']['secret_uuid'] = self.uuid

        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertEqual(self.user, tree.find('./auth').get('username'))
        self.assertEqual(secret_type, tree.find('./auth/secret').get('type'))
        self.assertEqual(self.uuid, tree.find('./auth/secret').get('uuid'))
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def test_libvirt_rbd_driver_auth_enabled_flags_override(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        secret_type = 'ceph'
        connection_info['data']['auth_enabled'] = True
        connection_info['data']['auth_username'] = self.user
        connection_info['data']['secret_type'] = secret_type
        connection_info['data']['secret_uuid'] = self.uuid

        flags_uuid = '37152720-1785-11e2-a740-af0c1d8b8e4b'
        flags_user = 'bar'
        self.flags(rbd_user=flags_user,
                   rbd_secret_uuid=flags_uuid,
                   group='libvirt')

        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertEqual(flags_user, tree.find('./auth').get('username'))
        self.assertEqual(secret_type, tree.find('./auth/secret').get('type'))
        self.assertEqual(flags_uuid, tree.find('./auth/secret').get('uuid'))
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def test_libvirt_rbd_driver_auth_disabled(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        secret_type = 'ceph'
        connection_info['data']['auth_enabled'] = False
        connection_info['data']['auth_username'] = self.user
        connection_info['data']['secret_type'] = secret_type
        connection_info['data']['secret_uuid'] = self.uuid

        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertIsNone(tree.find('./auth'))
        libvirt_driver.disconnect_volume(connection_info, "vde")

    def test_libvirt_rbd_driver_auth_disabled_flags_override(self):
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.rbd_connection(self.vol)
        secret_type = 'ceph'
        connection_info['data']['auth_enabled'] = False
        connection_info['data']['auth_username'] = self.user
        connection_info['data']['secret_type'] = secret_type
        connection_info['data']['secret_uuid'] = self.uuid

        # NOTE: Supplying the rbd_secret_uuid will enable authentication
        # locally in nova-compute even if not enabled in nova-volume/cinder
        flags_uuid = '37152720-1785-11e2-a740-af0c1d8b8e4b'
        flags_user = 'bar'
        self.flags(rbd_user=flags_user,
                   rbd_secret_uuid=flags_uuid,
                   group='libvirt')

        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertNetworkAndProtocolEquals(tree)
        self.assertEqual(flags_user, tree.find('./auth').get('username'))
        self.assertEqual(secret_type, tree.find('./auth/secret').get('type'))
        self.assertEqual(flags_uuid, tree.find('./auth/secret').get('uuid'))
        libvirt_driver.disconnect_volume(connection_info, "vde")

    @mock.patch.object(host.Host, 'find_secret')
    @mock.patch.object(host.Host, 'create_secret')
    @mock.patch.object(host.Host, 'delete_secret')
    def test_libvirt_iscsi_net_driver(self, mock_delete, mock_create,
                                      mock_find):
        mock_find.return_value = test_volume.FakeSecret()
        mock_create.return_value = test_volume.FakeSecret()
        libvirt_driver = net.LibvirtNetVolumeDriver(self.fake_conn)
        connection_info = self.iscsi_connection(self.vol, self.location,
                                                self.iqn, auth=True)
        secret_type = 'iscsi'
        flags_user = connection_info['data']['auth_username']
        conf = libvirt_driver.get_config(connection_info, self.disk_info)
        tree = conf.format_dom()
        self._assertISCSINetworkAndProtocolEquals(tree)
        self.assertEqual(flags_user, tree.find('./auth').get('username'))
        self.assertEqual(secret_type, tree.find('./auth/secret').get('type'))
        self.assertEqual(test_volume.SECRET_UUID,
                         tree.find('./auth/secret').get('uuid'))
        libvirt_driver.disconnect_volume(connection_info, 'vde')

#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import collections
import re

from oslo_utils import units
from oslo_vmware.objects import datastore as ds_obj

from nova import test
from nova.virt.vmwareapi import ds_util

ResultSet = collections.namedtuple('ResultSet', ['objects'])
ObjectContent = collections.namedtuple('ObjectContent', ['obj', 'propSet'])
DynamicProperty = collections.namedtuple('Property', ['name', 'val'])
MoRef = collections.namedtuple('ManagedObjectReference', ['value'])


class VMwareDSUtilDatastoreSelectionTestCase(test.NoDBTestCase):

    def setUp(self):
        super(VMwareDSUtilDatastoreSelectionTestCase, self).setUp()
        self.data = [
            ['VMFS', 'os-some-name', True, 'normal', 987654321, 12346789],
            ['NFS', 'another-name', True, 'normal', 9876543210, 123467890],
            ['BAD', 'some-name-bad', True, 'normal', 98765432100, 1234678900],
            ['VMFS', 'some-name-good', False, 'normal', 987654321, 12346789],
            ['VMFS', 'new-name', True, 'inMaintenance', 987654321, 12346789]
        ]

    def build_result_set(self, mock_data, name_list=None):
        # datastores will have a moref_id of ds-000 and
        # so on based on their index in the mock_data list
        if name_list is None:
            name_list = self.propset_name_list

        objects = []
        for id, row in enumerate(mock_data):
            obj = ObjectContent(
                obj=MoRef(value="ds-%03d" % id),
                propSet=[])
            for index, value in enumerate(row):
                obj.propSet.append(
                    DynamicProperty(name=name_list[index], val=row[index]))
            objects.append(obj)
        return ResultSet(objects=objects)

    @property
    def propset_name_list(self):
        return ['summary.type', 'summary.name', 'summary.accessible',
                'summary.maintenanceMode', 'summary.capacity',
                'summary.freeSpace']

    def test_filter_datastores_simple(self):
        datastores = self.build_result_set(self.data)
        best_match = ds_obj.Datastore(ref='fake_ref', name='ds',
                              capacity=0, freespace=0)
        rec = ds_util._select_datastore(None, datastores, best_match)

        self.assertIsNotNone(rec.ref, "could not find datastore!")
        self.assertEqual('ds-001', rec.ref.value,
                         "didn't find the right datastore!")
        self.assertEqual(123467890, rec.freespace,
                         "did not obtain correct freespace!")

    def test_filter_datastores_empty(self):
        data = []
        datastores = self.build_result_set(data)

        best_match = ds_obj.Datastore(ref='fake_ref', name='ds',
                              capacity=0, freespace=0)
        rec = ds_util._select_datastore(None, datastores, best_match)

        self.assertEqual(best_match, rec)

    def test_filter_datastores_no_match(self):
        datastores = self.build_result_set(self.data)
        datastore_regex = re.compile('no_match.*')

        best_match = ds_obj.Datastore(ref='fake_ref', name='ds',
                              capacity=0, freespace=0)
        rec = ds_util._select_datastore(None, datastores,
                                        best_match,
                                        datastore_regex)

        self.assertEqual(best_match, rec, "did not match datastore properly")

    def test_filter_datastores_specific_match(self):

        data = [
            ['VMFS', 'os-some-name', True, 'normal', 987654321, 1234678],
            ['NFS', 'another-name', True, 'normal', 9876543210, 123467890],
            ['BAD', 'some-name-bad', True, 'normal', 98765432100, 1234678900],
            ['VMFS', 'some-name-good', True, 'normal', 987654321, 12346789],
            ['VMFS', 'some-other-good', False, 'normal', 987654321000,
             12346789000],
            ['VMFS', 'new-name', True, 'inMaintenance', 987654321000,
             12346789000]
        ]
        # only the DS some-name-good is accessible and matches the regex
        datastores = self.build_result_set(data)
        datastore_regex = re.compile('.*-good$')

        best_match = ds_obj.Datastore(ref='fake_ref', name='ds',
                              capacity=0, freespace=0)
        rec = ds_util._select_datastore(None, datastores,
                                        best_match,
                                        datastore_regex)

        self.assertIsNotNone(rec, "could not find datastore!")
        self.assertEqual('ds-003', rec.ref.value,
                         "didn't find the right datastore!")
        self.assertNotEqual('ds-004', rec.ref.value,
                            "accepted an unreachable datastore!")
        self.assertEqual('some-name-good', rec.name)
        self.assertEqual(12346789, rec.freespace,
                         "did not obtain correct freespace!")
        self.assertEqual(987654321, rec.capacity,
                         "did not obtain correct capacity!")

    def test_filter_datastores_missing_props(self):
        data = [
            ['VMFS', 'os-some-name', 987654321, 1234678],
            ['NFS', 'another-name', 9876543210, 123467890],
        ]
        # no matches are expected when 'summary.accessible' is missing
        prop_names = ['summary.type', 'summary.name',
                      'summary.capacity', 'summary.freeSpace']
        datastores = self.build_result_set(data, prop_names)
        best_match = ds_obj.Datastore(ref='fake_ref', name='ds',
                              capacity=0, freespace=0)

        rec = ds_util._select_datastore(None, datastores, best_match)
        self.assertEqual(best_match, rec, "no matches were expected")

    def test_filter_datastores_best_match(self):
        data = [
            ['VMFS', 'spam-good', True, 20 * units.Gi, 10 * units.Gi],
            ['NFS', 'eggs-good', True, 40 * units.Gi, 15 * units.Gi],
            ['NFS41', 'nfs41-is-good', True, 35 * units.Gi, 12 * units.Gi],
            ['BAD', 'some-name-bad', True, 30 * units.Gi, 20 * units.Gi],
            ['VMFS', 'some-name-good', True, 50 * units.Gi, 5 * units.Gi],
            ['VMFS', 'some-other-good', True, 10 * units.Gi, 10 * units.Gi],
        ]

        datastores = self.build_result_set(data)
        datastore_regex = re.compile('.*-good$')

        # the current best match is better than all candidates
        best_match = ds_obj.Datastore(ref='ds-100', name='best-ds-good',
                              capacity=20 * units.Gi, freespace=19 * units.Gi)
        rec = ds_util._select_datastore(None,
                                        datastores,
                                        best_match,
                                        datastore_regex)
        self.assertEqual(best_match, rec, "did not match datastore properly")

# Copyright 2013 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from eventlet import greenthread
import mock
import uuid

try:
    import xmlrpclib
except ImportError:
    import six.moves.xmlrpc_client as xmlrpclib

from nova.compute import power_state
from nova.compute import task_states
from nova import context
from nova import exception
from nova import objects
from nova.objects import fields
from nova.pci import manager as pci_manager
from nova import test
from nova.tests.unit import fake_flavor
from nova.tests.unit import fake_instance
from nova.tests.unit.virt.xenapi import stubs
from nova import utils
from nova.virt import fake
from nova.virt.xenapi import agent as xenapi_agent
from nova.virt.xenapi.client import session as xenapi_session
from nova.virt.xenapi import fake as xenapi_fake
from nova.virt.xenapi import vm_utils
from nova.virt.xenapi import vmops
from nova.virt.xenapi import volume_utils
from nova.virt.xenapi import volumeops


class VMOpsTestBase(stubs.XenAPITestBaseNoDB):
    def setUp(self):
        super(VMOpsTestBase, self).setUp()
        self._setup_mock_vmops()
        self.vms = []

    def _setup_mock_vmops(self, product_brand=None, product_version=None):
        stubs.stubout_session(self.stubs, xenapi_fake.SessionBase)
        self._session = xenapi_session.XenAPISession('test_url', 'root',
                                                     'test_pass')
        self.vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())

    def create_vm(self, name, state="Running"):
        vm_ref = xenapi_fake.create_vm(name, state)
        self.vms.append(vm_ref)
        vm = xenapi_fake.get_record("VM", vm_ref)
        return vm, vm_ref

    def tearDown(self):
        super(VMOpsTestBase, self).tearDown()
        for vm in self.vms:
            xenapi_fake.destroy_vm(vm)


class VMOpsTestCase(VMOpsTestBase):
    def setUp(self):
        super(VMOpsTestCase, self).setUp()
        self._setup_mock_vmops()
        self.context = context.RequestContext('user', 'project')
        self.instance = fake_instance.fake_instance_obj(self.context)

    def _setup_mock_vmops(self, product_brand=None, product_version=None):
        self._session = self._get_mock_session(product_brand, product_version)
        self._vmops = vmops.VMOps(self._session, fake.FakeVirtAPI())

    def _get_mock_session(self, product_brand, product_version):
        class Mock(object):
            pass

        mock_session = Mock()
        mock_session.product_brand = product_brand
        mock_session.product_version = product_version
        return mock_session

    def _test_finish_revert_migration_after_crash(self, backup_made, new_made,
                                                  vm_shutdown=True):
        instance = {'name': 'foo',
                    'task_state': task_states.RESIZE_MIGRATING}
        context = 'fake_context'

        self.mox.StubOutWithMock(vm_utils, 'lookup')
        self.mox.StubOutWithMock(self._vmops, '_destroy')
        self.mox.StubOutWithMock(vm_utils, 'set_vm_name_label')
        self.mox.StubOutWithMock(self._vmops, '_attach_mapped_block_devices')
        self.mox.StubOutWithMock(self._vmops, '_start')
        self.mox.StubOutWithMock(vm_utils, 'is_vm_shutdown')

        vm_utils.lookup(self._session, 'foo-orig').AndReturn(
            backup_made and 'foo' or None)
        vm_utils.lookup(self._session, 'foo').AndReturn(
            (not backup_made or new_made) and 'foo' or None)
        if backup_made:
            if new_made:
                self._vmops._destroy(instance, 'foo')
            vm_utils.set_vm_name_label(self._session, 'foo', 'foo')
            self._vmops._attach_mapped_block_devices(instance, [])

        vm_utils.is_vm_shutdown(self._session, 'foo').AndReturn(vm_shutdown)
        if vm_shutdown:
            self._vmops._start(instance, 'foo')

        self.mox.ReplayAll()

        self._vmops.finish_revert_migration(context, instance, [])

    def test_finish_revert_migration_after_crash(self):
        self._test_finish_revert_migration_after_crash(True, True)

    def test_finish_revert_migration_after_crash_before_new(self):
        self._test_finish_revert_migration_after_crash(True, False)

    def test_finish_revert_migration_after_crash_before_backup(self):
        self._test_finish_revert_migration_after_crash(False, False)

    def test_xsm_sr_check_relaxed_cached(self):
        self.make_plugin_call_count = 0

        def fake_make_plugin_call(plugin, method, **args):
            self.make_plugin_call_count = self.make_plugin_call_count + 1
            return "true"

        self.stubs.Set(self._vmops, "_make_plugin_call",
                       fake_make_plugin_call)

        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())
        self.assertTrue(self._vmops._is_xsm_sr_check_relaxed())

        self.assertEqual(self.make_plugin_call_count, 1)

    def test_get_vm_opaque_ref_raises_instance_not_found(self):
        instance = {"name": "dummy"}
        self.mox.StubOutWithMock(vm_utils, 'lookup')
        vm_utils.lookup(self._session, instance['name'], False).AndReturn(None)
        self.mox.ReplayAll()

        self.assertRaises(exception.InstanceNotFound,
                self._vmops._get_vm_opaque_ref, instance)

    @mock.patch.object(vm_utils, 'destroy_vm')
    @mock.patch.object(vm_utils, 'clean_shutdown_vm')
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    def test_clean_shutdown_no_bdm_on_destroy(self, hard_shutdown_vm,
            clean_shutdown_vm, destroy_vm):
        vm_ref = 'vm_ref'
        self._vmops._destroy(self.instance, vm_ref, destroy_disks=False)
        hard_shutdown_vm.assert_called_once_with(self._vmops._session,
                self.instance, vm_ref)
        self.assertEqual(0, clean_shutdown_vm.call_count)

    @mock.patch.object(vm_utils, 'destroy_vm')
    @mock.patch.object(vm_utils, 'clean_shutdown_vm')
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    def test_clean_shutdown_with_bdm_on_destroy(self, hard_shutdown_vm,
            clean_shutdown_vm, destroy_vm):
        vm_ref = 'vm_ref'
        block_device_info = {'block_device_mapping': ['fake']}
        self._vmops._destroy(self.instance, vm_ref, destroy_disks=False,
                block_device_info=block_device_info)
        clean_shutdown_vm.assert_called_once_with(self._vmops._session,
                self.instance, vm_ref)
        self.assertEqual(0, hard_shutdown_vm.call_count)

    @mock.patch.object(vm_utils, 'destroy_vm')
    @mock.patch.object(vm_utils, 'clean_shutdown_vm', return_value=False)
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    def test_clean_shutdown_with_bdm_failed_on_destroy(self, hard_shutdown_vm,
            clean_shutdown_vm, destroy_vm):
        vm_ref = 'vm_ref'
        block_device_info = {'block_device_mapping': ['fake']}
        self._vmops._destroy(self.instance, vm_ref, destroy_disks=False,
                block_device_info=block_device_info)
        clean_shutdown_vm.assert_called_once_with(self._vmops._session,
                self.instance, vm_ref)
        hard_shutdown_vm.assert_called_once_with(self._vmops._session,
                self.instance, vm_ref)

    @mock.patch.object(vm_utils, 'try_auto_configure_disk')
    @mock.patch.object(vm_utils, 'create_vbd',
            side_effect=test.TestingException)
    def test_attach_disks_rescue_auto_disk_config_false(self, create_vbd,
            try_auto_config):
        ctxt = context.RequestContext('user', 'project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image_meta = objects.ImageMeta.from_dict(
            {'properties': {'auto_disk_config': 'false'}})
        vdis = {'root': {'ref': 'fake-ref'}}
        self.assertRaises(test.TestingException, self._vmops._attach_disks,
                instance, image_meta=image_meta, vm_ref=None,
                name_label=None, vdis=vdis, disk_image_type='fake',
                network_info=[], rescue=True)
        self.assertFalse(try_auto_config.called)

    @mock.patch.object(vm_utils, 'try_auto_configure_disk')
    @mock.patch.object(vm_utils, 'create_vbd',
            side_effect=test.TestingException)
    def test_attach_disks_rescue_auto_disk_config_true(self, create_vbd,
            try_auto_config):
        ctxt = context.RequestContext('user', 'project')
        instance = fake_instance.fake_instance_obj(ctxt)
        image_meta = objects.ImageMeta.from_dict(
            {'properties': {'auto_disk_config': 'true'}})
        vdis = {'root': {'ref': 'fake-ref'}}
        self.assertRaises(test.TestingException, self._vmops._attach_disks,
                instance, image_meta=image_meta, vm_ref=None,
                name_label=None, vdis=vdis, disk_image_type='fake',
                network_info=[], rescue=True)
        try_auto_config.assert_called_once_with(self._vmops._session,
                'fake-ref', instance.flavor.root_gb)


class InjectAutoDiskConfigTestCase(VMOpsTestBase):
    def test_inject_auto_disk_config_when_present(self):
        vm, vm_ref = self.create_vm("dummy")
        instance = {"name": "dummy", "uuid": "1234", "auto_disk_config": True}
        self.vmops._inject_auto_disk_config(instance, vm_ref)
        xenstore_data = vm['xenstore_data']
        self.assertEqual(xenstore_data['vm-data/auto-disk-config'], 'True')

    def test_inject_auto_disk_config_none_as_false(self):
        vm, vm_ref = self.create_vm("dummy")
        instance = {"name": "dummy", "uuid": "1234", "auto_disk_config": None}
        self.vmops._inject_auto_disk_config(instance, vm_ref)
        xenstore_data = vm['xenstore_data']
        self.assertEqual(xenstore_data['vm-data/auto-disk-config'], 'False')


class GetConsoleOutputTestCase(VMOpsTestBase):
    def test_get_console_output_works(self):
        ctxt = context.RequestContext('user', 'project')
        instance = fake_instance.fake_instance_obj(ctxt)
        self.mox.StubOutWithMock(self.vmops, '_get_last_dom_id')

        self.vmops._get_last_dom_id(instance, check_rescue=True).AndReturn(42)
        self.mox.ReplayAll()

        self.assertEqual("dom_id: 42", self.vmops.get_console_output(instance))

    def test_get_console_output_not_available(self):
        self.mox.StubOutWithMock(self.vmops, '_get_last_dom_id')

        ctxt = context.RequestContext('user', 'project')
        instance = fake_instance.fake_instance_obj(ctxt)
        # dom_id=0 used to trigger exception in fake XenAPI
        self.vmops._get_last_dom_id(instance, check_rescue=True).AndReturn(0)
        self.mox.ReplayAll()

        self.assertRaises(exception.ConsoleNotAvailable,
                self.vmops.get_console_output, instance)

    def test_get_dom_id_works(self):
        instance = {"name": "dummy"}
        vm, vm_ref = self.create_vm("dummy")
        self.assertEqual(vm["domid"], self.vmops._get_dom_id(instance))

    def test_get_dom_id_works_with_rescue_vm(self):
        instance = {"name": "dummy"}
        vm, vm_ref = self.create_vm("dummy-rescue")
        self.assertEqual(vm["domid"],
                self.vmops._get_dom_id(instance, check_rescue=True))

    def test_get_dom_id_raises_not_found(self):
        instance = {"name": "dummy"}
        self.create_vm("not-dummy")
        self.assertRaises(exception.NotFound, self.vmops._get_dom_id, instance)

    def test_get_dom_id_works_with_vmref(self):
        vm, vm_ref = self.create_vm("dummy")
        self.assertEqual(vm["domid"],
                         self.vmops._get_dom_id(vm_ref=vm_ref))


class SpawnTestCase(VMOpsTestBase):
    def _stub_out_common(self):
        self.mox.StubOutWithMock(self.vmops, '_ensure_instance_name_unique')
        self.mox.StubOutWithMock(self.vmops, '_ensure_enough_free_mem')
        self.mox.StubOutWithMock(self.vmops, '_update_instance_progress')
        self.mox.StubOutWithMock(vm_utils, 'determine_disk_image_type')
        self.mox.StubOutWithMock(self.vmops, '_get_vdis_for_instance')
        self.mox.StubOutWithMock(vm_utils, 'safe_destroy_vdis')
        self.mox.StubOutWithMock(self.vmops._volumeops,
                                 'safe_cleanup_from_vdis')
        self.mox.StubOutWithMock(self.vmops, '_resize_up_vdis')
        self.mox.StubOutWithMock(vm_utils,
                                 'create_kernel_and_ramdisk')
        self.mox.StubOutWithMock(vm_utils, 'destroy_kernel_ramdisk')
        self.mox.StubOutWithMock(self.vmops, '_create_vm_record')
        self.mox.StubOutWithMock(self.vmops, '_destroy')
        self.mox.StubOutWithMock(self.vmops, '_attach_disks')
        self.mox.StubOutWithMock(pci_manager, 'get_instance_pci_devs')
        self.mox.StubOutWithMock(vm_utils, 'set_other_config_pci')
        self.mox.StubOutWithMock(self.vmops, '_attach_orig_disks')
        self.mox.StubOutWithMock(self.vmops, 'inject_network_info')
        self.mox.StubOutWithMock(self.vmops, '_inject_hostname')
        self.mox.StubOutWithMock(self.vmops, '_inject_instance_metadata')
        self.mox.StubOutWithMock(self.vmops, '_inject_auto_disk_config')
        self.mox.StubOutWithMock(self.vmops, '_file_inject_vm_settings')
        self.mox.StubOutWithMock(self.vmops, '_create_vifs')
        self.mox.StubOutWithMock(self.vmops.firewall_driver,
                                 'setup_basic_filtering')
        self.mox.StubOutWithMock(self.vmops.firewall_driver,
                                 'prepare_instance_filter')
        self.mox.StubOutWithMock(self.vmops, '_start')
        self.mox.StubOutWithMock(self.vmops, '_wait_for_instance_to_start')
        self.mox.StubOutWithMock(self.vmops,
                                 '_configure_new_instance_with_agent')
        self.mox.StubOutWithMock(self.vmops, '_remove_hostname')
        self.mox.StubOutWithMock(self.vmops.firewall_driver,
                                 'apply_instance_filter')
        self.mox.StubOutWithMock(self.vmops, '_update_last_dom_id')
        self.mox.StubOutWithMock(self.vmops._session, 'call_xenapi')

    def _test_spawn(self, name_label_param=None, block_device_info_param=None,
                    rescue=False, include_root_vdi=True, throw_exception=None,
                    attach_pci_dev=False, neutron_exception=False):
        self._stub_out_common()

        instance = {"name": "dummy", "uuid": "fake_uuid"}
        name_label = name_label_param
        if name_label is None:
            name_label = "dummy"
        image_meta = objects.ImageMeta.from_dict({"id": "image_id"})
        context = "context"
        session = self.vmops._session
        injected_files = "fake_files"
        admin_password = "password"
        network_info = "net_info"
        steps = 10
        if rescue:
            steps += 1

        block_device_info = block_device_info_param
        if block_device_info and not block_device_info['root_device_name']:
            block_device_info = dict(block_device_info_param)
            block_device_info['root_device_name'] = \
                                                self.vmops.default_root_dev

        di_type = "di_type"
        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)
        step = 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        vdis = {"other": {"ref": "fake_ref_2", "osvol": True}}
        if include_root_vdi:
            vdis["root"] = {"ref": "fake_ref"}
        self.vmops._get_vdis_for_instance(context, instance,
                name_label, image_meta, di_type,
                block_device_info).AndReturn(vdis)
        self.vmops._resize_up_vdis(instance, vdis)
        step += 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        kernel_file = "kernel"
        ramdisk_file = "ramdisk"
        vm_utils.create_kernel_and_ramdisk(context, session,
                instance, name_label).AndReturn((kernel_file, ramdisk_file))
        step += 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        vm_ref = "fake_vm_ref"
        self.vmops._ensure_instance_name_unique(name_label)
        self.vmops._ensure_enough_free_mem(instance)
        self.vmops._create_vm_record(context, instance, name_label,
                di_type, kernel_file,
                ramdisk_file, image_meta, rescue).AndReturn(vm_ref)
        step += 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        self.vmops._attach_disks(instance, image_meta, vm_ref, name_label,
                            vdis, di_type, network_info, rescue,
                            admin_password, injected_files)
        if attach_pci_dev:
            fake_dev = {
                'created_at': None,
                'updated_at': None,
                'deleted_at': None,
                'deleted': None,
                'id': 1,
                'compute_node_id': 1,
                'address': '00:00.0',
                'vendor_id': '1234',
                'product_id': 'abcd',
                'dev_type': fields.PciDeviceType.STANDARD,
                'status': 'available',
                'dev_id': 'devid',
                'label': 'label',
                'instance_uuid': None,
                'extra_info': '{}',
            }
            pci_manager.get_instance_pci_devs(instance).AndReturn([fake_dev])
            vm_utils.set_other_config_pci(self.vmops._session,
                                          vm_ref,
                                          "0/0000:00:00.0")
        else:
            pci_manager.get_instance_pci_devs(instance).AndReturn([])
        step += 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        self.vmops._inject_instance_metadata(instance, vm_ref)
        self.vmops._inject_auto_disk_config(instance, vm_ref)
        self.vmops._inject_hostname(instance, vm_ref, rescue)
        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,
                                            network_info)
        self.vmops.inject_network_info(instance, network_info, vm_ref)
        step += 1
        self.vmops._update_instance_progress(context, instance, step, steps)

        if neutron_exception:
            events = [('network-vif-plugged', 1)]
            self.vmops._get_neutron_events(network_info,
                                           True, True).AndReturn(events)
            self.mox.StubOutWithMock(self.vmops, '_neutron_failed_callback')
            self.mox.StubOutWithMock(self.vmops._virtapi,
                                     'wait_for_instance_event')
            self.vmops._virtapi.wait_for_instance_event(instance, events,
                deadline=300,
                error_callback=self.vmops._neutron_failed_callback).\
                AndRaise(exception.VirtualInterfaceCreateException)
        else:
            self.vmops._create_vifs(instance, vm_ref, network_info)
            self.vmops.firewall_driver.setup_basic_filtering(instance,
                    network_info).AndRaise(NotImplementedError)
            self.vmops.firewall_driver.prepare_instance_filter(instance,
                                                               network_info)
            step += 1
            self.vmops._update_instance_progress(context, instance,
                                                 step, steps)

            if rescue:
                self.vmops._attach_orig_disks(instance, vm_ref)
                step += 1
                self.vmops._update_instance_progress(context, instance, step,
                                                     steps)
            start_pause = True
            self.vmops._start(instance, vm_ref, start_pause=start_pause)
            step += 1
            self.vmops._update_instance_progress(context, instance,
                                                 step, steps)
            self.vmops.firewall_driver.apply_instance_filter(instance,
                                                             network_info)
            step += 1
            self.vmops._update_instance_progress(context, instance,
                                                step, steps)
            self.vmops._session.call_xenapi('VM.unpause', vm_ref)
            self.vmops._wait_for_instance_to_start(instance, vm_ref)
            self.vmops._update_last_dom_id(vm_ref)
            self.vmops._configure_new_instance_with_agent(instance, vm_ref,
                    injected_files, admin_password)
            self.vmops._remove_hostname(instance, vm_ref)
            step += 1
            last_call = self.vmops._update_instance_progress(context, instance,
                                                 step, steps)

        if throw_exception:
            last_call.AndRaise(throw_exception)
        if throw_exception or neutron_exception:
            self.vmops._destroy(instance, vm_ref, network_info=network_info)
            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,
                                            kernel_file, ramdisk_file)
            vm_utils.safe_destroy_vdis(self.vmops._session, ["fake_ref"])
            self.vmops._volumeops.safe_cleanup_from_vdis(["fake_ref_2"])

        self.mox.ReplayAll()
        self.vmops.spawn(context, instance, image_meta, injected_files,
                         admin_password, network_info,
                         block_device_info_param, name_label_param, rescue)

    def test_spawn(self):
        self._test_spawn()

    def test_spawn_with_alternate_options(self):
        self._test_spawn(include_root_vdi=False, rescue=True,
                         name_label_param="bob",
                         block_device_info_param={"root_device_name": ""})

    def test_spawn_with_pci_available_on_the_host(self):
        self._test_spawn(attach_pci_dev=True)

    def test_spawn_performs_rollback_and_throws_exception(self):
        self.assertRaises(test.TestingException, self._test_spawn,
                          throw_exception=test.TestingException())

    def test_spawn_with_neutron(self):
        self.mox.StubOutWithMock(self.vmops, '_get_neutron_events')
        events = [('network-vif-plugged', 1)]
        network_info = "net_info"
        self.vmops._get_neutron_events(network_info,
                                    True, True).AndReturn(events)
        self.mox.StubOutWithMock(self.vmops,
                                 '_neutron_failed_callback')
        self._test_spawn()

    def test_spawn_with_neutron_exception(self):
        self.mox.StubOutWithMock(self.vmops, '_get_neutron_events')
        self.assertRaises(exception.VirtualInterfaceCreateException,
                          self._test_spawn, neutron_exception=True)

    def _test_finish_migration(self, power_on=True, resize_instance=True,
                               throw_exception=None, booted_from_volume=False):
        self._stub_out_common()
        self.mox.StubOutWithMock(volumeops.VolumeOps, "connect_volume")
        self.mox.StubOutWithMock(vm_utils, "import_all_migrated_disks")
        self.mox.StubOutWithMock(self.vmops, "_attach_mapped_block_devices")

        context = "context"
        migration = {}
        name_label = "dummy"
        instance = {"name": name_label, "uuid": "fake_uuid",
                "root_device_name": "/dev/xvda"}
        disk_info = "disk_info"
        network_info = "net_info"
        image_meta = objects.ImageMeta.from_dict({"id": "image_id"})
        block_device_info = {}
        import_root = True
        if booted_from_volume:
            block_device_info = {'block_device_mapping': [
                {'mount_device': '/dev/xvda',
                 'connection_info': {'data': 'fake-data'}}]}
            import_root = False
            volumeops.VolumeOps.connect_volume(
                    {'data': 'fake-data'}).AndReturn(('sr', 'vol-vdi-uuid'))
            self.vmops._session.call_xenapi('VDI.get_by_uuid',
                    'vol-vdi-uuid').AndReturn('vol-vdi-ref')
        session = self.vmops._session

        self.vmops._ensure_instance_name_unique(name_label)
        self.vmops._ensure_enough_free_mem(instance)

        di_type = "di_type"
        vm_utils.determine_disk_image_type(image_meta).AndReturn(di_type)

        root_vdi = {"ref": "fake_ref"}
        ephemeral_vdi = {"ref": "fake_ref_e"}
        vdis = {"root": root_vdi, "ephemerals": {4: ephemeral_vdi}}
        vm_utils.import_all_migrated_disks(self.vmops._session, instance,
                import_root=import_root).AndReturn(vdis)

        kernel_file = "kernel"
        ramdisk_file = "ramdisk"
        vm_utils.create_kernel_and_ramdisk(context, session,
                instance, name_label).AndReturn((kernel_file, ramdisk_file))

        vm_ref = "fake_vm_ref"
        rescue = False
        self.vmops._create_vm_record(context, instance, name_label,
                di_type, kernel_file,
                ramdisk_file, image_meta, rescue).AndReturn(vm_ref)

        if resize_instance:
            self.vmops._resize_up_vdis(instance, vdis)
        self.vmops._attach_disks(instance, image_meta, vm_ref, name_label,
                            vdis, di_type, network_info, False, None, None)
        self.vmops._attach_mapped_block_devices(instance, block_device_info)
        pci_manager.get_instance_pci_devs(instance).AndReturn([])

        self.vmops._inject_instance_metadata(instance, vm_ref)
        self.vmops._inject_auto_disk_config(instance, vm_ref)
        self.vmops._file_inject_vm_settings(instance, vm_ref, vdis,
                                            network_info)
        self.vmops.inject_network_info(instance, network_info, vm_ref)

        self.vmops._create_vifs(instance, vm_ref, network_info)
        self.vmops.firewall_driver.setup_basic_filtering(instance,
                network_info).AndRaise(NotImplementedError)
        self.vmops.firewall_driver.prepare_instance_filter(instance,
                                                           network_info)

        if power_on:
            self.vmops._start(instance, vm_ref, start_pause=True)

        self.vmops.firewall_driver.apply_instance_filter(instance,
                                                         network_info)
        if power_on:
            self.vmops._session.call_xenapi('VM.unpause', vm_ref)
            self.vmops._wait_for_instance_to_start(instance, vm_ref)
            self.vmops._update_last_dom_id(vm_ref)

        last_call = self.vmops._update_instance_progress(context, instance,
                                                        step=5, total_steps=5)
        if throw_exception:
            last_call.AndRaise(throw_exception)
            self.vmops._destroy(instance, vm_ref, network_info=network_info)
            vm_utils.destroy_kernel_ramdisk(self.vmops._session, instance,
                                            kernel_file, ramdisk_file)
            vm_utils.safe_destroy_vdis(self.vmops._session,
                                       ["fake_ref_e", "fake_ref"])

        self.mox.ReplayAll()
        self.vmops.finish_migration(context, migration, instance, disk_info,
                                    network_info, image_meta, resize_instance,
                                    block_device_info, power_on)

    def test_finish_migration(self):
        self._test_finish_migration()

    def test_finish_migration_no_power_on(self):
        self._test_finish_migration(power_on=False, resize_instance=False)

    def test_finish_migration_booted_from_volume(self):
        self._test_finish_migration(booted_from_volume=True)

    def test_finish_migrate_performs_rollback_on_error(self):
        self.assertRaises(test.TestingException, self._test_finish_migration,
                          power_on=False, resize_instance=False,
                          throw_exception=test.TestingException())

    def test_remove_hostname(self):
        vm, vm_ref = self.create_vm("dummy")
        instance = {"name": "dummy", "uuid": "1234", "auto_disk_config": None}
        self.mox.StubOutWithMock(self._session, 'call_xenapi')
        self._session.call_xenapi("VM.remove_from_xenstore_data", vm_ref,
                                  "vm-data/hostname")

        self.mox.ReplayAll()
        self.vmops._remove_hostname(instance, vm_ref)
        self.mox.VerifyAll()

    def test_reset_network(self):
        class mock_agent(object):
            def __init__(self):
                self.called = False

            def resetnetwork(self):
                self.called = True

        vm, vm_ref = self.create_vm("dummy")
        instance = {"name": "dummy", "uuid": "1234", "auto_disk_config": None}
        agent = mock_agent()

        self.mox.StubOutWithMock(self.vmops, 'agent_enabled')
        self.mox.StubOutWithMock(self.vmops, '_get_agent')
        self.mox.StubOutWithMock(self.vmops, '_inject_hostname')
        self.mox.StubOutWithMock(self.vmops, '_remove_hostname')

        self.vmops.agent_enabled(instance).AndReturn(True)
        self.vmops._get_agent(instance, vm_ref).AndReturn(agent)
        self.vmops._inject_hostname(instance, vm_ref, False)
        self.vmops._remove_hostname(instance, vm_ref)
        self.mox.ReplayAll()
        self.vmops.reset_network(instance)
        self.assertTrue(agent.called)
        self.mox.VerifyAll()

    def test_inject_hostname(self):
        instance = {"hostname": "dummy", "os_type": "fake", "uuid": "uuid"}
        vm_ref = "vm_ref"

        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')
        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname', 'dummy')

        self.mox.ReplayAll()
        self.vmops._inject_hostname(instance, vm_ref, rescue=False)

    def test_inject_hostname_with_rescue_prefix(self):
        instance = {"hostname": "dummy", "os_type": "fake", "uuid": "uuid"}
        vm_ref = "vm_ref"

        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')
        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',
                                          'RESCUE-dummy')

        self.mox.ReplayAll()
        self.vmops._inject_hostname(instance, vm_ref, rescue=True)

    def test_inject_hostname_with_windows_name_truncation(self):
        instance = {"hostname": "dummydummydummydummydummy",
                    "os_type": "windows", "uuid": "uuid"}
        vm_ref = "vm_ref"

        self.mox.StubOutWithMock(self.vmops, '_add_to_param_xenstore')
        self.vmops._add_to_param_xenstore(vm_ref, 'vm-data/hostname',
                                          'RESCUE-dummydum')

        self.mox.ReplayAll()
        self.vmops._inject_hostname(instance, vm_ref, rescue=True)

    def test_wait_for_instance_to_start(self):
        instance = {"uuid": "uuid"}
        vm_ref = "vm_ref"

        self.mox.StubOutWithMock(vm_utils, 'get_power_state')
        self.mox.StubOutWithMock(greenthread, 'sleep')
        vm_utils.get_power_state(self._session, vm_ref).AndReturn(
                                             power_state.SHUTDOWN)
        greenthread.sleep(0.5)
        vm_utils.get_power_state(self._session, vm_ref).AndReturn(
                                            power_state.RUNNING)

        self.mox.ReplayAll()
        self.vmops._wait_for_instance_to_start(instance, vm_ref)

    def test_attach_orig_disks(self):
        instance = {"name": "dummy"}
        vm_ref = "vm_ref"
        vbd_refs = {vmops.DEVICE_ROOT: "vdi_ref"}

        self.mox.StubOutWithMock(vm_utils, 'lookup')
        self.mox.StubOutWithMock(self.vmops, '_find_vdi_refs')
        self.mox.StubOutWithMock(vm_utils, 'create_vbd')

        vm_utils.lookup(self.vmops._session, "dummy").AndReturn("ref")
        self.vmops._find_vdi_refs("ref", exclude_volumes=True).AndReturn(
                vbd_refs)
        vm_utils.create_vbd(self.vmops._session, vm_ref, "vdi_ref",
                            vmops.DEVICE_RESCUE, bootable=False)

        self.mox.ReplayAll()
        self.vmops._attach_orig_disks(instance, vm_ref)

    def test_agent_update_setup(self):
        # agent updates need to occur after networking is configured
        instance = {'name': 'betelgeuse',
                    'uuid': '1-2-3-4-5-6'}
        vm_ref = 'vm_ref'
        agent = xenapi_agent.XenAPIBasedAgent(self.vmops._session,
                self.vmops._virtapi, instance, vm_ref)

        self.mox.StubOutWithMock(xenapi_agent, 'should_use_agent')
        self.mox.StubOutWithMock(self.vmops, '_get_agent')
        self.mox.StubOutWithMock(agent, 'get_version')
        self.mox.StubOutWithMock(agent, 'resetnetwork')
        self.mox.StubOutWithMock(agent, 'update_if_needed')

        xenapi_agent.should_use_agent(instance).AndReturn(True)
        self.vmops._get_agent(instance, vm_ref).AndReturn(agent)
        agent.get_version().AndReturn('1.2.3')
        agent.resetnetwork()
        agent.update_if_needed('1.2.3')

        self.mox.ReplayAll()
        self.vmops._configure_new_instance_with_agent(instance, vm_ref,
                None, None)

    @mock.patch.object(utils, 'is_neutron', return_value=True)
    def test_get_neutron_event(self, mock_is_neutron):
        network_info = [{"active": False, "id": 1},
                        {"active": True, "id": 2},
                        {"active": False, "id": 3},
                        {"id": 4}]
        power_on = True
        first_boot = True
        events = self.vmops._get_neutron_events(network_info,
                                                power_on, first_boot)
        self.assertEqual("network-vif-plugged", events[0][0])
        self.assertEqual(1, events[0][1])
        self.assertEqual("network-vif-plugged", events[1][0])
        self.assertEqual(3, events[1][1])

    @mock.patch.object(utils, 'is_neutron', return_value=False)
    def test_get_neutron_event_not_neutron_network(self, mock_is_neutron):
        network_info = [{"active": False, "id": 1},
                        {"active": True, "id": 2},
                        {"active": False, "id": 3},
                        {"id": 4}]
        power_on = True
        first_boot = True
        events = self.vmops._get_neutron_events(network_info,
                                                power_on, first_boot)
        self.assertEqual([], events)

    @mock.patch.object(utils, 'is_neutron', return_value=True)
    def test_get_neutron_event_power_off(self, mock_is_neutron):
        network_info = [{"active": False, "id": 1},
                        {"active": True, "id": 2},
                        {"active": False, "id": 3},
                        {"id": 4}]
        power_on = False
        first_boot = True
        events = self.vmops._get_neutron_events(network_info,
                                                power_on, first_boot)
        self.assertEqual([], events)

    @mock.patch.object(utils, 'is_neutron', return_value=True)
    def test_get_neutron_event_not_first_boot(self, mock_is_neutron):
        network_info = [{"active": False, "id": 1},
                        {"active": True, "id": 2},
                        {"active": False, "id": 3},
                        {"id": 4}]
        power_on = True
        first_boot = False
        events = self.vmops._get_neutron_events(network_info,
                                                power_on, first_boot)
        self.assertEqual([], events)


class DestroyTestCase(VMOpsTestBase):
    def setUp(self):
        super(DestroyTestCase, self).setUp()
        self.context = context.RequestContext(user_id=None, project_id=None)
        self.instance = fake_instance.fake_instance_obj(self.context)

    @mock.patch.object(vm_utils, 'lookup', side_effect=[None, None])
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    @mock.patch.object(volume_utils, 'find_sr_by_uuid')
    @mock.patch.object(volume_utils, 'forget_sr')
    def test_no_vm_no_bdm(self, forget_sr, find_sr_by_uuid, hard_shutdown_vm,
            lookup):
        self.vmops.destroy(self.instance, 'network_info',
                {'block_device_mapping': []})
        self.assertEqual(0, find_sr_by_uuid.call_count)
        self.assertEqual(0, forget_sr.call_count)
        self.assertEqual(0, hard_shutdown_vm.call_count)

    @mock.patch.object(vm_utils, 'lookup', side_effect=[None, None])
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    @mock.patch.object(volume_utils, 'find_sr_by_uuid', return_value=None)
    @mock.patch.object(volume_utils, 'forget_sr')
    def test_no_vm_orphaned_volume_no_sr(self, forget_sr, find_sr_by_uuid,
            hard_shutdown_vm, lookup):
        self.vmops.destroy(self.instance, 'network_info',
                {'block_device_mapping': [{'connection_info':
                    {'data': {'volume_id': 'fake-uuid'}}}]})
        find_sr_by_uuid.assert_called_once_with(self.vmops._session,
                'FA15E-D15C-fake-uuid')
        self.assertEqual(0, forget_sr.call_count)
        self.assertEqual(0, hard_shutdown_vm.call_count)

    @mock.patch.object(vm_utils, 'lookup', side_effect=[None, None])
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    @mock.patch.object(volume_utils, 'find_sr_by_uuid', return_value='sr_ref')
    @mock.patch.object(volume_utils, 'forget_sr')
    def test_no_vm_orphaned_volume_old_sr(self, forget_sr, find_sr_by_uuid,
            hard_shutdown_vm, lookup):
        self.vmops.destroy(self.instance, 'network_info',
                {'block_device_mapping': [{'connection_info':
                    {'data': {'volume_id': 'fake-uuid'}}}]})
        find_sr_by_uuid.assert_called_once_with(self.vmops._session,
                'FA15E-D15C-fake-uuid')
        forget_sr.assert_called_once_with(self.vmops._session, 'sr_ref')
        self.assertEqual(0, hard_shutdown_vm.call_count)

    @mock.patch.object(vm_utils, 'lookup', side_effect=[None, None])
    @mock.patch.object(vm_utils, 'hard_shutdown_vm')
    @mock.patch.object(volume_utils, 'find_sr_by_uuid',
                       side_effect=[None, 'sr_ref'])
    @mock.patch.object(volume_utils, 'forget_sr')
    @mock.patch.object(uuid, 'uuid5', return_value='fake-uuid')
    def test_no_vm_orphaned_volume(self, uuid5, forget_sr,
            find_sr_by_uuid, hard_shutdown_vm, lookup):
        fake_data = {'volume_id': 'fake-uuid',
                     'target_portal': 'host:port',
                     'target_iqn': 'iqn'}
        self.vmops.destroy(self.instance, 'network_info',
                {'block_device_mapping': [{'connection_info':
                                           {'data': fake_data}}]})
        call1 = mock.call(self.vmops._session, 'FA15E-D15C-fake-uuid')
        call2 = mock.call(self.vmops._session, 'fake-uuid')
        uuid5.assert_called_once_with(volume_utils.SR_NAMESPACE,
                                      'host/port/iqn')
        find_sr_by_uuid.assert_has_calls([call1, call2])
        forget_sr.assert_called_once_with(self.vmops._session, 'sr_ref')
        self.assertEqual(0, hard_shutdown_vm.call_count)


@mock.patch.object(vmops.VMOps, '_update_instance_progress')
@mock.patch.object(vmops.VMOps, '_get_vm_opaque_ref')
@mock.patch.object(vm_utils, 'get_sr_path')
@mock.patch.object(vmops.VMOps, '_detach_block_devices_from_orig_vm')
@mock.patch.object(vmops.VMOps, '_migrate_disk_resizing_down')
@mock.patch.object(vmops.VMOps, '_migrate_disk_resizing_up')
class MigrateDiskAndPowerOffTestCase(VMOpsTestBase):
    def setUp(self):
        super(MigrateDiskAndPowerOffTestCase, self).setUp()
        self.context = context.RequestContext('user', 'project')

    def test_migrate_disk_and_power_off_works_down(self,
                migrate_up, migrate_down, *mocks):
        instance = {"root_gb": 2, "ephemeral_gb": 0, "uuid": "uuid"}
        flavor = fake_flavor.fake_flavor_obj(self.context, root_gb=1,
                                             ephemeral_gb=0)

        self.vmops.migrate_disk_and_power_off(None, instance, None,
                flavor, None)

        self.assertFalse(migrate_up.called)
        self.assertTrue(migrate_down.called)

    def test_migrate_disk_and_power_off_works_up(self,
                migrate_up, migrate_down, *mocks):
        instance = {"root_gb": 1, "ephemeral_gb": 1, "uuid": "uuid"}
        flavor = fake_flavor.fake_flavor_obj(self.context, root_gb=2,
                                             ephemeral_gb=2)

        self.vmops.migrate_disk_and_power_off(None, instance, None,
                flavor, None)

        self.assertFalse(migrate_down.called)
        self.assertTrue(migrate_up.called)

    def test_migrate_disk_and_power_off_resize_down_ephemeral_fails(self,
                migrate_up, migrate_down, *mocks):
        instance = {"ephemeral_gb": 2}
        flavor = fake_flavor.fake_flavor_obj(self.context, ephemeral_gb=1)

        self.assertRaises(exception.ResizeError,
                          self.vmops.migrate_disk_and_power_off,
                          None, instance, None, flavor, None)


@mock.patch.object(vm_utils, 'get_vdi_for_vm_safely')
@mock.patch.object(vm_utils, 'migrate_vhd')
@mock.patch.object(vmops.VMOps, '_resize_ensure_vm_is_shutdown')
@mock.patch.object(vm_utils, 'get_all_vdi_uuids_for_vm')
@mock.patch.object(vmops.VMOps, '_update_instance_progress')
@mock.patch.object(vmops.VMOps, '_apply_orig_vm_name_label')
class MigrateDiskResizingUpTestCase(VMOpsTestBase):
    def _fake_snapshot_attached_here(self, session, instance, vm_ref, label,
                                     userdevice, post_snapshot_callback):
        self.assertIsInstance(instance, dict)
        if userdevice == '0':
            self.assertEqual("vm_ref", vm_ref)
            self.assertEqual("fake-snapshot", label)
            yield ["leaf", "parent", "grandp"]
        else:
            leaf = userdevice + "-leaf"
            parent = userdevice + "-parent"
            yield [leaf, parent]

    @mock.patch.object(volume_utils, 'is_booted_from_volume',
            return_value=False)
    def test_migrate_disk_resizing_up_works_no_ephemeral(self,
            mock_is_booted_from_volume,
            mock_apply_orig, mock_update_progress, mock_get_all_vdi_uuids,
            mock_shutdown, mock_migrate_vhd, mock_get_vdi_for_vm):
        context = "ctxt"
        instance = {"name": "fake", "uuid": "uuid"}
        dest = "dest"
        vm_ref = "vm_ref"
        sr_path = "sr_path"

        mock_get_all_vdi_uuids.return_value = None
        mock_get_vdi_for_vm.return_value = ({}, {"uuid": "root"})

        with mock.patch.object(vm_utils, '_snapshot_attached_here_impl',
                               self._fake_snapshot_attached_here):
            self.vmops._migrate_disk_resizing_up(context, instance, dest,
                                                 vm_ref, sr_path)

        mock_get_all_vdi_uuids.assert_called_once_with(self.vmops._session,
                vm_ref, min_userdevice=4)
        mock_apply_orig.assert_called_once_with(instance, vm_ref)
        mock_shutdown.assert_called_once_with(instance, vm_ref)

        m_vhd_expected = [mock.call(self.vmops._session, instance, "parent",
                                    dest, sr_path, 1),
                          mock.call(self.vmops._session, instance, "grandp",
                                    dest, sr_path, 2),
                          mock.call(self.vmops._session, instance, "root",
                                    dest, sr_path, 0)]
        self.assertEqual(m_vhd_expected, mock_migrate_vhd.call_args_list)

        prog_expected = [
            mock.call(context, instance, 1, 5),
            mock.call(context, instance, 2, 5),
            mock.call(context, instance, 3, 5),
            mock.call(context, instance, 4, 5)
            # 5/5: step to be executed by finish migration.
            ]
        self.assertEqual(prog_expected, mock_update_progress.call_args_list)

    @mock.patch.object(volume_utils, 'is_booted_from_volume',
            return_value=False)
    def test_migrate_disk_resizing_up_works_with_two_ephemeral(self,
            mock_is_booted_from_volume,
            mock_apply_orig, mock_update_progress, mock_get_all_vdi_uuids,
            mock_shutdown, mock_migrate_vhd, mock_get_vdi_for_vm):
        context = "ctxt"
        instance = {"name": "fake", "uuid": "uuid"}
        dest = "dest"
        vm_ref = "vm_ref"
        sr_path = "sr_path"

        mock_get_all_vdi_uuids.return_value = ["vdi-eph1", "vdi-eph2"]
        mock_get_vdi_for_vm.side_effect = [({}, {"uuid": "root"}),
                                           ({}, {"uuid": "4-root"}),
                                           ({}, {"uuid": "5-root"})]

        with mock.patch.object(vm_utils, '_snapshot_attached_here_impl',
                               self._fake_snapshot_attached_here):
            self.vmops._migrate_disk_resizing_up(context, instance, dest,
                                                 vm_ref, sr_path)

        mock_get_all_vdi_uuids.assert_called_once_with(self.vmops._session,
                vm_ref, min_userdevice=4)
        mock_apply_orig.assert_called_once_with(instance, vm_ref)
        mock_shutdown.assert_called_once_with(instance, vm_ref)

        m_vhd_expected = [mock.call(self.vmops._session, instance,
                                    "parent", dest, sr_path, 1),
                          mock.call(self.vmops._session, instance,
                                    "grandp", dest, sr_path, 2),
                          mock.call(self.vmops._session, instance,
                                    "4-parent", dest, sr_path, 1, 1),
                          mock.call(self.vmops._session, instance,
                                    "5-parent", dest, sr_path, 1, 2),
                          mock.call(self.vmops._session, instance,
                                    "root", dest, sr_path, 0),
                          mock.call(self.vmops._session, instance,
                                    "4-root", dest, sr_path, 0, 1),
                          mock.call(self.vmops._session, instance,
                                    "5-root", dest, sr_path, 0, 2)]
        self.assertEqual(m_vhd_expected, mock_migrate_vhd.call_args_list)

        prog_expected = [
            mock.call(context, instance, 1, 5),
            mock.call(context, instance, 2, 5),
            mock.call(context, instance, 3, 5),
            mock.call(context, instance, 4, 5)
            # 5/5: step to be executed by finish migration.
            ]
        self.assertEqual(prog_expected, mock_update_progress.call_args_list)

    @mock.patch.object(volume_utils, 'is_booted_from_volume',
            return_value=True)
    def test_migrate_disk_resizing_up_booted_from_volume(self,
            mock_is_booted_from_volume,
            mock_apply_orig, mock_update_progress, mock_get_all_vdi_uuids,
            mock_shutdown, mock_migrate_vhd, mock_get_vdi_for_vm):
        context = "ctxt"
        instance = {"name": "fake", "uuid": "uuid"}
        dest = "dest"
        vm_ref = "vm_ref"
        sr_path = "sr_path"

        mock_get_all_vdi_uuids.return_value = ["vdi-eph1", "vdi-eph2"]
        mock_get_vdi_for_vm.side_effect = [({}, {"uuid": "4-root"}),
                                           ({}, {"uuid": "5-root"})]

        with mock.patch.object(vm_utils, '_snapshot_attached_here_impl',
                self._fake_snapshot_attached_here):
            self.vmops._migrate_disk_resizing_up(context, instance, dest,
                                                 vm_ref, sr_path)

        mock_get_all_vdi_uuids.assert_called_once_with(self.vmops._session,
                vm_ref, min_userdevice=4)
        mock_apply_orig.assert_called_once_with(instance, vm_ref)
        mock_shutdown.assert_called_once_with(instance, vm_ref)

        m_vhd_expected = [mock.call(self.vmops._session, instance,
                                    "4-parent", dest, sr_path, 1, 1),
                          mock.call(self.vmops._session, instance,
                                    "5-parent", dest, sr_path, 1, 2),
                          mock.call(self.vmops._session, instance,
                                    "4-root", dest, sr_path, 0, 1),
                          mock.call(self.vmops._session, instance,
                                    "5-root", dest, sr_path, 0, 2)]
        self.assertEqual(m_vhd_expected, mock_migrate_vhd.call_args_list)

        prog_expected = [
            mock.call(context, instance, 1, 5),
            mock.call(context, instance, 2, 5),
            mock.call(context, instance, 3, 5),
            mock.call(context, instance, 4, 5)
            # 5/5: step to be executed by finish migration.
            ]
        self.assertEqual(prog_expected, mock_update_progress.call_args_list)

    @mock.patch.object(vmops.VMOps, '_restore_orig_vm_and_cleanup_orphan')
    @mock.patch.object(volume_utils, 'is_booted_from_volume',
            return_value=False)
    def test_migrate_disk_resizing_up_rollback(self,
            mock_is_booted_from_volume,
            mock_restore,
            mock_apply_orig, mock_update_progress, mock_get_all_vdi_uuids,
            mock_shutdown, mock_migrate_vhd, mock_get_vdi_for_vm):
        context = "ctxt"
        instance = {"name": "fake", "uuid": "fake"}
        dest = "dest"
        vm_ref = "vm_ref"
        sr_path = "sr_path"

        mock_migrate_vhd.side_effect = test.TestingException
        mock_restore.side_effect = test.TestingException

        with mock.patch.object(vm_utils, '_snapshot_attached_here_impl',
                               self._fake_snapshot_attached_here):
            self.assertRaises(exception.InstanceFaultRollback,
                              self.vmops._migrate_disk_resizing_up,
                              context, instance, dest, vm_ref, sr_path)

        mock_apply_orig.assert_called_once_with(instance, vm_ref)
        mock_restore.assert_called_once_with(instance)
        mock_migrate_vhd.assert_called_once_with(self.vmops._session,
                instance, "parent", dest, sr_path, 1)


class CreateVMRecordTestCase(VMOpsTestBase):
    @mock.patch.object(vm_utils, 'determine_vm_mode')
    @mock.patch.object(vm_utils, 'get_vm_device_id')
    @mock.patch.object(vm_utils, 'create_vm')
    def test_create_vm_record_with_vm_device_id(self, mock_create_vm,
            mock_get_vm_device_id, mock_determine_vm_mode):

        context = "context"
        instance = objects.Instance(vm_mode="vm_mode", uuid="uuid123")
        name_label = "dummy"
        disk_image_type = "vhd"
        kernel_file = "kernel"
        ramdisk_file = "ram"
        device_id = "0002"
        image_properties = {"xenapi_device_id": device_id}
        image_meta = objects.ImageMeta.from_dict(
            {"properties": image_properties})
        rescue = False
        session = "session"
        self.vmops._session = session
        mock_get_vm_device_id.return_value = device_id
        mock_determine_vm_mode.return_value = "vm_mode"

        self.vmops._create_vm_record(context, instance, name_label,
            disk_image_type, kernel_file, ramdisk_file, image_meta, rescue)

        mock_get_vm_device_id.assert_called_with(session, image_meta)
        mock_create_vm.assert_called_with(session, instance, name_label,
            kernel_file, ramdisk_file, False, device_id)


class BootableTestCase(VMOpsTestBase):

    def setUp(self):
        super(BootableTestCase, self).setUp()

        self.instance = {"name": "test", "uuid": "fake"}
        vm_rec, self.vm_ref = self.create_vm('test')

        # sanity check bootlock is initially disabled:
        self.assertEqual({}, vm_rec['blocked_operations'])

    def _get_blocked(self):
        vm_rec = self._session.call_xenapi("VM.get_record", self.vm_ref)
        return vm_rec['blocked_operations']

    def test_acquire_bootlock(self):
        self.vmops._acquire_bootlock(self.vm_ref)
        blocked = self._get_blocked()
        self.assertIn('start', blocked)

    def test_release_bootlock(self):
        self.vmops._acquire_bootlock(self.vm_ref)
        self.vmops._release_bootlock(self.vm_ref)
        blocked = self._get_blocked()
        self.assertNotIn('start', blocked)

    def test_set_bootable(self):
        self.vmops.set_bootable(self.instance, True)
        blocked = self._get_blocked()
        self.assertNotIn('start', blocked)

    def test_set_not_bootable(self):
        self.vmops.set_bootable(self.instance, False)
        blocked = self._get_blocked()
        self.assertIn('start', blocked)


@mock.patch.object(vm_utils, 'update_vdi_virtual_size', autospec=True)
class ResizeVdisTestCase(VMOpsTestBase):
    def test_dont_resize_root_volumes_osvol_false(self, mock_resize):
        instance = fake_instance.fake_db_instance(root_gb=20)
        vdis = {'root': {'osvol': False, 'ref': 'vdi_ref'}}
        self.vmops._resize_up_vdis(instance, vdis)
        self.assertTrue(mock_resize.called)

    def test_dont_resize_root_volumes_osvol_true(self, mock_resize):
        instance = fake_instance.fake_db_instance(root_gb=20)
        vdis = {'root': {'osvol': True}}
        self.vmops._resize_up_vdis(instance, vdis)
        self.assertFalse(mock_resize.called)

    def test_dont_resize_root_volumes_no_osvol(self, mock_resize):
        instance = fake_instance.fake_db_instance(root_gb=20)
        vdis = {'root': {}}
        self.vmops._resize_up_vdis(instance, vdis)
        self.assertFalse(mock_resize.called)

    @mock.patch.object(vm_utils, 'get_ephemeral_disk_sizes')
    def test_ensure_ephemeral_resize_with_root_volume(self, mock_sizes,
                                                       mock_resize):
        mock_sizes.return_value = [2000, 1000]
        instance = fake_instance.fake_db_instance(root_gb=20, ephemeral_gb=20)
        ephemerals = {"4": {"ref": 4}, "5": {"ref": 5}}
        vdis = {'root': {'osvol': True, 'ref': 'vdi_ref'},
                'ephemerals': ephemerals}
        with mock.patch.object(vm_utils, 'generate_single_ephemeral',
                               autospec=True) as g:
            self.vmops._resize_up_vdis(instance, vdis)
            self.assertEqual([mock.call(self.vmops._session, instance, 4,
                                        2000),
                              mock.call(self.vmops._session, instance, 5,
                                        1000)],
                             mock_resize.call_args_list)
            self.assertFalse(g.called)

    def test_resize_up_vdis_root(self, mock_resize):
        instance = {"root_gb": 20, "ephemeral_gb": 0}
        self.vmops._resize_up_vdis(instance, {"root": {"ref": "vdi_ref"}})
        mock_resize.assert_called_once_with(self.vmops._session, instance,
                                            "vdi_ref", 20)

    def test_resize_up_vdis_zero_disks(self, mock_resize):
        instance = {"root_gb": 0, "ephemeral_gb": 0}
        self.vmops._resize_up_vdis(instance, {"root": {}})
        self.assertFalse(mock_resize.called)

    def test_resize_up_vdis_no_vdis_like_initial_spawn(self, mock_resize):
        instance = {"root_gb": 0, "ephemeral_gb": 3000}
        vdis = {}

        self.vmops._resize_up_vdis(instance, vdis)

        self.assertFalse(mock_resize.called)

    @mock.patch.object(vm_utils, 'get_ephemeral_disk_sizes')
    def test_resize_up_vdis_ephemeral(self, mock_sizes, mock_resize):
        mock_sizes.return_value = [2000, 1000]
        instance = {"root_gb": 0, "ephemeral_gb": 3000}
        ephemerals = {"4": {"ref": 4}, "5": {"ref": 5}}
        vdis = {"ephemerals": ephemerals}

        self.vmops._resize_up_vdis(instance, vdis)

        mock_sizes.assert_called_once_with(3000)
        expected = [mock.call(self.vmops._session, instance, 4, 2000),
                    mock.call(self.vmops._session, instance, 5, 1000)]
        self.assertEqual(expected, mock_resize.call_args_list)

    @mock.patch.object(vm_utils, 'generate_single_ephemeral')
    @mock.patch.object(vm_utils, 'get_ephemeral_disk_sizes')
    def test_resize_up_vdis_ephemeral_with_generate(self, mock_sizes,
                                                    mock_generate,
                                                    mock_resize):
        mock_sizes.return_value = [2000, 1000]
        instance = {"root_gb": 0, "ephemeral_gb": 3000, "uuid": "a"}
        ephemerals = {"4": {"ref": 4}}
        vdis = {"ephemerals": ephemerals}

        self.vmops._resize_up_vdis(instance, vdis)

        mock_sizes.assert_called_once_with(3000)
        mock_resize.assert_called_once_with(self.vmops._session, instance,
                                            4, 2000)
        mock_generate.assert_called_once_with(self.vmops._session, instance,
                                              None, 5, 1000)


@mock.patch.object(vm_utils, 'remove_old_snapshots')
class CleanupFailedSnapshotTestCase(VMOpsTestBase):
    def test_post_interrupted_snapshot_cleanup(self, mock_remove):
        self.vmops._get_vm_opaque_ref = mock.Mock()
        self.vmops._get_vm_opaque_ref.return_value = "vm_ref"

        self.vmops.post_interrupted_snapshot_cleanup("context", "instance")

        mock_remove.assert_called_once_with(self.vmops._session,
                "instance", "vm_ref")


class XenstoreCallsTestCase(VMOpsTestBase):
    """Test cases for Read/Write/Delete/Update xenstore calls
    from vmops.
    """

    @mock.patch.object(vmops.VMOps, '_make_plugin_call')
    def test_read_from_xenstore(self, fake_xapi_call):
        fake_xapi_call.return_value = "fake_xapi_return"
        fake_instance = {"name": "fake_instance"}
        path = "attr/PVAddons/MajorVersion"
        self.assertEqual("fake_xapi_return",
                         self.vmops._read_from_xenstore(fake_instance, path,
                                                        vm_ref="vm_ref"))

    @mock.patch.object(vmops.VMOps, '_make_plugin_call')
    def test_read_from_xenstore_ignore_missing_path(self, fake_xapi_call):
        fake_instance = {"name": "fake_instance"}
        path = "attr/PVAddons/MajorVersion"
        self.vmops._read_from_xenstore(fake_instance, path, vm_ref="vm_ref")
        fake_xapi_call.assert_called_once_with('xenstore.py', 'read_record',
                                               fake_instance, vm_ref="vm_ref",
                                               path=path,
                                               ignore_missing_path='True')

    @mock.patch.object(vmops.VMOps, '_make_plugin_call')
    def test_read_from_xenstore_missing_path(self, fake_xapi_call):
        fake_instance = {"name": "fake_instance"}
        path = "attr/PVAddons/MajorVersion"
        self.vmops._read_from_xenstore(fake_instance, path, vm_ref="vm_ref",
                                       ignore_missing_path=False)
        fake_xapi_call.assert_called_once_with('xenstore.py', 'read_record',
                                               fake_instance, vm_ref="vm_ref",
                                               path=path,
                                               ignore_missing_path='False')


class LiveMigrateTestCase(VMOpsTestBase):

    @mock.patch.object(vmops.VMOps, '_ensure_host_in_aggregate')
    def _test_check_can_live_migrate_destination_shared_storage(
                                                self,
                                                shared,
                                                mock_ensure_host):
        fake_instance = {"name": "fake_instance", "host": "fake_host"}
        block_migration = None
        disk_over_commit = False
        ctxt = 'ctxt'

        with mock.patch.object(self._session, 'get_rec') as fake_sr_rec:
            fake_sr_rec.return_value = {'shared': shared}
            migrate_data_ret = self.vmops.check_can_live_migrate_destination(
                ctxt, fake_instance, block_migration, disk_over_commit)

        if shared:
            self.assertFalse(migrate_data_ret.block_migration)
        else:
            self.assertTrue(migrate_data_ret.block_migration)

    def test_check_can_live_migrate_destination_shared_storage(self):
        self._test_check_can_live_migrate_destination_shared_storage(True)

    def test_check_can_live_migrate_destination_shared_storage_false(self):
        self._test_check_can_live_migrate_destination_shared_storage(False)

    @mock.patch.object(vmops.VMOps, '_ensure_host_in_aggregate',
                       side_effect=exception.MigrationPreCheckError(reason=""))
    def test_check_can_live_migrate_destination_block_migration(
                                                self,
                                                mock_ensure_host):
        fake_instance = {"name": "fake_instance", "host": "fake_host"}
        block_migration = None
        disk_over_commit = False
        ctxt = 'ctxt'

        migrate_data_ret = self.vmops.check_can_live_migrate_destination(
            ctxt, fake_instance, block_migration, disk_over_commit)

        self.assertTrue(migrate_data_ret.block_migration)
        self.assertEqual(vm_utils.safe_find_sr(self._session),
                         migrate_data_ret.destination_sr_ref)
        self.assertEqual({'value': 'fake_migrate_data'},
                         migrate_data_ret.migrate_send_data)

    @mock.patch.object(vmops.objects.AggregateList, 'get_by_host')
    def test_get_host_uuid_from_aggregate_no_aggr(self, mock_get_by_host):
        mock_get_by_host.return_value = objects.AggregateList(objects=[])
        context = "ctx"
        hostname = "other_host"
        self.assertRaises(exception.MigrationPreCheckError,
                          self.vmops._get_host_uuid_from_aggregate,
                          context, hostname)

    @mock.patch.object(vmops.objects.AggregateList, 'get_by_host')
    def test_get_host_uuid_from_aggregate_bad_aggr(self, mock_get_by_host):
        context = "ctx"
        hostname = "other_host"
        fake_aggregate_obj = objects.Aggregate(hosts=['fake'],
                                               metadata={'this': 'that'})
        fake_aggr_list = objects.AggregateList(objects=[fake_aggregate_obj])
        mock_get_by_host.return_value = fake_aggr_list

        self.assertRaises(exception.MigrationPreCheckError,
                          self.vmops._get_host_uuid_from_aggregate,
                          context, hostname)

    @mock.patch.object(vmops.VMOps, 'connect_block_device_volumes')
    def test_pre_live_migration(self, mock_connect):
        migrate_data = objects.XenapiLiveMigrateData()
        migrate_data.block_migration = True
        sr_uuid_map = {"sr_uuid": "sr_ref"}
        mock_connect.return_value = {"sr_uuid": "sr_ref"}

        result = self.vmops.pre_live_migration(
                None, None, "bdi", None, None, migrate_data)

        self.assertTrue(result.block_migration)
        self.assertEqual(result.sr_uuid_map, sr_uuid_map)
        mock_connect.assert_called_once_with("bdi")

    def test_pre_live_migration_raises_with_no_data(self):
        self.assertRaises(exception.InvalidParameterValue,
                self.vmops.pre_live_migration,
                None, None, "bdi", None, None, None)


class LiveMigrateFakeVersionTestCase(VMOpsTestBase):
    @mock.patch.object(vmops.VMOps, '_pv_device_reported')
    @mock.patch.object(vmops.VMOps, '_pv_driver_version_reported')
    @mock.patch.object(vmops.VMOps, '_write_fake_pv_version')
    def test_ensure_pv_driver_info_for_live_migration(
        self,
        mock_write_fake_pv_version,
        mock_pv_driver_version_reported,
        mock_pv_device_reported):

        mock_pv_device_reported.return_value = True
        mock_pv_driver_version_reported.return_value = False
        fake_instance = {"name": "fake_instance"}
        self.vmops._ensure_pv_driver_info_for_live_migration(fake_instance,
                                                             "vm_rec")

        mock_write_fake_pv_version.assert_called_once_with(fake_instance,
                                                           "vm_rec")

    @mock.patch.object(vmops.VMOps, '_read_from_xenstore')
    def test_pv_driver_version_reported_None(self, fake_read_from_xenstore):
        fake_read_from_xenstore.return_value = '"None"'
        fake_instance = {"name": "fake_instance"}
        self.assertFalse(self.vmops._pv_driver_version_reported(fake_instance,
                                                                "vm_ref"))

    @mock.patch.object(vmops.VMOps, '_read_from_xenstore')
    def test_pv_driver_version_reported(self, fake_read_from_xenstore):
        fake_read_from_xenstore.return_value = '6.2.0'
        fake_instance = {"name": "fake_instance"}
        self.assertTrue(self.vmops._pv_driver_version_reported(fake_instance,
                                                               "vm_ref"))

    @mock.patch.object(vmops.VMOps, '_read_from_xenstore')
    def test_pv_device_reported(self, fake_read_from_xenstore):
        with mock.patch.object(self._session.VM, 'get_record') as fake_vm_rec:
            fake_vm_rec.return_value = {'VIFs': 'fake-vif-object'}
            with mock.patch.object(self._session, 'call_xenapi') as fake_call:
                fake_call.return_value = {'device': '0'}
                fake_read_from_xenstore.return_value = '4'
                fake_instance = {"name": "fake_instance"}
                self.assertTrue(self.vmops._pv_device_reported(fake_instance,
                                "vm_ref"))

    @mock.patch.object(vmops.VMOps, '_read_from_xenstore')
    def test_pv_device_not_reported(self, fake_read_from_xenstore):
        with mock.patch.object(self._session.VM, 'get_record') as fake_vm_rec:
            fake_vm_rec.return_value = {'VIFs': 'fake-vif-object'}
            with mock.patch.object(self._session, 'call_xenapi') as fake_call:
                fake_call.return_value = {'device': '0'}
                fake_read_from_xenstore.return_value = '0'
                fake_instance = {"name": "fake_instance"}
                self.assertFalse(self.vmops._pv_device_reported(fake_instance,
                                 "vm_ref"))

    @mock.patch.object(vmops.VMOps, '_read_from_xenstore')
    def test_pv_device_None_reported(self, fake_read_from_xenstore):
        with mock.patch.object(self._session.VM, 'get_record') as fake_vm_rec:
            fake_vm_rec.return_value = {'VIFs': 'fake-vif-object'}
            with mock.patch.object(self._session, 'call_xenapi') as fake_call:
                fake_call.return_value = {'device': '0'}
                fake_read_from_xenstore.return_value = '"None"'
                fake_instance = {"name": "fake_instance"}
                self.assertFalse(self.vmops._pv_device_reported(fake_instance,
                                 "vm_ref"))

    @mock.patch.object(vmops.VMOps, '_write_to_xenstore')
    def test_write_fake_pv_version(self, fake_write_to_xenstore):
        fake_write_to_xenstore.return_value = 'fake_return'
        fake_instance = {"name": "fake_instance"}
        with mock.patch.object(self._session, 'product_version') as version:
            version.return_value = ('6', '2', '0')
            self.assertIsNone(self.vmops._write_fake_pv_version(fake_instance,
                                                                "vm_ref"))


class LiveMigrateHelperTestCase(VMOpsTestBase):
    def test_connect_block_device_volumes_none(self):
        self.assertEqual({}, self.vmops.connect_block_device_volumes(None))

    @mock.patch.object(volumeops.VolumeOps, "connect_volume")
    def test_connect_block_device_volumes_calls_connect(self, mock_connect):
        with mock.patch.object(self.vmops._session,
                               "call_xenapi") as mock_session:
            mock_connect.return_value = ("sr_uuid", None)
            mock_session.return_value = "sr_ref"
            bdm = {"connection_info": "c_info"}
            bdi = {"block_device_mapping": [bdm]}
            result = self.vmops.connect_block_device_volumes(bdi)

            self.assertEqual({'sr_uuid': 'sr_ref'}, result)

            mock_connect.assert_called_once_with("c_info")
            mock_session.assert_called_once_with("SR.get_by_uuid",
                                                 "sr_uuid")

    def _call_live_migrate_command_with_migrate_send_data(self,
                                                          migrate_data):
        command_name = 'test_command'
        vm_ref = "vm_ref"

        def side_effect(method, *args):
            if method == "SR.get_by_uuid":
                return "sr_ref_new"
            xmlrpclib.dumps(args, method, allow_none=1)

        with mock.patch.object(self.vmops,
                               "_generate_vdi_map") as mock_gen_vdi_map, \
                mock.patch.object(self.vmops._session,
                                  'call_xenapi') as mock_call_xenapi:
            mock_call_xenapi.side_effect = side_effect
            mock_gen_vdi_map.side_effect = [
                    {"vdi": "sr_ref"}, {"vdi": "sr_ref_2"}]

            self.vmops._call_live_migrate_command(command_name,
                                                  vm_ref, migrate_data)

            expected_vdi_map = {'vdi': 'sr_ref'}
            if 'sr_uuid_map' in migrate_data:
                expected_vdi_map = {'vdi': 'sr_ref_2'}
            self.assertEqual(mock_call_xenapi.call_args_list[-1],
                mock.call('test_command', vm_ref,
                    migrate_data.migrate_send_data, True,
                    expected_vdi_map, {}, {}))

            self.assertEqual(mock_gen_vdi_map.call_args_list[0],
                mock.call(migrate_data.destination_sr_ref, vm_ref))
            if 'sr_uuid_map' in migrate_data:
                self.assertEqual(mock_gen_vdi_map.call_args_list[1],
                    mock.call(migrate_data.sr_uuid_map["sr_uuid2"], vm_ref,
                              "sr_ref_new"))

    def test_call_live_migrate_command_with_full_data(self):
        migrate_data = objects.XenapiLiveMigrateData()
        migrate_data.migrate_send_data = {"foo": "bar"}
        migrate_data.destination_sr_ref = "sr_ref"
        migrate_data.sr_uuid_map = {"sr_uuid2": "sr_ref_3"}
        self._call_live_migrate_command_with_migrate_send_data(migrate_data)

    def test_call_live_migrate_command_with_no_sr_uuid_map(self):
        migrate_data = objects.XenapiLiveMigrateData()
        migrate_data.migrate_send_data = {"foo": "baz"}
        migrate_data.destination_sr_ref = "sr_ref"
        self._call_live_migrate_command_with_migrate_send_data(migrate_data)

    def test_call_live_migrate_command_with_no_migrate_send_data(self):
        migrate_data = objects.XenapiLiveMigrateData()
        self.assertRaises(exception.InvalidParameterValue,
                self._call_live_migrate_command_with_migrate_send_data,
                migrate_data)


class RollbackLiveMigrateDestinationTestCase(VMOpsTestBase):
    @mock.patch.object(volume_utils, 'find_sr_by_uuid', return_value='sr_ref')
    @mock.patch.object(volume_utils, 'forget_sr')
    def test_rollback_dest_calls_sr_forget(self, forget_sr, sr_ref):
        block_device_info = {'block_device_mapping': [{'connection_info':
                                {'data': {'volume_id': 'fake-uuid',
                                          'target_iqn': 'fake-iqn',
                                          'target_portal': 'fake-portal'}}}]}
        self.vmops.rollback_live_migration_at_destination('instance',
                                                          block_device_info)
        forget_sr.assert_called_once_with(self.vmops._session, 'sr_ref')

    @mock.patch.object(volume_utils, 'forget_sr')
    @mock.patch.object(volume_utils, 'find_sr_by_uuid',
                       side_effect=test.TestingException)
    def test_rollback_dest_handles_exception(self, find_sr_ref, forget_sr):
        block_device_info = {'block_device_mapping': [{'connection_info':
                                {'data': {'volume_id': 'fake-uuid',
                                          'target_iqn': 'fake-iqn',
                                          'target_portal': 'fake-portal'}}}]}
        self.vmops.rollback_live_migration_at_destination('instance',
                                                          block_device_info)
        self.assertFalse(forget_sr.called)


@mock.patch.object(vmops.VMOps, '_resize_ensure_vm_is_shutdown')
@mock.patch.object(vmops.VMOps, '_apply_orig_vm_name_label')
@mock.patch.object(vmops.VMOps, '_update_instance_progress')
@mock.patch.object(vm_utils, 'get_vdi_for_vm_safely')
@mock.patch.object(vm_utils, 'resize_disk')
@mock.patch.object(vm_utils, 'migrate_vhd')
@mock.patch.object(vm_utils, 'destroy_vdi')
class MigrateDiskResizingDownTestCase(VMOpsTestBase):
    def test_migrate_disk_resizing_down_works_no_ephemeral(
        self,
        mock_destroy_vdi,
        mock_migrate_vhd,
        mock_resize_disk,
        mock_get_vdi_for_vm_safely,
        mock_update_instance_progress,
        mock_apply_orig_vm_name_label,
        mock_resize_ensure_vm_is_shutdown):

        context = "ctx"
        instance = {"name": "fake", "uuid": "uuid"}
        dest = "dest"
        vm_ref = "vm_ref"
        sr_path = "sr_path"
        instance_type = dict(root_gb=1)
        old_vdi_ref = "old_ref"
        new_vdi_ref = "new_ref"
        new_vdi_uuid = "new_uuid"

        mock_get_vdi_for_vm_safely.return_value = (old_vdi_ref, None)
        mock_resize_disk.return_value = (new_vdi_ref, new_vdi_uuid)

        self.vmops._migrate_disk_resizing_down(context, instance, dest,
                                               instance_type, vm_ref, sr_path)

        mock_get_vdi_for_vm_safely.assert_called_once_with(
            self.vmops._session,
            vm_ref)
        mock_resize_ensure_vm_is_shutdown.assert_called_once_with(
            instance, vm_ref)
        mock_apply_orig_vm_name_label.assert_called_once_with(
            instance, vm_ref)
        mock_resize_disk.assert_called_once_with(
            self.vmops._session,
            instance,
            old_vdi_ref,
            instance_type)
        mock_migrate_vhd.assert_called_once_with(
            self.vmops._session,
            instance,
            new_vdi_uuid,
            dest,
            sr_path, 0)
        mock_destroy_vdi.assert_called_once_with(
            self.vmops._session,
            new_vdi_ref)

        prog_expected = [
            mock.call(context, instance, 1, 5),
            mock.call(context, instance, 2, 5),
            mock.call(context, instance, 3, 5),
            mock.call(context, instance, 4, 5)
            # 5/5: step to be executed by finish migration.
            ]
        self.assertEqual(prog_expected,
                         mock_update_instance_progress.call_args_list)


class GetVdisForInstanceTestCase(VMOpsTestBase):
    """Tests get_vdis_for_instance utility method."""
    def setUp(self):
        super(GetVdisForInstanceTestCase, self).setUp()
        self.context = context.get_admin_context()
        self.context.auth_token = 'auth_token'
        self.session = mock.Mock()
        self.vmops._session = self.session
        self.instance = fake_instance.fake_instance_obj(self.context)
        self.name_label = 'name'
        self.image = 'fake_image_id'

    @mock.patch.object(volumeops.VolumeOps, "connect_volume",
                       return_value=("sr", "vdi_uuid"))
    def test_vdis_for_instance_bdi_password_scrubbed(self, get_uuid_mock):
        # setup fake data
        data = {'name_label': self.name_label,
                'sr_uuid': 'fake',
                'auth_password': 'scrubme'}
        bdm = [{'mount_device': '/dev/vda',
                'connection_info': {'data': data}}]
        bdi = {'root_device_name': 'vda',
               'block_device_mapping': bdm}

        # Tests that the parameters to the to_xml method are sanitized for
        # passwords when logged.
        def fake_debug(*args, **kwargs):
            if 'auth_password' in args[0]:
                self.assertNotIn('scrubme', args[0])
                fake_debug.matched = True

        fake_debug.matched = False

        with mock.patch.object(vmops.LOG, 'debug',
                               side_effect=fake_debug) as debug_mock:
            vdis = self.vmops._get_vdis_for_instance(self.context,
                    self.instance, self.name_label, self.image,
                    image_type=4, block_device_info=bdi)
            self.assertEqual(1, len(vdis))
            get_uuid_mock.assert_called_once_with({"data": data})
            # we don't care what the log message is, we just want to make sure
            # our stub method is called which asserts the password is scrubbed
            self.assertTrue(debug_mock.called)
            self.assertTrue(fake_debug.matched)

# coding=utf-8
#
# Copyright 2014 Hewlett-Packard Development Company, L.P.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

from oslo_config import cfg
from oslo_log import log as logging
from oslo_utils import importutils

from nova import exception
from nova.i18n import _


LOG = logging.getLogger(__name__)
CONF = cfg.CONF

ironic = None

# The API version required by the Ironic driver
IRONIC_API_VERSION = (1, 8)


class IronicClientWrapper(object):
    """Ironic client wrapper class that encapsulates retry logic."""

    def __init__(self):
        """Initialise the IronicClientWrapper for use.

        Initialise IronicClientWrapper by loading ironicclient
        dynamically so that ironicclient is not a dependency for
        Nova.
        """
        global ironic
        if ironic is None:
            ironic = importutils.import_module('ironicclient')
            # NOTE(deva): work around a lack of symbols in the current version.
            if not hasattr(ironic, 'exc'):
                ironic.exc = importutils.import_module('ironicclient.exc')
            if not hasattr(ironic, 'client'):
                ironic.client = importutils.import_module(
                                                    'ironicclient.client')
        self._cached_client = None

    def _invalidate_cached_client(self):
        """Tell the wrapper to invalidate the cached ironic-client."""
        self._cached_client = None

    def _get_client(self, retry_on_conflict=True):
        max_retries = CONF.ironic.api_max_retries if retry_on_conflict else 1
        retry_interval = (CONF.ironic.api_retry_interval
                          if retry_on_conflict else 0)

        # If we've already constructed a valid, authed client, just return
        # that.
        if retry_on_conflict and self._cached_client is not None:
            return self._cached_client

        auth_token = CONF.ironic.admin_auth_token
        if auth_token is None:
            kwargs = {'os_username': CONF.ironic.admin_username,
                      'os_password': CONF.ironic.admin_password,
                      'os_auth_url': CONF.ironic.admin_url,
                      'os_tenant_name': CONF.ironic.admin_tenant_name,
                      'os_service_type': 'baremetal',
                      'os_endpoint_type': 'public',
                      'ironic_url': CONF.ironic.api_endpoint}
        else:
            kwargs = {'os_auth_token': auth_token,
                      'ironic_url': CONF.ironic.api_endpoint}

        if CONF.ironic.cafile:
            kwargs['os_cacert'] = CONF.ironic.cafile
            # Set the old option for compat with old clients
            kwargs['ca_file'] = CONF.ironic.cafile

        # Retries for Conflict exception
        kwargs['max_retries'] = max_retries
        kwargs['retry_interval'] = retry_interval
        kwargs['os_ironic_api_version'] = '%d.%d' % IRONIC_API_VERSION
        try:
            cli = ironic.client.get_client(IRONIC_API_VERSION[0], **kwargs)
            # Cache the client so we don't have to reconstruct and
            # reauthenticate it every time we need it.
            if retry_on_conflict:
                self._cached_client = cli

        except ironic.exc.Unauthorized:
            msg = _("Unable to authenticate Ironic client.")
            LOG.error(msg)
            raise exception.NovaException(msg)

        return cli

    def _multi_getattr(self, obj, attr):
        """Support nested attribute path for getattr().

        :param obj: Root object.
        :param attr: Path of final attribute to get. E.g., "a.b.c.d"

        :returns: The value of the final named attribute.
        :raises: AttributeError will be raised if the path is invalid.
        """
        for attribute in attr.split("."):
            obj = getattr(obj, attribute)
        return obj

    def call(self, method, *args, **kwargs):
        """Call an Ironic client method and retry on errors.

        :param method: Name of the client method to call as a string.
        :param args: Client method arguments.
        :param kwargs: Client method keyword arguments.
        :param retry_on_conflict: Boolean value. Whether the request should be
                                  retried in case of a conflict error
                                  (HTTP 409) or not. If retry_on_conflict is
                                  False the cached instance of the client
                                  won't be used. Defaults to True.
        :raises: NovaException if all retries failed.
        """
        # TODO(dtantsur): drop these once ironicclient 0.8.0 is out and used in
        # global-requirements.
        retry_excs = (ironic.exc.ServiceUnavailable,
                      ironic.exc.ConnectionRefused)
        retry_on_conflict = kwargs.pop('retry_on_conflict', True)

        # num_attempts should be the times of retry + 1
        # eg. retry==0 just means  run once and no retry
        num_attempts = max(0, CONF.ironic.api_max_retries) + 1

        for attempt in range(1, num_attempts + 1):
            client = self._get_client(retry_on_conflict=retry_on_conflict)

            try:
                return self._multi_getattr(client, method)(*args, **kwargs)
            except ironic.exc.Unauthorized:
                # In this case, the authorization token of the cached
                # ironic-client probably expired. So invalidate the cached
                # client and the next try will start with a fresh one.
                self._invalidate_cached_client()
                LOG.debug("The Ironic client became unauthorized. "
                          "Will attempt to reauthorize and try again.")
            except retry_excs:
                pass

            # We want to perform this logic for all exception cases listed
            # above.
            msg = (_("Error contacting Ironic server for "
                     "'%(method)s'. Attempt %(attempt)d of %(total)d") %
                        {'method': method,
                         'attempt': attempt,
                         'total': num_attempts})
            if attempt == num_attempts:
                LOG.error(msg)
                raise exception.NovaException(msg)
            LOG.warning(msg)
            time.sleep(CONF.ironic.api_retry_interval)

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from nova.virt.libvirt.volume import fs


class LibvirtGPFSVolumeDriver(fs.LibvirtBaseFileSystemVolumeDriver):
    """Class for volumes backed by gpfs volume."""

    def _get_mount_point_base(self):
        return ''

    def get_config(self, connection_info, disk_info):
        """Returns xml for libvirt."""
        conf = super(LibvirtGPFSVolumeDriver,
                     self).get_config(connection_info, disk_info)
        conf.source_type = "file"
        conf.source_path = connection_info['data']['device_path']
        return conf

# Copyright (c) 2012 VMware, Inc.
# Copyright (c) 2011 Citrix Systems, Inc.
# Copyright 2011 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Utility classes for defining the time saving transfer of data from the reader
to the write using a LightQueue as a Pipe between the reader and the writer.
"""

from eventlet import event
from eventlet import greenthread
from eventlet import queue
from oslo_log import log as logging

from nova import exception
from nova.i18n import _, _LE
from nova import image
from nova import utils

LOG = logging.getLogger(__name__)
IMAGE_API = image.API()

IO_THREAD_SLEEP_TIME = .01
GLANCE_POLL_INTERVAL = 5
CHUNK_SIZE = 64 * 1024  # default chunk size for image transfer


class ThreadSafePipe(queue.LightQueue):
    """The pipe to hold the data which the reader writes to and the writer
    reads from.
    """

    def __init__(self, maxsize, transfer_size):
        queue.LightQueue.__init__(self, maxsize)
        self.transfer_size = transfer_size
        self.transferred = 0

    def read(self, chunk_size):
        """Read data from the pipe.

        Chunksize if ignored for we have ensured
        that the data chunks written to the pipe by readers is the same as the
        chunks asked for by the Writer.
        """
        if self.transfer_size == 0 or self.transferred < self.transfer_size:
            data_item = self.get()
            self.transferred += len(data_item)
            return data_item
        else:
            return ""

    def write(self, data):
        """Put a data item in the pipe."""
        self.put(data)

    def seek(self, offset, whence=0):
        """Set the file's current position at the offset."""
        pass

    def tell(self):
        """Get size of the file to be read."""
        return self.transfer_size

    def close(self):
        """A place-holder to maintain consistency."""
        pass


class GlanceWriteThread(object):
    """Ensures that image data is written to in the glance client and that
    it is in correct ('active')state.
    """

    def __init__(self, context, input, image_id,
            image_meta=None):
        if not image_meta:
            image_meta = {}

        self.context = context
        self.input = input
        self.image_id = image_id
        self.image_meta = image_meta
        self._running = False

    def start(self):
        self.done = event.Event()

        def _inner():
            """Function to do the image data transfer through an update
            and thereon checks if the state is 'active'.
            """
            try:
                IMAGE_API.update(self.context,
                                 self.image_id,
                                 self.image_meta,
                                 data=self.input)
                self._running = True
            except exception.ImageNotAuthorized as exc:
                self.done.send_exception(exc)

            while self._running:
                try:
                    image_meta = IMAGE_API.get(self.context,
                                               self.image_id)
                    image_status = image_meta.get("status")
                    if image_status == "active":
                        self.stop()
                        self.done.send(True)
                    # If the state is killed, then raise an exception.
                    elif image_status == "killed":
                        self.stop()
                        msg = (_("Glance image %s is in killed state") %
                                 self.image_id)
                        LOG.error(msg)
                        self.done.send_exception(exception.NovaException(msg))
                    elif image_status in ["saving", "queued"]:
                        greenthread.sleep(GLANCE_POLL_INTERVAL)
                    else:
                        self.stop()
                        msg = _("Glance image "
                                    "%(image_id)s is in unknown state "
                                    "- %(state)s") % {
                                            "image_id": self.image_id,
                                            "state": image_status}
                        LOG.error(msg)
                        self.done.send_exception(exception.NovaException(msg))
                except Exception as exc:
                    self.stop()
                    self.done.send_exception(exc)

        utils.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()

    def close(self):
        pass


class IOThread(object):
    """Class that reads chunks from the input file and writes them to the
    output file till the transfer is completely done.
    """

    def __init__(self, input, output):
        self.input = input
        self.output = output
        self._running = False
        self.got_exception = False

    def start(self):
        self.done = event.Event()

        def _inner():
            """Read data from the input and write the same to the output
            until the transfer completes.
            """
            self._running = True
            while self._running:
                try:
                    data = self.input.read(CHUNK_SIZE)
                    if not data:
                        self.stop()
                        self.done.send(True)
                    self.output.write(data)
                    greenthread.sleep(IO_THREAD_SLEEP_TIME)
                except Exception as exc:
                    self.stop()
                    LOG.exception(_LE('Read/Write data failed'))
                    self.done.send_exception(exc)

        utils.spawn(_inner)
        return self.done

    def stop(self):
        self._running = False

    def wait(self):
        return self.done.wait()

# Copyright (c) 2010 Citrix Systems, Inc.
# Copyright 2011 Piston Cloud Computing, Inc.
# Copyright 2012 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Helper methods for operations related to the management of VM records and
their attributes like VDIs, VIFs, as well as their lookup functions.
"""

import contextlib
import os
import time
import urllib
import uuid
from xml.dom import minidom
from xml.parsers import expat

from eventlet import greenthread
from oslo_concurrency import processutils
from oslo_config import cfg
from oslo_log import log as logging
from oslo_utils import excutils
from oslo_utils import importutils
from oslo_utils import strutils
from oslo_utils import timeutils
from oslo_utils import units
from oslo_utils import versionutils
import six
from six.moves import range
import six.moves.urllib.parse as urlparse

from nova.api.metadata import base as instance_metadata
from nova.compute import power_state
from nova.compute import task_states
from nova.compute import vm_mode
import nova.conf
from nova import exception
from nova.i18n import _, _LE, _LI, _LW
from nova.network import model as network_model
from nova import utils
from nova.virt import configdrive
from nova.virt import diagnostics
from nova.virt.disk import api as disk
from nova.virt.disk.vfs import localfs as vfsimpl
from nova.virt import hardware
from nova.virt.image import model as imgmodel
from nova.virt import netutils
from nova.virt.xenapi import agent
from nova.virt.xenapi.image import utils as image_utils

LOG = logging.getLogger(__name__)

xenapi_vm_utils_opts = [
    cfg.StrOpt('cache_images',
               default='all',
               choices=('all', 'some', 'none'),
               help='Cache glance images locally. `all` will cache all'
                    ' images, `some` will only cache images that have the'
                    ' image_property `cache_in_nova=True`, and `none` turns'
                    ' off caching entirely'),
    cfg.IntOpt('image_compression_level',
               min=1,
               max=9,
               help='Compression level for images, e.g., 9 for gzip -9.'
                    ' Range is 1-9, 9 being most compressed but most CPU'
                    ' intensive on dom0.'),
    cfg.StrOpt('default_os_type',
               default='linux',
               help='Default OS type'),
    cfg.IntOpt('block_device_creation_timeout',
               default=10,
               help='Time to wait for a block device to be created'),
    cfg.IntOpt('max_kernel_ramdisk_size',
               default=16 * units.Mi,
               help='Maximum size in bytes of kernel or ramdisk images'),
    cfg.StrOpt('sr_matching_filter',
               default='default-sr:true',
               help='Filter for finding the SR to be used to install guest '
                    'instances on. To use the Local Storage in default '
                    'XenServer/XCP installations set this flag to '
                    'other-config:i18n-key=local-storage. To select an SR '
                    'with a different matching criteria, you could set it to '
                    'other-config:my_favorite_sr=true. On the other hand, to '
                    'fall back on the Default SR, as displayed by XenCenter, '
                    'set this flag to: default-sr:true'),
    cfg.BoolOpt('sparse_copy',
                default=True,
                help='Whether to use sparse_copy for copying data on a '
                     'resize down (False will use standard dd). This speeds '
                     'up resizes down considerably since large runs of zeros '
                     'won\'t have to be rsynced'),
    cfg.IntOpt('num_vbd_unplug_retries',
               default=10,
               help='Maximum number of retries to unplug VBD. if <=0, '
                    'should try once and no retry'),
    cfg.StrOpt('torrent_images',
               default='none',
               choices=('all', 'some', 'none'),
               help='Whether or not to download images via Bit Torrent.'),
    cfg.StrOpt('ipxe_network_name',
               help='Name of network to use for booting iPXE ISOs'),
    cfg.StrOpt('ipxe_boot_menu_url',
               help='URL to the iPXE boot menu'),
    cfg.StrOpt('ipxe_mkisofs_cmd',
               default='mkisofs',
               help='Name and optionally path of the tool used for '
                    'ISO image creation'),
    ]

CONF = nova.conf.CONF
CONF.register_opts(xenapi_vm_utils_opts, 'xenserver')
CONF.import_opt('use_ipv6', 'nova.netconf')

XENAPI_POWER_STATE = {
    'Halted': power_state.SHUTDOWN,
    'Running': power_state.RUNNING,
    'Paused': power_state.PAUSED,
    'Suspended': power_state.SUSPENDED,
    'Crashed': power_state.CRASHED}


SECTOR_SIZE = 512
MBR_SIZE_SECTORS = 63
MBR_SIZE_BYTES = MBR_SIZE_SECTORS * SECTOR_SIZE
KERNEL_DIR = '/boot/guest'
MAX_VDI_CHAIN_SIZE = 16
PROGRESS_INTERVAL_SECONDS = 300

# Fudge factor to allow for the VHD chain to be slightly larger than
# the partitioned space. Otherwise, legitimate images near their
# maximum allowed size can fail on build with FlavorDiskSmallerThanImage.
VHD_SIZE_CHECK_FUDGE_FACTOR_GB = 10


class ImageType(object):
    """Enumeration class for distinguishing different image types

    | 0 - kernel image (goes on dom0's filesystem)
    | 1 - ramdisk image (goes on dom0's filesystem)
    | 2 - disk image (local SR, partitioned by objectstore plugin)
    | 3 - raw disk image (local SR, NOT partitioned by plugin)
    | 4 - vhd disk image (local SR, NOT inspected by XS, PV assumed for
    |     linux, HVM assumed for Windows)
    | 5 - ISO disk image (local SR, NOT partitioned by plugin)
    | 6 - config drive
    """

    KERNEL = 0
    RAMDISK = 1
    DISK = 2
    DISK_RAW = 3
    DISK_VHD = 4
    DISK_ISO = 5
    DISK_CONFIGDRIVE = 6
    _ids = (KERNEL, RAMDISK, DISK, DISK_RAW, DISK_VHD, DISK_ISO,
            DISK_CONFIGDRIVE)

    KERNEL_STR = "kernel"
    RAMDISK_STR = "ramdisk"
    DISK_STR = "root"
    DISK_RAW_STR = "os_raw"
    DISK_VHD_STR = "vhd"
    DISK_ISO_STR = "iso"
    DISK_CONFIGDRIVE_STR = "configdrive"
    _strs = (KERNEL_STR, RAMDISK_STR, DISK_STR, DISK_RAW_STR, DISK_VHD_STR,
             DISK_ISO_STR, DISK_CONFIGDRIVE_STR)

    @classmethod
    def to_string(cls, image_type):
        return dict(zip(cls._ids, ImageType._strs)).get(image_type)

    @classmethod
    def get_role(cls, image_type_id):
        """Get the role played by the image, based on its type."""
        return {
            cls.KERNEL: 'kernel',
            cls.RAMDISK: 'ramdisk',
            cls.DISK: 'root',
            cls.DISK_RAW: 'root',
            cls.DISK_VHD: 'root',
            cls.DISK_ISO: 'iso',
            cls.DISK_CONFIGDRIVE: 'configdrive'
        }.get(image_type_id)


def get_vm_device_id(session, image_meta):
    # NOTE: device_id should be 2 for windows VMs which run new xentools
    # (>=6.1). Refer to http://support.citrix.com/article/CTX135099 for more
    # information.
    device_id = image_meta.properties.get('hw_device_id')

    # The device_id is required to be set for hypervisor version 6.1 and above
    if device_id:
        hypervisor_version = session.product_version
        if _hypervisor_supports_device_id(hypervisor_version):
            return device_id
        else:
            msg = _("Device id %(id)s specified is not supported by "
                    "hypervisor version %(version)s") % {'id': device_id,
                    'version': hypervisor_version}
            raise exception.NovaException(msg)


def _hypervisor_supports_device_id(version):
    version_as_string = '.'.join(str(v) for v in version)
    return(versionutils.is_compatible('6.1', version_as_string))


def create_vm(session, instance, name_label, kernel, ramdisk,
              use_pv_kernel=False, device_id=None):
    """Create a VM record.  Returns new VM reference.
    the use_pv_kernel flag indicates whether the guest is HVM or PV

    There are 3 scenarios:

        1. Using paravirtualization, kernel passed in

        2. Using paravirtualization, kernel within the image

        3. Using hardware virtualization
    """
    flavor = instance.get_flavor()
    mem = str(int(flavor.memory_mb) * units.Mi)
    vcpus = str(flavor.vcpus)

    vcpu_weight = flavor.vcpu_weight
    vcpu_params = {}
    if vcpu_weight is not None:
        # NOTE(johngarbutt) bug in XenServer 6.1 and 6.2 means
        # we need to specify both weight and cap for either to apply
        vcpu_params = {"weight": str(vcpu_weight), "cap": "0"}

    cpu_mask_list = hardware.get_vcpu_pin_set()
    if cpu_mask_list:
        cpu_mask = hardware.format_cpu_spec(cpu_mask_list,
                                            allow_ranges=False)
        vcpu_params["mask"] = cpu_mask

    viridian = 'true' if instance['os_type'] == 'windows' else 'false'

    rec = {
        'actions_after_crash': 'destroy',
        'actions_after_reboot': 'restart',
        'actions_after_shutdown': 'destroy',
        'affinity': '',
        'blocked_operations': {},
        'ha_always_run': False,
        'ha_restart_priority': '',
        'HVM_boot_params': {},
        'HVM_boot_policy': '',
        'is_a_template': False,
        'memory_dynamic_min': mem,
        'memory_dynamic_max': mem,
        'memory_static_min': '0',
        'memory_static_max': mem,
        'memory_target': mem,
        'name_description': '',
        'name_label': name_label,
        'other_config': {'nova_uuid': str(instance['uuid'])},
        'PCI_bus': '',
        'platform': {'acpi': 'true', 'apic': 'true', 'pae': 'true',
                     'viridian': viridian, 'timeoffset': '0'},
        'PV_args': '',
        'PV_bootloader': '',
        'PV_bootloader_args': '',
        'PV_kernel': '',
        'PV_legacy_args': '',
        'PV_ramdisk': '',
        'recommendations': '',
        'tags': [],
        'user_version': '0',
        'VCPUs_at_startup': vcpus,
        'VCPUs_max': vcpus,
        'VCPUs_params': vcpu_params,
        'xenstore_data': {'vm-data/allowvssprovider': 'false'}}

    # Complete VM configuration record according to the image type
    # non-raw/raw with PV kernel/raw in HVM mode
    if use_pv_kernel:
        rec['platform']['nx'] = 'false'
        if instance['kernel_id']:
            # 1. Kernel explicitly passed in, use that
            rec['PV_args'] = 'root=/dev/xvda1'
            rec['PV_kernel'] = kernel
            rec['PV_ramdisk'] = ramdisk
        else:
            # 2. Use kernel within the image
            rec['PV_bootloader'] = 'pygrub'
    else:
        # 3. Using hardware virtualization
        rec['platform']['nx'] = 'true'
        rec['HVM_boot_params'] = {'order': 'dc'}
        rec['HVM_boot_policy'] = 'BIOS order'

    if device_id:
        rec['platform']['device_id'] = str(device_id).zfill(4)

    vm_ref = session.VM.create(rec)
    LOG.debug('Created VM', instance=instance)
    return vm_ref


def destroy_vm(session, instance, vm_ref):
    """Destroys a VM record."""
    try:
        session.VM.destroy(vm_ref)
    except session.XenAPI.Failure:
        LOG.exception(_LE('Destroy VM failed'))
        return

    LOG.debug("VM destroyed", instance=instance)


def clean_shutdown_vm(session, instance, vm_ref):
    if is_vm_shutdown(session, vm_ref):
        LOG.warning(_LW("VM already halted, skipping shutdown..."),
                    instance=instance)
        return True

    LOG.debug("Shutting down VM (cleanly)", instance=instance)
    try:
        session.call_xenapi('VM.clean_shutdown', vm_ref)
    except session.XenAPI.Failure:
        LOG.exception(_LE('Shutting down VM (cleanly) failed.'))
        return False
    return True


def hard_shutdown_vm(session, instance, vm_ref):
    if is_vm_shutdown(session, vm_ref):
        LOG.warning(_LW("VM already halted, skipping shutdown..."),
                    instance=instance)
        return True

    LOG.debug("Shutting down VM (hard)", instance=instance)
    try:
        session.call_xenapi('VM.hard_shutdown', vm_ref)
    except session.XenAPI.Failure:
        LOG.exception(_LE('Shutting down VM (hard) failed'))
        return False
    return True


def is_vm_shutdown(session, vm_ref):
    state = get_power_state(session, vm_ref)
    if state == power_state.SHUTDOWN:
        return True
    return False


def is_enough_free_mem(session, instance):
    flavor = instance.get_flavor()
    mem = int(flavor.memory_mb) * units.Mi
    host_free_mem = int(session.call_xenapi("host.compute_free_memory",
                                            session.host_ref))
    return host_free_mem >= mem


def _should_retry_unplug_vbd(err):
    # Retry if unplug failed with DEVICE_DETACH_REJECTED
    # For reasons which we don't understand,
    # we're seeing the device still in use, even when all processes
    # using the device should be dead.
    # Since XenServer 6.2, we also need to retry if we get
    # INTERNAL_ERROR, as that error goes away when you retry.
    return (err == 'DEVICE_DETACH_REJECTED'
            or
            err == 'INTERNAL_ERROR')


def unplug_vbd(session, vbd_ref, this_vm_ref):
    # make sure that perform at least once
    max_attempts = max(0, CONF.xenserver.num_vbd_unplug_retries) + 1
    for num_attempt in range(1, max_attempts + 1):
        try:
            if num_attempt > 1:
                greenthread.sleep(1)

            session.VBD.unplug(vbd_ref, this_vm_ref)
            return
        except session.XenAPI.Failure as exc:
            err = len(exc.details) > 0 and exc.details[0]
            if err == 'DEVICE_ALREADY_DETACHED':
                LOG.info(_LI('VBD %s already detached'), vbd_ref)
                return
            elif _should_retry_unplug_vbd(err):
                LOG.info(_LI('VBD %(vbd_ref)s uplug failed with "%(err)s", '
                             'attempt %(num_attempt)d/%(max_attempts)d'),
                         {'vbd_ref': vbd_ref, 'num_attempt': num_attempt,
                          'max_attempts': max_attempts, 'err': err})
            else:
                LOG.exception(_LE('Unable to unplug VBD'))
                raise exception.StorageError(
                        reason=_('Unable to unplug VBD %s') % vbd_ref)

    raise exception.StorageError(
            reason=_('Reached maximum number of retries '
                     'trying to unplug VBD %s')
                        % vbd_ref)


def destroy_vbd(session, vbd_ref):
    """Destroy VBD from host database."""
    try:
        session.call_xenapi('VBD.destroy', vbd_ref)
    except session.XenAPI.Failure:
        LOG.exception(_LE('Unable to destroy VBD'))
        raise exception.StorageError(
                reason=_('Unable to destroy VBD %s') % vbd_ref)


def create_vbd(session, vm_ref, vdi_ref, userdevice, vbd_type='disk',
               read_only=False, bootable=False, osvol=False,
               empty=False, unpluggable=True):
    """Create a VBD record and returns its reference."""
    vbd_rec = {}
    vbd_rec['VM'] = vm_ref
    if vdi_ref is None:
        vdi_ref = 'OpaqueRef:NULL'
    vbd_rec['VDI'] = vdi_ref
    vbd_rec['userdevice'] = str(userdevice)
    vbd_rec['bootable'] = bootable
    vbd_rec['mode'] = read_only and 'RO' or 'RW'
    vbd_rec['type'] = vbd_type
    vbd_rec['unpluggable'] = unpluggable
    vbd_rec['empty'] = empty
    vbd_rec['other_config'] = {}
    vbd_rec['qos_algorithm_type'] = ''
    vbd_rec['qos_algorithm_params'] = {}
    vbd_rec['qos_supported_algorithms'] = []
    LOG.debug('Creating %(vbd_type)s-type VBD for VM %(vm_ref)s,'
              ' VDI %(vdi_ref)s ... ',
              {'vbd_type': vbd_type, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref})
    vbd_ref = session.call_xenapi('VBD.create', vbd_rec)
    LOG.debug('Created VBD %(vbd_ref)s for VM %(vm_ref)s,'
              ' VDI %(vdi_ref)s.',
              {'vbd_ref': vbd_ref, 'vm_ref': vm_ref, 'vdi_ref': vdi_ref})
    if osvol:
        # set osvol=True in other-config to indicate this is an
        # attached nova (or cinder) volume
        session.call_xenapi('VBD.add_to_other_config',
                            vbd_ref, 'osvol', 'True')
    return vbd_ref


def attach_cd(session, vm_ref, vdi_ref, userdevice):
    """Create an empty VBD, then insert the CD."""
    vbd_ref = create_vbd(session, vm_ref, None, userdevice,
                         vbd_type='cd', read_only=True,
                         bootable=True, empty=True,
                         unpluggable=False)
    session.call_xenapi('VBD.insert', vbd_ref, vdi_ref)
    return vbd_ref


def destroy_vdi(session, vdi_ref):
    try:
        session.call_xenapi('VDI.destroy', vdi_ref)
    except session.XenAPI.Failure:
        LOG.debug("Unable to destroy VDI %s", vdi_ref, exc_info=True)
        msg = _("Unable to destroy VDI %s") % vdi_ref
        LOG.error(msg)
        raise exception.StorageError(reason=msg)


def safe_destroy_vdis(session, vdi_refs):
    """Tries to destroy the requested VDIs, but ignores any errors."""
    for vdi_ref in vdi_refs:
        try:
            destroy_vdi(session, vdi_ref)
        except exception.StorageError:
            LOG.debug("Ignoring error while destroying VDI: %s", vdi_ref)


def create_vdi(session, sr_ref, instance, name_label, disk_type, virtual_size,
               read_only=False):
    """Create a VDI record and returns its reference."""
    vdi_ref = session.call_xenapi("VDI.create",
         {'name_label': name_label,
          'name_description': disk_type,
          'SR': sr_ref,
          'virtual_size': str(virtual_size),
          'type': 'User',
          'sharable': False,
          'read_only': read_only,
          'xenstore_data': {},
          'other_config': _get_vdi_other_config(disk_type, instance=instance),
          'sm_config': {},
          'tags': []})
    LOG.debug('Created VDI %(vdi_ref)s (%(name_label)s,'
              ' %(virtual_size)s, %(read_only)s) on %(sr_ref)s.',
              {'vdi_ref': vdi_ref, 'name_label': name_label,
               'virtual_size': virtual_size, 'read_only': read_only,
               'sr_ref': sr_ref})
    return vdi_ref


@contextlib.contextmanager
def _dummy_vm(session, instance, vdi_ref):
    """This creates a temporary VM so that we can snapshot a VDI.

    VDI's can't be snapshotted directly since the API expects a `vm_ref`. To
    work around this, we need to create a temporary VM and then map the VDI to
    the VM using a temporary VBD.
    """
    name_label = "dummy"
    vm_ref = create_vm(session, instance, name_label, None, None)
    try:
        vbd_ref = create_vbd(session, vm_ref, vdi_ref, 'autodetect',
                             read_only=True)
        try:
            yield vm_ref
        finally:
            try:
                destroy_vbd(session, vbd_ref)
            except exception.StorageError:
                # destroy_vbd() will log error
                pass
    finally:
        destroy_vm(session, instance, vm_ref)


def _safe_copy_vdi(session, sr_ref, instance, vdi_to_copy_ref):
    """Copy a VDI and return the new VDIs reference.

    This function differs from the XenAPI `VDI.copy` call in that the copy is
    atomic and isolated, meaning we don't see half-downloaded images. It
    accomplishes this by copying the VDI's into a temporary directory and then
    atomically renaming them into the SR when the copy is completed.

    The correct long term solution is to fix `VDI.copy` so that it is atomic
    and isolated.
    """
    with _dummy_vm(session, instance, vdi_to_copy_ref) as vm_ref:
        label = "snapshot"
        with snapshot_attached_here(
                session, instance, vm_ref, label) as vdi_uuids:
            imported_vhds = session.call_plugin_serialized(
                'workarounds', 'safe_copy_vdis',
                sr_path=get_sr_path(session, sr_ref=sr_ref),
                vdi_uuids=vdi_uuids, uuid_stack=_make_uuid_stack())

    root_uuid = imported_vhds['root']['uuid']

    # rescan to discover new VHDs
    scan_default_sr(session)
    vdi_ref = session.call_xenapi('VDI.get_by_uuid', root_uuid)
    return vdi_ref


def _clone_vdi(session, vdi_to_clone_ref):
    """Clones a VDI and return the new VDIs reference."""
    vdi_ref = session.call_xenapi('VDI.clone', vdi_to_clone_ref)
    LOG.debug('Cloned VDI %(vdi_ref)s from VDI '
              '%(vdi_to_clone_ref)s',
              {'vdi_ref': vdi_ref, 'vdi_to_clone_ref': vdi_to_clone_ref})
    return vdi_ref


def _get_vdi_other_config(disk_type, instance=None):
    """Return metadata to store in VDI's other_config attribute.

    `nova_instance_uuid` is used to associate a VDI with a particular instance
    so that, if it becomes orphaned from an unclean shutdown of a
    compute-worker, we can safely detach it.
    """
    other_config = {'nova_disk_type': disk_type}

    # create_vdi may be called simply while creating a volume
    # hence information about instance may or may not be present
    if instance:
        other_config['nova_instance_uuid'] = instance['uuid']

    return other_config


def _set_vdi_info(session, vdi_ref, vdi_type, name_label, description,
                  instance):
    existing_other_config = session.call_xenapi('VDI.get_other_config',
                                                vdi_ref)

    session.call_xenapi('VDI.set_name_label', vdi_ref, name_label)
    session.call_xenapi('VDI.set_name_description', vdi_ref, description)

    other_config = _get_vdi_other_config(vdi_type, instance=instance)
    for key, value in six.iteritems(other_config):
        if key not in existing_other_config:
            session.call_xenapi(
                    "VDI.add_to_other_config", vdi_ref, key, value)


def _vm_get_vbd_refs(session, vm_ref):
    return session.call_xenapi("VM.get_VBDs", vm_ref)


def _vbd_get_rec(session, vbd_ref):
    return session.call_xenapi("VBD.get_record", vbd_ref)


def _vdi_get_rec(session, vdi_ref):
    return session.call_xenapi("VDI.get_record", vdi_ref)


def _vdi_get_uuid(session, vdi_ref):
    return session.call_xenapi("VDI.get_uuid", vdi_ref)


def _vdi_snapshot(session, vdi_ref):
    return session.call_xenapi("VDI.snapshot", vdi_ref, {})


def get_vdi_for_vm_safely(session, vm_ref, userdevice='0'):
    """Retrieves the primary VDI for a VM."""
    vbd_refs = _vm_get_vbd_refs(session, vm_ref)
    for vbd_ref in vbd_refs:
        vbd_rec = _vbd_get_rec(session, vbd_ref)
        # Convention dictates the primary VDI will be userdevice 0
        if vbd_rec['userdevice'] == userdevice:
            vdi_ref = vbd_rec['VDI']
            vdi_rec = _vdi_get_rec(session, vdi_ref)
            return vdi_ref, vdi_rec
    raise exception.NovaException(_("No primary VDI found for %s") % vm_ref)


def get_all_vdi_uuids_for_vm(session, vm_ref, min_userdevice=0):
    vbd_refs = _vm_get_vbd_refs(session, vm_ref)
    for vbd_ref in vbd_refs:
        vbd_rec = _vbd_get_rec(session, vbd_ref)
        if int(vbd_rec['userdevice']) >= min_userdevice:
            vdi_ref = vbd_rec['VDI']
            yield _vdi_get_uuid(session, vdi_ref)


def _try_strip_base_mirror_from_vdi(session, vdi_ref):
    try:
        session.call_xenapi("VDI.remove_from_sm_config", vdi_ref,
                            "base_mirror")
    except session.XenAPI.Failure:
        LOG.debug("Error while removing sm_config", exc_info=True)


def strip_base_mirror_from_vdis(session, vm_ref):
    # NOTE(johngarbutt) part of workaround for XenServer bug CA-98606
    vbd_refs = session.call_xenapi("VM.get_VBDs", vm_ref)
    for vbd_ref in vbd_refs:
        vdi_ref = session.call_xenapi("VBD.get_VDI", vbd_ref)
        _try_strip_base_mirror_from_vdi(session, vdi_ref)


def _delete_snapshots_in_vdi_chain(session, instance, vdi_uuid_chain, sr_ref):
    possible_snapshot_parents = vdi_uuid_chain[1:]

    if len(possible_snapshot_parents) == 0:
        LOG.debug("No VHD chain.", instance=instance)
        return

    snapshot_uuids = _child_vhds(session, sr_ref, possible_snapshot_parents,
                                 old_snapshots_only=True)
    number_of_snapshots = len(snapshot_uuids)

    if number_of_snapshots <= 0:
        LOG.debug("No snapshots to remove.", instance=instance)
        return

    vdi_refs = [session.VDI.get_by_uuid(vdi_uuid)
                for vdi_uuid in snapshot_uuids]
    safe_destroy_vdis(session, vdi_refs)

    # ensure garbage collector has been run
    _scan_sr(session, sr_ref)

    LOG.info(_LI("Deleted %s snapshots."), number_of_snapshots,
             instance=instance)


def remove_old_snapshots(session, instance, vm_ref):
    """See if there is an snapshot present that should be removed."""
    LOG.debug("Starting remove_old_snapshots for VM", instance=instance)
    vm_vdi_ref, vm_vdi_rec = get_vdi_for_vm_safely(session, vm_ref)
    chain = _walk_vdi_chain(session, vm_vdi_rec['uuid'])
    vdi_uuid_chain = [vdi_rec['uuid'] for vdi_rec in chain]
    sr_ref = vm_vdi_rec["SR"]
    _delete_snapshots_in_vdi_chain(session, instance, vdi_uuid_chain, sr_ref)


@contextlib.contextmanager
def snapshot_attached_here(session, instance, vm_ref, label, userdevice='0',
                           post_snapshot_callback=None):
    # impl method allow easier patching for tests
    return _snapshot_attached_here_impl(session, instance, vm_ref, label,
                                        userdevice, post_snapshot_callback)


def _snapshot_attached_here_impl(session, instance, vm_ref, label, userdevice,
                                 post_snapshot_callback):
    """Snapshot the root disk only.  Return a list of uuids for the vhds
    in the chain.
    """
    LOG.debug("Starting snapshot for VM", instance=instance)

    # Memorize the VDI chain so we can poll for coalesce
    vm_vdi_ref, vm_vdi_rec = get_vdi_for_vm_safely(session, vm_ref,
                                                   userdevice)
    chain = _walk_vdi_chain(session, vm_vdi_rec['uuid'])
    vdi_uuid_chain = [vdi_rec['uuid'] for vdi_rec in chain]
    sr_ref = vm_vdi_rec["SR"]

    # clean up after any interrupted snapshot attempts
    _delete_snapshots_in_vdi_chain(session, instance, vdi_uuid_chain, sr_ref)

    snapshot_ref = _vdi_snapshot(session, vm_vdi_ref)
    if post_snapshot_callback is not None:
        post_snapshot_callback(task_state=task_states.IMAGE_PENDING_UPLOAD)
    try:
        # When the VDI snapshot is taken a new parent is introduced.
        # If we have taken a snapshot before, the new parent can be coalesced.
        # We need to wait for this to happen before trying to copy the chain.
        _wait_for_vhd_coalesce(session, instance, sr_ref, vm_vdi_ref,
                               vdi_uuid_chain)

        snapshot_uuid = _vdi_get_uuid(session, snapshot_ref)
        chain = _walk_vdi_chain(session, snapshot_uuid)
        vdi_uuids = [vdi_rec['uuid'] for vdi_rec in chain]
        yield vdi_uuids
    finally:
        safe_destroy_vdis(session, [snapshot_ref])
        # TODO(johngarbut) we need to check the snapshot has been coalesced
        # now its associated VDI has been deleted.


def get_sr_path(session, sr_ref=None):
    """Return the path to our storage repository

    This is used when we're dealing with VHDs directly, either by taking
    snapshots or by restoring an image in the DISK_VHD format.
    """
    if sr_ref is None:
        sr_ref = safe_find_sr(session)
    pbd_rec = session.call_xenapi("PBD.get_all_records_where",
                                  'field "host"="%s" and '
                                  'field "SR"="%s"' %
                                  (session.host_ref, sr_ref))

    # NOTE(bobball): There can only be one PBD for a host/SR pair, but path is
    # not always present - older versions of XS do not set it.
    pbd_ref = list(pbd_rec.keys())[0]
    device_config = pbd_rec[pbd_ref]['device_config']
    if 'path' in device_config:
        return device_config['path']

    sr_rec = session.call_xenapi("SR.get_record", sr_ref)
    sr_uuid = sr_rec["uuid"]
    if sr_rec["type"] not in ["ext", "nfs"]:
        raise exception.NovaException(
            _("Only file-based SRs (ext/NFS) are supported by this feature."
              "  SR %(uuid)s is of type %(type)s") %
            {"uuid": sr_uuid, "type": sr_rec["type"]})

    return os.path.join(CONF.xenserver.sr_base_path, sr_uuid)


def destroy_cached_images(session, sr_ref, all_cached=False, dry_run=False):
    """Destroy used or unused cached images.

    A cached image that is being used by at least one VM is said to be 'used'.

    In the case of an 'unused' image, the cached image will be the only
    descendent of the base-copy. So when we delete the cached-image, the
    refcount will drop to zero and XenServer will automatically destroy the
    base-copy for us.

    The default behavior of this function is to destroy only 'unused' cached
    images. To destroy all cached images, use the `all_cached=True` kwarg.
    """
    cached_images = _find_cached_images(session, sr_ref)
    destroyed = set()

    def destroy_cached_vdi(vdi_uuid, vdi_ref):
        LOG.debug("Destroying cached VDI '%(vdi_uuid)s'")
        if not dry_run:
            destroy_vdi(session, vdi_ref)
        destroyed.add(vdi_uuid)

    for vdi_ref in cached_images.values():
        vdi_uuid = session.call_xenapi('VDI.get_uuid', vdi_ref)

        if all_cached:
            destroy_cached_vdi(vdi_uuid, vdi_ref)
            continue

        # Unused-Only: Search for siblings

        # Chain length greater than two implies a VM must be holding a ref to
        # the base-copy (otherwise it would have coalesced), so consider this
        # cached image used.
        chain = list(_walk_vdi_chain(session, vdi_uuid))
        if len(chain) > 2:
            continue
        elif len(chain) == 2:
            # Siblings imply cached image is used
            root_vdi_rec = chain[-1]
            children = _child_vhds(session, sr_ref, [root_vdi_rec['uuid']])
            if len(children) > 1:
                continue

        destroy_cached_vdi(vdi_uuid, vdi_ref)

    return destroyed


def _find_cached_images(session, sr_ref):
    """Return a dict(uuid=vdi_ref) representing all cached images."""
    cached_images = {}
    for vdi_ref, vdi_rec in _get_all_vdis_in_sr(session, sr_ref):
        try:
            image_id = vdi_rec['other_config']['image-id']
        except KeyError:
            continue

        cached_images[image_id] = vdi_ref

    return cached_images


def _find_cached_image(session, image_id, sr_ref):
    """Returns the vdi-ref of the cached image."""
    name_label = _get_image_vdi_label(image_id)
    recs = session.call_xenapi("VDI.get_all_records_where",
                               'field "name__label"="%s"'
                               % name_label)
    number_found = len(recs)
    if number_found > 0:
        if number_found > 1:
            LOG.warning(_LW("Multiple base images for image: %s"), image_id)
        return list(recs.keys())[0]


def _get_resize_func_name(session):
    brand = session.product_brand
    version = session.product_version

    # To maintain backwards compatibility. All recent versions
    # should use VDI.resize
    if version and brand:
        xcp = brand == 'XCP'
        r1_2_or_above = (version[0] == 1 and version[1] > 1) or version[0] > 1

        xenserver = brand == 'XenServer'
        r6_or_above = version[0] > 5

        if (xcp and not r1_2_or_above) or (xenserver and not r6_or_above):
            return 'VDI.resize_online'

    return 'VDI.resize'


def _vdi_get_virtual_size(session, vdi_ref):
    size = session.call_xenapi('VDI.get_virtual_size', vdi_ref)
    return int(size)


def _vdi_resize(session, vdi_ref, new_size):
    resize_func_name = _get_resize_func_name(session)
    session.call_xenapi(resize_func_name, vdi_ref, str(new_size))


def update_vdi_virtual_size(session, instance, vdi_ref, new_gb):
    virtual_size = _vdi_get_virtual_size(session, vdi_ref)
    new_disk_size = new_gb * units.Gi

    msg = ("Resizing up VDI %(vdi_ref)s from %(virtual_size)d "
           "to %(new_disk_size)d")
    LOG.debug(msg, {'vdi_ref': vdi_ref, 'virtual_size': virtual_size,
                    'new_disk_size': new_disk_size},
              instance=instance)

    if virtual_size < new_disk_size:
        # For resize up. Simple VDI resize will do the trick
        _vdi_resize(session, vdi_ref, new_disk_size)

    elif virtual_size == new_disk_size:
        LOG.debug("No need to change vdi virtual size.",
                  instance=instance)

    else:
        # NOTE(johngarbutt): we should never get here
        # but if we don't raise an exception, a user might be able to use
        # more storage than allowed by their chosen instance flavor
        msg = _("VDI %(vdi_ref)s is %(virtual_size)d bytes which is larger "
                "than flavor size of %(new_disk_size)d bytes.")
        msg = msg % {'vdi_ref': vdi_ref, 'virtual_size': virtual_size,
              'new_disk_size': new_disk_size}
        LOG.debug(msg, instance=instance)
        raise exception.ResizeError(reason=msg)


def resize_disk(session, instance, vdi_ref, flavor):
    size_gb = flavor.root_gb
    if size_gb == 0:
        reason = _("Can't resize a disk to 0 GB.")
        raise exception.ResizeError(reason=reason)

    sr_ref = safe_find_sr(session)
    clone_ref = _clone_vdi(session, vdi_ref)

    try:
        # Resize partition and filesystem down
        _auto_configure_disk(session, clone_ref, size_gb)

        # Create new VDI
        vdi_size = size_gb * units.Gi
        # NOTE(johannes): No resizing allowed for rescue instances, so
        # using instance['name'] is safe here
        new_ref = create_vdi(session, sr_ref, instance, instance['name'],
                             'root', vdi_size)

        new_uuid = session.call_xenapi('VDI.get_uuid', new_ref)

        # Manually copy contents over
        virtual_size = size_gb * units.Gi
        _copy_partition(session, clone_ref, new_ref, 1, virtual_size)

        return new_ref, new_uuid
    finally:
        destroy_vdi(session, clone_ref)


def _auto_configure_disk(session, vdi_ref, new_gb):
    """Partition and resize FS to match the size specified by
    flavors.root_gb.

    This is a fail-safe to prevent accidentally destroying data on a disk
    erroneously marked as auto_disk_config=True.

    The criteria for allowing resize are:

        1. 'auto_disk_config' must be true for the instance (and image).
           (If we've made it here, then auto_disk_config=True.)

        2. The disk must have only one partition.

        3. The file-system on the one partition must be ext3 or ext4.
    """
    if new_gb == 0:
        LOG.debug("Skipping auto_config_disk as destination size is 0GB")
        return

    with vdi_attached_here(session, vdi_ref, read_only=False) as dev:
        partitions = _get_partitions(dev)

        if len(partitions) != 1:
            reason = _('Disk must have only one partition.')
            raise exception.CannotResizeDisk(reason=reason)

        num, start, old_sectors, fstype, name, flags = partitions[0]
        if fstype not in ('ext3', 'ext4'):
            reason = _('Disk contains a filesystem '
                       'we are unable to resize: %s')
            raise exception.CannotResizeDisk(reason=(reason % fstype))

        if num != 1:
            reason = _('The only partition should be partition 1.')
            raise exception.CannotResizeDisk(reason=reason)

        new_sectors = new_gb * units.Gi / SECTOR_SIZE
        _resize_part_and_fs(dev, start, old_sectors, new_sectors, flags)


def try_auto_configure_disk(session, vdi_ref, new_gb):
    try:
        _auto_configure_disk(session, vdi_ref, new_gb)
    except exception.CannotResizeDisk as e:
        msg = _LW('Attempted auto_configure_disk failed because: %s')
        LOG.warning(msg % e)


def _make_partition(session, dev, partition_start, partition_end):
    dev_path = utils.make_dev_path(dev)

    # NOTE(bobball) If this runs in Dom0, parted will error trying
    # to re-read the partition table and return a generic error
    utils.execute('parted', '--script', dev_path,
                  'mklabel', 'msdos', run_as_root=True,
                  check_exit_code=not session.is_local_connection)

    utils.execute('parted', '--script', dev_path, '--',
                  'mkpart', 'primary',
                  partition_start,
                  partition_end,
                  run_as_root=True,
                  check_exit_code=not session.is_local_connection)

    partition_path = utils.make_dev_path(dev, partition=1)
    if session.is_local_connection:
        # Need to refresh the partitions
        utils.trycmd('kpartx', '-a', dev_path,
                     run_as_root=True,
                     discard_warnings=True)

        # Sometimes the partition gets created under /dev/mapper, depending
        # on the setup in dom0.
        mapper_path = '/dev/mapper/%s' % os.path.basename(partition_path)
        if os.path.exists(mapper_path):
            return mapper_path

    return partition_path


def _generate_disk(session, instance, vm_ref, userdevice, name_label,
                   disk_type, size_mb, fs_type, fs_label=None):
    """Steps to programmatically generate a disk:

        1. Create VDI of desired size

        2. Attach VDI to compute worker

        3. Create partition

        4. Create VBD between instance VM and VDI
    """
    # 1. Create VDI
    sr_ref = safe_find_sr(session)
    ONE_MEG = units.Mi
    virtual_size = size_mb * ONE_MEG
    vdi_ref = create_vdi(session, sr_ref, instance, name_label, disk_type,
                         virtual_size)

    try:
        # 2. Attach VDI to compute worker (VBD hotplug)
        with vdi_attached_here(session, vdi_ref, read_only=False) as dev:
            # 3. Create partition
            partition_start = "2048s"
            partition_end = "-0"

            partition_path = _make_partition(session, dev,
                                             partition_start, partition_end)

            if fs_type is not None:
                utils.mkfs(fs_type, partition_path, fs_label,
                           run_as_root=True)

        # 4. Create VBD between instance VM and VDI
        if vm_ref:
            create_vbd(session, vm_ref, vdi_ref, userdevice, bootable=False)
    except Exception:
        with excutils.save_and_reraise_exception():
            msg = "Error while generating disk number: %s" % userdevice
            LOG.debug(msg, instance=instance, exc_info=True)
            safe_destroy_vdis(session, [vdi_ref])

    return vdi_ref


def generate_swap(session, instance, vm_ref, userdevice, name_label, swap_mb):
    # NOTE(jk0): We use a FAT32 filesystem for the Windows swap
    # partition because that is what parted supports.
    is_windows = instance['os_type'] == "windows"
    fs_type = "vfat" if is_windows else "swap"

    _generate_disk(session, instance, vm_ref, userdevice, name_label,
                   'swap', swap_mb, fs_type)


def get_ephemeral_disk_sizes(total_size_gb):
    if not total_size_gb:
        return

    max_size_gb = 2000
    if total_size_gb % 1024 == 0:
        max_size_gb = 1024

    left_to_allocate = total_size_gb
    while left_to_allocate > 0:
        size_gb = min(max_size_gb, left_to_allocate)
        yield size_gb
        left_to_allocate -= size_gb


def generate_single_ephemeral(session, instance, vm_ref, userdevice,
                              size_gb, instance_name_label=None):
    if instance_name_label is None:
        instance_name_label = instance["name"]

    name_label = "%s ephemeral" % instance_name_label
    fs_label = "ephemeral"
    # TODO(johngarbutt) need to move DEVICE_EPHEMERAL from vmops to use it here
    label_number = int(userdevice) - 4
    if label_number > 0:
        name_label = "%s (%d)" % (name_label, label_number)
        fs_label = "ephemeral%d" % label_number

    return _generate_disk(session, instance, vm_ref, str(userdevice),
                          name_label, 'ephemeral', size_gb * 1024,
                          CONF.default_ephemeral_format, fs_label)


def generate_ephemeral(session, instance, vm_ref, first_userdevice,
                       instance_name_label, total_size_gb):
    # NOTE(johngarbutt): max possible size of a VHD disk is 2043GB
    sizes = get_ephemeral_disk_sizes(total_size_gb)
    first_userdevice = int(first_userdevice)

    vdi_refs = []
    try:
        for userdevice, size_gb in enumerate(sizes, start=first_userdevice):
            ref = generate_single_ephemeral(session, instance, vm_ref,
                                            userdevice, size_gb,
                                            instance_name_label)
            vdi_refs.append(ref)
    except Exception as exc:
        with excutils.save_and_reraise_exception():
            LOG.debug("Error when generating ephemeral disk. "
                      "Device: %(userdevice)s Size GB: %(size_gb)s "
                      "Error: %(exc)s", {
                            'userdevice': userdevice,
                            'size_gb': size_gb,
                            'exc': exc})
            safe_destroy_vdis(session, vdi_refs)


def generate_iso_blank_root_disk(session, instance, vm_ref, userdevice,
                                 name_label, size_gb):
    _generate_disk(session, instance, vm_ref, userdevice, name_label,
                   'user', size_gb * 1024, CONF.default_ephemeral_format)


def generate_configdrive(session, instance, vm_ref, userdevice,
                         network_info, admin_password=None, files=None):
    sr_ref = safe_find_sr(session)
    vdi_ref = create_vdi(session, sr_ref, instance, 'config-2',
                         'configdrive', configdrive.CONFIGDRIVESIZE_BYTES)

    try:
        with vdi_attached_here(session, vdi_ref, read_only=False) as dev:
            extra_md = {}
            if admin_password:
                extra_md['admin_pass'] = admin_password
            inst_md = instance_metadata.InstanceMetadata(instance,
                    content=files, extra_md=extra_md,
                    network_info=network_info)
            with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
                with utils.tempdir() as tmp_path:
                    tmp_file = os.path.join(tmp_path, 'configdrive')
                    cdb.make_drive(tmp_file)

                    dev_path = utils.make_dev_path(dev)
                    utils.execute('dd',
                                  'if=%s' % tmp_file,
                                  'of=%s' % dev_path,
                                  'oflag=direct,sync',
                                  run_as_root=True)

        create_vbd(session, vm_ref, vdi_ref, userdevice, bootable=False,
                   read_only=True)
    except Exception:
        with excutils.save_and_reraise_exception():
            msg = "Error while generating config drive"
            LOG.debug(msg, instance=instance, exc_info=True)
            safe_destroy_vdis(session, [vdi_ref])


def _create_kernel_image(context, session, instance, name_label, image_id,
                         image_type):
    """Creates kernel/ramdisk file from the image stored in the cache.
    If the image is not present in the cache, it streams it from glance.

    Returns: A list of dictionaries that describe VDIs
    """
    filename = ""
    if CONF.xenserver.cache_images:
        args = {}
        args['cached-image'] = image_id
        args['new-image-uuid'] = str(uuid.uuid4())
        filename = session.call_plugin('kernel', 'create_kernel_ramdisk', args)

    if filename == "":
        return _fetch_disk_image(context, session, instance, name_label,
                                 image_id, image_type)
    else:
        vdi_type = ImageType.to_string(image_type)
        return {vdi_type: dict(uuid=None, file=filename)}


def create_kernel_and_ramdisk(context, session, instance, name_label):
    kernel_file = None
    ramdisk_file = None
    if instance['kernel_id']:
        vdis = _create_kernel_image(context, session,
                instance, name_label, instance['kernel_id'],
                ImageType.KERNEL)
        kernel_file = vdis['kernel'].get('file')

    if instance['ramdisk_id']:
        vdis = _create_kernel_image(context, session,
                instance, name_label, instance['ramdisk_id'],
                ImageType.RAMDISK)
        ramdisk_file = vdis['ramdisk'].get('file')

    return kernel_file, ramdisk_file


def destroy_kernel_ramdisk(session, instance, kernel, ramdisk):
    args = {}
    if kernel:
        args['kernel-file'] = kernel
    if ramdisk:
        args['ramdisk-file'] = ramdisk
    if args:
        LOG.debug("Removing kernel/ramdisk files from dom0",
                    instance=instance)
        session.call_plugin('kernel', 'remove_kernel_ramdisk', args)


def _get_image_vdi_label(image_id):
    return 'Glance Image %s' % image_id


def _create_cached_image(context, session, instance, name_label,
                         image_id, image_type):
    sr_ref = safe_find_sr(session)
    sr_type = session.call_xenapi('SR.get_type', sr_ref)

    if CONF.use_cow_images and sr_type != "ext":
        LOG.warning(_LW("Fast cloning is only supported on default local SR "
                        "of type ext. SR on this system was found to be of "
                        "type %s. Ignoring the cow flag."), sr_type)

    @utils.synchronized('xenapi-image-cache' + image_id)
    def _create_cached_image_impl(context, session, instance, name_label,
                                  image_id, image_type, sr_ref):
        cache_vdi_ref = _find_cached_image(session, image_id, sr_ref)
        downloaded = False
        if cache_vdi_ref is None:
            downloaded = True
            vdis = _fetch_image(context, session, instance, name_label,
                                image_id, image_type)

            cache_vdi_ref = session.call_xenapi(
                    'VDI.get_by_uuid', vdis['root']['uuid'])

            session.call_xenapi('VDI.set_name_label', cache_vdi_ref,
                                _get_image_vdi_label(image_id))
            session.call_xenapi('VDI.set_name_description', cache_vdi_ref,
                                'root')
            session.call_xenapi('VDI.add_to_other_config',
                                cache_vdi_ref, 'image-id', str(image_id))

        if CONF.use_cow_images:
            new_vdi_ref = _clone_vdi(session, cache_vdi_ref)
        elif sr_type == 'ext':
            new_vdi_ref = _safe_copy_vdi(session, sr_ref, instance,
                                         cache_vdi_ref)
        else:
            new_vdi_ref = session.call_xenapi("VDI.copy", cache_vdi_ref,
                                              sr_ref)

        session.call_xenapi('VDI.set_name_label', new_vdi_ref, '')
        session.call_xenapi('VDI.set_name_description', new_vdi_ref, '')
        session.call_xenapi('VDI.remove_from_other_config',
                            new_vdi_ref, 'image-id')

        vdi_uuid = session.call_xenapi('VDI.get_uuid', new_vdi_ref)
        return downloaded, vdi_uuid

    downloaded, vdi_uuid = _create_cached_image_impl(context, session,
                                                     instance, name_label,
                                                     image_id, image_type,
                                                     sr_ref)

    vdis = {}
    vdi_type = ImageType.get_role(image_type)
    vdis[vdi_type] = dict(uuid=vdi_uuid, file=None)
    return downloaded, vdis


def create_image(context, session, instance, name_label, image_id,
                 image_type):
    """Creates VDI from the image stored in the local cache. If the image
    is not present in the cache, it streams it from glance.

    Returns: A list of dictionaries that describe VDIs
    """
    cache_images = CONF.xenserver.cache_images.lower()

    # Determine if the image is cacheable
    if image_type == ImageType.DISK_ISO:
        cache = False
    elif cache_images == 'all':
        cache = True
    elif cache_images == 'some':
        sys_meta = utils.instance_sys_meta(instance)
        try:
            cache = strutils.bool_from_string(sys_meta['image_cache_in_nova'])
        except KeyError:
            cache = False
    elif cache_images == 'none':
        cache = False
    else:
        LOG.warning(_LW("Unrecognized cache_images value '%s', defaulting to"
                        " True"), CONF.xenserver.cache_images)
        cache = True

    # Fetch (and cache) the image
    start_time = timeutils.utcnow()
    if cache:
        downloaded, vdis = _create_cached_image(context, session, instance,
                                                name_label, image_id,
                                                image_type)
    else:
        vdis = _fetch_image(context, session, instance, name_label,
                            image_id, image_type)
        downloaded = True
    duration = timeutils.delta_seconds(start_time, timeutils.utcnow())

    LOG.info(_LI("Image creation data, cacheable: %(cache)s, "
                 "downloaded: %(downloaded)s duration: %(duration).2f secs "
                 "for image %(image_id)s"),
             {'image_id': image_id, 'cache': cache, 'downloaded': downloaded,
              'duration': duration})

    for vdi_type, vdi in six.iteritems(vdis):
        vdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi['uuid'])
        _set_vdi_info(session, vdi_ref, vdi_type, name_label, vdi_type,
                      instance)

    return vdis


def _fetch_image(context, session, instance, name_label, image_id, image_type):
    """Fetch image from glance based on image type.

    Returns: A single filename if image_type is KERNEL or RAMDISK
             A list of dictionaries that describe VDIs, otherwise
    """
    if image_type == ImageType.DISK_VHD:
        vdis = _fetch_vhd_image(context, session, instance, image_id)
    else:
        vdis = _fetch_disk_image(context, session, instance, name_label,
                                 image_id, image_type)

    for vdi_type, vdi in six.iteritems(vdis):
        vdi_uuid = vdi['uuid']
        LOG.debug("Fetched VDIs of type '%(vdi_type)s' with UUID"
                  " '%(vdi_uuid)s'",
                  {'vdi_type': vdi_type, 'vdi_uuid': vdi_uuid},
                  instance=instance)

    return vdis


def _make_uuid_stack():
    # NOTE(sirp): The XenAPI plugins run under Python 2.4
    # which does not have the `uuid` module. To work around this,
    # we generate the uuids here (under Python 2.6+) and
    # pass them as arguments
    return [str(uuid.uuid4()) for i in range(MAX_VDI_CHAIN_SIZE)]


def _image_uses_bittorrent(context, instance):
    bittorrent = False
    torrent_images = CONF.xenserver.torrent_images.lower()

    if torrent_images == 'all':
        bittorrent = True
    elif torrent_images == 'some':
        sys_meta = utils.instance_sys_meta(instance)
        try:
            bittorrent = strutils.bool_from_string(
                sys_meta['image_bittorrent'])
        except KeyError:
            pass
    elif torrent_images == 'none':
        pass
    else:
        LOG.warning(_LW("Invalid value '%s' for torrent_images"),
                    torrent_images)

    return bittorrent


def _default_download_handler():
    # TODO(sirp):  This should be configurable like upload_handler
    return importutils.import_object(
            'nova.virt.xenapi.image.glance.GlanceStore')


def _choose_download_handler(context, instance):
    if _image_uses_bittorrent(context, instance):
        return importutils.import_object(
                'nova.virt.xenapi.image.bittorrent.BittorrentStore')
    else:
        return _default_download_handler()


def get_compression_level():
    level = CONF.xenserver.image_compression_level
    if level is not None and (level < 1 or level > 9):
        LOG.warning(_LW("Invalid value '%d' for image_compression_level"),
                    level)
        return None
    return level


def _fetch_vhd_image(context, session, instance, image_id):
    """Tell glance to download an image and put the VHDs into the SR

    Returns: A list of dictionaries that describe VDIs
    """
    LOG.debug("Asking xapi to fetch vhd image %s", image_id,
              instance=instance)

    handler = _choose_download_handler(context, instance)

    try:
        vdis = handler.download_image(context, session, instance, image_id)
    except Exception:
        default_handler = _default_download_handler()

        # Using type() instead of isinstance() so instance of subclass doesn't
        # test as equivalent
        if type(handler) == type(default_handler):
            raise

        LOG.exception(_LE("Download handler '%(handler)s' raised an"
                          " exception, falling back to default handler"
                          " '%(default_handler)s'"),
                      {'handler': handler,
                       'default_handler': default_handler})

        vdis = default_handler.download_image(
                context, session, instance, image_id)

    # Ensure we can see the import VHDs as VDIs
    scan_default_sr(session)

    vdi_uuid = vdis['root']['uuid']
    try:
        _check_vdi_size(context, session, instance, vdi_uuid)
    except Exception:
        with excutils.save_and_reraise_exception():
            msg = "Error while checking vdi size"
            LOG.debug(msg, instance=instance, exc_info=True)
            for vdi in vdis.values():
                vdi_uuid = vdi['uuid']
                vdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid)
                safe_destroy_vdis(session, [vdi_ref])

    return vdis


def _get_vdi_chain_size(session, vdi_uuid):
    """Compute the total size of a VDI chain, starting with the specified
    VDI UUID.

    This will walk the VDI chain to the root, add the size of each VDI into
    the total.
    """
    size_bytes = 0
    for vdi_rec in _walk_vdi_chain(session, vdi_uuid):
        cur_vdi_uuid = vdi_rec['uuid']
        vdi_size_bytes = int(vdi_rec['physical_utilisation'])
        LOG.debug('vdi_uuid=%(cur_vdi_uuid)s vdi_size_bytes='
                  '%(vdi_size_bytes)d',
                  {'cur_vdi_uuid': cur_vdi_uuid,
                   'vdi_size_bytes': vdi_size_bytes})
        size_bytes += vdi_size_bytes
    return size_bytes


def _check_vdi_size(context, session, instance, vdi_uuid):
    flavor = instance.get_flavor()
    allowed_size = (flavor.root_gb +
                    VHD_SIZE_CHECK_FUDGE_FACTOR_GB) * units.Gi
    if not flavor.root_gb:
        # root_gb=0 indicates that we're disabling size checks
        return

    size = _get_vdi_chain_size(session, vdi_uuid)
    if size > allowed_size:
        LOG.error(_LE("Image size %(size)d exceeded flavor "
                      "allowed size %(allowed_size)d"),
                  {'size': size, 'allowed_size': allowed_size},
                  instance=instance)

        raise exception.FlavorDiskSmallerThanImage(
            flavor_size=(flavor.root_gb * units.Gi),
            image_size=(size * units.Gi))


def _fetch_disk_image(context, session, instance, name_label, image_id,
                      image_type):
    """Fetch the image from Glance

    NOTE:
    Unlike _fetch_vhd_image, this method does not use the Glance
    plugin; instead, it streams the disks through domU to the VDI
    directly.

    Returns: A single filename if image_type is KERNEL_RAMDISK
             A list of dictionaries that describe VDIs, otherwise
    """
    # FIXME(sirp): Since the Glance plugin seems to be required for the
    # VHD disk, it may be worth using the plugin for both VHD and RAW and
    # DISK restores
    image_type_str = ImageType.to_string(image_type)
    LOG.debug("Fetching image %(image_id)s, type %(image_type_str)s",
              {'image_id': image_id, 'image_type_str': image_type_str},
              instance=instance)

    if image_type == ImageType.DISK_ISO:
        sr_ref = _safe_find_iso_sr(session)
    else:
        sr_ref = safe_find_sr(session)

    glance_image = image_utils.GlanceImage(context, image_id)
    if glance_image.is_raw_tgz():
        image = image_utils.RawTGZImage(glance_image)
    else:
        image = image_utils.RawImage(glance_image)

    virtual_size = image.get_size()
    vdi_size = virtual_size
    LOG.debug("Size for image %(image_id)s: %(virtual_size)d",
              {'image_id': image_id, 'virtual_size': virtual_size},
              instance=instance)
    if image_type == ImageType.DISK:
        # Make room for MBR.
        vdi_size += MBR_SIZE_BYTES
    elif (image_type in (ImageType.KERNEL, ImageType.RAMDISK) and
          vdi_size > CONF.xenserver.max_kernel_ramdisk_size):
        max_size = CONF.xenserver.max_kernel_ramdisk_size
        raise exception.NovaException(
            _("Kernel/Ramdisk image is too large: %(vdi_size)d bytes, "
              "max %(max_size)d bytes") %
            {'vdi_size': vdi_size, 'max_size': max_size})

    vdi_ref = create_vdi(session, sr_ref, instance, name_label,
                         image_type_str, vdi_size)
    # From this point we have a VDI on Xen host;
    # If anything goes wrong, we need to remember its uuid.
    try:
        filename = None
        vdi_uuid = session.call_xenapi("VDI.get_uuid", vdi_ref)

        with vdi_attached_here(session, vdi_ref, read_only=False) as dev:
            _stream_disk(
                session, image.stream_to, image_type, virtual_size, dev)

        if image_type in (ImageType.KERNEL, ImageType.RAMDISK):
            # We need to invoke a plugin for copying the
            # content of the VDI into the proper path.
            LOG.debug("Copying VDI %s to /boot/guest on dom0",
                      vdi_ref, instance=instance)

            args = {}
            args['vdi-ref'] = vdi_ref

            # Let the plugin copy the correct number of bytes.
            args['image-size'] = str(vdi_size)
            if CONF.xenserver.cache_images:
                args['cached-image'] = image_id
            filename = session.call_plugin('kernel', 'copy_vdi', args)

            # Remove the VDI as it is not needed anymore.
            destroy_vdi(session, vdi_ref)
            LOG.debug("Kernel/Ramdisk VDI %s destroyed", vdi_ref,
                      instance=instance)
            vdi_role = ImageType.get_role(image_type)
            return {vdi_role: dict(uuid=None, file=filename)}
        else:
            vdi_role = ImageType.get_role(image_type)
            return {vdi_role: dict(uuid=vdi_uuid, file=None)}
    except (session.XenAPI.Failure, IOError, OSError) as e:
        # We look for XenAPI and OS failures.
        LOG.exception(_LE("Failed to fetch glance image"),
                      instance=instance)
        e.args = e.args + ([dict(type=ImageType.to_string(image_type),
                                 uuid=vdi_uuid,
                                 file=filename)],)
        raise


def determine_disk_image_type(image_meta):
    """Disk Image Types are used to determine where the kernel will reside
    within an image. To figure out which type we're dealing with, we use
    the following rules:

    1. If we're using Glance, we can use the image_type field to
       determine the image_type

    2. If we're not using Glance, then we need to deduce this based on
       whether a kernel_id is specified.
    """
    if not image_meta.obj_attr_is_set("disk_format"):
        return None

    disk_format_map = {
        'ami': ImageType.DISK,
        'aki': ImageType.KERNEL,
        'ari': ImageType.RAMDISK,
        'raw': ImageType.DISK_RAW,
        'vhd': ImageType.DISK_VHD,
        'iso': ImageType.DISK_ISO,
    }

    try:
        image_type = disk_format_map[image_meta.disk_format]
    except KeyError:
        raise exception.InvalidDiskFormat(disk_format=image_meta.disk_format)

    LOG.debug("Detected %(type)s format for image %(image)s",
              {'type': ImageType.to_string(image_type),
               'image': image_meta})

    return image_type


def determine_vm_mode(instance, disk_image_type):
    current_mode = vm_mode.get_from_instance(instance)
    if current_mode == vm_mode.XEN or current_mode == vm_mode.HVM:
        return current_mode

    os_type = instance['os_type']
    if os_type == "linux":
        return vm_mode.XEN
    if os_type == "windows":
        return vm_mode.HVM

    # disk_image_type specific default for backwards compatibility
    if disk_image_type == ImageType.DISK_VHD or \
            disk_image_type == ImageType.DISK:
        return vm_mode.XEN

    # most images run OK as HVM
    return vm_mode.HVM


def set_vm_name_label(session, vm_ref, name_label):
    session.call_xenapi("VM.set_name_label", vm_ref, name_label)


def list_vms(session):
    vms = session.call_xenapi("VM.get_all_records_where",
                              'field "is_control_domain"="false" and '
                              'field "is_a_template"="false" and '
                              'field "resident_on"="%s"' % session.host_ref)
    for vm_ref in vms.keys():
        yield vm_ref, vms[vm_ref]


def lookup_vm_vdis(session, vm_ref):
    """Look for the VDIs that are attached to the VM."""
    # Firstly we get the VBDs, then the VDIs.
    # TODO(Armando): do we leave the read-only devices?
    vbd_refs = session.call_xenapi("VM.get_VBDs", vm_ref)
    vdi_refs = []
    if vbd_refs:
        for vbd_ref in vbd_refs:
            try:
                vdi_ref = session.call_xenapi("VBD.get_VDI", vbd_ref)
                # Test valid VDI
                vdi_uuid = session.call_xenapi("VDI.get_uuid", vdi_ref)
                LOG.debug('VDI %s is still available', vdi_uuid)
                vbd_other_config = session.call_xenapi("VBD.get_other_config",
                                                       vbd_ref)
                if not vbd_other_config.get('osvol'):
                    # This is not an attached volume
                    vdi_refs.append(vdi_ref)
            except session.XenAPI.Failure:
                LOG.exception(_LE('"Look for the VDIs failed'))
    return vdi_refs


def lookup(session, name_label, check_rescue=False):
    """Look the instance up and return it if available.
    :param:check_rescue: if True will return the 'name'-rescue vm if it
    exists, instead of just 'name'
    """
    if check_rescue:
        result = lookup(session, name_label + '-rescue', False)
        if result:
            return result
    vm_refs = session.call_xenapi("VM.get_by_name_label", name_label)
    n = len(vm_refs)
    if n == 0:
        return None
    elif n > 1:
        raise exception.InstanceExists(name=name_label)
    else:
        return vm_refs[0]


def preconfigure_instance(session, instance, vdi_ref, network_info):
    """Makes alterations to the image before launching as part of spawn.
    """
    key = str(instance['key_data'])
    net = netutils.get_injected_network_template(network_info)
    metadata = instance['metadata']

    # As mounting the image VDI is expensive, we only want do it once,
    # if at all, so determine whether it's required first, and then do
    # everything
    mount_required = key or net or metadata
    if not mount_required:
        return

    with vdi_attached_here(session, vdi_ref, read_only=False) as dev:
        _mounted_processing(dev, key, net, metadata)


def lookup_kernel_ramdisk(session, vm):
    vm_rec = session.call_xenapi("VM.get_record", vm)
    if 'PV_kernel' in vm_rec and 'PV_ramdisk' in vm_rec:
        return (vm_rec['PV_kernel'], vm_rec['PV_ramdisk'])
    else:
        return (None, None)


def is_snapshot(session, vm):
    vm_rec = session.call_xenapi("VM.get_record", vm)
    if 'is_a_template' in vm_rec and 'is_a_snapshot' in vm_rec:
        return vm_rec['is_a_template'] and vm_rec['is_a_snapshot']
    else:
        return False


def get_power_state(session, vm_ref):
    xapi_state = session.call_xenapi("VM.get_power_state", vm_ref)
    return XENAPI_POWER_STATE[xapi_state]


def compile_info(session, vm_ref):
    """Fill record with VM status information."""
    power_state = get_power_state(session, vm_ref)
    max_mem = session.call_xenapi("VM.get_memory_static_max", vm_ref)
    mem = session.call_xenapi("VM.get_memory_dynamic_max", vm_ref)
    num_cpu = session.call_xenapi("VM.get_VCPUs_max", vm_ref)

    return hardware.InstanceInfo(state=power_state,
                                 max_mem_kb=int(max_mem) >> 10,
                                 mem_kb=int(mem) >> 10,
                                 num_cpu=num_cpu)


def compile_instance_diagnostics(instance, vm_rec):
    vm_power_state_int = XENAPI_POWER_STATE[vm_rec['power_state']]
    vm_power_state = power_state.STATE_MAP[vm_power_state_int]
    config_drive = configdrive.required_by(instance)

    diags = diagnostics.Diagnostics(state=vm_power_state,
                                    driver='xenapi',
                                    config_drive=config_drive)

    for cpu_num in range(0, int(vm_rec['VCPUs_max'])):
        diags.add_cpu()

    for vif in vm_rec['VIFs']:
        diags.add_nic()

    for vbd in vm_rec['VBDs']:
        diags.add_disk()

    max_mem_bytes = int(vm_rec['memory_dynamic_max'])
    diags.memory_details.maximum = max_mem_bytes / units.Mi

    return diags


def compile_diagnostics(vm_rec):
    """Compile VM diagnostics data."""
    try:
        keys = []
        diags = {}
        vm_uuid = vm_rec["uuid"]
        xml = _get_rrd(_get_rrd_server(), vm_uuid)
        if xml:
            rrd = minidom.parseString(xml)
            for i, node in enumerate(rrd.firstChild.childNodes):
                # Provide the last update of the information
                if node.localName == 'lastupdate':
                    diags['last_update'] = node.firstChild.data

                # Create a list of the diagnostic keys (in their order)
                if node.localName == 'ds':
                    ref = node.childNodes
                    # Name and Value
                    if len(ref) > 6:
                        keys.append(ref[0].firstChild.data)

                # Read the last row of the first RRA to get the latest info
                if node.localName == 'rra':
                    rows = node.childNodes[4].childNodes
                    last_row = rows[rows.length - 1].childNodes
                    for j, value in enumerate(last_row):
                        diags[keys[j]] = value.firstChild.data
                    break

        return diags
    except expat.ExpatError as e:
        LOG.exception(_LE('Unable to parse rrd of %s'), e)
        return {"Unable to retrieve diagnostics": e}


def fetch_bandwidth(session):
    bw = session.call_plugin_serialized('bandwidth', 'fetch_all_bandwidth')
    return bw


def _scan_sr(session, sr_ref=None, max_attempts=4):
    if sr_ref:
        # NOTE(johngarbutt) xenapi will collapse any duplicate requests
        # for SR.scan if there is already a scan in progress.
        # However, we don't want that, because the scan may have started
        # before we modified the underlying VHDs on disk through a plugin.
        # Using our own mutex will reduce cases where our periodic SR scan
        # in host.update_status starts racing the sr.scan after a plugin call.
        @utils.synchronized('sr-scan-' + sr_ref)
        def do_scan(sr_ref):
            LOG.debug("Scanning SR %s", sr_ref)

            attempt = 1
            while True:
                try:
                    return session.call_xenapi('SR.scan', sr_ref)
                except session.XenAPI.Failure as exc:
                    with excutils.save_and_reraise_exception() as ctxt:
                        if exc.details[0] == 'SR_BACKEND_FAILURE_40':
                            if attempt < max_attempts:
                                ctxt.reraise = False
                                LOG.warning(_LW("Retry SR scan due to error: "
                                                "%s"), exc)
                                greenthread.sleep(2 ** attempt)
                                attempt += 1
        do_scan(sr_ref)


def scan_default_sr(session):
    """Looks for the system default SR and triggers a re-scan."""
    sr_ref = safe_find_sr(session)
    _scan_sr(session, sr_ref)
    return sr_ref


def safe_find_sr(session):
    """Same as _find_sr except raises a NotFound exception if SR cannot be
    determined
    """
    sr_ref = _find_sr(session)
    if sr_ref is None:
        raise exception.StorageRepositoryNotFound()
    return sr_ref


def _find_sr(session):
    """Return the storage repository to hold VM images."""
    host = session.host_ref
    try:
        tokens = CONF.xenserver.sr_matching_filter.split(':')
        filter_criteria = tokens[0]
        filter_pattern = tokens[1]
    except IndexError:
        # oops, flag is invalid
        LOG.warning(_LW("Flag sr_matching_filter '%s' does not respect "
                        "formatting convention"),
                    CONF.xenserver.sr_matching_filter)
        return None

    if filter_criteria == 'other-config':
        key, value = filter_pattern.split('=', 1)
        for sr_ref, sr_rec in session.get_all_refs_and_recs('SR'):
            if not (key in sr_rec['other_config'] and
                    sr_rec['other_config'][key] == value):
                continue
            for pbd_ref in sr_rec['PBDs']:
                pbd_rec = session.get_rec('PBD', pbd_ref)
                if pbd_rec and pbd_rec['host'] == host:
                    return sr_ref
    elif filter_criteria == 'default-sr' and filter_pattern == 'true':
        pool_ref = session.call_xenapi('pool.get_all')[0]
        sr_ref = session.call_xenapi('pool.get_default_SR', pool_ref)
        if sr_ref:
            return sr_ref
    # No SR found!
    LOG.error(_LE("XenAPI is unable to find a Storage Repository to "
                  "install guest instances on. Please check your "
                  "configuration (e.g. set a default SR for the pool) "
                  "and/or configure the flag 'sr_matching_filter'."))
    return None


def _safe_find_iso_sr(session):
    """Same as _find_iso_sr except raises a NotFound exception if SR
    cannot be determined
    """
    sr_ref = _find_iso_sr(session)
    if sr_ref is None:
        raise exception.NotFound(_('Cannot find SR of content-type ISO'))
    return sr_ref


def _find_iso_sr(session):
    """Return the storage repository to hold ISO images."""
    host = session.host_ref
    for sr_ref, sr_rec in session.get_all_refs_and_recs('SR'):
        LOG.debug("ISO: looking at SR %s", sr_rec)
        if not sr_rec['content_type'] == 'iso':
            LOG.debug("ISO: not iso content")
            continue
        if 'i18n-key' not in sr_rec['other_config']:
            LOG.debug("ISO: iso content_type, no 'i18n-key' key")
            continue
        if not sr_rec['other_config']['i18n-key'] == 'local-storage-iso':
            LOG.debug("ISO: iso content_type, i18n-key value not "
                      "'local-storage-iso'")
            continue

        LOG.debug("ISO: SR MATCHing our criteria")
        for pbd_ref in sr_rec['PBDs']:
            LOG.debug("ISO: ISO, looking to see if it is host local")
            pbd_rec = session.get_rec('PBD', pbd_ref)
            if not pbd_rec:
                LOG.debug("ISO: PBD %s disappeared", pbd_ref)
                continue
            pbd_rec_host = pbd_rec['host']
            LOG.debug("ISO: PBD matching, want %(pbd_rec)s, have %(host)s",
                      {'pbd_rec': pbd_rec, 'host': host})
            if pbd_rec_host == host:
                LOG.debug("ISO: SR with local PBD")
                return sr_ref
    return None


def _get_rrd_server():
    """Return server's scheme and address to use for retrieving RRD XMLs."""
    xs_url = urlparse.urlparse(CONF.xenserver.connection_url)
    return [xs_url.scheme, xs_url.netloc]


def _get_rrd(server, vm_uuid):
    """Return the VM RRD XML as a string."""
    try:
        xml = urllib.urlopen("%s://%s:%s@%s/vm_rrd?uuid=%s" % (
            server[0],
            CONF.xenserver.connection_username,
            CONF.xenserver.connection_password,
            server[1],
            vm_uuid))
        return xml.read()
    except IOError:
        LOG.exception(_LE('Unable to obtain RRD XML for VM %(vm_uuid)s with '
                          'server details: %(server)s.'),
                      {'vm_uuid': vm_uuid, 'server': server})
        return None


def _get_all_vdis_in_sr(session, sr_ref):
    for vdi_ref in session.call_xenapi('SR.get_VDIs', sr_ref):
        vdi_rec = session.get_rec('VDI', vdi_ref)
        # Check to make sure the record still exists. It may have
        # been deleted between the get_all call and get_rec call
        if vdi_rec:
            yield vdi_ref, vdi_rec


def get_instance_vdis_for_sr(session, vm_ref, sr_ref):
    """Return opaqueRef for all the vdis which live on sr."""
    for vbd_ref in session.call_xenapi('VM.get_VBDs', vm_ref):
        try:
            vdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref)
            if sr_ref == session.call_xenapi('VDI.get_SR', vdi_ref):
                yield vdi_ref
        except session.XenAPI.Failure:
            continue


def _get_vhd_parent_uuid(session, vdi_ref, vdi_rec=None):
    if vdi_rec is None:
        vdi_rec = session.call_xenapi("VDI.get_record", vdi_ref)

    if 'vhd-parent' not in vdi_rec['sm_config']:
        return None

    parent_uuid = vdi_rec['sm_config']['vhd-parent']
    vdi_uuid = vdi_rec['uuid']
    LOG.debug('VHD %(vdi_uuid)s has parent %(parent_uuid)s',
              {'vdi_uuid': vdi_uuid, 'parent_uuid': parent_uuid})
    return parent_uuid


def _walk_vdi_chain(session, vdi_uuid):
    """Yield vdi_recs for each element in a VDI chain."""
    scan_default_sr(session)
    while True:
        vdi_ref = session.call_xenapi("VDI.get_by_uuid", vdi_uuid)
        vdi_rec = session.call_xenapi("VDI.get_record", vdi_ref)
        yield vdi_rec

        parent_uuid = _get_vhd_parent_uuid(session, vdi_ref, vdi_rec)
        if not parent_uuid:
            break

        vdi_uuid = parent_uuid


def _is_vdi_a_snapshot(vdi_rec):
    """Ensure VDI is a snapshot, and not cached image."""
    is_a_snapshot = vdi_rec['is_a_snapshot']
    image_id = vdi_rec['other_config'].get('image-id')
    return is_a_snapshot and not image_id


def _child_vhds(session, sr_ref, vdi_uuid_list, old_snapshots_only=False):
    """Return the immediate children of a given VHD.

    This is not recursive, only the immediate children are returned.
    """
    children = set()
    for ref, rec in _get_all_vdis_in_sr(session, sr_ref):
        rec_uuid = rec['uuid']

        if rec_uuid in vdi_uuid_list:
            continue

        parent_uuid = _get_vhd_parent_uuid(session, ref, rec)
        if parent_uuid not in vdi_uuid_list:
            continue

        if old_snapshots_only and not _is_vdi_a_snapshot(rec):
            continue

        children.add(rec_uuid)

    return list(children)


def _count_children(session, parent_vdi_uuid, sr_ref):
    # Search for any other vdi which has the same parent as us to work out
    # whether we have siblings and therefore if coalesce is possible
    children = 0
    for _ref, rec in _get_all_vdis_in_sr(session, sr_ref):
        if (rec['sm_config'].get('vhd-parent') == parent_vdi_uuid):
            children = children + 1
    return children


def _wait_for_vhd_coalesce(session, instance, sr_ref, vdi_ref,
                           vdi_uuid_list):
    """Spin until the parent VHD is coalesced into one of the VDIs in the list

    vdi_uuid_list is a list of acceptable final parent VDIs for vdi_ref; once
    the parent of vdi_ref is in vdi_uuid_chain we consider the coalesce over.

    The use case is there are any number of VDIs between those in
    vdi_uuid_list and vdi_ref that we expect to be coalesced, but any of those
    in vdi_uuid_list may also be coalesced (except the base UUID - which is
    guaranteed to remain)
    """
    # If the base disk was a leaf node, there will be no coalescing
    # after a VDI snapshot.
    if len(vdi_uuid_list) == 1:
        LOG.debug("Old chain is single VHD, coalesce not possible.",
                  instance=instance)
        return

    # If the parent of the original disk has other children,
    # there will be no coalesce because of the VDI snapshot.
    # For example, the first snapshot for an instance that has been
    # spawned from a cached image, will not coalesce, because of this rule.
    parent_vdi_uuid = vdi_uuid_list[1]
    if _count_children(session, parent_vdi_uuid, sr_ref) > 1:
        LOG.debug("Parent has other children, coalesce is unlikely.",
                  instance=instance)
        return

    # When the VDI snapshot is taken, a new parent is created.
    # Assuming it is not one of the above cases, that new parent
    # can be coalesced, so we need to wait for that to happen.
    max_attempts = CONF.xenserver.vhd_coalesce_max_attempts
    # Remove the leaf node from list, to get possible good parents
    # when the coalesce has completed.
    # Its possible that other coalesce operation happen, so we need
    # to consider the full chain, rather than just the most recent parent.
    good_parent_uuids = vdi_uuid_list[1:]
    for i in range(max_attempts):
        # NOTE(sirp): This rescan is necessary to ensure the VM's `sm_config`
        # matches the underlying VHDs.
        # This can also kick XenServer into performing a pending coalesce.
        _scan_sr(session, sr_ref)
        parent_uuid = _get_vhd_parent_uuid(session, vdi_ref)
        if parent_uuid and (parent_uuid not in good_parent_uuids):
            LOG.debug("Parent %(parent_uuid)s not yet in parent list"
                      " %(good_parent_uuids)s, waiting for coalesce...",
                      {'parent_uuid': parent_uuid,
                       'good_parent_uuids': good_parent_uuids},
                      instance=instance)
        else:
            LOG.debug("Coalesce detected, because parent is: %s", parent_uuid,
                      instance=instance)
            return

        greenthread.sleep(CONF.xenserver.vhd_coalesce_poll_interval)

    msg = (_("VHD coalesce attempts exceeded (%d)"
             ", giving up...") % max_attempts)
    raise exception.NovaException(msg)


def _remap_vbd_dev(dev):
    """Return the appropriate location for a plugged-in VBD device

    Ubuntu Maverick moved xvd? -> sd?. This is considered a bug and will be
    fixed in future versions:
        https://bugs.launchpad.net/ubuntu/+source/linux/+bug/684875

    For now, we work around it by just doing a string replace.
    """
    # NOTE(sirp): This hack can go away when we pull support for Maverick
    should_remap = CONF.xenserver.remap_vbd_dev
    if not should_remap:
        return dev

    old_prefix = 'xvd'
    new_prefix = CONF.xenserver.remap_vbd_dev_prefix
    remapped_dev = dev.replace(old_prefix, new_prefix)

    return remapped_dev


def _wait_for_device(dev):
    """Wait for device node to appear."""
    for i in range(0, CONF.xenserver.block_device_creation_timeout):
        dev_path = utils.make_dev_path(dev)
        if os.path.exists(dev_path):
            return
        time.sleep(1)

    raise exception.StorageError(
        reason=_('Timeout waiting for device %s to be created') % dev)


def cleanup_attached_vdis(session):
    """Unplug any instance VDIs left after an unclean restart."""
    this_vm_ref = _get_this_vm_ref(session)

    vbd_refs = session.call_xenapi('VM.get_VBDs', this_vm_ref)
    for vbd_ref in vbd_refs:
        try:
            vdi_ref = session.call_xenapi('VBD.get_VDI', vbd_ref)
            vdi_rec = session.call_xenapi('VDI.get_record', vdi_ref)
        except session.XenAPI.Failure as e:
            if e.details[0] != 'HANDLE_INVALID':
                raise
            continue

        if 'nova_instance_uuid' in vdi_rec['other_config']:
            # Belongs to an instance and probably left over after an
            # unclean restart
            LOG.info(_LI('Disconnecting stale VDI %s from compute domU'),
                     vdi_rec['uuid'])
            unplug_vbd(session, vbd_ref, this_vm_ref)
            destroy_vbd(session, vbd_ref)


@contextlib.contextmanager
def vdi_attached_here(session, vdi_ref, read_only=False):
    this_vm_ref = _get_this_vm_ref(session)

    vbd_ref = create_vbd(session, this_vm_ref, vdi_ref, 'autodetect',
                         read_only=read_only, bootable=False)
    try:
        LOG.debug('Plugging VBD %s ... ', vbd_ref)
        session.VBD.plug(vbd_ref, this_vm_ref)
        try:
            LOG.debug('Plugging VBD %s done.', vbd_ref)
            orig_dev = session.call_xenapi("VBD.get_device", vbd_ref)
            LOG.debug('VBD %(vbd_ref)s plugged as %(orig_dev)s',
                      {'vbd_ref': vbd_ref, 'orig_dev': orig_dev})
            dev = _remap_vbd_dev(orig_dev)
            if dev != orig_dev:
                LOG.debug('VBD %(vbd_ref)s plugged into wrong dev, '
                          'remapping to %(dev)s',
                          {'vbd_ref': vbd_ref, 'dev': dev})
            _wait_for_device(dev)
            yield dev
        finally:
            utils.execute('sync', run_as_root=True)
            LOG.debug('Destroying VBD for VDI %s ... ', vdi_ref)
            unplug_vbd(session, vbd_ref, this_vm_ref)
    finally:
        try:
            destroy_vbd(session, vbd_ref)
        except exception.StorageError:
            # destroy_vbd() will log error
            pass
        LOG.debug('Destroying VBD for VDI %s done.', vdi_ref)


def _get_sys_hypervisor_uuid():
    with open('/sys/hypervisor/uuid') as f:
        return f.readline().strip()


def get_this_vm_uuid(session):
    if session and session.is_local_connection:
        # UUID is the control domain running on this host
        vms = session.call_xenapi("VM.get_all_records_where",
                                  'field "is_control_domain"="true" and '
                                  'field "resident_on"="%s"' %
                                  session.host_ref)
        return vms[list(vms.keys())[0]]['uuid']
    try:
        return _get_sys_hypervisor_uuid()
    except IOError:
        # Some guest kernels (without 5c13f8067745efc15f6ad0158b58d57c44104c25)
        # cannot read from uuid after a reboot.  Fall back to trying xenstore.
        # See https://bugs.launchpad.net/ubuntu/+source/xen-api/+bug/1081182
        domid, _ = utils.execute('xenstore-read', 'domid', run_as_root=True)
        vm_key, _ = utils.execute('xenstore-read',
                                 '/local/domain/%s/vm' % domid.strip(),
                                 run_as_root=True)
        return vm_key.strip()[4:]


def _get_this_vm_ref(session):
    return session.call_xenapi("VM.get_by_uuid", get_this_vm_uuid(session))


def _get_partitions(dev):
    """Return partition information (num, size, type) for a device."""
    dev_path = utils.make_dev_path(dev)
    out, _err = utils.execute('parted', '--script', '--machine',
                             dev_path, 'unit s', 'print',
                             run_as_root=True)
    lines = [line for line in out.split('\n') if line]
    partitions = []

    LOG.debug("Partitions:")
    for line in lines[2:]:
        line = line.rstrip(';')
        num, start, end, size, fstype, name, flags = line.split(':')
        num = int(num)
        start = int(start.rstrip('s'))
        end = int(end.rstrip('s'))
        size = int(size.rstrip('s'))
        LOG.debug("  %(num)s: %(fstype)s %(size)d sectors",
                  {'num': num, 'fstype': fstype, 'size': size})
        partitions.append((num, start, size, fstype, name, flags))

    return partitions


def _stream_disk(session, image_service_func, image_type, virtual_size, dev):
    offset = 0
    if image_type == ImageType.DISK:
        offset = MBR_SIZE_BYTES
        _write_partition(session, virtual_size, dev)

    dev_path = utils.make_dev_path(dev)

    with utils.temporary_chown(dev_path):
        with open(dev_path, 'wb') as f:
            f.seek(offset)
            image_service_func(f)


def _write_partition(session, virtual_size, dev):
    dev_path = utils.make_dev_path(dev)
    primary_first = MBR_SIZE_SECTORS
    primary_last = MBR_SIZE_SECTORS + (virtual_size / SECTOR_SIZE) - 1

    LOG.debug('Writing partition table %(primary_first)d %(primary_last)d'
              ' to %(dev_path)s...',
              {'primary_first': primary_first, 'primary_last': primary_last,
               'dev_path': dev_path})

    _make_partition(session, dev, "%ds" % primary_first, "%ds" % primary_last)
    LOG.debug('Writing partition table %s done.', dev_path)


def _repair_filesystem(partition_path):
    # Exit Code 1 = File system errors corrected
    #           2 = File system errors corrected, system needs a reboot
    utils.execute('e2fsck', '-f', '-y', partition_path, run_as_root=True,
        check_exit_code=[0, 1, 2])


def _resize_part_and_fs(dev, start, old_sectors, new_sectors, flags):
    """Resize partition and fileystem.

    This assumes we are dealing with a single primary partition and using
    ext3 or ext4.
    """
    size = new_sectors - start
    end = new_sectors - 1

    dev_path = utils.make_dev_path(dev)
    partition_path = utils.make_dev_path(dev, partition=1)

    # Replay journal if FS wasn't cleanly unmounted
    _repair_filesystem(partition_path)

    # Remove ext3 journal (making it ext2)
    utils.execute('tune2fs', '-O ^has_journal', partition_path,
                  run_as_root=True)

    if new_sectors < old_sectors:
        # Resizing down, resize filesystem before partition resize
        try:
            utils.execute('resize2fs', partition_path, '%ds' % size,
                          run_as_root=True)
        except processutils.ProcessExecutionError as exc:
            LOG.error(six.text_type(exc))
            reason = _("Shrinking the filesystem down with resize2fs "
                       "has failed, please check if you have "
                       "enough free space on your disk.")
            raise exception.ResizeError(reason=reason)

    utils.execute('parted', '--script', dev_path, 'rm', '1',
                  run_as_root=True)
    utils.execute('parted', '--script', dev_path, 'mkpart',
                  'primary',
                  '%ds' % start,
                  '%ds' % end,
                  run_as_root=True)
    if "boot" in flags.lower():
        utils.execute('parted', '--script', dev_path,
                      'set', '1', 'boot', 'on',
                      run_as_root=True)

    if new_sectors > old_sectors:
        # Resizing up, resize filesystem after partition resize
        utils.execute('resize2fs', partition_path, run_as_root=True)

    # Add back journal
    utils.execute('tune2fs', '-j', partition_path, run_as_root=True)


def _log_progress_if_required(left, last_log_time, virtual_size):
    if timeutils.is_older_than(last_log_time, PROGRESS_INTERVAL_SECONDS):
        last_log_time = timeutils.utcnow()
        complete_pct = float(virtual_size - left) / virtual_size * 100
        LOG.debug("Sparse copy in progress, "
                  "%(complete_pct).2f%% complete. "
                  "%(left)s bytes left to copy",
            {"complete_pct": complete_pct, "left": left})
    return last_log_time


def _sparse_copy(src_path, dst_path, virtual_size, block_size=4096):
    """Copy data, skipping long runs of zeros to create a sparse file."""
    start_time = last_log_time = timeutils.utcnow()
    EMPTY_BLOCK = '\0' * block_size
    bytes_read = 0
    skipped_bytes = 0
    left = virtual_size

    LOG.debug("Starting sparse_copy src=%(src_path)s dst=%(dst_path)s "
              "virtual_size=%(virtual_size)d block_size=%(block_size)d",
              {'src_path': src_path, 'dst_path': dst_path,
               'virtual_size': virtual_size, 'block_size': block_size})

    # NOTE(sirp): we need read/write access to the devices; since we don't have
    # the luxury of shelling out to a sudo'd command, we temporarily take
    # ownership of the devices.
    with utils.temporary_chown(src_path):
        with utils.temporary_chown(dst_path):
            with open(src_path, "r") as src:
                with open(dst_path, "w") as dst:
                    data = src.read(min(block_size, left))
                    while data:
                        if data == EMPTY_BLOCK:
                            dst.seek(block_size, os.SEEK_CUR)
                            left -= block_size
                            bytes_read += block_size
                            skipped_bytes += block_size
                        else:
                            dst.write(data)
                            data_len = len(data)
                            left -= data_len
                            bytes_read += data_len

                        if left <= 0:
                            break

                        data = src.read(min(block_size, left))
                        greenthread.sleep(0)
                        last_log_time = _log_progress_if_required(
                            left, last_log_time, virtual_size)

    duration = timeutils.delta_seconds(start_time, timeutils.utcnow())
    compression_pct = float(skipped_bytes) / bytes_read * 100

    LOG.debug("Finished sparse_copy in %(duration).2f secs, "
              "%(compression_pct).2f%% reduction in size",
              {'duration': duration, 'compression_pct': compression_pct})


def _copy_partition(session, src_ref, dst_ref, partition, virtual_size):
    # Part of disk taken up by MBR
    virtual_size -= MBR_SIZE_BYTES

    with vdi_attached_here(session, src_ref, read_only=True) as src:
        src_path = utils.make_dev_path(src, partition=partition)

        with vdi_attached_here(session, dst_ref, read_only=False) as dst:
            dst_path = utils.make_dev_path(dst, partition=partition)

            _write_partition(session, virtual_size, dst)

            if CONF.xenserver.sparse_copy:
                _sparse_copy(src_path, dst_path, virtual_size)
            else:
                num_blocks = virtual_size / SECTOR_SIZE
                utils.execute('dd',
                              'if=%s' % src_path,
                              'of=%s' % dst_path,
                              'count=%d' % num_blocks,
                              'iflag=direct,sync',
                              'oflag=direct,sync',
                              run_as_root=True)


def _mount_filesystem(dev_path, dir):
    """mounts the device specified by dev_path in dir."""
    try:
        _out, err = utils.execute('mount',
                                 '-t', 'ext2,ext3,ext4,reiserfs',
                                 dev_path, dir, run_as_root=True)
    except processutils.ProcessExecutionError as e:
        err = six.text_type(e)
    return err


def _mounted_processing(device, key, net, metadata):
    """Callback which runs with the image VDI attached."""
    # NB: Partition 1 hardcoded
    dev_path = utils.make_dev_path(device, partition=1)
    with utils.tempdir() as tmpdir:
        # Mount only Linux filesystems, to avoid disturbing NTFS images
        err = _mount_filesystem(dev_path, tmpdir)
        if not err:
            try:
                # This try block ensures that the umount occurs
                if not agent.find_guest_agent(tmpdir):
                    # TODO(berrange) passing in a None filename is
                    # rather dubious. We shouldn't be re-implementing
                    # the mount/unmount logic here either, when the
                    # VFSLocalFS impl has direct support for mount
                    # and unmount handling if it were passed a
                    # non-None filename
                    vfs = vfsimpl.VFSLocalFS(
                        imgmodel.LocalFileImage(None, imgmodel.FORMAT_RAW),
                        imgdir=tmpdir)
                    LOG.info(_LI('Manipulating interface files directly'))
                    # for xenapi, we don't 'inject' admin_password here,
                    # it's handled at instance startup time, nor do we
                    # support injecting arbitrary files here.
                    disk.inject_data_into_fs(vfs,
                                             key, net, metadata, None, None)
            finally:
                utils.execute('umount', dev_path, run_as_root=True)
        else:
            LOG.info(_LI('Failed to mount filesystem (expected for '
                         'non-linux instances): %s'), err)


def ensure_correct_host(session):
    """Ensure we're connected to the host we're running on. This is the
    required configuration for anything that uses vdi_attached_here.
    """
    this_vm_uuid = get_this_vm_uuid(session)

    try:
        session.call_xenapi('VM.get_by_uuid', this_vm_uuid)
    except session.XenAPI.Failure as exc:
        if exc.details[0] != 'UUID_INVALID':
            raise
        raise Exception(_('This domU must be running on the host '
                          'specified by connection_url'))


def import_all_migrated_disks(session, instance, import_root=True):
    root_vdi = None
    if import_root:
        root_vdi = _import_migrated_root_disk(session, instance)
    eph_vdis = _import_migrate_ephemeral_disks(session, instance)
    return {'root': root_vdi, 'ephemerals': eph_vdis}


def _import_migrated_root_disk(session, instance):
    chain_label = instance['uuid']
    vdi_label = instance['name']
    return _import_migrated_vhds(session, instance, chain_label, "root",
                                 vdi_label)


def _import_migrate_ephemeral_disks(session, instance):
    ephemeral_vdis = {}
    instance_uuid = instance['uuid']
    ephemeral_gb = instance.old_flavor.ephemeral_gb
    disk_sizes = get_ephemeral_disk_sizes(ephemeral_gb)
    for chain_number, _size in enumerate(disk_sizes, start=1):
        chain_label = instance_uuid + "_ephemeral_%d" % chain_number
        vdi_label = "%(name)s ephemeral (%(number)d)" % dict(
                        name=instance['name'], number=chain_number)
        ephemeral_vdi = _import_migrated_vhds(session, instance,
                                              chain_label, "ephemeral",
                                              vdi_label)
        userdevice = 3 + chain_number
        ephemeral_vdis[str(userdevice)] = ephemeral_vdi
    return ephemeral_vdis


def _import_migrated_vhds(session, instance, chain_label, disk_type,
                          vdi_label):
    """Move and possibly link VHDs via the XAPI plugin."""
    # TODO(johngarbutt) tidy up plugin params
    imported_vhds = session.call_plugin_serialized(
            'migration', 'move_vhds_into_sr', instance_uuid=chain_label,
            sr_path=get_sr_path(session), uuid_stack=_make_uuid_stack())

    # Now we rescan the SR so we find the VHDs
    scan_default_sr(session)

    vdi_uuid = imported_vhds['root']['uuid']
    vdi_ref = session.call_xenapi('VDI.get_by_uuid', vdi_uuid)

    # Set name-label so we can find if we need to clean up a failed migration
    _set_vdi_info(session, vdi_ref, disk_type, vdi_label,
                  disk_type, instance)

    return {'uuid': vdi_uuid, 'ref': vdi_ref}


def migrate_vhd(session, instance, vdi_uuid, dest, sr_path, seq_num,
                ephemeral_number=0):
    LOG.debug("Migrating VHD '%(vdi_uuid)s' with seq_num %(seq_num)d",
              {'vdi_uuid': vdi_uuid, 'seq_num': seq_num},
              instance=instance)
    chain_label = instance['uuid']
    if ephemeral_number:
        chain_label = instance['uuid'] + "_ephemeral_%d" % ephemeral_number
    try:
        # TODO(johngarbutt) tidy up plugin params
        session.call_plugin_serialized('migration', 'transfer_vhd',
                instance_uuid=chain_label, host=dest, vdi_uuid=vdi_uuid,
                sr_path=sr_path, seq_num=seq_num)
    except session.XenAPI.Failure:
        msg = "Failed to transfer vhd to new host"
        LOG.debug(msg, instance=instance, exc_info=True)
        raise exception.MigrationError(reason=msg)


def vm_ref_or_raise(session, instance_name):
    vm_ref = lookup(session, instance_name)
    if vm_ref is None:
        raise exception.InstanceNotFound(instance_id=instance_name)
    return vm_ref


def handle_ipxe_iso(session, instance, cd_vdi, network_info):
    """iPXE ISOs are a mechanism to allow the customer to roll their own
    image.

    To use this feature, a service provider needs to configure the
    appropriate Nova flags, roll an iPXE ISO, then distribute that image
    to customers via Glance.

    NOTE: `mkisofs` is not present by default in the Dom0, so the service
    provider can either add that package manually to Dom0 or include the
    `mkisofs` binary in the image itself.
    """
    boot_menu_url = CONF.xenserver.ipxe_boot_menu_url
    if not boot_menu_url:
        LOG.warning(_LW('ipxe_boot_menu_url not set, user will have to'
                        ' enter URL manually...'), instance=instance)
        return

    network_name = CONF.xenserver.ipxe_network_name
    if not network_name:
        LOG.warning(_LW('ipxe_network_name not set, user will have to'
                        ' enter IP manually...'), instance=instance)
        return

    network = None
    for vif in network_info:
        if vif['network']['label'] == network_name:
            network = vif['network']
            break

    if not network:
        LOG.warning(_LW("Unable to find network matching '%(network_name)s', "
                        "user will have to enter IP manually..."),
                    {'network_name': network_name}, instance=instance)
        return

    sr_path = get_sr_path(session)

    # Unpack IPv4 network info
    subnet = [sn for sn in network['subnets']
              if sn['version'] == 4][0]
    ip = subnet['ips'][0]

    ip_address = ip['address']
    netmask = network_model.get_netmask(ip, subnet)
    gateway = subnet['gateway']['address']
    dns = subnet['dns'][0]['address']

    try:
        session.call_plugin_serialized("ipxe", "inject", sr_path,
                cd_vdi['uuid'], boot_menu_url, ip_address, netmask,
                gateway, dns, CONF.xenserver.ipxe_mkisofs_cmd)
    except session.XenAPI.Failure as exc:
        _type, _method, error = exc.details[:3]
        if error == 'CommandNotFound':
            LOG.warning(_LW("ISO creation tool '%s' does not exist."),
                        CONF.xenserver.ipxe_mkisofs_cmd, instance=instance)
        else:
            raise


def set_other_config_pci(session, vm_ref, params):
    """Set the pci key of other-config parameter to params."""
    other_config = session.call_xenapi("VM.get_other_config", vm_ref)
    other_config['pci'] = params
    session.call_xenapi("VM.set_other_config", vm_ref, other_config)

######################################################################
#
# File: b2/part.py
#
# Copyright 2016 Backblaze Inc. All Rights Reserved.
#
# License https://www.backblaze.com/using_b2_code.html
#
######################################################################


class PartFactory(object):
    @classmethod
    def from_list_parts_dict(cls, part_dict):
        return Part(
            part_dict['fileId'], part_dict['partNumber'], part_dict['contentLength'],
            part_dict['contentSha1']
        )


class Part(object):
    def __init__(self, file_id, part_number, content_length, content_sha1):
        self.file_id = file_id
        self.part_number = part_number
        self.content_length = content_length
        self.content_sha1 = content_sha1

    def __repr__(self):
        return '<%s %s %s %s %s>' % (
            self.__class__.__name__, self.file_id, self.part_number, self.content_length,
            self.content_sha1
        )

    def __eq__(self, other):
        return isinstance(other, self.__class__) and self.__dict__ == other.__dict__

    def __ne__(self, other):
        return not (self == other)

"""
WSGI config for phantom_pdf_poc project.

It exposes the WSGI callable as a module-level variable named ``application``.

For more information on this file, see
https://docs.djangoproject.com/en/1.6/howto/deployment/wsgi/
"""

import os
os.environ.setdefault("DJANGO_SETTINGS_MODULE", "phantom_pdf_poc.settings")

from django.core.wsgi import get_wsgi_application
application = get_wsgi_application()

# -*- coding: utf-8 -*-
"""
    kaylee.manager.command
    ~~~~~~~~~~~~~~~~~~~~~~

    The module contains base Kaylee manager commands.

    :copyright: (c) 2013 by Zaur Nasibov.
    :license: MIT, see LICENSE for more details.
"""
import argparse


class BaseCommand(object):
    #: Command help text
    name = ''
    help = ''
    args = {}

    @classmethod
    def add_sub_parser(cls, sub_parsers_object):
        if cls.name.strip() == '':
            raise ValueError('{}.name is empty.'.format(cls.__name__))

        parser = sub_parsers_object.add_parser(cls.name, help=cls.help)
        for arg, arg_opts in cls.args.items():
            if isinstance(arg, str):
                arg = [arg, ]
            parser.add_argument(*arg, **arg_opts)
        parser.set_defaults(handler=cls.execute)

    @staticmethod
    def execute(ns):
        raise NotImplementedError('The command has no execute() static method.')


class AdminCommand(BaseCommand):
    #pylint: disable-msg=W0223
    #W0223: Method 'execute' is abstract in class 'BaseCommand'
    #       but is not overridden (throws NotImplementedError).
    pass


class LocalCommand(BaseCommand):
    #pylint: disable-msg=W0223
    #W0223: Method 'execute' is abstract in class 'BaseCommand'
    #       but is not overridden (throws NotImplementedError).
    pass


class CommandsManager(object):
    help = ''
    command_class = BaseCommand

    def __init__(self):
        self.parser = argparse.ArgumentParser(description=self.help)
        self.sub_parsers = self.parser.add_subparsers(
            help='Commands.')
        # add sub-commands
        from .commands import commands_classes
        for cmd_cls in commands_classes:
            if issubclass(cmd_cls, self.command_class):
                self.add_command(cmd_cls)

    def add_command(self, cmd_cls):
        cmd_cls.add_sub_parser(self.sub_parsers)

    def parse(self, argv=None):
        ns = self.parser.parse_args(argv)
        if 'handler' in ns:
            ns.handler(ns)

    @classmethod
    def execute_from_command_line(cls):
        try:
            cls().parse()
        except Exception as e:
            print(e)
            raise SystemExit(1)


class AdminCommandsManager(CommandsManager):
    help = 'Kaylee admin'
    command_class = AdminCommand


class LocalCommandsManager(CommandsManager):
    help = 'Kaylee local environment manager'
    command_class = LocalCommand

#pylint: disable-msg=F0401
#F0401:  Unable to import
###
import THIS_MODULE_DOES_NOT_EXIST_AND_IT_IS_FOR_IMPORT_ERROR_TEST

"""This is the Bcfg2 support for OpenSolaris packages."""

import pkg.client.image as image
import pkg.client.progress as progress

import Bcfg2.Client.Tools


class IPS(Bcfg2.Client.Tools.PkgTool):
    """The IPS driver implements OpenSolaris package operations."""
    name = 'IPS'
    pkgtype = 'ips'
    conflicts = ['SYSV']
    __handles__ = [('Package', 'ips')]
    __req__ = {'Package': ['name', 'version']}
    pkgtool = ('pkg install --no-refresh %s', ('%s', ['name']))

    def __init__(self, config):
        self.installed = {}
        self.pending_upgrades = set()
        self.image = image.Image()
        self.image.find_root('/', False)
        self.image.load_config()
        Bcfg2.Client.Tools.PkgTool.__init__(self, config)

    def RefreshPackages(self):
        self.installed = dict()
        self.image.history.operation_name = "list"
        self.image.load_catalogs(progress.NullProgressTracker())
        for (pfmri, pinfo) in self.image.inventory([], False):
            pname = pfmri.pkg_name
            pversion = pfmri.version.get_short_version()
            self.installed[pname] = pversion
            if pinfo['upgradable']:
                self.pending_upgrades.add(pname)

    def VerifyPackage(self, entry, _):
        """Verify package for entry."""
        pname = entry.get('name')
        if 'version' not in entry.attrib:
            self.logger.info("Cannot verify unversioned package %s" % (pname))
            return False
        if pname not in self.installed:
            self.logger.debug("IPS: Package %s not installed" % pname)
            return False
        if entry.get('version') == 'auto':
            if pname in self.pending_upgrades:
                return False
        elif entry.get('version') == 'any':
            pass
        else:
            if entry.get('version') != self.installed[pname]:
                self.logger.debug("IPS: Package %s: have %s want %s" %
                                  (pname, self.installed[pname],
                                   entry.get('version')))
                return False

        # need to implement pkg chksum validation
        return True

"""This contains all Bcfg2 Client modules"""

import os
import sys
import stat
import time
import fcntl
import socket
import fnmatch
import logging
import argparse
import tempfile
import Bcfg2.Logger
import Bcfg2.Options
from Bcfg2.Client import XML
from Bcfg2.Client import Proxy
from Bcfg2.Client import Tools
from Bcfg2.Utils import locked, Executor, safe_input
from Bcfg2.version import __version__
# pylint: disable=W0622
from Bcfg2.Compat import xmlrpclib, walk_packages, any, all, cmp
# pylint: enable=W0622


def cmpent(ent1, ent2):
    """Sort entries."""
    if ent1.tag != ent2.tag:
        return cmp(ent1.tag, ent2.tag)
    else:
        return cmp(ent1.get('name'), ent2.get('name'))


def matches_entry(entryspec, entry):
    """ Determine if the Decisions-style entry specification matches
    the entry.  Both are tuples of (tag, name).  The entryspec can
    handle the wildcard * in either position. """
    if entryspec == entry:
        return True
    return all(fnmatch.fnmatch(entry[i], entryspec[i]) for i in [0, 1])


def matches_white_list(entry, whitelist):
    """ Return True if (<entry tag>, <entry name>) is in the given
    whitelist. """
    return any(matches_entry(we, (entry.tag, entry.get('name')))
               for we in whitelist)


def passes_black_list(entry, blacklist):
    """ Return True if (<entry tag>, <entry name>) is not in the given
    blacklist. """
    return not any(matches_entry(be, (entry.tag, entry.get('name')))
                   for be in blacklist)


def prompt(msg):
    """ Helper to give a yes/no prompt to the user.  Flushes input
    buffers, handles exceptions, etc.  Returns True if the user
    answers in the affirmative, False otherwise.

    :param msg: The message to show to the user.  The message is not
                altered in any way for display; i.e., it should
                contain "[y/N]" if desired, etc.
    :type msg: string
    :returns: bool - True if yes, False if no """
    try:
        ans = safe_input(msg)
        return ans in ['y', 'Y']
    except UnicodeEncodeError:
        ans = input(msg.encode('utf-8'))
        return ans in ['y', 'Y']
    except (EOFError, KeyboardInterrupt):
        # handle ^C
        raise SystemExit(1)
    except:
        print("Error while reading input: %s" % sys.exc_info()[1])
        return False


class ClientDriverAction(Bcfg2.Options.ComponentAction):
    """ Action to load client drivers """
    bases = ['Bcfg2.Client.Tools']
    fail_silently = True


class Client(object):
    """ The main Bcfg2 client class """

    options = Proxy.ComponentProxy.options + [
        Bcfg2.Options.Common.syslog,
        Bcfg2.Options.Common.interactive,
        Bcfg2.Options.BooleanOption(
            "-q", "--quick", help="Disable some checksum verification"),
        Bcfg2.Options.Option(
            cf=('client', 'probe_timeout'),
            type=Bcfg2.Options.Types.timeout,
            help="Timeout when running client probes"),
        Bcfg2.Options.Option(
            "-b", "--only-bundles", default=[],
            type=Bcfg2.Options.Types.colon_list,
            help='Only configure the given bundle(s)'),
        Bcfg2.Options.Option(
            "-B", "--except-bundles", default=[],
            type=Bcfg2.Options.Types.colon_list,
            help='Configure everything except the given bundle(s)'),
        Bcfg2.Options.ExclusiveOptionGroup(
            Bcfg2.Options.BooleanOption(
                "-Q", "--bundle-quick",
                help='Only verify the given bundle(s)'),
            Bcfg2.Options.Option(
                '-r', '--remove',
                choices=['all', 'services', 'packages', 'users'],
                help='Force removal of additional configuration items')),
        Bcfg2.Options.ExclusiveOptionGroup(
            Bcfg2.Options.PathOption(
                '-f', '--file', type=argparse.FileType('rb'),
                help='Configure from a file rather than querying the server'),
            Bcfg2.Options.PathOption(
                '-c', '--cache', type=argparse.FileType('wb'),
                help='Store the configuration in a file')),
        Bcfg2.Options.BooleanOption(
            '--exit-on-probe-failure', default=True,
            cf=('client', 'exit_on_probe_failure'),
            help="The client should exit if a probe fails"),
        Bcfg2.Options.Option(
            '-p', '--profile', cf=('client', 'profile'),
            help='Assert the given profile for the host'),
        Bcfg2.Options.Option(
            '-l', '--decision', cf=('client', 'decision'),
            choices=['whitelist', 'blacklist', 'none'],
            help='Run client in server decision list mode'),
        Bcfg2.Options.BooleanOption(
            "-O", "--no-lock", help='Omit lock check'),
        Bcfg2.Options.PathOption(
            cf=('components', 'lockfile'), default='/var/lock/bcfg2.run',
            help='Client lock file'),
        Bcfg2.Options.BooleanOption(
            "-n", "--dry-run", help='Do not actually change the system'),
        Bcfg2.Options.Option(
            "-D", "--drivers", cf=('client', 'drivers'),
            type=Bcfg2.Options.Types.comma_list,
            default=[m[1] for m in walk_packages(path=Tools.__path__)],
            action=ClientDriverAction, help='Client drivers'),
        Bcfg2.Options.BooleanOption(
            "-e", "--show-extra", help='Enable extra entry output'),
        Bcfg2.Options.BooleanOption(
            "-k", "--kevlar", help='Run in bulletproof mode'),
        Bcfg2.Options.BooleanOption(
            "-i", "--only-important",
            help='Only configure the important entries')]

    def __init__(self):
        self.config = None
        self._proxy = None
        self.logger = logging.getLogger('bcfg2')
        self.cmd = Executor(Bcfg2.Options.setup.probe_timeout)
        self.tools = []
        self.times = dict()
        self.times['initialization'] = time.time()

        if Bcfg2.Options.setup.bundle_quick:
            if (not Bcfg2.Options.setup.only_bundles and
                    not Bcfg2.Options.setup.except_bundles):
                self.logger.error("-Q option requires -b or -B")
                raise SystemExit(1)
        if Bcfg2.Options.setup.remove == 'services':
            self.logger.error("Service removal is nonsensical; "
                              "removed services will only be disabled")
        if not Bcfg2.Options.setup.server.startswith('https://'):
            Bcfg2.Options.setup.server = \
                'https://' + Bcfg2.Options.setup.server

        #: A dict of the state of each entry.  Keys are the entries.
        #: Values are boolean: True means that the entry is good,
        #: False means that the entry is bad.
        self.states = {}
        self.whitelist = []
        self.blacklist = []
        self.removal = []
        self.unhandled = []
        self.logger = logging.getLogger(__name__)

    def _probe_failure(self, probename, msg):
        """ handle failure of a probe in the way the user wants us to
        (exit or continue) """
        message = "Failed to execute probe %s: %s" % (probename, msg)
        if Bcfg2.Options.setup.exit_on_probe_failure:
            self.fatal_error(message)
        else:
            self.logger.error(message)

    def run_probe(self, probe):
        """Execute probe."""
        name = probe.get('name')
        self.logger.info("Running probe %s" % name)
        ret = XML.Element("probe-data", name=name, source=probe.get('source'))
        try:
            scripthandle, scriptname = tempfile.mkstemp()
            if sys.hexversion >= 0x03000000:
                script = os.fdopen(scripthandle, 'w',
                                   encoding=Bcfg2.Options.setup.encoding)
            else:
                script = os.fdopen(scripthandle, 'w')
            try:
                script.write("#!%s\n" %
                             (probe.attrib.get('interpreter', '/bin/sh')))
                if sys.hexversion >= 0x03000000:
                    script.write(probe.text)
                else:
                    script.write(probe.text.encode('utf-8'))
                script.close()
                os.chmod(scriptname,
                         stat.S_IRUSR | stat.S_IRGRP | stat.S_IROTH |
                         stat.S_IXUSR | stat.S_IXGRP | stat.S_IXOTH |
                         stat.S_IWUSR)  # 0755
                rv = self.cmd.run(scriptname)
                if rv.stderr:
                    self.logger.warning("Probe %s has error output: %s" %
                                        (name, rv.stderr))
                if not rv.success:
                    self._probe_failure(name, "Return value %s" % rv.retval)
                self.logger.info("Probe %s has result:" % name)
                self.logger.info(rv.stdout)
                if sys.hexversion >= 0x03000000:
                    ret.text = rv.stdout
                else:
                    ret.text = rv.stdout.decode('utf-8')
            finally:
                os.unlink(scriptname)
        except SystemExit:
            raise
        except:
            self._probe_failure(name, sys.exc_info()[1])
        return ret

    def fatal_error(self, message):
        """Signal a fatal error."""
        self.logger.error("Fatal error: %s" % (message))
        raise SystemExit(1)

    @property
    def proxy(self):
        """ get an XML-RPC proxy to the server """
        if self._proxy is None:
            self._proxy = Proxy.ComponentProxy()
        return self._proxy

    def run_probes(self):
        """ run probes and upload probe data """
        try:
            probes = XML.XML(str(self.proxy.GetProbes()))
        except (Proxy.ProxyError,
                Proxy.CertificateError,
                socket.gaierror,
                socket.error):
            err = sys.exc_info()[1]
            self.fatal_error("Failed to download probes from bcfg2: %s" % err)
        except XML.ParseError:
            err = sys.exc_info()[1]
            self.fatal_error("Server returned invalid probe requests: %s" %
                             err)

        self.times['probe_download'] = time.time()

        # execute probes
        probedata = XML.Element("ProbeData")
        for probe in probes.findall(".//probe"):
            probedata.append(self.run_probe(probe))

        if len(probes.findall(".//probe")) > 0:
            try:
                # upload probe responses
                self.proxy.RecvProbeData(
                    XML.tostring(probedata,
                                 xml_declaration=False).decode('utf-8'))
            except Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.fatal_error("Failed to upload probe data: %s" % err)

        self.times['probe_upload'] = time.time()

    def get_config(self):
        """ load the configuration, either from the cached
        configuration file (-f), or from the server """
        if Bcfg2.Options.setup.file:
            # read config from file
            try:
                self.logger.debug("Reading cached configuration from %s" %
                                  Bcfg2.Options.setup.file.name)
                return Bcfg2.Options.setup.file.read()
            except IOError:
                self.fatal_error("Failed to read cached configuration from: %s"
                                 % Bcfg2.Options.setup.file.name)
        else:
            # retrieve config from server
            if Bcfg2.Options.setup.profile:
                try:
                    self.proxy.AssertProfile(Bcfg2.Options.setup.profile)
                except Proxy.ProxyError:
                    err = sys.exc_info()[1]
                    self.fatal_error("Failed to set client profile: %s" % err)

            try:
                self.proxy.DeclareVersion(__version__)
            except (xmlrpclib.Fault,
                    Proxy.ProxyError,
                    Proxy.CertificateError,
                    socket.gaierror,
                    socket.error):
                err = sys.exc_info()[1]
                self.fatal_error("Failed to declare version: %s" % err)

            self.run_probes()

            if Bcfg2.Options.setup.decision in ['whitelist', 'blacklist']:
                try:
                    # TODO: read decision list from --decision-list
                    Bcfg2.Options.setup.decision_list = \
                        self.proxy.GetDecisionList(
                            Bcfg2.Options.setup.decision)
                    self.logger.info("Got decision list from server:")
                    self.logger.info(Bcfg2.Options.setup.decision_list)
                except Proxy.ProxyError:
                    err = sys.exc_info()[1]
                    self.fatal_error("Failed to get decision list: %s" % err)

            try:
                rawconfig = self.proxy.GetConfig().encode('utf-8')
            except Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.fatal_error("Failed to download configuration from "
                                 "Bcfg2: %s" % err)

            self.times['config_download'] = time.time()

        if Bcfg2.Options.setup.cache:
            try:
                Bcfg2.Options.setup.cache.write(rawconfig)
                os.chmod(Bcfg2.Options.setup.cache.name, 384)  # 0600
            except IOError:
                self.logger.warning("Failed to write config cache file %s" %
                                    (Bcfg2.Options.setup.cache))
            self.times['caching'] = time.time()

        return rawconfig

    def parse_config(self, rawconfig):
        """ Parse the XML configuration received from the Bcfg2 server """
        try:
            self.config = XML.XML(rawconfig)
        except XML.ParseError:
            syntax_error = sys.exc_info()[1]
            self.fatal_error("The configuration could not be parsed: %s" %
                             syntax_error)

        self.load_tools()

        # find entries not handled by any tools
        self.unhandled = [entry for struct in self.config
                          for entry in struct
                          if entry not in self.handled]

        if self.unhandled:
            self.logger.error("The following entries are not handled by any "
                              "tool:")
            for entry in self.unhandled:
                self.logger.error("%s:%s:%s" % (entry.tag, entry.get('type'),
                                                entry.get('name')))

        # find duplicates
        self.find_dups(self.config)

        pkgs = [(entry.get('name'), entry.get('origin'))
                for struct in self.config
                for entry in struct
                if entry.tag == 'Package']
        if pkgs:
            self.logger.debug("The following packages are specified in bcfg2:")
            self.logger.debug([pkg[0] for pkg in pkgs if pkg[1] is None])
            self.logger.debug("The following packages are prereqs added by "
                              "Packages:")
            self.logger.debug([pkg[0] for pkg in pkgs if pkg[1] == 'Packages'])

        self.times['config_parse'] = time.time()

    def run(self):
        """Perform client execution phase."""
        # begin configuration
        self.times['start'] = time.time()

        self.logger.info("Starting Bcfg2 client run at %s" %
                         self.times['start'])

        self.parse_config(self.get_config().decode('utf-8'))

        if self.config.tag == 'error':
            self.fatal_error("Server error: %s" % (self.config.text))

        if Bcfg2.Options.setup.bundle_quick:
            newconfig = XML.XML('<Configuration/>')
            for bundle in self.config.getchildren():
                name = bundle.get("name")
                if (name and (name in Bcfg2.Options.setup.only_bundles or
                              name not in Bcfg2.Options.setup.except_bundles)):
                    newconfig.append(bundle)
            self.config = newconfig

        if not Bcfg2.Options.setup.no_lock:
            # check lock here
            try:
                lockfile = open(Bcfg2.Options.setup.lockfile, 'w')
                if locked(lockfile.fileno()):
                    self.fatal_error("Another instance of Bcfg2 is running. "
                                     "If you want to bypass the check, run "
                                     "with the -O/--no-lock option")
            except SystemExit:
                raise
            except:
                lockfile = None
                self.logger.error("Failed to open lockfile %s: %s" %
                                  (Bcfg2.Options.setup.lockfile,
                                   sys.exc_info()[1]))

        # execute the configuration
        self.Execute()

        if not Bcfg2.Options.setup.no_lock:
            # unlock here
            if lockfile:
                try:
                    fcntl.lockf(lockfile.fileno(), fcntl.LOCK_UN)
                    os.remove(Bcfg2.Options.setup.lockfile)
                except OSError:
                    self.logger.error("Failed to unlock lockfile %s" %
                                      lockfile.name)

        if (not Bcfg2.Options.setup.file and
                not Bcfg2.Options.setup.bundle_quick):
            # upload statistics
            feedback = self.GenerateStats()

            try:
                self.proxy.RecvStats(
                    XML.tostring(feedback,
                                 xml_declaration=False).decode('utf-8'))
            except Proxy.ProxyError:
                err = sys.exc_info()[1]
                self.logger.error("Failed to upload configuration statistics: "
                                  "%s" % err)
                raise SystemExit(2)

        self.logger.info("Finished Bcfg2 client run at %s" % time.time())

    def load_tools(self):
        """ Load all applicable client tools """
        for tool in Bcfg2.Options.setup.drivers:
            try:
                self.tools.append(tool(self.config))
            except Tools.ToolInstantiationError:
                continue
            except:
                self.logger.error("Failed to instantiate tool %s" % tool,
                                  exc_info=1)

        for tool in self.tools[:]:
            for conflict in getattr(tool, 'conflicts', []):
                for item in self.tools:
                    if item.name == conflict:
                        self.tools.remove(item)

        self.logger.info("Loaded tool drivers:")
        self.logger.info([tool.name for tool in self.tools])

        deprecated = [tool.name for tool in self.tools if tool.deprecated]
        if deprecated:
            self.logger.warning("Loaded deprecated tool drivers:")
            self.logger.warning(deprecated)
        experimental = [tool.name for tool in self.tools if tool.experimental]
        if experimental:
            self.logger.warning("Loaded experimental tool drivers:")
            self.logger.warning(experimental)

    def find_dups(self, config):
        """ Find duplicate entries and warn about them """
        entries = dict()
        for struct in config:
            for entry in struct:
                for tool in self.tools:
                    if tool.handlesEntry(entry):
                        pkey = tool.primarykey(entry)
                        if pkey in entries:
                            entries[pkey] += 1
                        else:
                            entries[pkey] = 1
        multi = [e for e, c in entries.items() if c > 1]
        if multi:
            self.logger.debug("The following entries are included multiple "
                              "times:")
            for entry in multi:
                self.logger.debug(entry)

    def promptFilter(self, msg, entries):
        """Filter a supplied list based on user input."""
        ret = []
        entries.sort(key=lambda e: e.tag + ":" + e.get('name'))
        for entry in entries[:]:
            if entry in self.unhandled:
                # don't prompt for entries that can't be installed
                continue
            if 'qtext' in entry.attrib:
                iprompt = entry.get('qtext')
            else:
                iprompt = msg % (entry.tag, entry.get('name'))
            if prompt(iprompt):
                ret.append(entry)
        return ret

    def __getattr__(self, name):
        if name in ['extra', 'handled', 'modified', '__important__']:
            ret = []
            for tool in self.tools:
                ret += getattr(tool, name)
            return ret
        elif name in self.__dict__:
            return self.__dict__[name]
        raise AttributeError(name)

    def InstallImportant(self):
        """Install important entries

        We also process the decision mode stuff here because we want to prevent
        non-whitelisted/blacklisted 'important' entries from being installed
        prior to determining the decision mode on the client.
        """
        # Need to process decision stuff early so that dryrun mode
        # works with it
        self.whitelist = [entry for entry in self.states
                          if not self.states[entry]]
        if not Bcfg2.Options.setup.file:
            if Bcfg2.Options.setup.decision == 'whitelist':
                dwl = Bcfg2.Options.setup.decision_list
                w_to_rem = [e for e in self.whitelist
                            if not matches_white_list(e, dwl)]
                if w_to_rem:
                    self.logger.info("In whitelist mode: "
                                     "suppressing installation of:")
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in w_to_rem])
                    self.whitelist = [x for x in self.whitelist
                                      if x not in w_to_rem]
            elif Bcfg2.Options.setup.decision == 'blacklist':
                b_to_rem = \
                    [e for e in self.whitelist
                     if not
                     passes_black_list(e, Bcfg2.Options.setup.decision_list)]
                if b_to_rem:
                    self.logger.info("In blacklist mode: "
                                     "suppressing installation of:")
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in b_to_rem])
                    self.whitelist = [x for x in self.whitelist
                                      if x not in b_to_rem]

        # take care of important entries first
        if (not Bcfg2.Options.setup.dry_run or
                Bcfg2.Options.setup.only_important):
            important_installs = set()
            for parent in self.config.findall(".//Path/.."):
                name = parent.get("name")
                if not name or (name in Bcfg2.Options.setup.except_bundles and
                                name not in Bcfg2.Options.setup.only_bundles):
                    continue
                for cfile in parent.findall("./Path"):
                    if (cfile.get('name') not in self.__important__ or
                            cfile.get('type') != 'file' or
                            cfile not in self.whitelist):
                        continue
                    tools = [t for t in self.tools
                             if t.handlesEntry(cfile) and t.canVerify(cfile)]
                    if not tools:
                        continue
                    if Bcfg2.Options.setup.dry_run:
                        important_installs.add(cfile)
                        continue
                    if (Bcfg2.Options.setup.interactive and not
                            self.promptFilter("Install %s: %s? (y/N):",
                                              [cfile])):
                        self.whitelist.remove(cfile)
                        continue
                    try:
                        self.states[cfile] = tools[0].InstallPath(cfile)
                        if self.states[cfile]:
                            tools[0].modified.append(cfile)
                    except:  # pylint: disable=W0702
                        self.logger.error("Unexpected tool failure",
                                          exc_info=1)
                    cfile.set('qtext', '')
                    if tools[0].VerifyPath(cfile, []):
                        self.whitelist.remove(cfile)
            if Bcfg2.Options.setup.dry_run and len(important_installs) > 0:
                self.logger.info("In dryrun mode: "
                                 "suppressing entry installation for:")
                self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                  for e in important_installs])

    def Inventory(self):
        """
           Verify all entries,
           find extra entries,
           and build up workqueues

        """
        # initialize all states
        for struct in self.config.getchildren():
            for entry in struct.getchildren():
                self.states[entry] = False
        for tool in self.tools:
            try:
                self.states.update(tool.Inventory())
            except KeyboardInterrupt:
                raise
            except:  # pylint: disable=W0702
                self.logger.error("%s.Inventory() call failed:" % tool.name,
                                  exc_info=1)

    def Decide(self):  # pylint: disable=R0912
        """Set self.whitelist based on user interaction."""
        iprompt = "Install %s: %s? (y/N): "
        rprompt = "Remove %s: %s? (y/N): "
        if Bcfg2.Options.setup.remove:
            if Bcfg2.Options.setup.remove == 'all':
                self.removal = self.extra
            elif Bcfg2.Options.setup.remove == 'services':
                self.removal = [entry for entry in self.extra
                                if entry.tag == 'Service']
            elif Bcfg2.Options.setup.remove == 'packages':
                self.removal = [entry for entry in self.extra
                                if entry.tag == 'Package']
            elif Bcfg2.Options.setup.remove == 'users':
                self.removal = [entry for entry in self.extra
                                if entry.tag in ['POSIXUser', 'POSIXGroup']]

        candidates = [entry for entry in self.states
                      if not self.states[entry]]

        if Bcfg2.Options.setup.dry_run:
            if self.whitelist:
                self.logger.info("In dryrun mode: "
                                 "suppressing entry installation for:")
                self.logger.info(["%s:%s" % (entry.tag, entry.get('name'))
                                  for entry in self.whitelist])
                self.whitelist = []
            if self.removal:
                self.logger.info("In dryrun mode: "
                                 "suppressing entry removal for:")
                self.logger.info(["%s:%s" % (entry.tag, entry.get('name'))
                                  for entry in self.removal])
            self.removal = []

        # Here is where most of the work goes
        # first perform bundle filtering
        all_bundle_names = [b.get('name')
                            for b in self.config.findall('./Bundle')]
        bundles = self.config.getchildren()
        if Bcfg2.Options.setup.only_bundles:
            # warn if non-existent bundle given
            for bundle in Bcfg2.Options.setup.only_bundles:
                if bundle not in all_bundle_names:
                    self.logger.info("Warning: Bundle %s not found" % bundle)
            bundles = [b for b in bundles
                       if b.get('name') in Bcfg2.Options.setup.only_bundles]
        if Bcfg2.Options.setup.except_bundles:
            # warn if non-existent bundle given
            if not Bcfg2.Options.setup.bundle_quick:
                for bundle in Bcfg2.Options.setup.except_bundles:
                    if bundle not in all_bundle_names:
                        self.logger.info("Warning: Bundle %s not found" %
                                         bundle)
            bundles = [
                b for b in bundles
                if b.get('name') not in Bcfg2.Options.setup.except_bundles]
        self.whitelist = [e for e in self.whitelist
                          if any(e in b for b in bundles)]

        # first process prereq actions
        for bundle in bundles[:]:
            if bundle.tag == 'Bundle':
                bmodified = any((item in self.whitelist or
                                 item in self.modified) for item in bundle)
            else:
                bmodified = False
            actions = [a for a in bundle.findall('./Action')
                       if (a.get('timing') in ['pre', 'both'] and
                           (bmodified or a.get('when') == 'always'))]
            # now we process all "pre" and "both" actions that are either
            # always or the bundle has been modified
            if Bcfg2.Options.setup.interactive:
                self.promptFilter(iprompt, actions)
            self.DispatchInstallCalls(actions)

            if bundle.tag != 'Bundle':
                continue

            # need to test to fail entries in whitelist
            if not all(self.states[a] for a in actions):
                # then display bundles forced off with entries
                self.logger.info("%s %s failed prerequisite action" %
                                 (bundle.tag, bundle.get('name')))
                bundles.remove(bundle)
                b_to_remv = [ent for ent in self.whitelist if ent in bundle]
                if b_to_remv:
                    self.logger.info("Not installing entries from %s %s" %
                                     (bundle.tag, bundle.get('name')))
                    self.logger.info(["%s:%s" % (e.tag, e.get('name'))
                                      for e in b_to_remv])
                    for ent in b_to_remv:
                        self.whitelist.remove(ent)

        self.logger.debug("Installing entries in the following bundle(s):")
        self.logger.debug("  %s" % ", ".join(b.get("name") for b in bundles
                                             if b.get("name")))

        if Bcfg2.Options.setup.interactive:
            self.whitelist = self.promptFilter(iprompt, self.whitelist)
            self.removal = self.promptFilter(rprompt, self.removal)

        for entry in candidates:
            if entry not in self.whitelist:
                self.blacklist.append(entry)

    def DispatchInstallCalls(self, entries):
        """Dispatch install calls to underlying tools."""
        for tool in self.tools:
            handled = [entry for entry in entries if tool.canInstall(entry)]
            if not handled:
                continue
            try:
                self.states.update(tool.Install(handled))
            except KeyboardInterrupt:
                raise
            except:  # pylint: disable=W0702
                self.logger.error("%s.Install() call failed:" % tool.name,
                                  exc_info=1)

    def Install(self):
        """Install all entries."""
        self.DispatchInstallCalls(self.whitelist)
        mods = self.modified
        mbundles = [struct for struct in self.config.findall('Bundle')
                    if any(True for mod in mods if mod in struct)]

        if self.modified:
            # Handle Bundle interdeps
            if mbundles:
                self.logger.info("The Following Bundles have been modified:")
                self.logger.info([mbun.get('name') for mbun in mbundles])
            tbm = [(t, b) for t in self.tools for b in mbundles]
            for tool, bundle in tbm:
                try:
                    self.states.update(tool.Inventory(structures=[bundle]))
                except KeyboardInterrupt:
                    raise
                except:  # pylint: disable=W0702
                    self.logger.error("%s.Inventory() call failed:" %
                                      tool.name,
                                      exc_info=1)
            clobbered = [entry for bundle in mbundles for entry in bundle
                         if (not self.states[entry] and
                             entry not in self.blacklist)]
            if clobbered:
                self.logger.debug("Found clobbered entries:")
                self.logger.debug(["%s:%s" % (entry.tag, entry.get('name'))
                                   for entry in clobbered])
                if not Bcfg2.Options.setup.interactive:
                    self.DispatchInstallCalls(clobbered)

        all_bundles = self.config.findall('./Bundle')
        mbundles.extend(self._get_all_modified_bundles(mbundles, all_bundles))

        for bundle in all_bundles:
            if (Bcfg2.Options.setup.only_bundles and
                    bundle.get('name') not in
                    Bcfg2.Options.setup.only_bundles):
                # prune out unspecified bundles when running with -b
                continue
            if bundle in mbundles:
                continue

            self.logger.debug("Bundle %s was not modified" %
                              bundle.get('name'))
            for tool in self.tools:
                try:
                    self.states.update(tool.BundleNotUpdated(bundle))
                except KeyboardInterrupt:
                    raise
                except:  # pylint: disable=W0702
                    self.logger.error('%s.BundleNotUpdated(%s:%s) call failed:'
                                      % (tool.name, bundle.tag,
                                         bundle.get('name')), exc_info=1)

        for indep in self.config.findall('.//Independent'):
            for tool in self.tools:
                try:
                    self.states.update(tool.BundleNotUpdated(indep))
                except KeyboardInterrupt:
                    raise
                except:  # pylint: disable=W0702
                    self.logger.error("%s.BundleNotUpdated(%s:%s) call failed:"
                                      % (tool.name, indep.tag,
                                         indep.get("name")), exc_info=1)

    def _get_all_modified_bundles(self, mbundles, all_bundles):
        """This gets all modified bundles by calling BundleUpdated until no
        new bundles get added to the modification list."""
        new_mbundles = mbundles
        add_mbundles = []

        while new_mbundles:
            for bundle in self.config.findall('./Bundle'):
                if (Bcfg2.Options.setup.only_bundles and
                        bundle.get('name') not in
                        Bcfg2.Options.setup.only_bundles):
                    # prune out unspecified bundles when running with -b
                    continue
                if bundle not in new_mbundles:
                    continue

                self.logger.debug('Bundle %s was modified' %
                                  bundle.get('name'))
                for tool in self.tools:
                    try:
                        self.states.update(tool.BundleUpdated(bundle))
                    except:  # pylint: disable=W0702
                        self.logger.error('%s.BundleUpdated(%s:%s) call '
                                          'failed:' % (tool.name, bundle.tag,
                                                       bundle.get("name")),
                                          exc_info=1)

            mods = self.modified
            new_mbundles = [struct for struct in all_bundles
                            if any(True for mod in mods if mod in struct) and
                            struct not in mbundles + add_mbundles]
            add_mbundles.extend(new_mbundles)

        return add_mbundles

    def Remove(self):
        """Remove extra entries."""
        for tool in self.tools:
            extras = [entry for entry in self.removal
                      if tool.handlesEntry(entry)]
            if extras:
                try:
                    tool.Remove(extras)
                except:  # pylint: disable=W0702
                    self.logger.error("%s.Remove() failed" % tool.name,
                                      exc_info=1)

    def CondDisplayState(self, phase):
        """Conditionally print tracing information."""
        self.logger.info('Phase: %s' % phase)
        self.logger.info('Correct entries:        %d' %
                         list(self.states.values()).count(True))
        self.logger.info('Incorrect entries:      %d' %
                         list(self.states.values()).count(False))
        if phase == 'final' and list(self.states.values()).count(False):
            for entry in sorted(self.states.keys(), key=lambda e: e.tag + ":" +
                                e.get('name')):
                if not self.states[entry]:
                    etype = entry.get('type')
                    if etype:
                        self.logger.info("%s:%s:%s" % (entry.tag, etype,
                                                       entry.get('name')))
                    else:
                        self.logger.info("%s:%s" % (entry.tag,
                                                    entry.get('name')))
        self.logger.info('Total managed entries: %d' %
                         len(list(self.states.values())))
        self.logger.info('Unmanaged entries:      %d' % len(self.extra))
        if phase == 'final' and Bcfg2.Options.setup.show_extra:
            for entry in sorted(self.extra,
                                key=lambda e: e.tag + ":" + e.get('name')):
                etype = entry.get('type')
                if etype:
                    self.logger.info("%s:%s:%s" % (entry.tag, etype,
                                                   entry.get('name')))
                else:
                    self.logger.info("%s:%s" % (entry.tag,
                                                entry.get('name')))

        if ((list(self.states.values()).count(False) == 0) and not self.extra):
            self.logger.info('All entries correct.')

    def ReInventory(self):
        """Recheck everything."""
        if not Bcfg2.Options.setup.dry_run and Bcfg2.Options.setup.kevlar:
            self.logger.info("Rechecking system inventory")
            self.Inventory()

    def Execute(self):
        """Run all methods."""
        self.Inventory()
        self.times['inventory'] = time.time()
        self.CondDisplayState('initial')
        self.InstallImportant()
        if not Bcfg2.Options.setup.only_important:
            self.Decide()
            self.Install()
            self.times['install'] = time.time()
            self.Remove()
            self.times['remove'] = time.time()

        if self.modified:
            self.ReInventory()
            self.times['reinventory'] = time.time()
        self.times['finished'] = time.time()
        self.CondDisplayState('final')

    def GenerateStats(self):
        """Generate XML summary of execution statistics."""
        states = {}
        for (item, val) in list(self.states.items()):
            if not Bcfg2.Options.setup.only_important or \
               item.get('important', 'false').lower() == 'true':
                states[item] = val

        feedback = XML.Element("upload-statistics")
        stats = XML.SubElement(feedback,
                               'Statistics', total=str(len(states)),
                               version='2.0',
                               revision=self.config.get('revision', '-1'))
        flags = XML.SubElement(stats, "Flags")
        XML.SubElement(flags, "Flag", name="dry_run",
                       value=str(Bcfg2.Options.setup.dry_run))
        XML.SubElement(flags, "Flag", name="only_important",
                       value=str(Bcfg2.Options.setup.only_important))
        good_entries = [key for key, val in list(states.items()) if val]
        good = len(good_entries)
        stats.set('good', str(good))
        if any(not val for val in list(states.values())):
            stats.set('state', 'dirty')
        else:
            stats.set('state', 'clean')

        # List bad elements of the configuration
        for (data, ename) in [(self.modified, 'Modified'),
                              (self.extra, "Extra"),
                              (good_entries, "Good"),
                              ([entry for entry in states
                                if not states[entry]], "Bad")]:
            container = XML.SubElement(stats, ename)
            for item in data:
                item.set('qtext', '')
                container.append(item)
                item.text = None

        timeinfo = XML.Element("OpStamps")
        feedback.append(stats)
        for (event, timestamp) in list(self.times.items()):
            timeinfo.set(event, str(timestamp))
        stats.append(timeinfo)
        return feedback

# -*- coding: utf-8 -*-
from __future__ import unicode_literals

from django.db import models, migrations


class Migration(migrations.Migration):

    dependencies = [
        ('Reporting', '0003_expand_hash_key'),
    ]

    operations = [
        migrations.AlterField(
            model_name='interaction',
            name='profile',
            field=models.ForeignKey(related_name='+', to='Reporting.Group', null=True),
            preserve_default=True,
        ),
    ]

"""This provides bundle clauses with translation functionality."""

import os
import re
import sys
import copy
import fnmatch
import lxml.etree
from Bcfg2.Server.Plugin import StructFile, Plugin, Structure, \
    StructureValidator, XMLDirectoryBacked, Generator
from Bcfg2.version import Bcfg2VersionInfo
from genshi.template import TemplateError


class BundleFile(StructFile):
    """ Representation of a bundle XML file """
    bundle_name_re = re.compile(r'^(?P<name>.*)\.(xml|genshi)$')

    def __init__(self, filename, should_monitor=False):
        StructFile.__init__(self, filename, should_monitor=should_monitor)
        if self.name.endswith(".genshi"):
            self.logger.warning("Bundler: %s: Bundle filenames ending with "
                                ".genshi are deprecated; add the Genshi XML "
                                "namespace to a .xml bundle instead" %
                                self.name)

    def Index(self):
        StructFile.Index(self)
        if self.xdata.get("name"):
            self.logger.warning("Bundler: %s: Explicitly specifying bundle "
                                "names is deprecated" % self.name)

    @property
    def bundle_name(self):
        """ The name of the bundle, as determined from the filename """
        return self.bundle_name_re.match(
            os.path.basename(self.name)).group("name")


class Bundler(Plugin,
              Structure,
              StructureValidator,
              XMLDirectoryBacked):
    """ The bundler creates dependent clauses based on the
    bundle/translation scheme from Bcfg1. """
    __author__ = 'bcfg-dev@mcs.anl.gov'
    __child__ = BundleFile
    patterns = re.compile(r'^.*\.(?:xml|genshi)$')

    def __init__(self, core):
        Plugin.__init__(self, core)
        Structure.__init__(self)
        StructureValidator.__init__(self)
        XMLDirectoryBacked.__init__(self, self.data)
        #: Bundles by bundle name, rather than filename
        self.bundles = dict()

    def HandleEvent(self, event):
        XMLDirectoryBacked.HandleEvent(self, event)
        self.bundles = dict([(b.bundle_name, b)
                             for b in self.entries.values()])

    def validate_structures(self, metadata, structures):
        """ Translate <Path glob='...'/> entries into <Path name='...'/>
        entries """
        for struct in structures:
            for pathglob in struct.xpath("//Path[@glob]"):
                for plugin in self.core.plugins_by_type(Generator):
                    for match in fnmatch.filter(plugin.Entries['Path'].keys(),
                                                pathglob.get("glob")):
                        lxml.etree.SubElement(pathglob.getparent(),
                                              "Path", name=match)
                pathglob.getparent().remove(pathglob)

    def BuildStructures(self, metadata):
        bundleset = []
        bundles = copy.copy(metadata.bundles)
        bundles_added = set(bundles)
        while bundles:
            bundlename = bundles.pop()
            try:
                bundle = self.bundles[bundlename]
            except KeyError:
                self.logger.error("Bundler: Bundle %s does not exist" %
                                  bundlename)
                continue

            try:
                data = bundle.XMLMatch(metadata)
            except TemplateError:
                err = sys.exc_info()[1]
                self.logger.error("Bundler: Failed to render templated bundle "
                                  "%s: %s" % (bundlename, err))
                continue
            except:
                self.logger.error("Bundler: Unexpected bundler error for %s" %
                                  bundlename, exc_info=1)
                continue

            if data.get("independent", "false").lower() == "true":
                data.tag = "Independent"
                del data.attrib['independent']

            data.set("name", bundlename)

            for child in data.findall("Bundle"):
                if child.getchildren():
                    # XInclude'd bundle -- "flatten" it so there
                    # aren't extra Bundle tags, since other bits in
                    # Bcfg2 only handle the direct children of the
                    # top-level Bundle tag
                    if data.get("name"):
                        self.logger.warning("Bundler: In file XIncluded from "
                                            "%s: Explicitly specifying "
                                            "bundle names is deprecated" %
                                            self.name)
                    for el in child.getchildren():
                        data.append(el)
                    data.remove(child)
                else:
                    # no children -- wat
                    self.logger.warning("Bundler: Useless empty Bundle tag "
                                        "in %s" % self.name)
                    data.remove(child)

            for child in data.findall('RequiredBundle'):
                if child.get("name"):
                    # dependent bundle -- add it to the list of
                    # bundles for this client
                    if child.get("name") not in bundles_added:
                        bundles.add(child.get("name"))
                        bundles_added.add(child.get("name"))
                    if child.get('inherit_modification', 'false') == 'true':
                        if metadata.version_info >= \
                           Bcfg2VersionInfo('1.4.0pre2'):
                            lxml.etree.SubElement(data, 'Bundle',
                                                  name=child.get('name'))
                        else:
                            self.logger.warning(
                                'Bundler: inherit_modification="true" is '
                                'only supported for clients starting '
                                '1.4.0pre2')
                    data.remove(child)
                else:
                    # no name -- wat
                    self.logger.warning('Bundler: Missing required name in '
                                        'RequiredBundle tag in %s' %
                                        self.name)
                    data.remove(child)

            bundleset.append(data)
        return bundleset

"""Revision interface for Bcfg2 repos using mercurial.
"""

import sys
from mercurial import ui, hg
import Bcfg2.Server.Plugin


class Hg(Bcfg2.Server.Plugin.Version):
    """Revision interface for Bcfg2 repos using mercurial.
    """

    __author__ = 'bcfg-dev@mcs.anl.gov'
    __vcs_metadata_path__ = ".hg"

    def __init__(self, core):
        Bcfg2.Server.Plugin.Version.__init__(self, core)
        self.logger.debug("Initialized hg plugin with hg directory %s" %
                          self.vcs_path)

    def get_revision(self):
        """Read hg revision information for the Bcfg2 repository."""
        try:
            repo_path = Bcfg2.Options.setup.vcs_root + "/"
            repo = hg.repository(ui.ui(), repo_path)
            tip = repo.changelog.tip()
            return repo.changelog.rev(tip)
        except hg.error.RepoError:
            err = sys.exc_info()[1]
            msg = "Failed to read hg repository: %s" % err
            self.logger.error(msg)
            raise Bcfg2.Server.Plugin.PluginExecutionError(msg)

__all__ = ['manage', 'nisauth', 'reports', 'settings', 'backends', 'urls', 'importscript']

"""test component loading."""

import argparse
import os

from Bcfg2.Options import Option, BooleanOption, PathOption, ComponentAction, \
    get_parser, new_parser, Types, ConfigFileAction, Common

from testsuite.Testsrc.Testlib.TestOptions import make_config, One, Two, \
    OptionTestCase


# create a bunch of fake components for testing component loading options

class ChildOne(object):
    """fake component for testing component loading."""
    options = [Option("--child-one")]


class ChildTwo(object):
    """fake component for testing component loading."""
    options = [Option("--child-two")]


class ChildComponentAction(ComponentAction):
    """child component loader action."""
    islist = False
    mapping = {"one": ChildOne,
               "two": ChildTwo}


class ComponentOne(object):
    """fake component for testing component loading."""
    options = [BooleanOption("--one")]


class ComponentTwo(object):
    """fake component for testing component loading."""
    options = [Option("--child", default="one", action=ChildComponentAction)]


class ComponentThree(object):
    """fake component for testing component loading."""
    options = [BooleanOption("--three")]


class ConfigFileComponent(object):
    """fake component for testing component loading."""
    options = [Option("--config2", action=ConfigFileAction),
               Option(cf=("config", "test"), dest="config2_test",
                      default="bar")]


class PathComponent(object):
    """fake component for testing <repository> macros in child components."""
    options = [PathOption(cf=("test", "test_path")),
               PathOption(cf=("test", "test_path_default"),
                          default="<repository>/test/default")]


class ParentComponentAction(ComponentAction):
    """parent component loader action."""
    mapping = {"one": ComponentOne,
               "two": ComponentTwo,
               "three": ComponentThree,
               "config": ConfigFileComponent,
               "path": PathComponent}


class TestComponentOptions(OptionTestCase):
    """test cases for component loading."""

    def setUp(self):
        OptionTestCase.setUp(self)
        self.options = [
            Option("--parent", type=Types.comma_list,
                   default=["one", "two"], action=ParentComponentAction)]

        self.result = argparse.Namespace()
        new_parser()
        self.parser = get_parser(components=[self], namespace=self.result,
                                 description="component testing parser")

    @make_config()
    def test_loading_components(self, config_file):
        """load a single component during option parsing."""
        self.parser.parse(["-C", config_file, "--parent", "one"])
        self.assertEqual(self.result.parent, [ComponentOne])

    @make_config()
    def test_component_option(self, config_file):
        """use options from a component loaded during option parsing."""
        self.parser.parse(["--one", "-C", config_file, "--parent", "one"])
        self.assertEqual(self.result.parent, [ComponentOne])
        self.assertTrue(self.result.one)

    @make_config()
    def test_multi_component_load(self, config_file):
        """load multiple components during option parsing."""
        self.parser.parse(["-C", config_file, "--parent", "one,three"])
        self.assertEqual(self.result.parent, [ComponentOne, ComponentThree])

    @make_config()
    def test_multi_component_options(self, config_file):
        """use options from multiple components during option parsing."""
        self.parser.parse(["-C", config_file, "--three",
                           "--parent", "one,three", "--one"])
        self.assertEqual(self.result.parent, [ComponentOne, ComponentThree])
        self.assertTrue(self.result.one)
        self.assertTrue(self.result.three)

    @make_config()
    def test_component_default_not_loaded(self, config_file):
        """options from default but unused components not available."""
        self.assertRaises(
            SystemExit,
            self.parser.parse,
            ["-C", config_file, "--child", "one", "--parent", "one"])

    @make_config()
    def test_tiered_components(self, config_file):
        """load child component."""
        self.parser.parse(["-C", config_file, "--parent", "two",
                           "--child", "one"])
        self.assertEqual(self.result.parent, [ComponentTwo])
        self.assertEqual(self.result.child, ChildOne)

    @make_config()
    def test_options_tiered_components(self, config_file):
        """use options from child component."""
        self.parser.parse(["--child-one", "foo", "-C", config_file, "--parent",
                           "two", "--child", "one"])
        self.assertEqual(self.result.parent, [ComponentTwo])
        self.assertEqual(self.result.child, ChildOne)
        self.assertEqual(self.result.child_one, "foo")

    @make_config()
    def test_bogus_component(self, config_file):
        """error out with bad component name."""
        self.assertRaises(SystemExit,
                          self.parser.parse,
                          ["-C", config_file, "--parent", "blargle"])

    @make_config()
    @make_config({"config": {"test": "foo"}})
    def test_config_component(self, config1, config2):
        """load component with alternative config file."""
        self.parser.parse(["-C", config1, "--config2", config2,
                           "--parent", "config"])
        self.assertEqual(self.result.config2, config2)
        self.assertEqual(self.result.config2_test, "foo")

    @make_config()
    def test_config_component_no_file(self, config_file):
        """load component with missing alternative config file."""
        self.parser.parse(["-C", config_file, "--parent", "config"])
        self.assertEqual(self.result.config2, None)

    @make_config({"test": {"test_path": "<repository>/test"}})
    def test_macros_in_component_options(self, config_file):
        """fix up <repository> macros in component options."""
        self.parser.add_options([Common.repository])
        self.parser.parse(["-C", config_file, "-Q", "/foo/bar",
                           "--parent", "path"])
        self.assertEqual(self.result.test_path, "/foo/bar/test")
        self.assertEqual(self.result.test_path_default,
                         "/foo/bar/test/default")


class ImportComponentAction(ComponentAction):
    """action that imports real classes for testing."""
    islist = False
    bases = ["testsuite.Testsrc.Testlib.TestOptions"]


class ImportModuleAction(ImportComponentAction):
    """action that only imports modules for testing."""
    module = True


class TestImportComponentOptions(OptionTestCase):
    """test cases for component loading."""

    def setUp(self):
        self.options = [Option("--cls", cf=("config", "cls"),
                               action=ImportComponentAction),
                        Option("--module", action=ImportModuleAction)]

        self.result = argparse.Namespace()
        new_parser()
        self.parser = get_parser(components=[self], namespace=self.result)

    @make_config()
    def test_import_component(self, config_file):
        """load class components by importing."""
        self.parser.parse(["-C", config_file, "--cls", "One"])
        self.assertEqual(self.result.cls, One.One)

    @make_config()
    def test_import_module(self, config_file):
        """load module components by importing."""
        self.parser.parse(["-C", config_file, "--module", "One"])
        self.assertEqual(self.result.module, One)

    @make_config()
    def test_import_full_path(self, config_file):
        """load components by importing the full path."""
        self.parser.parse(["-C", config_file, "--cls", "os.path"])
        self.assertEqual(self.result.cls, os.path)

    @make_config()
    def test_import_bogus_class(self, config_file):
        """fail to load class component that cannot be imported."""
        self.assertRaises(SystemExit,
                          self.parser.parse,
                          ["-C", config_file, "--cls", "Three"])

    @make_config()
    def test_import_bogus_module(self, config_file):
        """fail to load module component that cannot be imported."""
        self.assertRaises(SystemExit,
                          self.parser.parse,
                          ["-C", config_file, "--module", "Three"])

    @make_config()
    def test_import_bogus_path(self, config_file):
        """fail to load component that cannot be imported by full path."""
        self.assertRaises(SystemExit,
                          self.parser.parse,
                          ["-C", config_file, "--cls", "Bcfg2.No.Such.Thing"])

    @make_config({"config": {"test": "foo", "cls": "Two"}})
    def test_default_from_config_for_component_options(self, config_file):
        """use default value from config file for options added by dynamic loaded component."""
        self.parser.parse(["-C", config_file])
        self.assertEqual(self.result.cls, Two.Two)
        self.assertEqual(self.result.test, "foo")

import os
import sys
import lxml.etree
from mock import Mock, MagicMock, patch
from Bcfg2.Server.Plugins.Cfg.CfgJinja2Generator import *

# add all parent testsuite directories to sys.path to allow (most)
# relative imports in python 2.4
path = os.path.dirname(__file__)
while path != "/":
    if os.path.basename(path).lower().startswith("test"):
        sys.path.append(path)
    if os.path.basename(path) == "testsuite":
        break
    path = os.path.dirname(path)
from common import *
from TestServer.TestPlugins.TestCfg.Test_init import TestCfgGenerator


class TestCfgJinja2Generator(TestCfgGenerator):
    test_obj = CfgJinja2Generator

    @skipUnless(HAS_JINJA2, "Jinja2 libraries not found, skipping")
    def setUp(self):
        TestCfgGenerator.setUp(self)
        set_setup_default("repository", datastore)

    def test__init(self):
        TestCfgGenerator.test__init(self)
        cgg = self.get_obj()
        self.assertIsInstance(cgg.loader, cgg.__loader_cls__)
        self.assertIsInstance(cgg.environment, cgg.__environment_cls__)

    @patch("Bcfg2.Server.Plugins.Cfg.CfgJinja2Generator.Environment")
    @patch("Bcfg2.Server.Plugins.Cfg.CfgJinja2Generator.get_template_data")
    def test_get_data(self, mock_get_template_data, mock_Environment):
        cgg = self.get_obj()
        entry = lxml.etree.Element("Path", name="/test.txt")
        metadata = Mock()

        # self.template is currently None
        self.assertRaises(PluginExecutionError,
                          cgg.get_data, entry, metadata)

        cgg.template = mock_Environment.return_value.get_template.return_value

        template_vars = dict(name=entry.get("name"),
                             metadata=metadata,
                             path=cgg.name,
                             source_path=cgg.name,
                             repo=datastore)
        mock_get_template_data.return_value = template_vars

        tmpl = mock_Environment.return_value.get_template.return_value
        self.assertEqual(cgg.get_data(entry, metadata),
                         tmpl.render.return_value)
        tmpl.render.assert_called_with(template_vars)

    def test_handle_event(self):
        cgg = self.get_obj()
        cgg.environment = Mock()
        event = Mock()
        cgg.handle_event(event)
        cgg.environment.get_template.assert_called_with(
            cgg.name)

        cgg.environment.reset_mock()
        cgg.environment.get_template.side_effect = OSError
        self.assertRaises(PluginExecutionError,
                          cgg.handle_event, event)
        cgg.environment.get_template.assert_called_with(
            cgg.name)

#!/usr/bin/env python

'''Build debian/ubuntu package indexes'''

# Original code from Bcfg2 sources

import gzip
import os
import sys
import subprocess

# Compatibility imports
from Bcfg2.Compat import StringIO
from Bcfg2.Compat import ConfigParser
from Bcfg2.Compat import urlopen

def debug(msg):
    '''print debug messages'''
    if '-v' in sys.argv:
        sys.stdout.write(msg)


def get_as_list(somestring):
    """ Input : a string like this : 'a, g, f,w'
        Output : a list like this : ['a', 'g', 'f', 'w'] """
    return somestring.replace(' ', '').split(',')


def list_contains_all_the_same_values(l):
    if len(l) == 0:
        return True
    # The list contains all the same values if all elements in
    # the list are equal to the first element.
    first = l[0]
    for elem in l:
        if first != elem:
            return False
    return True


class SourceURL:
    def __init__(self, deb_url):
        deb_url_tokens = deb_url.split()
        # ex: deb http://somemirror.com/ubuntu dapper main restricted universe
        self.url = deb_url_tokens[1]
        self.distribution = deb_url_tokens[2]
        self.sections = deb_url_tokens[3:]

    def __str__(self):
        return "deb %s %s %s" % (self.url, self.distribution, ' '.join(self.sections))

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))


class Source:
    def __init__(self, confparser, section, bcfg2_repos_prefix):
        self.filename = "%s/Pkgmgr/%s.xml" % (bcfg2_repos_prefix, section)
        self.groups = get_as_list(confparser.get(section, "group_names"))
        self.priority = confparser.getint(section, "priority")
        self.architectures = get_as_list(confparser.get(section, "architectures"))

        self.source_urls = []
        self.source_urls.append(SourceURL(confparser.get(section, "deb_url")))
        # Agregate urls in the form of deb_url0, deb_url1, ... to deb_url9
        for i in range(10):  # 0 to 9
            option_name = "deb_url%s" % i
            if confparser.has_option(section, option_name):
                self.source_urls.append(SourceURL(confparser.get(section, option_name)))

        self.file = None
        self.indent_level = 0

    def __str__(self):
        return """File: %s
Groups: %s
Priority: %s
Architectures: %s
Source URLS: %s""" % (self.filename, self.groups, self.priority, self.architectures, self.source_urls)

    def __repr__(self):
        return "<%s %s>" % (self.__class__.__name__, str(self))

    def _open_file(self):
        self.file = open(self.filename + '~', 'w')

    def _close_file(self):
        self.file.close()

    def _write_to_file(self, msg):
        self.file.write("%s%s\n" % (self.indent_level * '    ', msg))

    def _rename_file(self):
        os.rename(self.filename + '~', self.filename)

    def _pkg_version_is_older(self, version1, version2):
        """ Use dpkg to compare the two version
            Return true if version1 < version2 """
        # Avoid forking a new process if the two strings are equals
        if version1 == version2:
            return False
        (status, output) = subprocess.getstatusoutput("/usr/bin/dpkg --compare-versions %s lt %s" % (version1,
                                                                                                     version2))
        #print "%s dpkg --compare-versions %s lt %s" % (status, version1, version2)
        return status == 0

    def _update_pkgdata(self, pkgdata, source_url):
        for section in source_url.sections:
            for arch in self.architectures:
                url = "%s/dists/%s/%s/binary-%s/Packages.gz" % (source_url.url, source_url.distribution, section, arch)
                debug("Processing url %s\n" % (url))
                try:
                    data = urlopen(url)
                    buf = StringIO(''.join(data.readlines()))
                    reader = gzip.GzipFile(fileobj=buf)
                    for line in reader.readlines():
                        if line[:8] == 'Package:':
                            pkgname = line.split(' ')[1].strip()
                        elif line[:8] == 'Version:':
                            version = line.split(' ')[1].strip()
                            if pkgname in pkgdata:
                                if arch in pkgdata[pkgname]:
                                    # The package is listed twice for the same architecture
                                    # We keep the most recent version
                                    old_version = pkgdata[pkgname][arch]
                                    if self._pkg_version_is_older(old_version, version):
                                        pkgdata[pkgname][arch] = version
                                else:
                                    # The package data exists for another architecture,
                                    # but not for this one. Add it.
                                    pkgdata[pkgname][arch] = version
                            else:
                                # First entry for this package
                                pkgdata[pkgname] = {arch: version}
                        else:
                            continue
                except:
                    raise Exception("Could not process URL %s\n%s\nPlease "
                                    "verify the URL." % (url, sys.exc_info()[1]))
        return pkgdata

    def _get_sorted_pkg_keys(self, pkgdata):
        pkgs = []
        for k in list(pkgdata.keys()):
            pkgs.append(k)
        pkgs.sort()
        return pkgs

    def _write_common_entries(self, pkgdata):
        # Write entries for packages that have the same version
        # across all architectures
        #coalesced = 0
        for pkg in self._get_sorted_pkg_keys(pkgdata):
            # Dictionary of archname: pkgversion
            # (There is exactly one version per architecture)
            archdata = pkgdata[pkg]
            # List of versions for all architectures of this package
            pkgversions = list(archdata.values())
            # If the versions for all architectures are the same
            if list_contains_all_the_same_values(pkgversions):
                # Write the package data
                ver = pkgversions[0]
                self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, ver))
                #coalesced += 1
                # Remove this package entry
                del pkgdata[pkg]

    def _write_perarch_entries(self, pkgdata):
        # Write entries that are left, i.e. packages that have different
        # versions per architecture
        #perarch = 0
        if pkgdata:
            for arch in self.architectures:
                self._write_to_file('<Group name="%s">' % (arch))
                self.indent_level = self.indent_level + 1
                for pkg in self._get_sorted_pkg_keys(pkgdata):
                    if arch in pkgdata[pkg]:
                        self._write_to_file('<Package name="%s" version="%s"/>' % (pkg, pkgdata[pkg][arch]))
                        #perarch += 1
                self.indent_level = self.indent_level - 1
                self._write_to_file('</Group>')
        #debug("Got %s coalesced, %s per-arch\n" % (coalesced, perarch))

    def process(self):
        '''Build package indices for source'''

        # First, build the pkgdata structure without touching the file,
        # so the file does not contain incomplete informations if the
        # network in not reachable.
        pkgdata = {}
        for source_url in self.source_urls:
            pkgdata = self._update_pkgdata(pkgdata, source_url)

        # Construct the file.
        self._open_file()
        for source_url in self.source_urls:
            self._write_to_file('<!-- %s -->' % source_url)

        self._write_to_file('<PackageList priority="%s" type="deb">' % self.priority)

        self.indent_level = self.indent_level + 1
        for group in self.groups:
            self._write_to_file('<Group name="%s">' % group)
            self.indent_level = self.indent_level + 1

        self._write_common_entries(pkgdata)
        self._write_perarch_entries(pkgdata)

        for group in self.groups:
            self.indent_level = self.indent_level - 1
            self._write_to_file('</Group>')
        self.indent_level = self.indent_level - 1
        self._write_to_file('</PackageList>')
        self._close_file()
        self._rename_file()

if __name__ == '__main__':
    main_conf_parser = ConfigParser.SafeConfigParser()
    main_conf_parser.read(['/etc/bcfg2.conf'])
    repo = main_conf_parser.get('server', 'repository')

    confparser = ConfigParser.SafeConfigParser()
    confparser.read(os.path.join(repo, "etc/debian-pkglist.conf"))

    # We read the whole configuration file before processing each entries
    # to avoid doing work if there is a problem in the file.
    sources_list = []
    for section in confparser.sections():
        sources_list.append(Source(confparser, section, repo))

    for source in sources_list:
        source.process()

from django.db import models

from vote.compat import AUTH_USER_MODEL
from vote.managers import VotableManager

# Create your models here.
class Comment(models.Model):
    user = models.ForeignKey(AUTH_USER_MODEL)
    content = models.TextField()
    create_at = models.DateTimeField(auto_now_add=True)
    update_at = models.DateTimeField(auto_now=True)

    votes = VotableManager()

class CustomVoteComment(models.Model):
    user = models.ForeignKey(AUTH_USER_MODEL)
    content = models.TextField()
    create_at = models.DateTimeField(auto_now_add=True)
    update_at = models.DateTimeField(auto_now=True)

    custom_votes = VotableManager()


__author__ = 'Matteo'

from unittest import TestCase

from addic7ed_cli.episode import Episode, search
from addic7ed_cli.util import file_to_query, normalize_release, parse_release


def s(*args):
    return set(args)


class TestAddic7ed(TestCase):

    maxDiff = None

    def test_search(self):
        result = search('homeland 2x02')
        self.assertEqual(result, [
            Episode(
                'http://www.addic7ed.com/serie/Homeland/2/2/Beirut_Is_Back',
                'Homeland - 02x02 - Beirut Is Back')
        ])

    def test_search_multiple(self):
        result = search('black mirror 01x')
        self.assertEqual(result, [
            Episode(
                'serie/Black_Mirror_%25282011%2529/1/1/The_National_Anthem',
                'Black Mirror (2011) - 01x01 - The National Anthem'),
            Episode(
                'serie/Black_Mirror_%25282011%2529/1/2/15_Million_Merits',
                'Black Mirror (2011) - 01x02 - 15 Million Merits'),
            Episode(
                'serie/Black_Mirror_%25282011%2529/1/3/'
                'The_Entire_History_of_You',
                'Black Mirror (2011) - 01x03 - The Entire History of You'),
        ])

    def file_to_query(self, filename, query, version=set()):
        q, v = file_to_query(filename)
        self.assertEqual(query, q)
        self.assertEqual(version, v)

    def test_file_to_query(self):
        self.file_to_query('Homeland.S02E02.PROPER.720p.HDTV.x264-EVOLVE.mkv',
                           'homeland 2x02',
                           s('proper', 'evolve'))
        self.file_to_query('CSI.S13E06.720p.HDTV.X264-DIMENSION.mkv',
                           'csi 13x06',
                           s('dimension', 'sys', 'lol'))
        self.file_to_query('Youre.the.Worst.S02E02.720p.HDTV.X264-DIMENSION[EtHD].mkv',
                           "you're the worst 2x02",
                           s('dimension', 'sys', 'lol'))

    def test_file_to_query_stopword(self):
        self.file_to_query('Foo.and.Bar.S02E23.PLOP.mkv',
                           'foo bar 2x23',
                           s('plop'))

    def test_file_to_query_exceptions(self):
        self.file_to_query('CSI.New.York.S09E10.720p.HDTV.X264-YOLO.mkv',
                           'csi ny 9x10',
                           s('yolo'))

    def test_file_to_query_number_in_title(self):
        self.file_to_query('Dont.Apartment.23.S02E05.720p.HDTV.X264'
                           '-DIMENSION.mkv',
                           'don\'t apartment 23 2x05',
                           s('dimension', 'sys', 'lol'))

    def test_file_to_query_noseason(self):
        self.file_to_query('Foo.23.mkv', 'foo 23')

    def test_file_to_query_nonumber(self):
        self.file_to_query('Foo bar.mkv', 'foo bar', s('foo', 'bar'))

    def test_file_to_query_threenumbers(self):
        self.file_to_query('The.Serie.223.MDR.mkv', 'the serie 2x23', s('mdr'))
        self.file_to_query('hannibal.210.hdtv-lol', 'hannibal 2x10')

    def test_file_to_query_fournumbers(self):
        self.file_to_query('The.Serie.1234.MDR.mkv',
                           'the serie 12x34',
                           s('mdr'))
        self.file_to_query('hannibal.1234.hdtv-lol', 'hannibal 12x34')

    def test_file_to_query_season_episode(self):
        self.file_to_query('The Serie Season 4 Episode 03 - Foo',
                           'the serie 4x03', s('foo'))

    def test_episode(self):
        result = Episode('serie/Homeland/2/2/Beirut_Is_Back')
        result.fetch_versions()
        self.assertEqual(result.title, 'Homeland - 02x02 - Beirut Is Back')
        versions = result.filter_versions(['english', 'french'], s('evolve'))
        self.assertEqual('English', versions[1].language)
        self.assertEqual('/original/67365/2', versions[1].url)
        self.assertFalse(versions[0].hearing_impaired)
        self.assertTrue(versions[1].hearing_impaired)

    def test_unicode_episode(self):
        search('family guy 10x12')[0].fetch_versions()

        # doing another query after that should not raise any exception
        search('family guy 10x11')

    def test_normalize_release(self):
        self.assertEqual(s('immerse', 'asap', 'xii'),
                         normalize_release(s('immerse', '720p')))

        self.assertEqual(s('lol', 'sys', 'dimension'),
                         normalize_release(s('lol')))
        self.assertEqual(s('mdr'), normalize_release(s('mdr')))

    def test_parse_release(self):
        self.assertEqual(s('webdl', 'bs'),
                         parse_release('WEBDL-BS Resync from DIMENSION.'))
        self.assertEqual(s('webdl', 'bs'),
                         parse_release('WEB-DL-BS'))

######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

# Big5 frequency table
# by Taiwan's Mandarin Promotion Council
# <http://www.edu.tw:81/mandr/>
#
# 128  --> 0.42261
# 256  --> 0.57851
# 512  --> 0.74851
# 1024 --> 0.89384
# 2048 --> 0.97583
#
# Ideal Distribution Ratio = 0.74851/(1-0.74851) =2.98
# Random Distribution Ration = 512/(5401-512)=0.105
#
# Typical Distribution Ratio about 25% of Ideal one, still much higher than RDR

BIG5_TYPICAL_DISTRIBUTION_RATIO = 0.75

#Char to FreqOrder table
BIG5_TABLE_SIZE = 5376

Big5CharToFreqOrder = (
   1,1801,1506, 255,1431, 198,   9,  82,   6,5008, 177, 202,3681,1256,2821, 110, #   16
3814,  33,3274, 261,  76,  44,2114,  16,2946,2187,1176, 659,3971,  26,3451,2653, #   32
1198,3972,3350,4202, 410,2215, 302, 590, 361,1964,   8, 204,  58,4510,5009,1932, #   48
  63,5010,5011, 317,1614,  75, 222, 159,4203,2417,1480,5012,3555,3091, 224,2822, #   64
3682,   3,  10,3973,1471,  29,2787,1135,2866,1940, 873, 130,3275,1123, 312,5013, #   80
4511,2052, 507, 252, 682,5014, 142,1915, 124, 206,2947,  34,3556,3204,  64, 604, #   96
5015,2501,1977,1978, 155,1991, 645, 641,1606,5016,3452, 337,  72, 406,5017,  80, #  112
 630, 238,3205,1509, 263, 939,1092,2654, 756,1440,1094,3453, 449,  69,2987, 591, #  128
 179,2096, 471, 115,2035,1844,  60,  50,2988, 134, 806,1869, 734,2036,3454, 180, #  144
 995,1607, 156, 537,2907, 688,5018, 319,1305, 779,2145, 514,2379, 298,4512, 359, #  160
2502,  90,2716,1338, 663,  11, 906,1099,2553,  20,2441, 182, 532,1716,5019, 732, #  176
1376,4204,1311,1420,3206,  25,2317,1056, 113, 399, 382,1950, 242,3455,2474, 529, #  192
3276, 475,1447,3683,5020, 117,  21, 656, 810,1297,2300,2334,3557,5021, 126,4205, #  208
 706, 456, 150, 613,4513,  71,1118,2037,4206, 145,3092,  85, 835, 486,2115,1246, #  224
1426, 428, 727,1285,1015, 800, 106, 623, 303,1281,5022,2128,2359, 347,3815, 221, #  240
3558,3135,5023,1956,1153,4207,  83, 296,1199,3093, 192, 624,  93,5024, 822,1898, #  256
2823,3136, 795,2065, 991,1554,1542,1592,  27,  43,2867, 859, 139,1456, 860,4514, #  272
 437, 712,3974, 164,2397,3137, 695, 211,3037,2097, 195,3975,1608,3559,3560,3684, #  288
3976, 234, 811,2989,2098,3977,2233,1441,3561,1615,2380, 668,2077,1638, 305, 228, #  304
1664,4515, 467, 415,5025, 262,2099,1593, 239, 108, 300, 200,1033, 512,1247,2078, #  320
5026,5027,2176,3207,3685,2682, 593, 845,1062,3277,  88,1723,2038,3978,1951, 212, #  336
 266, 152, 149, 468,1899,4208,4516,  77, 187,5028,3038,  37,   5,2990,5029,3979, #  352
5030,5031,  39,2524,4517,2908,3208,2079,  55, 148,  74,4518, 545, 483,1474,1029, #  368
1665, 217,1870,1531,3138,1104,2655,4209,  24, 172,3562, 900,3980,3563,3564,4519, #  384
  32,1408,2824,1312, 329, 487,2360,2251,2717, 784,2683,   4,3039,3351,1427,1789, #  400
 188, 109, 499,5032,3686,1717,1790, 888,1217,3040,4520,5033,3565,5034,3352,1520, #  416
3687,3981, 196,1034, 775,5035,5036, 929,1816, 249, 439,  38,5037,1063,5038, 794, #  432
3982,1435,2301,  46, 178,3278,2066,5039,2381,5040, 214,1709,4521, 804,  35, 707, #  448
 324,3688,1601,2554, 140, 459,4210,5041,5042,1365, 839, 272, 978,2262,2580,3456, #  464
2129,1363,3689,1423, 697, 100,3094,  48,  70,1231, 495,3139,2196,5043,1294,5044, #  480
2080, 462, 586,1042,3279, 853, 256, 988, 185,2382,3457,1698, 434,1084,5045,3458, #  496
 314,2625,2788,4522,2335,2336, 569,2285, 637,1817,2525, 757,1162,1879,1616,3459, #  512
 287,1577,2116, 768,4523,1671,2868,3566,2526,1321,3816, 909,2418,5046,4211, 933, #  528
3817,4212,2053,2361,1222,4524, 765,2419,1322, 786,4525,5047,1920,1462,1677,2909, #  544
1699,5048,4526,1424,2442,3140,3690,2600,3353,1775,1941,3460,3983,4213, 309,1369, #  560
1130,2825, 364,2234,1653,1299,3984,3567,3985,3986,2656, 525,1085,3041, 902,2001, #  576
1475, 964,4527, 421,1845,1415,1057,2286, 940,1364,3141, 376,4528,4529,1381,   7, #  592
2527, 983,2383, 336,1710,2684,1846, 321,3461, 559,1131,3042,2752,1809,1132,1313, #  608
 265,1481,1858,5049, 352,1203,2826,3280, 167,1089, 420,2827, 776, 792,1724,3568, #  624
4214,2443,3281,5050,4215,5051, 446, 229, 333,2753, 901,3818,1200,1557,4530,2657, #  640
1921, 395,2754,2685,3819,4216,1836, 125, 916,3209,2626,4531,5052,5053,3820,5054, #  656
5055,5056,4532,3142,3691,1133,2555,1757,3462,1510,2318,1409,3569,5057,2146, 438, #  672
2601,2910,2384,3354,1068, 958,3043, 461, 311,2869,2686,4217,1916,3210,4218,1979, #  688
 383, 750,2755,2627,4219, 274, 539, 385,1278,1442,5058,1154,1965, 384, 561, 210, #  704
  98,1295,2556,3570,5059,1711,2420,1482,3463,3987,2911,1257, 129,5060,3821, 642, #  720
 523,2789,2790,2658,5061, 141,2235,1333,  68, 176, 441, 876, 907,4220, 603,2602, #  736
 710, 171,3464, 404, 549,  18,3143,2398,1410,3692,1666,5062,3571,4533,2912,4534, #  752
5063,2991, 368,5064, 146, 366,  99, 871,3693,1543, 748, 807,1586,1185,  22,2263, #  768
 379,3822,3211,5065,3212, 505,1942,2628,1992,1382,2319,5066, 380,2362, 218, 702, #  784
1818,1248,3465,3044,3572,3355,3282,5067,2992,3694, 930,3283,3823,5068,  59,5069, #  800
 585, 601,4221, 497,3466,1112,1314,4535,1802,5070,1223,1472,2177,5071, 749,1837, #  816
 690,1900,3824,1773,3988,1476, 429,1043,1791,2236,2117, 917,4222, 447,1086,1629, #  832
5072, 556,5073,5074,2021,1654, 844,1090, 105, 550, 966,1758,2828,1008,1783, 686, #  848
1095,5075,2287, 793,1602,5076,3573,2603,4536,4223,2948,2302,4537,3825, 980,2503, #  864
 544, 353, 527,4538, 908,2687,2913,5077, 381,2629,1943,1348,5078,1341,1252, 560, #  880
3095,5079,3467,2870,5080,2054, 973, 886,2081, 143,4539,5081,5082, 157,3989, 496, #  896
4224,  57, 840, 540,2039,4540,4541,3468,2118,1445, 970,2264,1748,1966,2082,4225, #  912
3144,1234,1776,3284,2829,3695, 773,1206,2130,1066,2040,1326,3990,1738,1725,4226, #  928
 279,3145,  51,1544,2604, 423,1578,2131,2067, 173,4542,1880,5083,5084,1583, 264, #  944
 610,3696,4543,2444, 280, 154,5085,5086,5087,1739, 338,1282,3096, 693,2871,1411, #  960
1074,3826,2445,5088,4544,5089,5090,1240, 952,2399,5091,2914,1538,2688, 685,1483, #  976
4227,2475,1436, 953,4228,2055,4545, 671,2400,  79,4229,2446,3285, 608, 567,2689, #  992
3469,4230,4231,1691, 393,1261,1792,2401,5092,4546,5093,5094,5095,5096,1383,1672, # 1008
3827,3213,1464, 522,1119, 661,1150, 216, 675,4547,3991,1432,3574, 609,4548,2690, # 1024
2402,5097,5098,5099,4232,3045,   0,5100,2476, 315, 231,2447, 301,3356,4549,2385, # 1040
5101, 233,4233,3697,1819,4550,4551,5102,  96,1777,1315,2083,5103, 257,5104,1810, # 1056
3698,2718,1139,1820,4234,2022,1124,2164,2791,1778,2659,5105,3097, 363,1655,3214, # 1072
5106,2993,5107,5108,5109,3992,1567,3993, 718, 103,3215, 849,1443, 341,3357,2949, # 1088
1484,5110,1712, 127,  67, 339,4235,2403, 679,1412, 821,5111,5112, 834, 738, 351, # 1104
2994,2147, 846, 235,1497,1881, 418,1993,3828,2719, 186,1100,2148,2756,3575,1545, # 1120
1355,2950,2872,1377, 583,3994,4236,2581,2995,5113,1298,3699,1078,2557,3700,2363, # 1136
  78,3829,3830, 267,1289,2100,2002,1594,4237, 348, 369,1274,2197,2178,1838,4552, # 1152
1821,2830,3701,2757,2288,2003,4553,2951,2758, 144,3358, 882,4554,3995,2759,3470, # 1168
4555,2915,5114,4238,1726, 320,5115,3996,3046, 788,2996,5116,2831,1774,1327,2873, # 1184
3997,2832,5117,1306,4556,2004,1700,3831,3576,2364,2660, 787,2023, 506, 824,3702, # 1200
 534, 323,4557,1044,3359,2024,1901, 946,3471,5118,1779,1500,1678,5119,1882,4558, # 1216
 165, 243,4559,3703,2528, 123, 683,4239, 764,4560,  36,3998,1793, 589,2916, 816, # 1232
 626,1667,3047,2237,1639,1555,1622,3832,3999,5120,4000,2874,1370,1228,1933, 891, # 1248
2084,2917, 304,4240,5121, 292,2997,2720,3577, 691,2101,4241,1115,4561, 118, 662, # 1264
5122, 611,1156, 854,2386,1316,2875,   2, 386, 515,2918,5123,5124,3286, 868,2238, # 1280
1486, 855,2661, 785,2216,3048,5125,1040,3216,3578,5126,3146, 448,5127,1525,5128, # 1296
2165,4562,5129,3833,5130,4242,2833,3579,3147, 503, 818,4001,3148,1568, 814, 676, # 1312
1444, 306,1749,5131,3834,1416,1030, 197,1428, 805,2834,1501,4563,5132,5133,5134, # 1328
1994,5135,4564,5136,5137,2198,  13,2792,3704,2998,3149,1229,1917,5138,3835,2132, # 1344
5139,4243,4565,2404,3580,5140,2217,1511,1727,1120,5141,5142, 646,3836,2448, 307, # 1360
5143,5144,1595,3217,5145,5146,5147,3705,1113,1356,4002,1465,2529,2530,5148, 519, # 1376
5149, 128,2133,  92,2289,1980,5150,4003,1512, 342,3150,2199,5151,2793,2218,1981, # 1392
3360,4244, 290,1656,1317, 789, 827,2365,5152,3837,4566, 562, 581,4004,5153, 401, # 1408
4567,2252,  94,4568,5154,1399,2794,5155,1463,2025,4569,3218,1944,5156, 828,1105, # 1424
4245,1262,1394,5157,4246, 605,4570,5158,1784,2876,5159,2835, 819,2102, 578,2200, # 1440
2952,5160,1502, 436,3287,4247,3288,2836,4005,2919,3472,3473,5161,2721,2320,5162, # 1456
5163,2337,2068,  23,4571, 193, 826,3838,2103, 699,1630,4248,3098, 390,1794,1064, # 1472
3581,5164,1579,3099,3100,1400,5165,4249,1839,1640,2877,5166,4572,4573, 137,4250, # 1488
 598,3101,1967, 780, 104, 974,2953,5167, 278, 899, 253, 402, 572, 504, 493,1339, # 1504
5168,4006,1275,4574,2582,2558,5169,3706,3049,3102,2253, 565,1334,2722, 863,  41, # 1520
5170,5171,4575,5172,1657,2338,  19, 463,2760,4251, 606,5173,2999,3289,1087,2085, # 1536
1323,2662,3000,5174,1631,1623,1750,4252,2691,5175,2878, 791,2723,2663,2339, 232, # 1552
2421,5176,3001,1498,5177,2664,2630, 755,1366,3707,3290,3151,2026,1609, 119,1918, # 1568
3474, 862,1026,4253,5178,4007,3839,4576,4008,4577,2265,1952,2477,5179,1125, 817, # 1584
4254,4255,4009,1513,1766,2041,1487,4256,3050,3291,2837,3840,3152,5180,5181,1507, # 1600
5182,2692, 733,  40,1632,1106,2879, 345,4257, 841,2531, 230,4578,3002,1847,3292, # 1616
3475,5183,1263, 986,3476,5184, 735, 879, 254,1137, 857, 622,1300,1180,1388,1562, # 1632
4010,4011,2954, 967,2761,2665,1349, 592,2134,1692,3361,3003,1995,4258,1679,4012, # 1648
1902,2188,5185, 739,3708,2724,1296,1290,5186,4259,2201,2202,1922,1563,2605,2559, # 1664
1871,2762,3004,5187, 435,5188, 343,1108, 596,  17,1751,4579,2239,3477,3709,5189, # 1680
4580, 294,3582,2955,1693, 477, 979, 281,2042,3583, 643,2043,3710,2631,2795,2266, # 1696
1031,2340,2135,2303,3584,4581, 367,1249,2560,5190,3585,5191,4582,1283,3362,2005, # 1712
 240,1762,3363,4583,4584, 836,1069,3153, 474,5192,2149,2532, 268,3586,5193,3219, # 1728
1521,1284,5194,1658,1546,4260,5195,3587,3588,5196,4261,3364,2693,1685,4262, 961, # 1744
1673,2632, 190,2006,2203,3841,4585,4586,5197, 570,2504,3711,1490,5198,4587,2633, # 1760
3293,1957,4588, 584,1514, 396,1045,1945,5199,4589,1968,2449,5200,5201,4590,4013, # 1776
 619,5202,3154,3294, 215,2007,2796,2561,3220,4591,3221,4592, 763,4263,3842,4593, # 1792
5203,5204,1958,1767,2956,3365,3712,1174, 452,1477,4594,3366,3155,5205,2838,1253, # 1808
2387,2189,1091,2290,4264, 492,5206, 638,1169,1825,2136,1752,4014, 648, 926,1021, # 1824
1324,4595, 520,4596, 997, 847,1007, 892,4597,3843,2267,1872,3713,2405,1785,4598, # 1840
1953,2957,3103,3222,1728,4265,2044,3714,4599,2008,1701,3156,1551,  30,2268,4266, # 1856
5207,2027,4600,3589,5208, 501,5209,4267, 594,3478,2166,1822,3590,3479,3591,3223, # 1872
 829,2839,4268,5210,1680,3157,1225,4269,5211,3295,4601,4270,3158,2341,5212,4602, # 1888
4271,5213,4015,4016,5214,1848,2388,2606,3367,5215,4603, 374,4017, 652,4272,4273, # 1904
 375,1140, 798,5216,5217,5218,2366,4604,2269, 546,1659, 138,3051,2450,4605,5219, # 1920
2254, 612,1849, 910, 796,3844,1740,1371, 825,3845,3846,5220,2920,2562,5221, 692, # 1936
 444,3052,2634, 801,4606,4274,5222,1491, 244,1053,3053,4275,4276, 340,5223,4018, # 1952
1041,3005, 293,1168,  87,1357,5224,1539, 959,5225,2240, 721, 694,4277,3847, 219, # 1968
1478, 644,1417,3368,2666,1413,1401,1335,1389,4019,5226,5227,3006,2367,3159,1826, # 1984
 730,1515, 184,2840,  66,4607,5228,1660,2958, 246,3369, 378,1457, 226,3480, 975, # 2000
4020,2959,1264,3592, 674, 696,5229, 163,5230,1141,2422,2167, 713,3593,3370,4608, # 2016
4021,5231,5232,1186,  15,5233,1079,1070,5234,1522,3224,3594, 276,1050,2725, 758, # 2032
1126, 653,2960,3296,5235,2342, 889,3595,4022,3104,3007, 903,1250,4609,4023,3481, # 2048
3596,1342,1681,1718, 766,3297, 286,  89,2961,3715,5236,1713,5237,2607,3371,3008, # 2064
5238,2962,2219,3225,2880,5239,4610,2505,2533, 181, 387,1075,4024, 731,2190,3372, # 2080
5240,3298, 310, 313,3482,2304, 770,4278,  54,3054, 189,4611,3105,3848,4025,5241, # 2096
1230,1617,1850, 355,3597,4279,4612,3373, 111,4280,3716,1350,3160,3483,3055,4281, # 2112
2150,3299,3598,5242,2797,4026,4027,3009, 722,2009,5243,1071, 247,1207,2343,2478, # 2128
1378,4613,2010, 864,1437,1214,4614, 373,3849,1142,2220, 667,4615, 442,2763,2563, # 2144
3850,4028,1969,4282,3300,1840, 837, 170,1107, 934,1336,1883,5244,5245,2119,4283, # 2160
2841, 743,1569,5246,4616,4284, 582,2389,1418,3484,5247,1803,5248, 357,1395,1729, # 2176
3717,3301,2423,1564,2241,5249,3106,3851,1633,4617,1114,2086,4285,1532,5250, 482, # 2192
2451,4618,5251,5252,1492, 833,1466,5253,2726,3599,1641,2842,5254,1526,1272,3718, # 2208
4286,1686,1795, 416,2564,1903,1954,1804,5255,3852,2798,3853,1159,2321,5256,2881, # 2224
4619,1610,1584,3056,2424,2764, 443,3302,1163,3161,5257,5258,4029,5259,4287,2506, # 2240
3057,4620,4030,3162,2104,1647,3600,2011,1873,4288,5260,4289, 431,3485,5261, 250, # 2256
  97,  81,4290,5262,1648,1851,1558, 160, 848,5263, 866, 740,1694,5264,2204,2843, # 2272
3226,4291,4621,3719,1687, 950,2479, 426, 469,3227,3720,3721,4031,5265,5266,1188, # 2288
 424,1996, 861,3601,4292,3854,2205,2694, 168,1235,3602,4293,5267,2087,1674,4622, # 2304
3374,3303, 220,2565,1009,5268,3855, 670,3010, 332,1208, 717,5269,5270,3603,2452, # 2320
4032,3375,5271, 513,5272,1209,2882,3376,3163,4623,1080,5273,5274,5275,5276,2534, # 2336
3722,3604, 815,1587,4033,4034,5277,3605,3486,3856,1254,4624,1328,3058,1390,4035, # 2352
1741,4036,3857,4037,5278, 236,3858,2453,3304,5279,5280,3723,3859,1273,3860,4625, # 2368
5281, 308,5282,4626, 245,4627,1852,2480,1307,2583, 430, 715,2137,2454,5283, 270, # 2384
 199,2883,4038,5284,3606,2727,1753, 761,1754, 725,1661,1841,4628,3487,3724,5285, # 2400
5286, 587,  14,3305, 227,2608, 326, 480,2270, 943,2765,3607, 291, 650,1884,5287, # 2416
1702,1226, 102,1547,  62,3488, 904,4629,3489,1164,4294,5288,5289,1224,1548,2766, # 2432
 391, 498,1493,5290,1386,1419,5291,2056,1177,4630, 813, 880,1081,2368, 566,1145, # 2448
4631,2291,1001,1035,2566,2609,2242, 394,1286,5292,5293,2069,5294,  86,1494,1730, # 2464
4039, 491,1588, 745, 897,2963, 843,3377,4040,2767,2884,3306,1768, 998,2221,2070, # 2480
 397,1827,1195,1970,3725,3011,3378, 284,5295,3861,2507,2138,2120,1904,5296,4041, # 2496
2151,4042,4295,1036,3490,1905, 114,2567,4296, 209,1527,5297,5298,2964,2844,2635, # 2512
2390,2728,3164, 812,2568,5299,3307,5300,1559, 737,1885,3726,1210, 885,  28,2695, # 2528
3608,3862,5301,4297,1004,1780,4632,5302, 346,1982,2222,2696,4633,3863,1742, 797, # 2544
1642,4043,1934,1072,1384,2152, 896,4044,3308,3727,3228,2885,3609,5303,2569,1959, # 2560
4634,2455,1786,5304,5305,5306,4045,4298,1005,1308,3728,4299,2729,4635,4636,1528, # 2576
2610, 161,1178,4300,1983, 987,4637,1101,4301, 631,4046,1157,3229,2425,1343,1241, # 2592
1016,2243,2570, 372, 877,2344,2508,1160, 555,1935, 911,4047,5307, 466,1170, 169, # 2608
1051,2921,2697,3729,2481,3012,1182,2012,2571,1251,2636,5308, 992,2345,3491,1540, # 2624
2730,1201,2071,2406,1997,2482,5309,4638, 528,1923,2191,1503,1874,1570,2369,3379, # 2640
3309,5310, 557,1073,5311,1828,3492,2088,2271,3165,3059,3107, 767,3108,2799,4639, # 2656
1006,4302,4640,2346,1267,2179,3730,3230, 778,4048,3231,2731,1597,2667,5312,4641, # 2672
5313,3493,5314,5315,5316,3310,2698,1433,3311, 131,  95,1504,4049, 723,4303,3166, # 2688
1842,3610,2768,2192,4050,2028,2105,3731,5317,3013,4051,1218,5318,3380,3232,4052, # 2704
4304,2584, 248,1634,3864, 912,5319,2845,3732,3060,3865, 654,  53,5320,3014,5321, # 2720
1688,4642, 777,3494,1032,4053,1425,5322, 191, 820,2121,2846, 971,4643, 931,3233, # 2736
 135, 664, 783,3866,1998, 772,2922,1936,4054,3867,4644,2923,3234, 282,2732, 640, # 2752
1372,3495,1127, 922, 325,3381,5323,5324, 711,2045,5325,5326,4055,2223,2800,1937, # 2768
4056,3382,2224,2255,3868,2305,5327,4645,3869,1258,3312,4057,3235,2139,2965,4058, # 2784
4059,5328,2225, 258,3236,4646, 101,1227,5329,3313,1755,5330,1391,3314,5331,2924, # 2800
2057, 893,5332,5333,5334,1402,4305,2347,5335,5336,3237,3611,5337,5338, 878,1325, # 2816
1781,2801,4647, 259,1385,2585, 744,1183,2272,4648,5339,4060,2509,5340, 684,1024, # 2832
4306,5341, 472,3612,3496,1165,3315,4061,4062, 322,2153, 881, 455,1695,1152,1340, # 2848
 660, 554,2154,4649,1058,4650,4307, 830,1065,3383,4063,4651,1924,5342,1703,1919, # 2864
5343, 932,2273, 122,5344,4652, 947, 677,5345,3870,2637, 297,1906,1925,2274,4653, # 2880
2322,3316,5346,5347,4308,5348,4309,  84,4310, 112, 989,5349, 547,1059,4064, 701, # 2896
3613,1019,5350,4311,5351,3497, 942, 639, 457,2306,2456, 993,2966, 407, 851, 494, # 2912
4654,3384, 927,5352,1237,5353,2426,3385, 573,4312, 680, 921,2925,1279,1875, 285, # 2928
 790,1448,1984, 719,2168,5354,5355,4655,4065,4066,1649,5356,1541, 563,5357,1077, # 2944
5358,3386,3061,3498, 511,3015,4067,4068,3733,4069,1268,2572,3387,3238,4656,4657, # 2960
5359, 535,1048,1276,1189,2926,2029,3167,1438,1373,2847,2967,1134,2013,5360,4313, # 2976
1238,2586,3109,1259,5361, 700,5362,2968,3168,3734,4314,5363,4315,1146,1876,1907, # 2992
4658,2611,4070, 781,2427, 132,1589, 203, 147, 273,2802,2407, 898,1787,2155,4071, # 3008
4072,5364,3871,2803,5365,5366,4659,4660,5367,3239,5368,1635,3872, 965,5369,1805, # 3024
2699,1516,3614,1121,1082,1329,3317,4073,1449,3873,  65,1128,2848,2927,2769,1590, # 3040
3874,5370,5371,  12,2668,  45, 976,2587,3169,4661, 517,2535,1013,1037,3240,5372, # 3056
3875,2849,5373,3876,5374,3499,5375,2612, 614,1999,2323,3877,3110,2733,2638,5376, # 3072
2588,4316, 599,1269,5377,1811,3735,5378,2700,3111, 759,1060, 489,1806,3388,3318, # 3088
1358,5379,5380,2391,1387,1215,2639,2256, 490,5381,5382,4317,1759,2392,2348,5383, # 3104
4662,3878,1908,4074,2640,1807,3241,4663,3500,3319,2770,2349, 874,5384,5385,3501, # 3120
3736,1859,  91,2928,3737,3062,3879,4664,5386,3170,4075,2669,5387,3502,1202,1403, # 3136
3880,2969,2536,1517,2510,4665,3503,2511,5388,4666,5389,2701,1886,1495,1731,4076, # 3152
2370,4667,5390,2030,5391,5392,4077,2702,1216, 237,2589,4318,2324,4078,3881,4668, # 3168
4669,2703,3615,3504, 445,4670,5393,5394,5395,5396,2771,  61,4079,3738,1823,4080, # 3184
5397, 687,2046, 935, 925, 405,2670, 703,1096,1860,2734,4671,4081,1877,1367,2704, # 3200
3389, 918,2106,1782,2483, 334,3320,1611,1093,4672, 564,3171,3505,3739,3390, 945, # 3216
2641,2058,4673,5398,1926, 872,4319,5399,3506,2705,3112, 349,4320,3740,4082,4674, # 3232
3882,4321,3741,2156,4083,4675,4676,4322,4677,2408,2047, 782,4084, 400, 251,4323, # 3248
1624,5400,5401, 277,3742, 299,1265, 476,1191,3883,2122,4324,4325,1109, 205,5402, # 3264
2590,1000,2157,3616,1861,5403,5404,5405,4678,5406,4679,2573, 107,2484,2158,4085, # 3280
3507,3172,5407,1533, 541,1301, 158, 753,4326,2886,3617,5408,1696, 370,1088,4327, # 3296
4680,3618, 579, 327, 440, 162,2244, 269,1938,1374,3508, 968,3063,  56,1396,3113, # 3312
2107,3321,3391,5409,1927,2159,4681,3016,5410,3619,5411,5412,3743,4682,2485,5413, # 3328
2804,5414,1650,4683,5415,2613,5416,5417,4086,2671,3392,1149,3393,4087,3884,4088, # 3344
5418,1076,  49,5419, 951,3242,3322,3323, 450,2850, 920,5420,1812,2805,2371,4328, # 3360
1909,1138,2372,3885,3509,5421,3243,4684,1910,1147,1518,2428,4685,3886,5422,4686, # 3376
2393,2614, 260,1796,3244,5423,5424,3887,3324, 708,5425,3620,1704,5426,3621,1351, # 3392
1618,3394,3017,1887, 944,4329,3395,4330,3064,3396,4331,5427,3744, 422, 413,1714, # 3408
3325, 500,2059,2350,4332,2486,5428,1344,1911, 954,5429,1668,5430,5431,4089,2409, # 3424
4333,3622,3888,4334,5432,2307,1318,2512,3114, 133,3115,2887,4687, 629,  31,2851, # 3440
2706,3889,4688, 850, 949,4689,4090,2970,1732,2089,4335,1496,1853,5433,4091, 620, # 3456
3245, 981,1242,3745,3397,1619,3746,1643,3326,2140,2457,1971,1719,3510,2169,5434, # 3472
3246,5435,5436,3398,1829,5437,1277,4690,1565,2048,5438,1636,3623,3116,5439, 869, # 3488
2852, 655,3890,3891,3117,4092,3018,3892,1310,3624,4691,5440,5441,5442,1733, 558, # 3504
4692,3747, 335,1549,3065,1756,4336,3748,1946,3511,1830,1291,1192, 470,2735,2108, # 3520
2806, 913,1054,4093,5443,1027,5444,3066,4094,4693, 982,2672,3399,3173,3512,3247, # 3536
3248,1947,2807,5445, 571,4694,5446,1831,5447,3625,2591,1523,2429,5448,2090, 984, # 3552
4695,3749,1960,5449,3750, 852, 923,2808,3513,3751, 969,1519, 999,2049,2325,1705, # 3568
5450,3118, 615,1662, 151, 597,4095,2410,2326,1049, 275,4696,3752,4337, 568,3753, # 3584
3626,2487,4338,3754,5451,2430,2275, 409,3249,5452,1566,2888,3514,1002, 769,2853, # 3600
 194,2091,3174,3755,2226,3327,4339, 628,1505,5453,5454,1763,2180,3019,4096, 521, # 3616
1161,2592,1788,2206,2411,4697,4097,1625,4340,4341, 412,  42,3119, 464,5455,2642, # 3632
4698,3400,1760,1571,2889,3515,2537,1219,2207,3893,2643,2141,2373,4699,4700,3328, # 3648
1651,3401,3627,5456,5457,3628,2488,3516,5458,3756,5459,5460,2276,2092, 460,5461, # 3664
4701,5462,3020, 962, 588,3629, 289,3250,2644,1116,  52,5463,3067,1797,5464,5465, # 3680
5466,1467,5467,1598,1143,3757,4342,1985,1734,1067,4702,1280,3402, 465,4703,1572, # 3696
 510,5468,1928,2245,1813,1644,3630,5469,4704,3758,5470,5471,2673,1573,1534,5472, # 3712
5473, 536,1808,1761,3517,3894,3175,2645,5474,5475,5476,4705,3518,2929,1912,2809, # 3728
5477,3329,1122, 377,3251,5478, 360,5479,5480,4343,1529, 551,5481,2060,3759,1769, # 3744
2431,5482,2930,4344,3330,3120,2327,2109,2031,4706,1404, 136,1468,1479, 672,1171, # 3760
3252,2308, 271,3176,5483,2772,5484,2050, 678,2736, 865,1948,4707,5485,2014,4098, # 3776
2971,5486,2737,2227,1397,3068,3760,4708,4709,1735,2931,3403,3631,5487,3895, 509, # 3792
2854,2458,2890,3896,5488,5489,3177,3178,4710,4345,2538,4711,2309,1166,1010, 552, # 3808
 681,1888,5490,5491,2972,2973,4099,1287,1596,1862,3179, 358, 453, 736, 175, 478, # 3824
1117, 905,1167,1097,5492,1854,1530,5493,1706,5494,2181,3519,2292,3761,3520,3632, # 3840
4346,2093,4347,5495,3404,1193,2489,4348,1458,2193,2208,1863,1889,1421,3331,2932, # 3856
3069,2182,3521, 595,2123,5496,4100,5497,5498,4349,1707,2646, 223,3762,1359, 751, # 3872
3121, 183,3522,5499,2810,3021, 419,2374, 633, 704,3897,2394, 241,5500,5501,5502, # 3888
 838,3022,3763,2277,2773,2459,3898,1939,2051,4101,1309,3122,2246,1181,5503,1136, # 3904
2209,3899,2375,1446,4350,2310,4712,5504,5505,4351,1055,2615, 484,3764,5506,4102, # 3920
 625,4352,2278,3405,1499,4353,4103,5507,4104,4354,3253,2279,2280,3523,5508,5509, # 3936
2774, 808,2616,3765,3406,4105,4355,3123,2539, 526,3407,3900,4356, 955,5510,1620, # 3952
4357,2647,2432,5511,1429,3766,1669,1832, 994, 928,5512,3633,1260,5513,5514,5515, # 3968
1949,2293, 741,2933,1626,4358,2738,2460, 867,1184, 362,3408,1392,5516,5517,4106, # 3984
4359,1770,1736,3254,2934,4713,4714,1929,2707,1459,1158,5518,3070,3409,2891,1292, # 4000
1930,2513,2855,3767,1986,1187,2072,2015,2617,4360,5519,2574,2514,2170,3768,2490, # 4016
3332,5520,3769,4715,5521,5522, 666,1003,3023,1022,3634,4361,5523,4716,1814,2257, # 4032
 574,3901,1603, 295,1535, 705,3902,4362, 283, 858, 417,5524,5525,3255,4717,4718, # 4048
3071,1220,1890,1046,2281,2461,4107,1393,1599, 689,2575, 388,4363,5526,2491, 802, # 4064
5527,2811,3903,2061,1405,2258,5528,4719,3904,2110,1052,1345,3256,1585,5529, 809, # 4080
5530,5531,5532, 575,2739,3524, 956,1552,1469,1144,2328,5533,2329,1560,2462,3635, # 4096
3257,4108, 616,2210,4364,3180,2183,2294,5534,1833,5535,3525,4720,5536,1319,3770, # 4112
3771,1211,3636,1023,3258,1293,2812,5537,5538,5539,3905, 607,2311,3906, 762,2892, # 4128
1439,4365,1360,4721,1485,3072,5540,4722,1038,4366,1450,2062,2648,4367,1379,4723, # 4144
2593,5541,5542,4368,1352,1414,2330,2935,1172,5543,5544,3907,3908,4724,1798,1451, # 4160
5545,5546,5547,5548,2936,4109,4110,2492,2351, 411,4111,4112,3637,3333,3124,4725, # 4176
1561,2674,1452,4113,1375,5549,5550,  47,2974, 316,5551,1406,1591,2937,3181,5552, # 4192
1025,2142,3125,3182, 354,2740, 884,2228,4369,2412, 508,3772, 726,3638, 996,2433, # 4208
3639, 729,5553, 392,2194,1453,4114,4726,3773,5554,5555,2463,3640,2618,1675,2813, # 4224
 919,2352,2975,2353,1270,4727,4115,  73,5556,5557, 647,5558,3259,2856,2259,1550, # 4240
1346,3024,5559,1332, 883,3526,5560,5561,5562,5563,3334,2775,5564,1212, 831,1347, # 4256
4370,4728,2331,3909,1864,3073, 720,3910,4729,4730,3911,5565,4371,5566,5567,4731, # 4272
5568,5569,1799,4732,3774,2619,4733,3641,1645,2376,4734,5570,2938, 669,2211,2675, # 4288
2434,5571,2893,5572,5573,1028,3260,5574,4372,2413,5575,2260,1353,5576,5577,4735, # 4304
3183, 518,5578,4116,5579,4373,1961,5580,2143,4374,5581,5582,3025,2354,2355,3912, # 4320
 516,1834,1454,4117,2708,4375,4736,2229,2620,1972,1129,3642,5583,2776,5584,2976, # 4336
1422, 577,1470,3026,1524,3410,5585,5586, 432,4376,3074,3527,5587,2594,1455,2515, # 4352
2230,1973,1175,5588,1020,2741,4118,3528,4737,5589,2742,5590,1743,1361,3075,3529, # 4368
2649,4119,4377,4738,2295, 895, 924,4378,2171, 331,2247,3076, 166,1627,3077,1098, # 4384
5591,1232,2894,2231,3411,4739, 657, 403,1196,2377, 542,3775,3412,1600,4379,3530, # 4400
5592,4740,2777,3261, 576, 530,1362,4741,4742,2540,2676,3776,4120,5593, 842,3913, # 4416
5594,2814,2032,1014,4121, 213,2709,3413, 665, 621,4380,5595,3777,2939,2435,5596, # 4432
2436,3335,3643,3414,4743,4381,2541,4382,4744,3644,1682,4383,3531,1380,5597, 724, # 4448
2282, 600,1670,5598,1337,1233,4745,3126,2248,5599,1621,4746,5600, 651,4384,5601, # 4464
1612,4385,2621,5602,2857,5603,2743,2312,3078,5604, 716,2464,3079, 174,1255,2710, # 4480
4122,3645, 548,1320,1398, 728,4123,1574,5605,1891,1197,3080,4124,5606,3081,3082, # 4496
3778,3646,3779, 747,5607, 635,4386,4747,5608,5609,5610,4387,5611,5612,4748,5613, # 4512
3415,4749,2437, 451,5614,3780,2542,2073,4388,2744,4389,4125,5615,1764,4750,5616, # 4528
4390, 350,4751,2283,2395,2493,5617,4391,4126,2249,1434,4127, 488,4752, 458,4392, # 4544
4128,3781, 771,1330,2396,3914,2576,3184,2160,2414,1553,2677,3185,4393,5618,2494, # 4560
2895,2622,1720,2711,4394,3416,4753,5619,2543,4395,5620,3262,4396,2778,5621,2016, # 4576
2745,5622,1155,1017,3782,3915,5623,3336,2313, 201,1865,4397,1430,5624,4129,5625, # 4592
5626,5627,5628,5629,4398,1604,5630, 414,1866, 371,2595,4754,4755,3532,2017,3127, # 4608
4756,1708, 960,4399, 887, 389,2172,1536,1663,1721,5631,2232,4130,2356,2940,1580, # 4624
5632,5633,1744,4757,2544,4758,4759,5634,4760,5635,2074,5636,4761,3647,3417,2896, # 4640
4400,5637,4401,2650,3418,2815, 673,2712,2465, 709,3533,4131,3648,4402,5638,1148, # 4656
 502, 634,5639,5640,1204,4762,3649,1575,4763,2623,3783,5641,3784,3128, 948,3263, # 4672
 121,1745,3916,1110,5642,4403,3083,2516,3027,4132,3785,1151,1771,3917,1488,4133, # 4688
1987,5643,2438,3534,5644,5645,2094,5646,4404,3918,1213,1407,2816, 531,2746,2545, # 4704
3264,1011,1537,4764,2779,4405,3129,1061,5647,3786,3787,1867,2897,5648,2018, 120, # 4720
4406,4407,2063,3650,3265,2314,3919,2678,3419,1955,4765,4134,5649,3535,1047,2713, # 4736
1266,5650,1368,4766,2858, 649,3420,3920,2546,2747,1102,2859,2679,5651,5652,2000, # 4752
5653,1111,3651,2977,5654,2495,3921,3652,2817,1855,3421,3788,5655,5656,3422,2415, # 4768
2898,3337,3266,3653,5657,2577,5658,3654,2818,4135,1460, 856,5659,3655,5660,2899, # 4784
2978,5661,2900,3922,5662,4408, 632,2517, 875,3923,1697,3924,2296,5663,5664,4767, # 4800
3028,1239, 580,4768,4409,5665, 914, 936,2075,1190,4136,1039,2124,5666,5667,5668, # 4816
5669,3423,1473,5670,1354,4410,3925,4769,2173,3084,4137, 915,3338,4411,4412,3339, # 4832
1605,1835,5671,2748, 398,3656,4413,3926,4138, 328,1913,2860,4139,3927,1331,4414, # 4848
3029, 937,4415,5672,3657,4140,4141,3424,2161,4770,3425, 524, 742, 538,3085,1012, # 4864
5673,5674,3928,2466,5675, 658,1103, 225,3929,5676,5677,4771,5678,4772,5679,3267, # 4880
1243,5680,4142, 963,2250,4773,5681,2714,3658,3186,5682,5683,2596,2332,5684,4774, # 4896
5685,5686,5687,3536, 957,3426,2547,2033,1931,2941,2467, 870,2019,3659,1746,2780, # 4912
2781,2439,2468,5688,3930,5689,3789,3130,3790,3537,3427,3791,5690,1179,3086,5691, # 4928
3187,2378,4416,3792,2548,3188,3131,2749,4143,5692,3428,1556,2549,2297, 977,2901, # 4944
2034,4144,1205,3429,5693,1765,3430,3189,2125,1271, 714,1689,4775,3538,5694,2333, # 4960
3931, 533,4417,3660,2184, 617,5695,2469,3340,3539,2315,5696,5697,3190,5698,5699, # 4976
3932,1988, 618, 427,2651,3540,3431,5700,5701,1244,1690,5702,2819,4418,4776,5703, # 4992
3541,4777,5704,2284,1576, 473,3661,4419,3432, 972,5705,3662,5706,3087,5707,5708, # 5008
4778,4779,5709,3793,4145,4146,5710, 153,4780, 356,5711,1892,2902,4420,2144, 408, # 5024
 803,2357,5712,3933,5713,4421,1646,2578,2518,4781,4782,3934,5714,3935,4422,5715, # 5040
2416,3433, 752,5716,5717,1962,3341,2979,5718, 746,3030,2470,4783,4423,3794, 698, # 5056
4784,1893,4424,3663,2550,4785,3664,3936,5719,3191,3434,5720,1824,1302,4147,2715, # 5072
3937,1974,4425,5721,4426,3192, 823,1303,1288,1236,2861,3542,4148,3435, 774,3938, # 5088
5722,1581,4786,1304,2862,3939,4787,5723,2440,2162,1083,3268,4427,4149,4428, 344, # 5104
1173, 288,2316, 454,1683,5724,5725,1461,4788,4150,2597,5726,5727,4789, 985, 894, # 5120
5728,3436,3193,5729,1914,2942,3795,1989,5730,2111,1975,5731,4151,5732,2579,1194, # 5136
 425,5733,4790,3194,1245,3796,4429,5734,5735,2863,5736, 636,4791,1856,3940, 760, # 5152
1800,5737,4430,2212,1508,4792,4152,1894,1684,2298,5738,5739,4793,4431,4432,2213, # 5168
 479,5740,5741, 832,5742,4153,2496,5743,2980,2497,3797, 990,3132, 627,1815,2652, # 5184
4433,1582,4434,2126,2112,3543,4794,5744, 799,4435,3195,5745,4795,2113,1737,3031, # 5200
1018, 543, 754,4436,3342,1676,4796,4797,4154,4798,1489,5746,3544,5747,2624,2903, # 5216
4155,5748,5749,2981,5750,5751,5752,5753,3196,4799,4800,2185,1722,5754,3269,3270, # 5232
1843,3665,1715, 481, 365,1976,1857,5755,5756,1963,2498,4801,5757,2127,3666,3271, # 5248
 433,1895,2064,2076,5758, 602,2750,5759,5760,5761,5762,5763,3032,1628,3437,5764, # 5264
3197,4802,4156,2904,4803,2519,5765,2551,2782,5766,5767,5768,3343,4804,2905,5769, # 5280
4805,5770,2864,4806,4807,1221,2982,4157,2520,5771,5772,5773,1868,1990,5774,5775, # 5296
5776,1896,5777,5778,4808,1897,4158, 318,5779,2095,4159,4437,5780,5781, 485,5782, # 5312
 938,3941, 553,2680, 116,5783,3942,3667,5784,3545,2681,2783,3438,3344,2820,5785, # 5328
3668,2943,4160,1747,2944,2983,5786,5787, 207,5788,4809,5789,4810,2521,5790,3033, # 5344
 890,3669,3943,5791,1878,3798,3439,5792,2186,2358,3440,1652,5793,5794,5795, 941, # 5360
2299, 208,3546,4161,2020, 330,4438,3944,2906,2499,3799,4439,4811,5796,5797,5798, # 5376  #last 512
#Everything below is of no interest for detection purpose
2522,1613,4812,5799,3345,3945,2523,5800,4162,5801,1637,4163,2471,4813,3946,5802, # 5392
2500,3034,3800,5803,5804,2195,4814,5805,2163,5806,5807,5808,5809,5810,5811,5812, # 5408
5813,5814,5815,5816,5817,5818,5819,5820,5821,5822,5823,5824,5825,5826,5827,5828, # 5424
5829,5830,5831,5832,5833,5834,5835,5836,5837,5838,5839,5840,5841,5842,5843,5844, # 5440
5845,5846,5847,5848,5849,5850,5851,5852,5853,5854,5855,5856,5857,5858,5859,5860, # 5456
5861,5862,5863,5864,5865,5866,5867,5868,5869,5870,5871,5872,5873,5874,5875,5876, # 5472
5877,5878,5879,5880,5881,5882,5883,5884,5885,5886,5887,5888,5889,5890,5891,5892, # 5488
5893,5894,5895,5896,5897,5898,5899,5900,5901,5902,5903,5904,5905,5906,5907,5908, # 5504
5909,5910,5911,5912,5913,5914,5915,5916,5917,5918,5919,5920,5921,5922,5923,5924, # 5520
5925,5926,5927,5928,5929,5930,5931,5932,5933,5934,5935,5936,5937,5938,5939,5940, # 5536
5941,5942,5943,5944,5945,5946,5947,5948,5949,5950,5951,5952,5953,5954,5955,5956, # 5552
5957,5958,5959,5960,5961,5962,5963,5964,5965,5966,5967,5968,5969,5970,5971,5972, # 5568
5973,5974,5975,5976,5977,5978,5979,5980,5981,5982,5983,5984,5985,5986,5987,5988, # 5584
5989,5990,5991,5992,5993,5994,5995,5996,5997,5998,5999,6000,6001,6002,6003,6004, # 5600
6005,6006,6007,6008,6009,6010,6011,6012,6013,6014,6015,6016,6017,6018,6019,6020, # 5616
6021,6022,6023,6024,6025,6026,6027,6028,6029,6030,6031,6032,6033,6034,6035,6036, # 5632
6037,6038,6039,6040,6041,6042,6043,6044,6045,6046,6047,6048,6049,6050,6051,6052, # 5648
6053,6054,6055,6056,6057,6058,6059,6060,6061,6062,6063,6064,6065,6066,6067,6068, # 5664
6069,6070,6071,6072,6073,6074,6075,6076,6077,6078,6079,6080,6081,6082,6083,6084, # 5680
6085,6086,6087,6088,6089,6090,6091,6092,6093,6094,6095,6096,6097,6098,6099,6100, # 5696
6101,6102,6103,6104,6105,6106,6107,6108,6109,6110,6111,6112,6113,6114,6115,6116, # 5712
6117,6118,6119,6120,6121,6122,6123,6124,6125,6126,6127,6128,6129,6130,6131,6132, # 5728
6133,6134,6135,6136,6137,6138,6139,6140,6141,6142,6143,6144,6145,6146,6147,6148, # 5744
6149,6150,6151,6152,6153,6154,6155,6156,6157,6158,6159,6160,6161,6162,6163,6164, # 5760
6165,6166,6167,6168,6169,6170,6171,6172,6173,6174,6175,6176,6177,6178,6179,6180, # 5776
6181,6182,6183,6184,6185,6186,6187,6188,6189,6190,6191,6192,6193,6194,6195,6196, # 5792
6197,6198,6199,6200,6201,6202,6203,6204,6205,6206,6207,6208,6209,6210,6211,6212, # 5808
6213,6214,6215,6216,6217,6218,6219,6220,6221,6222,6223,3670,6224,6225,6226,6227, # 5824
6228,6229,6230,6231,6232,6233,6234,6235,6236,6237,6238,6239,6240,6241,6242,6243, # 5840
6244,6245,6246,6247,6248,6249,6250,6251,6252,6253,6254,6255,6256,6257,6258,6259, # 5856
6260,6261,6262,6263,6264,6265,6266,6267,6268,6269,6270,6271,6272,6273,6274,6275, # 5872
6276,6277,6278,6279,6280,6281,6282,6283,6284,6285,4815,6286,6287,6288,6289,6290, # 5888
6291,6292,4816,6293,6294,6295,6296,6297,6298,6299,6300,6301,6302,6303,6304,6305, # 5904
6306,6307,6308,6309,6310,6311,4817,4818,6312,6313,6314,6315,6316,6317,6318,4819, # 5920
6319,6320,6321,6322,6323,6324,6325,6326,6327,6328,6329,6330,6331,6332,6333,6334, # 5936
6335,6336,6337,4820,6338,6339,6340,6341,6342,6343,6344,6345,6346,6347,6348,6349, # 5952
6350,6351,6352,6353,6354,6355,6356,6357,6358,6359,6360,6361,6362,6363,6364,6365, # 5968
6366,6367,6368,6369,6370,6371,6372,6373,6374,6375,6376,6377,6378,6379,6380,6381, # 5984
6382,6383,6384,6385,6386,6387,6388,6389,6390,6391,6392,6393,6394,6395,6396,6397, # 6000
6398,6399,6400,6401,6402,6403,6404,6405,6406,6407,6408,6409,6410,3441,6411,6412, # 6016
6413,6414,6415,6416,6417,6418,6419,6420,6421,6422,6423,6424,6425,4440,6426,6427, # 6032
6428,6429,6430,6431,6432,6433,6434,6435,6436,6437,6438,6439,6440,6441,6442,6443, # 6048
6444,6445,6446,6447,6448,6449,6450,6451,6452,6453,6454,4821,6455,6456,6457,6458, # 6064
6459,6460,6461,6462,6463,6464,6465,6466,6467,6468,6469,6470,6471,6472,6473,6474, # 6080
6475,6476,6477,3947,3948,6478,6479,6480,6481,3272,4441,6482,6483,6484,6485,4442, # 6096
6486,6487,6488,6489,6490,6491,6492,6493,6494,6495,6496,4822,6497,6498,6499,6500, # 6112
6501,6502,6503,6504,6505,6506,6507,6508,6509,6510,6511,6512,6513,6514,6515,6516, # 6128
6517,6518,6519,6520,6521,6522,6523,6524,6525,6526,6527,6528,6529,6530,6531,6532, # 6144
6533,6534,6535,6536,6537,6538,6539,6540,6541,6542,6543,6544,6545,6546,6547,6548, # 6160
6549,6550,6551,6552,6553,6554,6555,6556,2784,6557,4823,6558,6559,6560,6561,6562, # 6176
6563,6564,6565,6566,6567,6568,6569,3949,6570,6571,6572,4824,6573,6574,6575,6576, # 6192
6577,6578,6579,6580,6581,6582,6583,4825,6584,6585,6586,3950,2785,6587,6588,6589, # 6208
6590,6591,6592,6593,6594,6595,6596,6597,6598,6599,6600,6601,6602,6603,6604,6605, # 6224
6606,6607,6608,6609,6610,6611,6612,4826,6613,6614,6615,4827,6616,6617,6618,6619, # 6240
6620,6621,6622,6623,6624,6625,4164,6626,6627,6628,6629,6630,6631,6632,6633,6634, # 6256
3547,6635,4828,6636,6637,6638,6639,6640,6641,6642,3951,2984,6643,6644,6645,6646, # 6272
6647,6648,6649,4165,6650,4829,6651,6652,4830,6653,6654,6655,6656,6657,6658,6659, # 6288
6660,6661,6662,4831,6663,6664,6665,6666,6667,6668,6669,6670,6671,4166,6672,4832, # 6304
3952,6673,6674,6675,6676,4833,6677,6678,6679,4167,6680,6681,6682,3198,6683,6684, # 6320
6685,6686,6687,6688,6689,6690,6691,6692,6693,6694,6695,6696,6697,4834,6698,6699, # 6336
6700,6701,6702,6703,6704,6705,6706,6707,6708,6709,6710,6711,6712,6713,6714,6715, # 6352
6716,6717,6718,6719,6720,6721,6722,6723,6724,6725,6726,6727,6728,6729,6730,6731, # 6368
6732,6733,6734,4443,6735,6736,6737,6738,6739,6740,6741,6742,6743,6744,6745,4444, # 6384
6746,6747,6748,6749,6750,6751,6752,6753,6754,6755,6756,6757,6758,6759,6760,6761, # 6400
6762,6763,6764,6765,6766,6767,6768,6769,6770,6771,6772,6773,6774,6775,6776,6777, # 6416
6778,6779,6780,6781,4168,6782,6783,3442,6784,6785,6786,6787,6788,6789,6790,6791, # 6432
4169,6792,6793,6794,6795,6796,6797,6798,6799,6800,6801,6802,6803,6804,6805,6806, # 6448
6807,6808,6809,6810,6811,4835,6812,6813,6814,4445,6815,6816,4446,6817,6818,6819, # 6464
6820,6821,6822,6823,6824,6825,6826,6827,6828,6829,6830,6831,6832,6833,6834,6835, # 6480
3548,6836,6837,6838,6839,6840,6841,6842,6843,6844,6845,6846,4836,6847,6848,6849, # 6496
6850,6851,6852,6853,6854,3953,6855,6856,6857,6858,6859,6860,6861,6862,6863,6864, # 6512
6865,6866,6867,6868,6869,6870,6871,6872,6873,6874,6875,6876,6877,3199,6878,6879, # 6528
6880,6881,6882,4447,6883,6884,6885,6886,6887,6888,6889,6890,6891,6892,6893,6894, # 6544
6895,6896,6897,6898,6899,6900,6901,6902,6903,6904,4170,6905,6906,6907,6908,6909, # 6560
6910,6911,6912,6913,6914,6915,6916,6917,6918,6919,6920,6921,6922,6923,6924,6925, # 6576
6926,6927,4837,6928,6929,6930,6931,6932,6933,6934,6935,6936,3346,6937,6938,4838, # 6592
6939,6940,6941,4448,6942,6943,6944,6945,6946,4449,6947,6948,6949,6950,6951,6952, # 6608
6953,6954,6955,6956,6957,6958,6959,6960,6961,6962,6963,6964,6965,6966,6967,6968, # 6624
6969,6970,6971,6972,6973,6974,6975,6976,6977,6978,6979,6980,6981,6982,6983,6984, # 6640
6985,6986,6987,6988,6989,6990,6991,6992,6993,6994,3671,6995,6996,6997,6998,4839, # 6656
6999,7000,7001,7002,3549,7003,7004,7005,7006,7007,7008,7009,7010,7011,7012,7013, # 6672
7014,7015,7016,7017,7018,7019,7020,7021,7022,7023,7024,7025,7026,7027,7028,7029, # 6688
7030,4840,7031,7032,7033,7034,7035,7036,7037,7038,4841,7039,7040,7041,7042,7043, # 6704
7044,7045,7046,7047,7048,7049,7050,7051,7052,7053,7054,7055,7056,7057,7058,7059, # 6720
7060,7061,7062,7063,7064,7065,7066,7067,7068,7069,7070,2985,7071,7072,7073,7074, # 6736
7075,7076,7077,7078,7079,7080,4842,7081,7082,7083,7084,7085,7086,7087,7088,7089, # 6752
7090,7091,7092,7093,7094,7095,7096,7097,7098,7099,7100,7101,7102,7103,7104,7105, # 6768
7106,7107,7108,7109,7110,7111,7112,7113,7114,7115,7116,7117,7118,4450,7119,7120, # 6784
7121,7122,7123,7124,7125,7126,7127,7128,7129,7130,7131,7132,7133,7134,7135,7136, # 6800
7137,7138,7139,7140,7141,7142,7143,4843,7144,7145,7146,7147,7148,7149,7150,7151, # 6816
7152,7153,7154,7155,7156,7157,7158,7159,7160,7161,7162,7163,7164,7165,7166,7167, # 6832
7168,7169,7170,7171,7172,7173,7174,7175,7176,7177,7178,7179,7180,7181,7182,7183, # 6848
7184,7185,7186,7187,7188,4171,4172,7189,7190,7191,7192,7193,7194,7195,7196,7197, # 6864
7198,7199,7200,7201,7202,7203,7204,7205,7206,7207,7208,7209,7210,7211,7212,7213, # 6880
7214,7215,7216,7217,7218,7219,7220,7221,7222,7223,7224,7225,7226,7227,7228,7229, # 6896
7230,7231,7232,7233,7234,7235,7236,7237,7238,7239,7240,7241,7242,7243,7244,7245, # 6912
7246,7247,7248,7249,7250,7251,7252,7253,7254,7255,7256,7257,7258,7259,7260,7261, # 6928
7262,7263,7264,7265,7266,7267,7268,7269,7270,7271,7272,7273,7274,7275,7276,7277, # 6944
7278,7279,7280,7281,7282,7283,7284,7285,7286,7287,7288,7289,7290,7291,7292,7293, # 6960
7294,7295,7296,4844,7297,7298,7299,7300,7301,7302,7303,7304,7305,7306,7307,7308, # 6976
7309,7310,7311,7312,7313,7314,7315,7316,4451,7317,7318,7319,7320,7321,7322,7323, # 6992
7324,7325,7326,7327,7328,7329,7330,7331,7332,7333,7334,7335,7336,7337,7338,7339, # 7008
7340,7341,7342,7343,7344,7345,7346,7347,7348,7349,7350,7351,7352,7353,4173,7354, # 7024
7355,4845,7356,7357,7358,7359,7360,7361,7362,7363,7364,7365,7366,7367,7368,7369, # 7040
7370,7371,7372,7373,7374,7375,7376,7377,7378,7379,7380,7381,7382,7383,7384,7385, # 7056
7386,7387,7388,4846,7389,7390,7391,7392,7393,7394,7395,7396,7397,7398,7399,7400, # 7072
7401,7402,7403,7404,7405,3672,7406,7407,7408,7409,7410,7411,7412,7413,7414,7415, # 7088
7416,7417,7418,7419,7420,7421,7422,7423,7424,7425,7426,7427,7428,7429,7430,7431, # 7104
7432,7433,7434,7435,7436,7437,7438,7439,7440,7441,7442,7443,7444,7445,7446,7447, # 7120
7448,7449,7450,7451,7452,7453,4452,7454,3200,7455,7456,7457,7458,7459,7460,7461, # 7136
7462,7463,7464,7465,7466,7467,7468,7469,7470,7471,7472,7473,7474,4847,7475,7476, # 7152
7477,3133,7478,7479,7480,7481,7482,7483,7484,7485,7486,7487,7488,7489,7490,7491, # 7168
7492,7493,7494,7495,7496,7497,7498,7499,7500,7501,7502,3347,7503,7504,7505,7506, # 7184
7507,7508,7509,7510,7511,7512,7513,7514,7515,7516,7517,7518,7519,7520,7521,4848, # 7200
7522,7523,7524,7525,7526,7527,7528,7529,7530,7531,7532,7533,7534,7535,7536,7537, # 7216
7538,7539,7540,7541,7542,7543,7544,7545,7546,7547,7548,7549,3801,4849,7550,7551, # 7232
7552,7553,7554,7555,7556,7557,7558,7559,7560,7561,7562,7563,7564,7565,7566,7567, # 7248
7568,7569,3035,7570,7571,7572,7573,7574,7575,7576,7577,7578,7579,7580,7581,7582, # 7264
7583,7584,7585,7586,7587,7588,7589,7590,7591,7592,7593,7594,7595,7596,7597,7598, # 7280
7599,7600,7601,7602,7603,7604,7605,7606,7607,7608,7609,7610,7611,7612,7613,7614, # 7296
7615,7616,4850,7617,7618,3802,7619,7620,7621,7622,7623,7624,7625,7626,7627,7628, # 7312
7629,7630,7631,7632,4851,7633,7634,7635,7636,7637,7638,7639,7640,7641,7642,7643, # 7328
7644,7645,7646,7647,7648,7649,7650,7651,7652,7653,7654,7655,7656,7657,7658,7659, # 7344
7660,7661,7662,7663,7664,7665,7666,7667,7668,7669,7670,4453,7671,7672,7673,7674, # 7360
7675,7676,7677,7678,7679,7680,7681,7682,7683,7684,7685,7686,7687,7688,7689,7690, # 7376
7691,7692,7693,7694,7695,7696,7697,3443,7698,7699,7700,7701,7702,4454,7703,7704, # 7392
7705,7706,7707,7708,7709,7710,7711,7712,7713,2472,7714,7715,7716,7717,7718,7719, # 7408
7720,7721,7722,7723,7724,7725,7726,7727,7728,7729,7730,7731,3954,7732,7733,7734, # 7424
7735,7736,7737,7738,7739,7740,7741,7742,7743,7744,7745,7746,7747,7748,7749,7750, # 7440
3134,7751,7752,4852,7753,7754,7755,4853,7756,7757,7758,7759,7760,4174,7761,7762, # 7456
7763,7764,7765,7766,7767,7768,7769,7770,7771,7772,7773,7774,7775,7776,7777,7778, # 7472
7779,7780,7781,7782,7783,7784,7785,7786,7787,7788,7789,7790,7791,7792,7793,7794, # 7488
7795,7796,7797,7798,7799,7800,7801,7802,7803,7804,7805,4854,7806,7807,7808,7809, # 7504
7810,7811,7812,7813,7814,7815,7816,7817,7818,7819,7820,7821,7822,7823,7824,7825, # 7520
4855,7826,7827,7828,7829,7830,7831,7832,7833,7834,7835,7836,7837,7838,7839,7840, # 7536
7841,7842,7843,7844,7845,7846,7847,3955,7848,7849,7850,7851,7852,7853,7854,7855, # 7552
7856,7857,7858,7859,7860,3444,7861,7862,7863,7864,7865,7866,7867,7868,7869,7870, # 7568
7871,7872,7873,7874,7875,7876,7877,7878,7879,7880,7881,7882,7883,7884,7885,7886, # 7584
7887,7888,7889,7890,7891,4175,7892,7893,7894,7895,7896,4856,4857,7897,7898,7899, # 7600
7900,2598,7901,7902,7903,7904,7905,7906,7907,7908,4455,7909,7910,7911,7912,7913, # 7616
7914,3201,7915,7916,7917,7918,7919,7920,7921,4858,7922,7923,7924,7925,7926,7927, # 7632
7928,7929,7930,7931,7932,7933,7934,7935,7936,7937,7938,7939,7940,7941,7942,7943, # 7648
7944,7945,7946,7947,7948,7949,7950,7951,7952,7953,7954,7955,7956,7957,7958,7959, # 7664
7960,7961,7962,7963,7964,7965,7966,7967,7968,7969,7970,7971,7972,7973,7974,7975, # 7680
7976,7977,7978,7979,7980,7981,4859,7982,7983,7984,7985,7986,7987,7988,7989,7990, # 7696
7991,7992,7993,7994,7995,7996,4860,7997,7998,7999,8000,8001,8002,8003,8004,8005, # 7712
8006,8007,8008,8009,8010,8011,8012,8013,8014,8015,8016,4176,8017,8018,8019,8020, # 7728
8021,8022,8023,4861,8024,8025,8026,8027,8028,8029,8030,8031,8032,8033,8034,8035, # 7744
8036,4862,4456,8037,8038,8039,8040,4863,8041,8042,8043,8044,8045,8046,8047,8048, # 7760
8049,8050,8051,8052,8053,8054,8055,8056,8057,8058,8059,8060,8061,8062,8063,8064, # 7776
8065,8066,8067,8068,8069,8070,8071,8072,8073,8074,8075,8076,8077,8078,8079,8080, # 7792
8081,8082,8083,8084,8085,8086,8087,8088,8089,8090,8091,8092,8093,8094,8095,8096, # 7808
8097,8098,8099,4864,4177,8100,8101,8102,8103,8104,8105,8106,8107,8108,8109,8110, # 7824
8111,8112,8113,8114,8115,8116,8117,8118,8119,8120,4178,8121,8122,8123,8124,8125, # 7840
8126,8127,8128,8129,8130,8131,8132,8133,8134,8135,8136,8137,8138,8139,8140,8141, # 7856
8142,8143,8144,8145,4865,4866,8146,8147,8148,8149,8150,8151,8152,8153,8154,8155, # 7872
8156,8157,8158,8159,8160,8161,8162,8163,8164,8165,4179,8166,8167,8168,8169,8170, # 7888
8171,8172,8173,8174,8175,8176,8177,8178,8179,8180,8181,4457,8182,8183,8184,8185, # 7904
8186,8187,8188,8189,8190,8191,8192,8193,8194,8195,8196,8197,8198,8199,8200,8201, # 7920
8202,8203,8204,8205,8206,8207,8208,8209,8210,8211,8212,8213,8214,8215,8216,8217, # 7936
8218,8219,8220,8221,8222,8223,8224,8225,8226,8227,8228,8229,8230,8231,8232,8233, # 7952
8234,8235,8236,8237,8238,8239,8240,8241,8242,8243,8244,8245,8246,8247,8248,8249, # 7968
8250,8251,8252,8253,8254,8255,8256,3445,8257,8258,8259,8260,8261,8262,4458,8263, # 7984
8264,8265,8266,8267,8268,8269,8270,8271,8272,4459,8273,8274,8275,8276,3550,8277, # 8000
8278,8279,8280,8281,8282,8283,8284,8285,8286,8287,8288,8289,4460,8290,8291,8292, # 8016
8293,8294,8295,8296,8297,8298,8299,8300,8301,8302,8303,8304,8305,8306,8307,4867, # 8032
8308,8309,8310,8311,8312,3551,8313,8314,8315,8316,8317,8318,8319,8320,8321,8322, # 8048
8323,8324,8325,8326,4868,8327,8328,8329,8330,8331,8332,8333,8334,8335,8336,8337, # 8064
8338,8339,8340,8341,8342,8343,8344,8345,8346,8347,8348,8349,8350,8351,8352,8353, # 8080
8354,8355,8356,8357,8358,8359,8360,8361,8362,8363,4869,4461,8364,8365,8366,8367, # 8096
8368,8369,8370,4870,8371,8372,8373,8374,8375,8376,8377,8378,8379,8380,8381,8382, # 8112
8383,8384,8385,8386,8387,8388,8389,8390,8391,8392,8393,8394,8395,8396,8397,8398, # 8128
8399,8400,8401,8402,8403,8404,8405,8406,8407,8408,8409,8410,4871,8411,8412,8413, # 8144
8414,8415,8416,8417,8418,8419,8420,8421,8422,4462,8423,8424,8425,8426,8427,8428, # 8160
8429,8430,8431,8432,8433,2986,8434,8435,8436,8437,8438,8439,8440,8441,8442,8443, # 8176
8444,8445,8446,8447,8448,8449,8450,8451,8452,8453,8454,8455,8456,8457,8458,8459, # 8192
8460,8461,8462,8463,8464,8465,8466,8467,8468,8469,8470,8471,8472,8473,8474,8475, # 8208
8476,8477,8478,4180,8479,8480,8481,8482,8483,8484,8485,8486,8487,8488,8489,8490, # 8224
8491,8492,8493,8494,8495,8496,8497,8498,8499,8500,8501,8502,8503,8504,8505,8506, # 8240
8507,8508,8509,8510,8511,8512,8513,8514,8515,8516,8517,8518,8519,8520,8521,8522, # 8256
8523,8524,8525,8526,8527,8528,8529,8530,8531,8532,8533,8534,8535,8536,8537,8538, # 8272
8539,8540,8541,8542,8543,8544,8545,8546,8547,8548,8549,8550,8551,8552,8553,8554, # 8288
8555,8556,8557,8558,8559,8560,8561,8562,8563,8564,4872,8565,8566,8567,8568,8569, # 8304
8570,8571,8572,8573,4873,8574,8575,8576,8577,8578,8579,8580,8581,8582,8583,8584, # 8320
8585,8586,8587,8588,8589,8590,8591,8592,8593,8594,8595,8596,8597,8598,8599,8600, # 8336
8601,8602,8603,8604,8605,3803,8606,8607,8608,8609,8610,8611,8612,8613,4874,3804, # 8352
8614,8615,8616,8617,8618,8619,8620,8621,3956,8622,8623,8624,8625,8626,8627,8628, # 8368
8629,8630,8631,8632,8633,8634,8635,8636,8637,8638,2865,8639,8640,8641,8642,8643, # 8384
8644,8645,8646,8647,8648,8649,8650,8651,8652,8653,8654,8655,8656,4463,8657,8658, # 8400
8659,4875,4876,8660,8661,8662,8663,8664,8665,8666,8667,8668,8669,8670,8671,8672, # 8416
8673,8674,8675,8676,8677,8678,8679,8680,8681,4464,8682,8683,8684,8685,8686,8687, # 8432
8688,8689,8690,8691,8692,8693,8694,8695,8696,8697,8698,8699,8700,8701,8702,8703, # 8448
8704,8705,8706,8707,8708,8709,2261,8710,8711,8712,8713,8714,8715,8716,8717,8718, # 8464
8719,8720,8721,8722,8723,8724,8725,8726,8727,8728,8729,8730,8731,8732,8733,4181, # 8480
8734,8735,8736,8737,8738,8739,8740,8741,8742,8743,8744,8745,8746,8747,8748,8749, # 8496
8750,8751,8752,8753,8754,8755,8756,8757,8758,8759,8760,8761,8762,8763,4877,8764, # 8512
8765,8766,8767,8768,8769,8770,8771,8772,8773,8774,8775,8776,8777,8778,8779,8780, # 8528
8781,8782,8783,8784,8785,8786,8787,8788,4878,8789,4879,8790,8791,8792,4880,8793, # 8544
8794,8795,8796,8797,8798,8799,8800,8801,4881,8802,8803,8804,8805,8806,8807,8808, # 8560
8809,8810,8811,8812,8813,8814,8815,3957,8816,8817,8818,8819,8820,8821,8822,8823, # 8576
8824,8825,8826,8827,8828,8829,8830,8831,8832,8833,8834,8835,8836,8837,8838,8839, # 8592
8840,8841,8842,8843,8844,8845,8846,8847,4882,8848,8849,8850,8851,8852,8853,8854, # 8608
8855,8856,8857,8858,8859,8860,8861,8862,8863,8864,8865,8866,8867,8868,8869,8870, # 8624
8871,8872,8873,8874,8875,8876,8877,8878,8879,8880,8881,8882,8883,8884,3202,8885, # 8640
8886,8887,8888,8889,8890,8891,8892,8893,8894,8895,8896,8897,8898,8899,8900,8901, # 8656
8902,8903,8904,8905,8906,8907,8908,8909,8910,8911,8912,8913,8914,8915,8916,8917, # 8672
8918,8919,8920,8921,8922,8923,8924,4465,8925,8926,8927,8928,8929,8930,8931,8932, # 8688
4883,8933,8934,8935,8936,8937,8938,8939,8940,8941,8942,8943,2214,8944,8945,8946, # 8704
8947,8948,8949,8950,8951,8952,8953,8954,8955,8956,8957,8958,8959,8960,8961,8962, # 8720
8963,8964,8965,4884,8966,8967,8968,8969,8970,8971,8972,8973,8974,8975,8976,8977, # 8736
8978,8979,8980,8981,8982,8983,8984,8985,8986,8987,8988,8989,8990,8991,8992,4885, # 8752
8993,8994,8995,8996,8997,8998,8999,9000,9001,9002,9003,9004,9005,9006,9007,9008, # 8768
9009,9010,9011,9012,9013,9014,9015,9016,9017,9018,9019,9020,9021,4182,9022,9023, # 8784
9024,9025,9026,9027,9028,9029,9030,9031,9032,9033,9034,9035,9036,9037,9038,9039, # 8800
9040,9041,9042,9043,9044,9045,9046,9047,9048,9049,9050,9051,9052,9053,9054,9055, # 8816
9056,9057,9058,9059,9060,9061,9062,9063,4886,9064,9065,9066,9067,9068,9069,4887, # 8832
9070,9071,9072,9073,9074,9075,9076,9077,9078,9079,9080,9081,9082,9083,9084,9085, # 8848
9086,9087,9088,9089,9090,9091,9092,9093,9094,9095,9096,9097,9098,9099,9100,9101, # 8864
9102,9103,9104,9105,9106,9107,9108,9109,9110,9111,9112,9113,9114,9115,9116,9117, # 8880
9118,9119,9120,9121,9122,9123,9124,9125,9126,9127,9128,9129,9130,9131,9132,9133, # 8896
9134,9135,9136,9137,9138,9139,9140,9141,3958,9142,9143,9144,9145,9146,9147,9148, # 8912
9149,9150,9151,4888,9152,9153,9154,9155,9156,9157,9158,9159,9160,9161,9162,9163, # 8928
9164,9165,9166,9167,9168,9169,9170,9171,9172,9173,9174,9175,4889,9176,9177,9178, # 8944
9179,9180,9181,9182,9183,9184,9185,9186,9187,9188,9189,9190,9191,9192,9193,9194, # 8960
9195,9196,9197,9198,9199,9200,9201,9202,9203,4890,9204,9205,9206,9207,9208,9209, # 8976
9210,9211,9212,9213,9214,9215,9216,9217,9218,9219,9220,9221,9222,4466,9223,9224, # 8992
9225,9226,9227,9228,9229,9230,9231,9232,9233,9234,9235,9236,9237,9238,9239,9240, # 9008
9241,9242,9243,9244,9245,4891,9246,9247,9248,9249,9250,9251,9252,9253,9254,9255, # 9024
9256,9257,4892,9258,9259,9260,9261,4893,4894,9262,9263,9264,9265,9266,9267,9268, # 9040
9269,9270,9271,9272,9273,4467,9274,9275,9276,9277,9278,9279,9280,9281,9282,9283, # 9056
9284,9285,3673,9286,9287,9288,9289,9290,9291,9292,9293,9294,9295,9296,9297,9298, # 9072
9299,9300,9301,9302,9303,9304,9305,9306,9307,9308,9309,9310,9311,9312,9313,9314, # 9088
9315,9316,9317,9318,9319,9320,9321,9322,4895,9323,9324,9325,9326,9327,9328,9329, # 9104
9330,9331,9332,9333,9334,9335,9336,9337,9338,9339,9340,9341,9342,9343,9344,9345, # 9120
9346,9347,4468,9348,9349,9350,9351,9352,9353,9354,9355,9356,9357,9358,9359,9360, # 9136
9361,9362,9363,9364,9365,9366,9367,9368,9369,9370,9371,9372,9373,4896,9374,4469, # 9152
9375,9376,9377,9378,9379,4897,9380,9381,9382,9383,9384,9385,9386,9387,9388,9389, # 9168
9390,9391,9392,9393,9394,9395,9396,9397,9398,9399,9400,9401,9402,9403,9404,9405, # 9184
9406,4470,9407,2751,9408,9409,3674,3552,9410,9411,9412,9413,9414,9415,9416,9417, # 9200
9418,9419,9420,9421,4898,9422,9423,9424,9425,9426,9427,9428,9429,3959,9430,9431, # 9216
9432,9433,9434,9435,9436,4471,9437,9438,9439,9440,9441,9442,9443,9444,9445,9446, # 9232
9447,9448,9449,9450,3348,9451,9452,9453,9454,9455,9456,9457,9458,9459,9460,9461, # 9248
9462,9463,9464,9465,9466,9467,9468,9469,9470,9471,9472,4899,9473,9474,9475,9476, # 9264
9477,4900,9478,9479,9480,9481,9482,9483,9484,9485,9486,9487,9488,3349,9489,9490, # 9280
9491,9492,9493,9494,9495,9496,9497,9498,9499,9500,9501,9502,9503,9504,9505,9506, # 9296
9507,9508,9509,9510,9511,9512,9513,9514,9515,9516,9517,9518,9519,9520,4901,9521, # 9312
9522,9523,9524,9525,9526,4902,9527,9528,9529,9530,9531,9532,9533,9534,9535,9536, # 9328
9537,9538,9539,9540,9541,9542,9543,9544,9545,9546,9547,9548,9549,9550,9551,9552, # 9344
9553,9554,9555,9556,9557,9558,9559,9560,9561,9562,9563,9564,9565,9566,9567,9568, # 9360
9569,9570,9571,9572,9573,9574,9575,9576,9577,9578,9579,9580,9581,9582,9583,9584, # 9376
3805,9585,9586,9587,9588,9589,9590,9591,9592,9593,9594,9595,9596,9597,9598,9599, # 9392
9600,9601,9602,4903,9603,9604,9605,9606,9607,4904,9608,9609,9610,9611,9612,9613, # 9408
9614,4905,9615,9616,9617,9618,9619,9620,9621,9622,9623,9624,9625,9626,9627,9628, # 9424
9629,9630,9631,9632,4906,9633,9634,9635,9636,9637,9638,9639,9640,9641,9642,9643, # 9440
4907,9644,9645,9646,9647,9648,9649,9650,9651,9652,9653,9654,9655,9656,9657,9658, # 9456
9659,9660,9661,9662,9663,9664,9665,9666,9667,9668,9669,9670,9671,9672,4183,9673, # 9472
9674,9675,9676,9677,4908,9678,9679,9680,9681,4909,9682,9683,9684,9685,9686,9687, # 9488
9688,9689,9690,4910,9691,9692,9693,3675,9694,9695,9696,2945,9697,9698,9699,9700, # 9504
9701,9702,9703,9704,9705,4911,9706,9707,9708,9709,9710,9711,9712,9713,9714,9715, # 9520
9716,9717,9718,9719,9720,9721,9722,9723,9724,9725,9726,9727,9728,9729,9730,9731, # 9536
9732,9733,9734,9735,4912,9736,9737,9738,9739,9740,4913,9741,9742,9743,9744,9745, # 9552
9746,9747,9748,9749,9750,9751,9752,9753,9754,9755,9756,9757,9758,4914,9759,9760, # 9568
9761,9762,9763,9764,9765,9766,9767,9768,9769,9770,9771,9772,9773,9774,9775,9776, # 9584
9777,9778,9779,9780,9781,9782,4915,9783,9784,9785,9786,9787,9788,9789,9790,9791, # 9600
9792,9793,4916,9794,9795,9796,9797,9798,9799,9800,9801,9802,9803,9804,9805,9806, # 9616
9807,9808,9809,9810,9811,9812,9813,9814,9815,9816,9817,9818,9819,9820,9821,9822, # 9632
9823,9824,9825,9826,9827,9828,9829,9830,9831,9832,9833,9834,9835,9836,9837,9838, # 9648
9839,9840,9841,9842,9843,9844,9845,9846,9847,9848,9849,9850,9851,9852,9853,9854, # 9664
9855,9856,9857,9858,9859,9860,9861,9862,9863,9864,9865,9866,9867,9868,4917,9869, # 9680
9870,9871,9872,9873,9874,9875,9876,9877,9878,9879,9880,9881,9882,9883,9884,9885, # 9696
9886,9887,9888,9889,9890,9891,9892,4472,9893,9894,9895,9896,9897,3806,9898,9899, # 9712
9900,9901,9902,9903,9904,9905,9906,9907,9908,9909,9910,9911,9912,9913,9914,4918, # 9728
9915,9916,9917,4919,9918,9919,9920,9921,4184,9922,9923,9924,9925,9926,9927,9928, # 9744
9929,9930,9931,9932,9933,9934,9935,9936,9937,9938,9939,9940,9941,9942,9943,9944, # 9760
9945,9946,4920,9947,9948,9949,9950,9951,9952,9953,9954,9955,4185,9956,9957,9958, # 9776
9959,9960,9961,9962,9963,9964,9965,4921,9966,9967,9968,4473,9969,9970,9971,9972, # 9792
9973,9974,9975,9976,9977,4474,9978,9979,9980,9981,9982,9983,9984,9985,9986,9987, # 9808
9988,9989,9990,9991,9992,9993,9994,9995,9996,9997,9998,9999,10000,10001,10002,10003, # 9824
10004,10005,10006,10007,10008,10009,10010,10011,10012,10013,10014,10015,10016,10017,10018,10019, # 9840
10020,10021,4922,10022,4923,10023,10024,10025,10026,10027,10028,10029,10030,10031,10032,10033, # 9856
10034,10035,10036,10037,10038,10039,10040,10041,10042,10043,10044,10045,10046,10047,10048,4924, # 9872
10049,10050,10051,10052,10053,10054,10055,10056,10057,10058,10059,10060,10061,10062,10063,10064, # 9888
10065,10066,10067,10068,10069,10070,10071,10072,10073,10074,10075,10076,10077,10078,10079,10080, # 9904
10081,10082,10083,10084,10085,10086,10087,4475,10088,10089,10090,10091,10092,10093,10094,10095, # 9920
10096,10097,4476,10098,10099,10100,10101,10102,10103,10104,10105,10106,10107,10108,10109,10110, # 9936
10111,2174,10112,10113,10114,10115,10116,10117,10118,10119,10120,10121,10122,10123,10124,10125, # 9952
10126,10127,10128,10129,10130,10131,10132,10133,10134,10135,10136,10137,10138,10139,10140,3807, # 9968
4186,4925,10141,10142,10143,10144,10145,10146,10147,4477,4187,10148,10149,10150,10151,10152, # 9984
10153,4188,10154,10155,10156,10157,10158,10159,10160,10161,4926,10162,10163,10164,10165,10166, #10000
10167,10168,10169,10170,10171,10172,10173,10174,10175,10176,10177,10178,10179,10180,10181,10182, #10016
10183,10184,10185,10186,10187,10188,10189,10190,10191,10192,3203,10193,10194,10195,10196,10197, #10032
10198,10199,10200,4478,10201,10202,10203,10204,4479,10205,10206,10207,10208,10209,10210,10211, #10048
10212,10213,10214,10215,10216,10217,10218,10219,10220,10221,10222,10223,10224,10225,10226,10227, #10064
10228,10229,10230,10231,10232,10233,10234,4927,10235,10236,10237,10238,10239,10240,10241,10242, #10080
10243,10244,10245,10246,10247,10248,10249,10250,10251,10252,10253,10254,10255,10256,10257,10258, #10096
10259,10260,10261,10262,10263,10264,10265,10266,10267,10268,10269,10270,10271,10272,10273,4480, #10112
4928,4929,10274,10275,10276,10277,10278,10279,10280,10281,10282,10283,10284,10285,10286,10287, #10128
10288,10289,10290,10291,10292,10293,10294,10295,10296,10297,10298,10299,10300,10301,10302,10303, #10144
10304,10305,10306,10307,10308,10309,10310,10311,10312,10313,10314,10315,10316,10317,10318,10319, #10160
10320,10321,10322,10323,10324,10325,10326,10327,10328,10329,10330,10331,10332,10333,10334,4930, #10176
10335,10336,10337,10338,10339,10340,10341,10342,4931,10343,10344,10345,10346,10347,10348,10349, #10192
10350,10351,10352,10353,10354,10355,3088,10356,2786,10357,10358,10359,10360,4189,10361,10362, #10208
10363,10364,10365,10366,10367,10368,10369,10370,10371,10372,10373,10374,10375,4932,10376,10377, #10224
10378,10379,10380,10381,10382,10383,10384,10385,10386,10387,10388,10389,10390,10391,10392,4933, #10240
10393,10394,10395,4934,10396,10397,10398,10399,10400,10401,10402,10403,10404,10405,10406,10407, #10256
10408,10409,10410,10411,10412,3446,10413,10414,10415,10416,10417,10418,10419,10420,10421,10422, #10272
10423,4935,10424,10425,10426,10427,10428,10429,10430,4936,10431,10432,10433,10434,10435,10436, #10288
10437,10438,10439,10440,10441,10442,10443,4937,10444,10445,10446,10447,4481,10448,10449,10450, #10304
10451,10452,10453,10454,10455,10456,10457,10458,10459,10460,10461,10462,10463,10464,10465,10466, #10320
10467,10468,10469,10470,10471,10472,10473,10474,10475,10476,10477,10478,10479,10480,10481,10482, #10336
10483,10484,10485,10486,10487,10488,10489,10490,10491,10492,10493,10494,10495,10496,10497,10498, #10352
10499,10500,10501,10502,10503,10504,10505,4938,10506,10507,10508,10509,10510,2552,10511,10512, #10368
10513,10514,10515,10516,3447,10517,10518,10519,10520,10521,10522,10523,10524,10525,10526,10527, #10384
10528,10529,10530,10531,10532,10533,10534,10535,10536,10537,10538,10539,10540,10541,10542,10543, #10400
4482,10544,4939,10545,10546,10547,10548,10549,10550,10551,10552,10553,10554,10555,10556,10557, #10416
10558,10559,10560,10561,10562,10563,10564,10565,10566,10567,3676,4483,10568,10569,10570,10571, #10432
10572,3448,10573,10574,10575,10576,10577,10578,10579,10580,10581,10582,10583,10584,10585,10586, #10448
10587,10588,10589,10590,10591,10592,10593,10594,10595,10596,10597,10598,10599,10600,10601,10602, #10464
10603,10604,10605,10606,10607,10608,10609,10610,10611,10612,10613,10614,10615,10616,10617,10618, #10480
10619,10620,10621,10622,10623,10624,10625,10626,10627,4484,10628,10629,10630,10631,10632,4940, #10496
10633,10634,10635,10636,10637,10638,10639,10640,10641,10642,10643,10644,10645,10646,10647,10648, #10512
10649,10650,10651,10652,10653,10654,10655,10656,4941,10657,10658,10659,2599,10660,10661,10662, #10528
10663,10664,10665,10666,3089,10667,10668,10669,10670,10671,10672,10673,10674,10675,10676,10677, #10544
10678,10679,10680,4942,10681,10682,10683,10684,10685,10686,10687,10688,10689,10690,10691,10692, #10560
10693,10694,10695,10696,10697,4485,10698,10699,10700,10701,10702,10703,10704,4943,10705,3677, #10576
10706,10707,10708,10709,10710,10711,10712,4944,10713,10714,10715,10716,10717,10718,10719,10720, #10592
10721,10722,10723,10724,10725,10726,10727,10728,4945,10729,10730,10731,10732,10733,10734,10735, #10608
10736,10737,10738,10739,10740,10741,10742,10743,10744,10745,10746,10747,10748,10749,10750,10751, #10624
10752,10753,10754,10755,10756,10757,10758,10759,10760,10761,4946,10762,10763,10764,10765,10766, #10640
10767,4947,4948,10768,10769,10770,10771,10772,10773,10774,10775,10776,10777,10778,10779,10780, #10656
10781,10782,10783,10784,10785,10786,10787,10788,10789,10790,10791,10792,10793,10794,10795,10796, #10672
10797,10798,10799,10800,10801,10802,10803,10804,10805,10806,10807,10808,10809,10810,10811,10812, #10688
10813,10814,10815,10816,10817,10818,10819,10820,10821,10822,10823,10824,10825,10826,10827,10828, #10704
10829,10830,10831,10832,10833,10834,10835,10836,10837,10838,10839,10840,10841,10842,10843,10844, #10720
10845,10846,10847,10848,10849,10850,10851,10852,10853,10854,10855,10856,10857,10858,10859,10860, #10736
10861,10862,10863,10864,10865,10866,10867,10868,10869,10870,10871,10872,10873,10874,10875,10876, #10752
10877,10878,4486,10879,10880,10881,10882,10883,10884,10885,4949,10886,10887,10888,10889,10890, #10768
10891,10892,10893,10894,10895,10896,10897,10898,10899,10900,10901,10902,10903,10904,10905,10906, #10784
10907,10908,10909,10910,10911,10912,10913,10914,10915,10916,10917,10918,10919,4487,10920,10921, #10800
10922,10923,10924,10925,10926,10927,10928,10929,10930,10931,10932,4950,10933,10934,10935,10936, #10816
10937,10938,10939,10940,10941,10942,10943,10944,10945,10946,10947,10948,10949,4488,10950,10951, #10832
10952,10953,10954,10955,10956,10957,10958,10959,4190,10960,10961,10962,10963,10964,10965,10966, #10848
10967,10968,10969,10970,10971,10972,10973,10974,10975,10976,10977,10978,10979,10980,10981,10982, #10864
10983,10984,10985,10986,10987,10988,10989,10990,10991,10992,10993,10994,10995,10996,10997,10998, #10880
10999,11000,11001,11002,11003,11004,11005,11006,3960,11007,11008,11009,11010,11011,11012,11013, #10896
11014,11015,11016,11017,11018,11019,11020,11021,11022,11023,11024,11025,11026,11027,11028,11029, #10912
11030,11031,11032,4951,11033,11034,11035,11036,11037,11038,11039,11040,11041,11042,11043,11044, #10928
11045,11046,11047,4489,11048,11049,11050,11051,4952,11052,11053,11054,11055,11056,11057,11058, #10944
4953,11059,11060,11061,11062,11063,11064,11065,11066,11067,11068,11069,11070,11071,4954,11072, #10960
11073,11074,11075,11076,11077,11078,11079,11080,11081,11082,11083,11084,11085,11086,11087,11088, #10976
11089,11090,11091,11092,11093,11094,11095,11096,11097,11098,11099,11100,11101,11102,11103,11104, #10992
11105,11106,11107,11108,11109,11110,11111,11112,11113,11114,11115,3808,11116,11117,11118,11119, #11008
11120,11121,11122,11123,11124,11125,11126,11127,11128,11129,11130,11131,11132,11133,11134,4955, #11024
11135,11136,11137,11138,11139,11140,11141,11142,11143,11144,11145,11146,11147,11148,11149,11150, #11040
11151,11152,11153,11154,11155,11156,11157,11158,11159,11160,11161,4956,11162,11163,11164,11165, #11056
11166,11167,11168,11169,11170,11171,11172,11173,11174,11175,11176,11177,11178,11179,11180,4957, #11072
11181,11182,11183,11184,11185,11186,4958,11187,11188,11189,11190,11191,11192,11193,11194,11195, #11088
11196,11197,11198,11199,11200,3678,11201,11202,11203,11204,11205,11206,4191,11207,11208,11209, #11104
11210,11211,11212,11213,11214,11215,11216,11217,11218,11219,11220,11221,11222,11223,11224,11225, #11120
11226,11227,11228,11229,11230,11231,11232,11233,11234,11235,11236,11237,11238,11239,11240,11241, #11136
11242,11243,11244,11245,11246,11247,11248,11249,11250,11251,4959,11252,11253,11254,11255,11256, #11152
11257,11258,11259,11260,11261,11262,11263,11264,11265,11266,11267,11268,11269,11270,11271,11272, #11168
11273,11274,11275,11276,11277,11278,11279,11280,11281,11282,11283,11284,11285,11286,11287,11288, #11184
11289,11290,11291,11292,11293,11294,11295,11296,11297,11298,11299,11300,11301,11302,11303,11304, #11200
11305,11306,11307,11308,11309,11310,11311,11312,11313,11314,3679,11315,11316,11317,11318,4490, #11216
11319,11320,11321,11322,11323,11324,11325,11326,11327,11328,11329,11330,11331,11332,11333,11334, #11232
11335,11336,11337,11338,11339,11340,11341,11342,11343,11344,11345,11346,11347,4960,11348,11349, #11248
11350,11351,11352,11353,11354,11355,11356,11357,11358,11359,11360,11361,11362,11363,11364,11365, #11264
11366,11367,11368,11369,11370,11371,11372,11373,11374,11375,11376,11377,3961,4961,11378,11379, #11280
11380,11381,11382,11383,11384,11385,11386,11387,11388,11389,11390,11391,11392,11393,11394,11395, #11296
11396,11397,4192,11398,11399,11400,11401,11402,11403,11404,11405,11406,11407,11408,11409,11410, #11312
11411,4962,11412,11413,11414,11415,11416,11417,11418,11419,11420,11421,11422,11423,11424,11425, #11328
11426,11427,11428,11429,11430,11431,11432,11433,11434,11435,11436,11437,11438,11439,11440,11441, #11344
11442,11443,11444,11445,11446,11447,11448,11449,11450,11451,11452,11453,11454,11455,11456,11457, #11360
11458,11459,11460,11461,11462,11463,11464,11465,11466,11467,11468,11469,4963,11470,11471,4491, #11376
11472,11473,11474,11475,4964,11476,11477,11478,11479,11480,11481,11482,11483,11484,11485,11486, #11392
11487,11488,11489,11490,11491,11492,4965,11493,11494,11495,11496,11497,11498,11499,11500,11501, #11408
11502,11503,11504,11505,11506,11507,11508,11509,11510,11511,11512,11513,11514,11515,11516,11517, #11424
11518,11519,11520,11521,11522,11523,11524,11525,11526,11527,11528,11529,3962,11530,11531,11532, #11440
11533,11534,11535,11536,11537,11538,11539,11540,11541,11542,11543,11544,11545,11546,11547,11548, #11456
11549,11550,11551,11552,11553,11554,11555,11556,11557,11558,11559,11560,11561,11562,11563,11564, #11472
4193,4194,11565,11566,11567,11568,11569,11570,11571,11572,11573,11574,11575,11576,11577,11578, #11488
11579,11580,11581,11582,11583,11584,11585,11586,11587,11588,11589,11590,11591,4966,4195,11592, #11504
11593,11594,11595,11596,11597,11598,11599,11600,11601,11602,11603,11604,3090,11605,11606,11607, #11520
11608,11609,11610,4967,11611,11612,11613,11614,11615,11616,11617,11618,11619,11620,11621,11622, #11536
11623,11624,11625,11626,11627,11628,11629,11630,11631,11632,11633,11634,11635,11636,11637,11638, #11552
11639,11640,11641,11642,11643,11644,11645,11646,11647,11648,11649,11650,11651,11652,11653,11654, #11568
11655,11656,11657,11658,11659,11660,11661,11662,11663,11664,11665,11666,11667,11668,11669,11670, #11584
11671,11672,11673,11674,4968,11675,11676,11677,11678,11679,11680,11681,11682,11683,11684,11685, #11600
11686,11687,11688,11689,11690,11691,11692,11693,3809,11694,11695,11696,11697,11698,11699,11700, #11616
11701,11702,11703,11704,11705,11706,11707,11708,11709,11710,11711,11712,11713,11714,11715,11716, #11632
11717,11718,3553,11719,11720,11721,11722,11723,11724,11725,11726,11727,11728,11729,11730,4969, #11648
11731,11732,11733,11734,11735,11736,11737,11738,11739,11740,4492,11741,11742,11743,11744,11745, #11664
11746,11747,11748,11749,11750,11751,11752,4970,11753,11754,11755,11756,11757,11758,11759,11760, #11680
11761,11762,11763,11764,11765,11766,11767,11768,11769,11770,11771,11772,11773,11774,11775,11776, #11696
11777,11778,11779,11780,11781,11782,11783,11784,11785,11786,11787,11788,11789,11790,4971,11791, #11712
11792,11793,11794,11795,11796,11797,4972,11798,11799,11800,11801,11802,11803,11804,11805,11806, #11728
11807,11808,11809,11810,4973,11811,11812,11813,11814,11815,11816,11817,11818,11819,11820,11821, #11744
11822,11823,11824,11825,11826,11827,11828,11829,11830,11831,11832,11833,11834,3680,3810,11835, #11760
11836,4974,11837,11838,11839,11840,11841,11842,11843,11844,11845,11846,11847,11848,11849,11850, #11776
11851,11852,11853,11854,11855,11856,11857,11858,11859,11860,11861,11862,11863,11864,11865,11866, #11792
11867,11868,11869,11870,11871,11872,11873,11874,11875,11876,11877,11878,11879,11880,11881,11882, #11808
11883,11884,4493,11885,11886,11887,11888,11889,11890,11891,11892,11893,11894,11895,11896,11897, #11824
11898,11899,11900,11901,11902,11903,11904,11905,11906,11907,11908,11909,11910,11911,11912,11913, #11840
11914,11915,4975,11916,11917,11918,11919,11920,11921,11922,11923,11924,11925,11926,11927,11928, #11856
11929,11930,11931,11932,11933,11934,11935,11936,11937,11938,11939,11940,11941,11942,11943,11944, #11872
11945,11946,11947,11948,11949,4976,11950,11951,11952,11953,11954,11955,11956,11957,11958,11959, #11888
11960,11961,11962,11963,11964,11965,11966,11967,11968,11969,11970,11971,11972,11973,11974,11975, #11904
11976,11977,11978,11979,11980,11981,11982,11983,11984,11985,11986,11987,4196,11988,11989,11990, #11920
11991,11992,4977,11993,11994,11995,11996,11997,11998,11999,12000,12001,12002,12003,12004,12005, #11936
12006,12007,12008,12009,12010,12011,12012,12013,12014,12015,12016,12017,12018,12019,12020,12021, #11952
12022,12023,12024,12025,12026,12027,12028,12029,12030,12031,12032,12033,12034,12035,12036,12037, #11968
12038,12039,12040,12041,12042,12043,12044,12045,12046,12047,12048,12049,12050,12051,12052,12053, #11984
12054,12055,12056,12057,12058,12059,12060,12061,4978,12062,12063,12064,12065,12066,12067,12068, #12000
12069,12070,12071,12072,12073,12074,12075,12076,12077,12078,12079,12080,12081,12082,12083,12084, #12016
12085,12086,12087,12088,12089,12090,12091,12092,12093,12094,12095,12096,12097,12098,12099,12100, #12032
12101,12102,12103,12104,12105,12106,12107,12108,12109,12110,12111,12112,12113,12114,12115,12116, #12048
12117,12118,12119,12120,12121,12122,12123,4979,12124,12125,12126,12127,12128,4197,12129,12130, #12064
12131,12132,12133,12134,12135,12136,12137,12138,12139,12140,12141,12142,12143,12144,12145,12146, #12080
12147,12148,12149,12150,12151,12152,12153,12154,4980,12155,12156,12157,12158,12159,12160,4494, #12096
12161,12162,12163,12164,3811,12165,12166,12167,12168,12169,4495,12170,12171,4496,12172,12173, #12112
12174,12175,12176,3812,12177,12178,12179,12180,12181,12182,12183,12184,12185,12186,12187,12188, #12128
12189,12190,12191,12192,12193,12194,12195,12196,12197,12198,12199,12200,12201,12202,12203,12204, #12144
12205,12206,12207,12208,12209,12210,12211,12212,12213,12214,12215,12216,12217,12218,12219,12220, #12160
12221,4981,12222,12223,12224,12225,12226,12227,12228,12229,12230,12231,12232,12233,12234,12235, #12176
4982,12236,12237,12238,12239,12240,12241,12242,12243,12244,12245,4983,12246,12247,12248,12249, #12192
4984,12250,12251,12252,12253,12254,12255,12256,12257,12258,12259,12260,12261,12262,12263,12264, #12208
4985,12265,4497,12266,12267,12268,12269,12270,12271,12272,12273,12274,12275,12276,12277,12278, #12224
12279,12280,12281,12282,12283,12284,12285,12286,12287,4986,12288,12289,12290,12291,12292,12293, #12240
12294,12295,12296,2473,12297,12298,12299,12300,12301,12302,12303,12304,12305,12306,12307,12308, #12256
12309,12310,12311,12312,12313,12314,12315,12316,12317,12318,12319,3963,12320,12321,12322,12323, #12272
12324,12325,12326,12327,12328,12329,12330,12331,12332,4987,12333,12334,12335,12336,12337,12338, #12288
12339,12340,12341,12342,12343,12344,12345,12346,12347,12348,12349,12350,12351,12352,12353,12354, #12304
12355,12356,12357,12358,12359,3964,12360,12361,12362,12363,12364,12365,12366,12367,12368,12369, #12320
12370,3965,12371,12372,12373,12374,12375,12376,12377,12378,12379,12380,12381,12382,12383,12384, #12336
12385,12386,12387,12388,12389,12390,12391,12392,12393,12394,12395,12396,12397,12398,12399,12400, #12352
12401,12402,12403,12404,12405,12406,12407,12408,4988,12409,12410,12411,12412,12413,12414,12415, #12368
12416,12417,12418,12419,12420,12421,12422,12423,12424,12425,12426,12427,12428,12429,12430,12431, #12384
12432,12433,12434,12435,12436,12437,12438,3554,12439,12440,12441,12442,12443,12444,12445,12446, #12400
12447,12448,12449,12450,12451,12452,12453,12454,12455,12456,12457,12458,12459,12460,12461,12462, #12416
12463,12464,4989,12465,12466,12467,12468,12469,12470,12471,12472,12473,12474,12475,12476,12477, #12432
12478,12479,12480,4990,12481,12482,12483,12484,12485,12486,12487,12488,12489,4498,12490,12491, #12448
12492,12493,12494,12495,12496,12497,12498,12499,12500,12501,12502,12503,12504,12505,12506,12507, #12464
12508,12509,12510,12511,12512,12513,12514,12515,12516,12517,12518,12519,12520,12521,12522,12523, #12480
12524,12525,12526,12527,12528,12529,12530,12531,12532,12533,12534,12535,12536,12537,12538,12539, #12496
12540,12541,12542,12543,12544,12545,12546,12547,12548,12549,12550,12551,4991,12552,12553,12554, #12512
12555,12556,12557,12558,12559,12560,12561,12562,12563,12564,12565,12566,12567,12568,12569,12570, #12528
12571,12572,12573,12574,12575,12576,12577,12578,3036,12579,12580,12581,12582,12583,3966,12584, #12544
12585,12586,12587,12588,12589,12590,12591,12592,12593,12594,12595,12596,12597,12598,12599,12600, #12560
12601,12602,12603,12604,12605,12606,12607,12608,12609,12610,12611,12612,12613,12614,12615,12616, #12576
12617,12618,12619,12620,12621,12622,12623,12624,12625,12626,12627,12628,12629,12630,12631,12632, #12592
12633,12634,12635,12636,12637,12638,12639,12640,12641,12642,12643,12644,12645,12646,4499,12647, #12608
12648,12649,12650,12651,12652,12653,12654,12655,12656,12657,12658,12659,12660,12661,12662,12663, #12624
12664,12665,12666,12667,12668,12669,12670,12671,12672,12673,12674,12675,12676,12677,12678,12679, #12640
12680,12681,12682,12683,12684,12685,12686,12687,12688,12689,12690,12691,12692,12693,12694,12695, #12656
12696,12697,12698,4992,12699,12700,12701,12702,12703,12704,12705,12706,12707,12708,12709,12710, #12672
12711,12712,12713,12714,12715,12716,12717,12718,12719,12720,12721,12722,12723,12724,12725,12726, #12688
12727,12728,12729,12730,12731,12732,12733,12734,12735,12736,12737,12738,12739,12740,12741,12742, #12704
12743,12744,12745,12746,12747,12748,12749,12750,12751,12752,12753,12754,12755,12756,12757,12758, #12720
12759,12760,12761,12762,12763,12764,12765,12766,12767,12768,12769,12770,12771,12772,12773,12774, #12736
12775,12776,12777,12778,4993,2175,12779,12780,12781,12782,12783,12784,12785,12786,4500,12787, #12752
12788,12789,12790,12791,12792,12793,12794,12795,12796,12797,12798,12799,12800,12801,12802,12803, #12768
12804,12805,12806,12807,12808,12809,12810,12811,12812,12813,12814,12815,12816,12817,12818,12819, #12784
12820,12821,12822,12823,12824,12825,12826,4198,3967,12827,12828,12829,12830,12831,12832,12833, #12800
12834,12835,12836,12837,12838,12839,12840,12841,12842,12843,12844,12845,12846,12847,12848,12849, #12816
12850,12851,12852,12853,12854,12855,12856,12857,12858,12859,12860,12861,4199,12862,12863,12864, #12832
12865,12866,12867,12868,12869,12870,12871,12872,12873,12874,12875,12876,12877,12878,12879,12880, #12848
12881,12882,12883,12884,12885,12886,12887,4501,12888,12889,12890,12891,12892,12893,12894,12895, #12864
12896,12897,12898,12899,12900,12901,12902,12903,12904,12905,12906,12907,12908,12909,12910,12911, #12880
12912,4994,12913,12914,12915,12916,12917,12918,12919,12920,12921,12922,12923,12924,12925,12926, #12896
12927,12928,12929,12930,12931,12932,12933,12934,12935,12936,12937,12938,12939,12940,12941,12942, #12912
12943,12944,12945,12946,12947,12948,12949,12950,12951,12952,12953,12954,12955,12956,1772,12957, #12928
12958,12959,12960,12961,12962,12963,12964,12965,12966,12967,12968,12969,12970,12971,12972,12973, #12944
12974,12975,12976,12977,12978,12979,12980,12981,12982,12983,12984,12985,12986,12987,12988,12989, #12960
12990,12991,12992,12993,12994,12995,12996,12997,4502,12998,4503,12999,13000,13001,13002,13003, #12976
4504,13004,13005,13006,13007,13008,13009,13010,13011,13012,13013,13014,13015,13016,13017,13018, #12992
13019,13020,13021,13022,13023,13024,13025,13026,13027,13028,13029,3449,13030,13031,13032,13033, #13008
13034,13035,13036,13037,13038,13039,13040,13041,13042,13043,13044,13045,13046,13047,13048,13049, #13024
13050,13051,13052,13053,13054,13055,13056,13057,13058,13059,13060,13061,13062,13063,13064,13065, #13040
13066,13067,13068,13069,13070,13071,13072,13073,13074,13075,13076,13077,13078,13079,13080,13081, #13056
13082,13083,13084,13085,13086,13087,13088,13089,13090,13091,13092,13093,13094,13095,13096,13097, #13072
13098,13099,13100,13101,13102,13103,13104,13105,13106,13107,13108,13109,13110,13111,13112,13113, #13088
13114,13115,13116,13117,13118,3968,13119,4995,13120,13121,13122,13123,13124,13125,13126,13127, #13104
4505,13128,13129,13130,13131,13132,13133,13134,4996,4506,13135,13136,13137,13138,13139,4997, #13120
13140,13141,13142,13143,13144,13145,13146,13147,13148,13149,13150,13151,13152,13153,13154,13155, #13136
13156,13157,13158,13159,4998,13160,13161,13162,13163,13164,13165,13166,13167,13168,13169,13170, #13152
13171,13172,13173,13174,13175,13176,4999,13177,13178,13179,13180,13181,13182,13183,13184,13185, #13168
13186,13187,13188,13189,13190,13191,13192,13193,13194,13195,13196,13197,13198,13199,13200,13201, #13184
13202,13203,13204,13205,13206,5000,13207,13208,13209,13210,13211,13212,13213,13214,13215,13216, #13200
13217,13218,13219,13220,13221,13222,13223,13224,13225,13226,13227,4200,5001,13228,13229,13230, #13216
13231,13232,13233,13234,13235,13236,13237,13238,13239,13240,3969,13241,13242,13243,13244,3970, #13232
13245,13246,13247,13248,13249,13250,13251,13252,13253,13254,13255,13256,13257,13258,13259,13260, #13248
13261,13262,13263,13264,13265,13266,13267,13268,3450,13269,13270,13271,13272,13273,13274,13275, #13264
13276,5002,13277,13278,13279,13280,13281,13282,13283,13284,13285,13286,13287,13288,13289,13290, #13280
13291,13292,13293,13294,13295,13296,13297,13298,13299,13300,13301,13302,3813,13303,13304,13305, #13296
13306,13307,13308,13309,13310,13311,13312,13313,13314,13315,13316,13317,13318,13319,13320,13321, #13312
13322,13323,13324,13325,13326,13327,13328,4507,13329,13330,13331,13332,13333,13334,13335,13336, #13328
13337,13338,13339,13340,13341,5003,13342,13343,13344,13345,13346,13347,13348,13349,13350,13351, #13344
13352,13353,13354,13355,13356,13357,13358,13359,13360,13361,13362,13363,13364,13365,13366,13367, #13360
5004,13368,13369,13370,13371,13372,13373,13374,13375,13376,13377,13378,13379,13380,13381,13382, #13376
13383,13384,13385,13386,13387,13388,13389,13390,13391,13392,13393,13394,13395,13396,13397,13398, #13392
13399,13400,13401,13402,13403,13404,13405,13406,13407,13408,13409,13410,13411,13412,13413,13414, #13408
13415,13416,13417,13418,13419,13420,13421,13422,13423,13424,13425,13426,13427,13428,13429,13430, #13424
13431,13432,4508,13433,13434,13435,4201,13436,13437,13438,13439,13440,13441,13442,13443,13444, #13440
13445,13446,13447,13448,13449,13450,13451,13452,13453,13454,13455,13456,13457,5005,13458,13459, #13456
13460,13461,13462,13463,13464,13465,13466,13467,13468,13469,13470,4509,13471,13472,13473,13474, #13472
13475,13476,13477,13478,13479,13480,13481,13482,13483,13484,13485,13486,13487,13488,13489,13490, #13488
13491,13492,13493,13494,13495,13496,13497,13498,13499,13500,13501,13502,13503,13504,13505,13506, #13504
13507,13508,13509,13510,13511,13512,13513,13514,13515,13516,13517,13518,13519,13520,13521,13522, #13520
13523,13524,13525,13526,13527,13528,13529,13530,13531,13532,13533,13534,13535,13536,13537,13538, #13536
13539,13540,13541,13542,13543,13544,13545,13546,13547,13548,13549,13550,13551,13552,13553,13554, #13552
13555,13556,13557,13558,13559,13560,13561,13562,13563,13564,13565,13566,13567,13568,13569,13570, #13568
13571,13572,13573,13574,13575,13576,13577,13578,13579,13580,13581,13582,13583,13584,13585,13586, #13584
13587,13588,13589,13590,13591,13592,13593,13594,13595,13596,13597,13598,13599,13600,13601,13602, #13600
13603,13604,13605,13606,13607,13608,13609,13610,13611,13612,13613,13614,13615,13616,13617,13618, #13616
13619,13620,13621,13622,13623,13624,13625,13626,13627,13628,13629,13630,13631,13632,13633,13634, #13632
13635,13636,13637,13638,13639,13640,13641,13642,5006,13643,13644,13645,13646,13647,13648,13649, #13648
13650,13651,5007,13652,13653,13654,13655,13656,13657,13658,13659,13660,13661,13662,13663,13664, #13664
13665,13666,13667,13668,13669,13670,13671,13672,13673,13674,13675,13676,13677,13678,13679,13680, #13680
13681,13682,13683,13684,13685,13686,13687,13688,13689,13690,13691,13692,13693,13694,13695,13696, #13696
13697,13698,13699,13700,13701,13702,13703,13704,13705,13706,13707,13708,13709,13710,13711,13712, #13712
13713,13714,13715,13716,13717,13718,13719,13720,13721,13722,13723,13724,13725,13726,13727,13728, #13728
13729,13730,13731,13732,13733,13734,13735,13736,13737,13738,13739,13740,13741,13742,13743,13744, #13744
13745,13746,13747,13748,13749,13750,13751,13752,13753,13754,13755,13756,13757,13758,13759,13760, #13760
13761,13762,13763,13764,13765,13766,13767,13768,13769,13770,13771,13772,13773,13774,3273,13775, #13776
13776,13777,13778,13779,13780,13781,13782,13783,13784,13785,13786,13787,13788,13789,13790,13791, #13792
13792,13793,13794,13795,13796,13797,13798,13799,13800,13801,13802,13803,13804,13805,13806,13807, #13808
13808,13809,13810,13811,13812,13813,13814,13815,13816,13817,13818,13819,13820,13821,13822,13823, #13824
13824,13825,13826,13827,13828,13829,13830,13831,13832,13833,13834,13835,13836,13837,13838,13839, #13840
13840,13841,13842,13843,13844,13845,13846,13847,13848,13849,13850,13851,13852,13853,13854,13855, #13856
13856,13857,13858,13859,13860,13861,13862,13863,13864,13865,13866,13867,13868,13869,13870,13871, #13872
13872,13873,13874,13875,13876,13877,13878,13879,13880,13881,13882,13883,13884,13885,13886,13887, #13888
13888,13889,13890,13891,13892,13893,13894,13895,13896,13897,13898,13899,13900,13901,13902,13903, #13904
13904,13905,13906,13907,13908,13909,13910,13911,13912,13913,13914,13915,13916,13917,13918,13919, #13920
13920,13921,13922,13923,13924,13925,13926,13927,13928,13929,13930,13931,13932,13933,13934,13935, #13936
13936,13937,13938,13939,13940,13941,13942,13943,13944,13945,13946,13947,13948,13949,13950,13951, #13952
13952,13953,13954,13955,13956,13957,13958,13959,13960,13961,13962,13963,13964,13965,13966,13967, #13968
13968,13969,13970,13971,13972) #13973

# flake8: noqa

"""The match_hostname() function from Python 3.3.3, essential when using SSL."""

# Note: This file is under the PSF license as the code comes from the python
# stdlib.   http://docs.python.org/3/license.html

import re

__version__ = '3.4.0.2'

class CertificateError(ValueError):
    pass


def _dnsname_match(dn, hostname, max_wildcards=1):
    """Matching according to RFC 6125, section 6.4.3

    http://tools.ietf.org/html/rfc6125#section-6.4.3
    """
    pats = []
    if not dn:
        return False

    # Ported from python3-syntax:
    # leftmost, *remainder = dn.split(r'.')
    parts = dn.split(r'.')
    leftmost = parts[0]
    remainder = parts[1:]

    wildcards = leftmost.count('*')
    if wildcards > max_wildcards:
        # Issue #17980: avoid denials of service by refusing more
        # than one wildcard per fragment.  A survey of established
        # policy among SSL implementations showed it to be a
        # reasonable choice.
        raise CertificateError(
            "too many wildcards in certificate DNS name: " + repr(dn))

    # speed up common case w/o wildcards
    if not wildcards:
        return dn.lower() == hostname.lower()

    # RFC 6125, section 6.4.3, subitem 1.
    # The client SHOULD NOT attempt to match a presented identifier in which
    # the wildcard character comprises a label other than the left-most label.
    if leftmost == '*':
        # When '*' is a fragment by itself, it matches a non-empty dotless
        # fragment.
        pats.append('[^.]+')
    elif leftmost.startswith('xn--') or hostname.startswith('xn--'):
        # RFC 6125, section 6.4.3, subitem 3.
        # The client SHOULD NOT attempt to match a presented identifier
        # where the wildcard character is embedded within an A-label or
        # U-label of an internationalized domain name.
        pats.append(re.escape(leftmost))
    else:
        # Otherwise, '*' matches any dotless string, e.g. www*
        pats.append(re.escape(leftmost).replace(r'\*', '[^.]*'))

    # add the remaining fragments, ignore any wildcards
    for frag in remainder:
        pats.append(re.escape(frag))

    pat = re.compile(r'\A' + r'\.'.join(pats) + r'\Z', re.IGNORECASE)
    return pat.match(hostname)


def match_hostname(cert, hostname):
    """Verify that *cert* (in decoded format as returned by
    SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125
    rules are followed, but IP addresses are not accepted for *hostname*.

    CertificateError is raised on failure. On success, the function
    returns nothing.
    """
    if not cert:
        raise ValueError("empty or no certificate")
    dnsnames = []
    san = cert.get('subjectAltName', ())
    for key, value in san:
        if key == 'DNS':
            if _dnsname_match(value, hostname):
                return
            dnsnames.append(value)
    if not dnsnames:
        # The subject is only checked when there is no dNSName entry
        # in subjectAltName
        for sub in cert.get('subject', ()):
            for key, value in sub:
                # XXX according to RFC 2818, the most specific Common Name
                # must be used.
                if key == 'commonName':
                    if _dnsname_match(value, hostname):
                        return
                    dnsnames.append(value)
    if len(dnsnames) > 1:
        raise CertificateError("hostname %r "
            "doesn't match either of %s"
            % (hostname, ', '.join(map(repr, dnsnames))))
    elif len(dnsnames) == 1:
        raise CertificateError("hostname %r "
            "doesn't match %r"
            % (hostname, dnsnames[0]))
    else:
        raise CertificateError("no appropriate commonName or "
            "subjectAltName fields were found")

#!/usr/bin/python
# -*- coding: utf-8 -*-
__author__ = 'BernardoGO'


import config
from server.utils import info
from server.handlers.requestHandler import requestHandler
from server.support import multithreadSupport
from server.utils import statusCheck
from server.utils import checkFolders
from server.utils import parseArgs
import argparse

import sys
if sys.version_info >= (3, 0):
    import http.server as http
else:
    from BaseHTTPServer import HTTPServer as http

def main():
    parseArgs.parseAll()

    checkFolders.createIfNotExists()
    if config.__ENABLE_MULTITHREADING__ == False:
        server = http((config.__LISTEN_ADDRESS__, config.__INTERNAL_PORT__), requestHandler)
    else:
        server = multithreadSupport.ThreadedHTTPServer((config.__LISTEN_ADDRESS__, config.__INTERNAL_PORT__),
                                                       requestHandler)
    statusCheck.printConfigs()


    if parseArgs.parsed.ver == True:
        print ("Server Version: "+str(info.__SRV_VERSION__))
        return

    if parseArgs.parsed.test == False:
        print('Starting server, use <Ctrl-C> to stop\nStarted.')
        server.serve_forever()

if __name__ == '__main__':
    main()



VERSION = (1, 2, 1)
__version__ = '.'.join(map(str, VERSION))

default_app_config = 'cachalot.apps.CachalotConfig'

#!/usr/bin/env python

import os
from setuptools import setup, find_packages
from cachalot import __version__


CURRENT_PATH = os.path.abspath(os.path.dirname(__file__))

with open(os.path.join(CURRENT_PATH, 'requirements.txt')) as f:
    required = f.read().splitlines()


setup(
    name='django-cachalot',
    version=__version__,
    author='Bertrand Bordage',
    author_email='bordage.bertrand@gmail.com',
    url='https://github.com/BertrandBordage/django-cachalot',
    description='Caches your Django ORM queries '
                'and automatically invalidates them.',
    long_description=open('README.rst').read(),
    classifiers=[
        'Development Status :: 5 - Production/Stable',
        'Framework :: Django',
        'Intended Audience :: Developers',
        'License :: OSI Approved :: BSD License',
        'Operating System :: OS Independent',
        'Programming Language :: Python :: 2',
        'Programming Language :: Python :: 2.7',
        'Programming Language :: Python :: 3',
        'Programming Language :: Python :: 3.2',
        'Programming Language :: Python :: 3.3',
        'Programming Language :: Python :: 3.4',
        'Programming Language :: Python :: 3.5',
        'Topic :: Internet :: WWW/HTTP',
    ],
    license='BSD',
    packages=find_packages(),
    install_requires=required,
    include_package_data=True,
    zip_safe=False,
)

from django.db.models import QuerySet
from django.utils.six import string_types

from .sql.base import to_alphanum, from_alphanum


class Path:
    def __init__(self, field, value):
        self.field = field
        self.attname = getattr(self.field, 'attname', None)
        self.field_bound = self.attname is not None
        self.qs = (self.field.model._default_manager.all()
                   if self.field_bound else QuerySet())
        self.value = value

    def __repr__(self):
        if self.field_bound:
            return '<Path %s %s>' % (self.field, self.value)
        return '<Path %s>' % self.value

    def __str__(self):
        return str(self.value)

    def __eq__(self, other):
        if isinstance(other, Path):
            other = other.value
        return self.value == other

    def __ne__(self, other):
        if isinstance(other, Path):
            other = other.value
        return self.value != other

    def __lt__(self, other):
        # We simulate the effects of a NULLS LAST.
        if self.value is None:
            return False
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return True
        return self.value < other

    def __le__(self, other):
        # We simulate the effects of a NULLS LAST.
        if self.value is None:
            return False
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return True
        return self.value <= other

    def __gt__(self, other):
        # We simulate the effects of a NULLS LAST.
        if self.value is None:
            return True
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return False
        return self.value > other

    def __ge__(self, other):
        # We simulate the effects of a NULLS LAST.
        if self.value is None:
            return True
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return False
        return self.value >= other

    def get_children(self):
        if self.value is None:
            return self.qs.none()
        return self.qs.filter(
            **{self.attname + '__match': self.value + '.*{1}'})

    def get_ancestors(self, include_self=False):
        if self.value is None or (self.is_root() and not include_self):
            return self.qs.none()
        paths = []
        path = ''
        for part in self.value.split('.'):
            if path:
                path += '.'
            path += part
            paths.append(path)
        if not include_self:
            paths.pop()
        return self.qs.filter(**{self.attname + '__in': paths})

    def get_descendants(self, include_self=False):
        if self.value is None:
            return self.qs.none()
        return self.qs.filter(
            **{self.attname + '__match': self.value + ('.*' if include_self
                                                       else '.*{1,}')})

    def get_siblings(self, include_self=False):
        if self.value is None:
            return self.qs.none()
        qs = self.qs
        match = '*{1}'
        if not self.is_root():
            match = self.value.rsplit('.', 1)[0] + '.' + match
        if not include_self:
            qs = qs.exclude(**{self.attname: self.value})
        return qs.filter(**{self.attname + '__match': match})

    def get_prev_siblings(self, include_self=False):
        if self.value is None:
            return self.qs.none()
        siblings = self.get_siblings(include_self=include_self)
        lookup = '__lte' if include_self else '__lt'
        return (siblings.filter(**{self.attname + lookup: self.value})
                .order_by('-' + self.attname))

    def get_next_siblings(self, include_self=False):
        if self.value is None:
            return self.qs.none()
        siblings = self.get_siblings(include_self=include_self)
        lookup = '__gte' if include_self else '__gt'
        return (siblings.filter(**{self.attname + lookup: self.value})
                .order_by(self.attname))

    def get_prev_sibling(self):
        if self.value is None:
            return None

        # TODO: Handle the case where the trigger is not in place.

        if self.is_root():
            parent_path = ''
            current_label = self.value
        else:
            parent_path, current_label = self.value.rsplit('.', 1)
            parent_path += '.'
        if not current_label.lstrip('0'):
            return
        prev_label = parent_path + to_alphanum(
            from_alphanum(current_label) - 1, len(current_label))
        return self.qs.get(**{self.attname: prev_label})

    def get_next_sibling(self):
        if self.value is None:
            return None

        # TODO: Handle the case where the trigger is not in place.

        if self.is_root():
            parent_path = ''
            current_label = self.value
        else:
            parent_path, current_label = self.value.rsplit('.', 1)
            parent_path += '.'
        next_label = parent_path + to_alphanum(
            from_alphanum(current_label) + 1, len(current_label))
        return self.qs.filter(**{self.attname: next_label}).first()

    def get_level(self):
        if self.value is not None:
            return self.value.count('.') + 1

    def is_root(self):
        if self.value is not None:
            return '.' not in self.value

    def is_leaf(self):
        if self.value is not None:
            return not self.get_children().exists()

    def is_ancestor_of(self, other, include_self=False):
        if self.value is None:
            return False
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return False
        if not isinstance(other, string_types):
            raise TypeError('`other` must be a `Path` instance or a string.')
        if not include_self and self.value == other:
            return False
        return other.startswith(self.value)

    def is_descendant_of(self, other, include_self=False):
        if self.value is None:
            return False
        if isinstance(other, Path):
            other = other.value
        if other is None:
            return False
        if not isinstance(other, string_types):
            raise TypeError('`other` must be a `Path` instance or a string.')
        if not include_self and self.value == other:
            return False
        return self.value.startswith(other)


# Tells psycopg2 how to prepare a Path object for the database,
# in case it doesn't go through the ORM.
try:
    import psycopg2
except ImportError:
    pass
else:
    from psycopg2.extensions import adapt, register_adapter, AsIs

    def adapt_path(path):
        return AsIs('%s::ltree' % adapt(path.value))

    register_adapter(Path, adapt_path)

# -*- coding: utf-8 -*-

# Form implementation generated from reading ui file 'help.ui'
#
# Created: Wed Jan 14 22:42:39 2015
#      by: PyQt4 UI code generator 4.9.4
#
# WARNING! All changes made in this file will be lost!

from PyQt4 import QtCore, QtGui

try:
    _fromUtf8 = QtCore.QString.fromUtf8
except AttributeError:
    _fromUtf8 = lambda s: s

class Ui_helpDialog(object):
    def setupUi(self, helpDialog):
        helpDialog.setObjectName(_fromUtf8("helpDialog"))
        helpDialog.resize(335, 96)
        self.formLayout = QtGui.QFormLayout(helpDialog)
        self.formLayout.setObjectName(_fromUtf8("formLayout"))
        self.labelHelpURI = QtGui.QLabel(helpDialog)
        self.labelHelpURI.setOpenExternalLinks(True)
        self.labelHelpURI.setObjectName(_fromUtf8("labelHelpURI"))
        self.formLayout.setWidget(1, QtGui.QFormLayout.LabelRole, self.labelHelpURI)
        self.label = QtGui.QLabel(helpDialog)
        self.label.setWordWrap(True)
        self.label.setObjectName(_fromUtf8("label"))
        self.formLayout.setWidget(0, QtGui.QFormLayout.SpanningRole, self.label)
        spacerItem = QtGui.QSpacerItem(40, 20, QtGui.QSizePolicy.Expanding, QtGui.QSizePolicy.Minimum)
        self.formLayout.setItem(2, QtGui.QFormLayout.LabelRole, spacerItem)
        self.buttonBox = QtGui.QDialogButtonBox(helpDialog)
        self.buttonBox.setOrientation(QtCore.Qt.Horizontal)
        self.buttonBox.setStandardButtons(QtGui.QDialogButtonBox.Ok)
        self.buttonBox.setObjectName(_fromUtf8("buttonBox"))
        self.formLayout.setWidget(2, QtGui.QFormLayout.FieldRole, self.buttonBox)

        self.retranslateUi(helpDialog)
        QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL(_fromUtf8("accepted()")), helpDialog.accept)
        QtCore.QObject.connect(self.buttonBox, QtCore.SIGNAL(_fromUtf8("rejected()")), helpDialog.reject)
        QtCore.QMetaObject.connectSlotsByName(helpDialog)

    def retranslateUi(self, helpDialog):
        helpDialog.setWindowTitle(QtGui.QApplication.translate("helpDialog", "Help", None, QtGui.QApplication.UnicodeUTF8))
        self.labelHelpURI.setText(QtGui.QApplication.translate("helpDialog", "<a href=\"https://bitmessage.org/wiki/PyBitmessage_Help\">https://bitmessage.org/wiki/PyBitmessage_Help</a>", None, QtGui.QApplication.UnicodeUTF8))
        self.label.setText(QtGui.QApplication.translate("helpDialog", "As Bitmessage is a collaborative project, help can be found online in the Bitmessage Wiki:", None, QtGui.QApplication.UnicodeUTF8))


from helper_sql import *

def insert(t):
    sqlExecute('''INSERT INTO sent VALUES (?,?,?,?,?,?,?,?,?,?,?,?,?,?,?)''', *t)

"""
Module for reporting into http://www.blazemeter.com/ service

Copyright 2015 BlazeMeter Inc.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""
import copy
import json
import logging
import math
import os
import sys
import time
import traceback
import zipfile

import yaml
from urwid import Pile, Text

from bzt import ManualShutdown
from bzt.engine import Reporter, Provisioning, ScenarioExecutor, Configuration, Service
from bzt.modules.aggregator import DataPoint, KPISet, ConsolidatingAggregator, ResultsProvider, AggregatorListener
from bzt.modules.console import WidgetProvider
from bzt.modules.jmeter import JMeterExecutor
from bzt.six import BytesIO, text_type, iteritems, HTTPError, urlencode, Request, urlopen, r_input, URLError, \
    string_types
from bzt.utils import to_json, dehumanize_time, MultiPartForm, BetterDict, open_browser


class BlazeMeterUploader(Reporter, AggregatorListener):
    """
    Reporter class

    :type client: BlazeMeterClient
    """

    def __init__(self):
        super(BlazeMeterUploader, self).__init__()
        self.browser_open = 'start'
        self.client = BlazeMeterClient(self.log)
        self.test_id = ""
        self.kpi_buffer = []
        self.send_interval = 30
        self.sess_name = None
        self._last_status_check = time.time()

    def prepare(self):
        """
        Read options for uploading, check that they're sane
        """
        super(BlazeMeterUploader, self).prepare()
        self.client.logger_limit = self.settings.get("request-logging-limit", self.client.logger_limit)
        self.client.address = self.settings.get("address", self.client.address)
        self.client.data_address = self.settings.get("data-address", self.client.data_address)
        self.client.timeout = dehumanize_time(self.settings.get("timeout", self.client.timeout))
        self.send_interval = dehumanize_time(self.settings.get("send-interval", self.send_interval))
        self.browser_open = self.settings.get("browser-open", self.browser_open)
        token = self.settings.get("token", "")
        if not token:
            self.log.warning("No BlazeMeter API key provided, will upload anonymously")
        self.client.token = token

        self.client.active_session_id = self.parameters.get("session-id", None)
        self.client.test_id = self.parameters.get("test-id", None)
        self.client.user_id = self.parameters.get("user-id", None)
        self.client.data_signature = self.parameters.get("signature", None)
        self.client.kpi_target = self.parameters.get("kpi-target", self.client.kpi_target)

        if not self.client.test_id:
            try:
                self.client.ping()  # to check connectivity and auth
            except HTTPError:
                self.log.error("Cannot reach online results storage, maybe the address/token is wrong")
                raise

            if token:
                finder = ProjectFinder(self.parameters, self.settings, self.client, self.engine)
                self.test_id = finder.resolve_test_id({"type": "external"}, self.engine.config, [])

        self.sess_name = self.parameters.get("report-name", self.settings.get("report-name", self.sess_name))
        if self.sess_name == 'ask' and sys.stdin.isatty():
            self.sess_name = r_input("Please enter report-name: ")

        if isinstance(self.engine.aggregator, ResultsProvider):
            self.engine.aggregator.add_listener(self)

    def startup(self):
        """
        Initiate online test
        """
        super(BlazeMeterUploader, self).startup()

        if not self.client.active_session_id:
            try:
                url = self.client.start_online(self.test_id, self.sess_name)
                self.log.info("Started data feeding: %s", url)
                if self.browser_open in ('start', 'both'):
                    open_browser(url)
            except KeyboardInterrupt:
                raise
            except BaseException as exc:
                self.log.debug("Exception: %s", traceback.format_exc())
                self.log.warning("Failed to start feeding: %s", exc)
                raise

    def __get_jtls_and_more(self):
        """
        Compress all files in artifacts dir to single zipfile
        :return: BytesIO
        """
        mfile = BytesIO()
        max_file_size = self.settings.get('artifact-upload-size-limit', 10) * 1024 * 1024  # 10MB
        with zipfile.ZipFile(mfile, mode='w', compression=zipfile.ZIP_DEFLATED, allowZip64=True) as zfh:
            for handler in self.engine.log.parent.handlers:
                if isinstance(handler, logging.FileHandler):
                    zfh.write(handler.baseFilename, os.path.basename(handler.baseFilename))

            for root, _dirs, files in os.walk(self.engine.artifacts_dir):
                for filename in files:
                    if os.path.getsize(os.path.join(root, filename)) <= max_file_size:
                        zfh.write(os.path.join(root, filename),
                                  os.path.join(os.path.relpath(root, self.engine.artifacts_dir), filename))
                    else:
                        msg = "File %s exceeds maximum size quota of %s and won't be included into upload"
                        self.log.warning(msg, filename, max_file_size)
        return mfile

    def __upload_artifacts(self):
        """
        If token provided, upload artifacts folder contents and jmeter_log
        else: jmeter_log only
        :return:
        """
        if self.client.token:
            self.log.info("Uploading all artifacts as jtls_and_more.zip ...")
            mfile = self.__get_jtls_and_more()
            self.client.upload_file("jtls_and_more.zip", mfile.getvalue())

        for executor in self.engine.provisioning.executors:
            if isinstance(executor, JMeterExecutor):
                if executor.jmeter_log:
                    self.log.info("Uploading %s", executor.jmeter_log)
                    self.client.upload_file(executor.jmeter_log)

    def post_process(self):
        """
        Upload results if possible
        """
        if not self.client.active_session_id:
            self.log.debug("No feeding session obtained, nothing to finalize")
            return

        try:
            self.__send_data(self.kpi_buffer, False, True)
            self.kpi_buffer = []
        finally:
            self._postproc_phase2()

        if self.client.results_url:
            if self.browser_open in ('end', 'both'):
                open_browser(self.client.results_url)
            self.log.info("Online report link: %s", self.client.results_url)

    def _postproc_phase2(self):
        try:
            self.__upload_artifacts()
        except IOError:
            self.log.warning("Failed artifact upload: %s", traceback.format_exc())
        finally:
            self.set_last_status_check(self.parameters.get('forced-last-check', self._last_status_check))
            tries = self.send_interval  # NOTE: you dirty one...
            while not self._last_status_check and tries > 0:
                self.log.info("Waiting for ping...")
                time.sleep(self.send_interval)
                tries -= 1

            self._postproc_phase3()

    def _postproc_phase3(self):
        try:
            self.client.end_online()
            if self.engine.stopping_reason:
                note = "%s: %s" % (self.engine.stopping_reason.__class__.__name__, str(self.engine.stopping_reason))
                sess = self.client.get_session(self.client.active_session_id)
                if 'note' in sess:
                    note += "\n" + sess['note']
                self.client.update_session(self.client.active_session_id, {"note": note})
        except KeyboardInterrupt:
            raise
        except BaseException as exc:
            self.log.warning("Failed to finish online: %s", exc)

    def check(self):
        """
        Send data if any in buffer

        :return:
        """
        self.log.debug("KPI bulk buffer len: %s", len(self.kpi_buffer))
        if len(self.kpi_buffer):
            if self.client.last_ts < (time.time() - self.send_interval):
                self.__send_data(self.kpi_buffer)
                self.kpi_buffer = []
        return super(BlazeMeterUploader, self).check()

    def __send_data(self, data, do_check=True, is_final=False):
        """
        :param data: list[bzt.modules.aggregator.DataPoint]
        :return:
        """
        if not self.client.active_session_id:
            return

        try:
            self.client.send_kpi_data(data, do_check, is_final)
        except IOError as _:
            self.log.debug("Error sending data: %s", traceback.format_exc())
            self.log.warning("Failed to send data, will retry in %s sec...", self.client.timeout)
            try:
                time.sleep(self.client.timeout)
                self.client.send_kpi_data(data, do_check, is_final)
                self.log.info("Succeeded with retry")
            except IOError as _:
                self.log.error("Fatal error sending data: %s", traceback.format_exc())
                self.log.warning("Will skip failed data and continue running")

        if not data:
            return

        try:
            self.client.send_error_summary(data)
        except IOError as exc:
            self.log.debug("Failed sending error summary: %s", traceback.format_exc())
            self.log.warning("Failed to send error summary: %s", exc)

    def aggregated_second(self, data):
        """
        Send online data
        :param data: DataPoint
        :return:
        """
        self.kpi_buffer.append(data)

    def set_last_status_check(self, value):
        self._last_status_check = value
        self.log.debug("Set last check time to: %s", self._last_status_check)


class ProjectFinder(object):
    def __init__(self, parameters, settings, client, engine):
        super(ProjectFinder, self).__init__()
        self.default_test_name = "Taurus Test"
        self.client = client
        self.parameters = parameters
        self.settings = settings
        self.engine = engine
        self.test_name = None

    def resolve_test_id(self, test_config, taurus_config, rfiles):
        proj_name = self.parameters.get("project", self.settings.get("project", None))
        if isinstance(proj_name, (int, float)):
            proj_id = int(proj_name)
            self.engine.log.debug("Treating project name as ID: %s", proj_id)
        elif proj_name is not None:
            proj_id = self.client.project_by_name(proj_name)
        else:
            proj_id = None

        self.test_name = self.parameters.get("test", self.settings.get("test", self.default_test_name))
        return self.client.test_by_name(self.test_name, test_config, taurus_config, rfiles, proj_id)


class BlazeMeterClient(object):
    """ Service client class """

    def __init__(self, parent_logger):
        self.kpi_target = 'labels_bulk'
        self.logger_limit = 256
        self.user_id = None
        self.test_id = None
        self.log = parent_logger.getChild(self.__class__.__name__)
        self.token = None
        self.address = "https://a.blazemeter.com"
        self.data_address = "https://data.blazemeter.com"
        self.results_url = None
        self.active_session_id = None  # FIXME: it's not good using it for both session id and master ID
        self.data_signature = None
        self.first_ts = sys.maxsize
        self.last_ts = 0
        self.timeout = 10

    def _request(self, url, data=None, headers=None, checker=None, method=None):
        if not headers:
            headers = {}
        if self.token:
            headers["X-Api-Key"] = self.token

        log_method = 'GET' if data is None else 'POST'
        if method:
            log_method = method

        self.log.debug("Request: %s %s %s", log_method, url, data[:self.logger_limit] if data else None)
        # .encode("utf-8") is probably better
        data = data.encode() if isinstance(data, text_type) else data
        req = Request(url, data, headers)
        if method:
            req.get_method = lambda: method

        response = urlopen(req, timeout=self.timeout)

        if checker:
            checker(response)

        resp = response.read()
        if not isinstance(resp, str):
            resp = resp.decode()

        self.log.debug("Response: %s", resp[:self.logger_limit] if resp else None)
        try:
            return json.loads(resp) if len(resp) else {}
        except ValueError:
            self.log.warning("Non-JSON response from API: %s", resp)
            raise

    def start_online(self, test_id, session_name):
        """
        Start online test

        :type test_id: str
        :return:
        """
        self.log.info("Initiating data feeding...")
        data = urlencode({})

        if self.token:
            url = self.address + "/api/latest/tests/%s/start-external" % test_id
        else:
            url = self.address + "/api/latest/sessions"

        resp = self._request(url, data)

        self.active_session_id = str(resp['result']['session']['id'])
        self.data_signature = str(resp['result']['signature'])
        self.test_id = test_id
        self.user_id = str(resp['result']['session']['userId'])
        if self.token:
            self.results_url = self.address + '/app/#reports/%s' % self.active_session_id
            if session_name:
                url = self.address + "/api/latest/sessions/%s" % self.active_session_id
                self._request(url, to_json({"name": str(session_name)}),
                              headers={"Content-Type": "application/json"}, method='PATCH')
        else:
            self.test_id = resp['result']['session']['testId']
            self.results_url = resp['result']['publicTokenUrl']
        return self.results_url

    def start_taurus(self, test_id):
        """
        Start online test

        :type test_id: str
        :return:
        """
        self.log.info("Initiating cloud test with %s ...", self.address)
        data = urlencode({})

        url = self.address + "/api/latest/tests/%s/start" % test_id

        resp = self._request(url, data)

        self.log.debug("Response: %s", resp['result'])
        self.active_session_id = str(resp['result']['id'])
        self.results_url = self.address + '/app/#reports/%s' % self.active_session_id
        return self.results_url

    def end_online(self):
        """
        Finish online test
        """
        if not self.active_session_id:
            self.log.debug("Feeding not started, so not stopping")
        else:
            self.log.info("Ending data feeding...")
            if self.token:
                url = self.address + "/api/latest/sessions/%s/terminate"
                self._request(url % self.active_session_id)
            else:
                url = self.address + "/api/latest/sessions/%s/terminateExternal"
                data = {"signature": self.data_signature, "testId": self.test_id, "sessionId": self.active_session_id}
                self._request(url % self.active_session_id, json.dumps(data))

    def end_master(self, master_id):
        if master_id:
            self.log.info("Ending cloud test...")
            url = self.address + "/api/latest/masters/%s/terminate"
            self._request(url % master_id)

    def project_by_name(self, proj_name):
        """
        :type proj_name: str
        :rtype: int
        """
        projects = self.get_projects()
        matching = []
        for project in projects:
            if project['name'] == proj_name:
                matching.append(project['id'])

        if len(matching) > 1:
            self.log.warning("Several projects IDs matched with '%s': %s", proj_name, matching)
            raise ValueError("Project name is ambiguous, please use project ID instead of name to distinguish it")
        elif len(matching) == 1:
            return matching[0]
        else:
            self.log.info("Creating project '%s'...", proj_name)
            return self.create_project(proj_name)

    def test_by_name(self, name, configuration, taurus_config, resource_files, proj_id):
        """

        :type name: str
        :rtype: str
        """
        tests = self.get_tests()
        test_id = None
        for test in tests:
            self.log.debug("Test: %s", test)
            if "name" in test and test['name'] == name:
                if test['configuration']['type'] == configuration['type']:
                    if not proj_id or proj_id == test['projectId']:
                        test_id = test['id']
                        self.log.debug("Matched: %s", test)

        if not test_id:
            self.log.debug("Creating new test")
            url = self.address + '/api/latest/tests'
            data = {"name": name, "projectId": proj_id, "configuration": configuration}
            hdr = {"Content-Type": " application/json"}
            resp = self._request(url, json.dumps(data), headers=hdr)
            test_id = resp['result']['id']

        if configuration['type'] == 'taurus':  # FIXME: this is weird way to code, subclass it or something
            self.log.debug("Uploading files into the test: %s", resource_files)
            url = '%s/api/latest/tests/%s/files' % (self.address, test_id)

            body = MultiPartForm()

            body.add_file_as_string('script', 'taurus.yml', yaml.dump(taurus_config, default_flow_style=False,
                                                                      explicit_start=True, canonical=False))

            for rfile in resource_files:
                body.add_file('files[]', rfile)

            hdr = {"Content-Type": body.get_content_type()}
            _ = self._request(url, body.form_as_bytes(), headers=hdr)

        self.log.debug("Using test ID: %s", test_id)
        return test_id

    def get_tests(self):
        """

        :rtype: list
        """
        tests = self._request(self.address + '/api/latest/tests')
        self.log.debug("Tests for user: %s", len(tests['result']))
        return tests['result']

    def send_kpi_data(self, data_buffer, is_check_response=True, is_final=False):
        """
        Sends online data

        :param is_check_response:
        :type data_buffer: list[bzt.modules.aggregator.DataPoint]
        """
        data = []

        for sec in data_buffer:
            self.first_ts = min(self.first_ts, sec[DataPoint.TIMESTAMP])
            self.last_ts = max(self.last_ts, sec[DataPoint.TIMESTAMP])

            for lbl, item in iteritems(sec[DataPoint.CURRENT]):
                if lbl == '':
                    label = "ALL"
                else:
                    label = lbl

                json_item = None
                for lbl_item in data:
                    if lbl_item["name"] == label:
                        json_item = lbl_item
                        break

                if not json_item:
                    json_item = self.__label_skel(label)
                    data.append(json_item)

                interval_item = self.__interval_json(item, sec)
                for r_code, cnt in iteritems(item[KPISet.RESP_CODES]):
                    interval_item['rc'].append({"n": cnt, "rc": r_code})

                json_item['intervals'].append(interval_item)

                cumul = sec[DataPoint.CUMULATIVE][lbl]
                json_item['n'] = cumul[KPISet.SAMPLE_COUNT]
                json_item["summary"] = self.__summary_json(cumul)

        data = {"labels": data, "sourceID": id(self)}
        if is_final:
            data['final'] = True

        url = self.data_address + "/submit.php?session_id=%s&signature=%s&test_id=%s&user_id=%s"
        url = url % (self.active_session_id, self.data_signature, self.test_id, self.user_id)
        url += "&pq=0&target=%s&update=1" % self.kpi_target
        hdr = {"Content-Type": " application/json"}
        response = self._request(url, to_json(data), headers=hdr)

        if response and 'response_code' in response and response['response_code'] != 200:
            raise RuntimeError("Failed to feed data, response code %s" % response['response_code'])

        if response and 'result' in response and is_check_response:
            result = response['result']['session']
            self.log.debug("Result: %s", result)
            if 'statusCode' in result and result['statusCode'] > 100:
                self.log.info("Test was stopped through Web UI: %s", result['status'])
                raise ManualShutdown("The test was interrupted through Web UI")

    def __label_skel(self, name):
        return {
            "n": None,
            "name": name,
            "interval": 1,
            "intervals": [],
            "samplesNotCounted": 0,
            "assertionsNotCounted": 0,
            "failedEmbeddedResources": [],
            "failedEmbeddedResourcesSpilloverCount": 0,
            "otherErrorsCount": 0,
            "errors": [],
            "percentileHistogram": [],
            "percentileHistogramLatency": [],
            "percentileHistogramBytes": [],
            "empty": False,
        }

    def __summary_json(self, cumul):
        return {
            "first": self.first_ts,
            "last": self.last_ts,
            "duration": self.last_ts - self.first_ts,
            "failed": cumul[KPISet.FAILURES],
            "hits": cumul[KPISet.SAMPLE_COUNT],

            "avg": int(1000 * cumul[KPISet.AVG_RESP_TIME]),
            "min": int(1000 * cumul[KPISet.PERCENTILES]["0.0"]) if "0.0" in cumul[KPISet.PERCENTILES] else 0,
            "max": int(1000 * cumul[KPISet.PERCENTILES]["100.0"]) if "100.0" in cumul[KPISet.PERCENTILES] else 0,
            "std": int(1000 * cumul[KPISet.STDEV_RESP_TIME]),
            "tp90": int(1000 * cumul[KPISet.PERCENTILES]["90.0"]) if "90.0" in cumul[KPISet.PERCENTILES] else 0,
            "tp95": int(1000 * cumul[KPISet.PERCENTILES]["95.0"]) if "95.0" in cumul[KPISet.PERCENTILES] else 0,
            "tp99": int(1000 * cumul[KPISet.PERCENTILES]["99.0"]) if "99.0" in cumul[KPISet.PERCENTILES] else 0,

            "latencyAvg": int(1000 * cumul[KPISet.AVG_LATENCY]),
            "latencyMax": 0,
            "latencyMin": 0,
            "latencySTD": 0,

            "bytes": 0,
            "bytesMax": 0,
            "bytesMin": 0,
            "bytesAvg": 0,
            "bytesSTD": 0,

            "otherErrorsSpillcount": 0,
        }

    def __interval_json(self, item, sec):
        return {
            "ec": item[KPISet.FAILURES],
            "ts": sec[DataPoint.TIMESTAMP],
            "na": item[KPISet.CONCURRENCY],
            "n": item[KPISet.SAMPLE_COUNT],
            "failed": item[KPISet.FAILURES],
            "rc": [],  # filled later
            "t": {
                "min": int(1000 * item[KPISet.PERCENTILES]["0.0"]) if "0.0" in item[KPISet.PERCENTILES] else 0,
                "max": int(1000 * item[KPISet.PERCENTILES]["100.0"]) if "100.0" in item[KPISet.PERCENTILES] else 0,
                "sum": 1000 * item[KPISet.AVG_RESP_TIME] * item[KPISet.SAMPLE_COUNT],
                "n": item[KPISet.SAMPLE_COUNT],
                "std": 1000 * item[KPISet.STDEV_RESP_TIME],
                "avg": 1000 * item[KPISet.AVG_RESP_TIME]
            },
            "lt": {
                "min": 0,
                "max": 0,
                "sum": 1000 * item[KPISet.AVG_LATENCY] * item[KPISet.SAMPLE_COUNT],
                "n": 1000 * item[KPISet.SAMPLE_COUNT],
                "std": 0,
                "avg": 1000 * item[KPISet.AVG_LATENCY]
            },
            "by": {
                "min": 0,
                "max": 0,
                "sum": 0,
                "n": 0,
                "std": 0,
                "avg": 0
            },
        }

    def ping(self):
        """
        Quick check if we can access the service
        """
        self._request(self.address + '/api/latest/web/version')

    def upload_file(self, filename, contents=None):
        """
        Upload single artifact

        :type filename: str
        :type contents: str
        :raise IOError:
        """
        body = MultiPartForm()

        if contents is None:
            body.add_file('file', filename)
        else:
            body.add_file_as_string('file', filename, contents)

        url = self.address + "/api/latest/image/%s/files?signature=%s"
        url = url % (self.active_session_id, self.data_signature)
        hdr = {"Content-Type": body.get_content_type()}
        response = self._request(url, body.form_as_bytes(), headers=hdr)
        if not response['result']:
            raise IOError("Upload failed: %s" % response)

    def send_error_summary(self, data_buffer):
        """
        Sends error summary file

        :type data_buffer: list[bzt.modules.aggregator.DataPoint]
        """
        if not data_buffer:
            return

        recent = data_buffer[-1]
        if not recent[DataPoint.CUMULATIVE][''][KPISet.ERRORS]:
            return

        errors = self.__errors_skel(recent[DataPoint.TIMESTAMP], self.active_session_id, self.test_id, self.user_id)
        for label, label_data in iteritems(recent[DataPoint.CUMULATIVE]):
            if not label_data[KPISet.ERRORS]:
                continue

            if label == '':
                label = 'ALL'

            error_item = self.__error_item_skel(label)
            for err_item in label_data[KPISet.ERRORS]:
                if err_item["type"] == KPISet.ERRTYPE_ASSERT:
                    error_item['assertionsCount'] += err_item['cnt']
                    error_item['assertions'].append({
                        "name": "All Assertions",
                        "failureMessage": err_item['msg'],
                        "failure": True,
                        "error": False,
                        "count": err_item['cnt']
                    })
                else:
                    error_item['count'] += err_item['cnt']
                    error_item['responseInfo'].append({
                        "description": err_item['msg'],
                        "code": err_item['rc'],
                        "count": err_item['cnt'],
                    })
            errors['summery']['labels'].append(error_item)

        self.upload_file("sample.jtl.blazemeter.summery.json", to_json(errors))

    def __errors_skel(self, t_stamp, sess_id, test_id, user_id):
        return {
            "reportInfo": {
                "sessionId": sess_id,
                "timestamp": t_stamp,
                "userId": user_id,
                "testId": test_id,
                "type": "SUMMERY",
                # "testName": test_name
            },
            "timestamp": t_stamp,
            "summery": {
                "labels": [],
                "empty": False
            }
        }

    def __error_item_skel(self, label):
        return {
            "name": label,

            "count": 0,
            "responseInfo": [],

            "assertionsCount": 0,
            "assertions": [],

            "embeddedResourcesCount": 0,
            "embeddedResources": [],
        }

    def get_session(self, session_id):
        sess = self._request(self.address + '/api/latest/sessions/%s' % session_id)
        return sess['result']

    def get_master(self, master_id):
        sess = self._request(self.address + '/api/latest/masters/%s' % master_id)
        return sess['result']

    def get_master_status(self, master_id):
        sess = self._request(self.address + '/api/latest/masters/%s/status' % master_id)
        return sess['result']

    def get_master_sessions(self, master_id):
        sess = self._request(self.address + '/api/latest/masters/%s/sessions' % master_id)
        return sess['result']['sessions'] if 'sessions' in sess['result'] else sess['result']

    def get_projects(self):
        data = self._request(self.address + '/api/latest/projects')
        return data['result']

    def create_project(self, proj_name):
        hdr = {"Content-Type": "application/json"}
        data = self._request(self.address + '/api/latest/projects', to_json({"name": str(proj_name)}), headers=hdr)
        return data['result']['id']

    def get_user_info(self):
        res = self._request(self.address + '/api/latest/user')
        return res

    def get_kpis(self, master_id, min_ts):
        params = [
            ("interval", 1),
            ("from", min_ts),
            ("master_ids[]", master_id),
        ]
        for item in ('t', 'lt', 'by', 'n', 'ec', 'ts', 'na'):
            params.append(("kpis[]", item))

        labels = self.get_labels(master_id)
        for label in labels:
            params.append(("labels[]", label['id']))

        url = self.address + "/api/latest/data/kpis?" + urlencode(params)
        res = self._request(url)
        return res['result']

    def get_labels(self, master_id):
        url = self.address + "/api/latest/data/labels?" + urlencode({'master_id': master_id})
        res = self._request(url)
        return res['result']

    def update_session(self, active_session_id, data):
        hdr = {"Content-Type": "application/json"}
        data = self._request(self.address + '/api/latest/sessions/%s' % active_session_id, to_json(data),
                             headers=hdr, method="PUT")
        return data['result']

    def get_available_locations(self):
        user_info = self.get_user_info()
        return {str(x['id']): x for x in user_info['locations'] if not x['id'].startswith('harbor-')}


class CloudProvisioning(Provisioning, WidgetProvider):
    """
    :type client: BlazeMeterClient
    :type results_reader: ResultsFromBZA
    """

    LOC = "locations"

    def __init__(self):
        super(CloudProvisioning, self).__init__()
        self.results_reader = None
        self.client = BlazeMeterClient(self.log)
        self.test_id = None
        self.test_name = None
        self.__last_master_status = None
        self.browser_open = 'start'
        self.widget = None

    def prepare(self):
        if self.settings.get("dump-locations", False):
            self.log.warning("Dumping available locations instead of running the test")
            self._configure_client()
            info = self.client.get_user_info()
            locations = self.client.get_available_locations()
            for item in info['locations']:
                if item['id'] in locations:
                    self.log.info("Location: %s\t%s", item['id'], item['title'])
            raise ManualShutdown("Done listing locations")

        super(CloudProvisioning, self).prepare()
        self.browser_open = self.settings.get("browser-open", self.browser_open)
        self._configure_client()

        self.__prepare_locations()
        config = self.__get_config_for_cloud()
        rfiles = self.__get_rfiles()

        def file_replacer(value, key, container):
            if isinstance(value, string_types):
                if value in rfiles:
                    container[key] = os.path.basename(value)
                    if container[key] != value:
                        self.log.debug("Replaced %s with %s", value, container[key])

        BetterDict.traverse(config, file_replacer)

        bza_plugin = self.__get_bza_test_config()
        finder = ProjectFinder(self.parameters, self.settings, self.client, self.engine)
        finder.default_test_name = "Taurus Cloud Test"
        self.test_id = finder.resolve_test_id(bza_plugin, config, rfiles)
        self.test_name = finder.test_name
        self.widget = CloudProvWidget(self)

        if isinstance(self.engine.aggregator, ConsolidatingAggregator):
            self.results_reader = ResultsFromBZA(self.client)
            self.results_reader.log = self.log
            self.engine.aggregator.add_underling(self.results_reader)

    def _configure_client(self):
        self.client.logger_limit = self.settings.get("request-logging-limit", self.client.logger_limit)
        # TODO: go to "blazemeter" section for these settings by default?
        self.client.address = self.settings.get("address", self.client.address)
        self.client.token = self.settings.get("token", self.client.token)
        self.client.timeout = dehumanize_time(self.settings.get("timeout", self.client.timeout))
        if not self.client.token:
            bmmod = self.engine.instantiate_module('blazemeter')
            self.client.token = bmmod.settings.get("token")
            if not self.client.token:
                raise ValueError("You must provide API token to use cloud provisioning")

    def __prepare_locations(self):
        available_locations = self.client.get_available_locations()
        for executor in self.executors:
            locations = self._get_locations(available_locations, executor)
            executor.get_load()  # we need it to resolve load settings into full form

            for location in locations.keys():
                if location not in available_locations:
                    self.log.warning("List of supported locations for you is: %s", sorted(available_locations.keys()))
                    raise ValueError("Invalid location requested: %s" % location)

    def __get_config_for_cloud(self):
        config = copy.deepcopy(self.engine.config)

        if not isinstance(config[ScenarioExecutor.EXEC], list):
            config[ScenarioExecutor.EXEC] = [config[ScenarioExecutor.EXEC]]

        provisioning = config.pop(Provisioning.PROV)
        for execution in config[ScenarioExecutor.EXEC]:
            execution[ScenarioExecutor.CONCURR] = execution.get(ScenarioExecutor.CONCURR).get(provisioning, None)
            execution[ScenarioExecutor.THRPT] = execution.get(ScenarioExecutor.THRPT).get(provisioning, None)

        for key in list(config.keys()):
            if key not in ("scenarios", ScenarioExecutor.EXEC, "included-configs", Service.SERV):
                config.pop(key)

        assert isinstance(config, Configuration)
        config.dump(self.engine.create_artifact("cloud", ""))
        return config

    def __get_rfiles(self):
        rfiles = []
        for executor in self.executors:
            rfiles += executor.get_resource_files()
        self.log.debug("All resource files are: %s", rfiles)
        return [self.engine.find_file(x) for x in rfiles]

    def __get_bza_test_config(self):
        bza_plugin = {
            "type": "taurus",
            "plugins": {
                "taurus": {
                    "filename": ""  # without this line it does not work
                }
            }
        }
        return bza_plugin

    def _get_locations(self, available_locations, executor):
        locations = executor.execution.get(self.LOC, BetterDict())
        if not locations:
            for location in available_locations.values():
                if location['sandbox']:
                    locations.merge({location['id']: 1})
        if not locations:
            self.log.warning("List of supported locations for you is: %s", sorted(available_locations.keys()))
            raise ValueError("No sandbox location available, please specify locations manually")
        return locations

    def startup(self):
        super(CloudProvisioning, self).startup()
        self.client.start_taurus(self.test_id)
        self.log.info("Started cloud test: %s", self.client.results_url)
        if self.client.results_url:
            if self.browser_open in ('start', 'both'):
                open_browser(self.client.results_url)

    def check(self):
        # TODO: throttle down requests
        try:
            master = self.client.get_master_status(self.client.active_session_id)
        except URLError:
            self.log.warning("Failed to get test status, will retry in %s seconds...", self.client.timeout)
            self.log.debug("Full exception: %s", traceback.format_exc())
            time.sleep(self.client.timeout)
            master = self.client.get_master_status(self.client.active_session_id)
            self.log.info("Succeeded with retry")

        if "status" in master and master['status'] != self.__last_master_status:
            self.__last_master_status = master['status']
            self.log.info("Cloud test status: %s", self.__last_master_status)

        if self.results_reader is not None and 'progress' in master and master['progress'] >= 100:
            self.results_reader.master_id = self.client.active_session_id

        if 'progress' in master and master['progress'] > 100:
            self.log.info("Test was stopped in the cloud: %s", master['status'])
            status = self.client.get_master(self.client.active_session_id)
            if 'note' in status and status['note']:
                self.log.warning("Cloud test has probably failed with message: %s", status['note'])

            self.client.active_session_id = None
            return True

        self.widget.update()
        return super(CloudProvisioning, self).check()

    def post_process(self):
        self.client.end_master(self.client.active_session_id)
        if self.client.results_url:
            if self.browser_open in ('end', 'both'):
                open_browser(self.client.results_url)

    def weight_locations(self, locations, load, available_locations):
        total = float(sum(locations.values()))
        for loc_name, share in iteritems(locations):
            loc_info = available_locations[loc_name]
            limits = loc_info['limits']

            if load.duration > limits['duration'] * 60:
                msg = "Test duration %s exceeds limit %s for location %s"
                self.log.warning(msg, load.duration, limits['duration'] * 60, loc_name)

            if load.concurrency:
                locations[loc_name] = int(math.ceil(load.concurrency * share / total / limits['threadsPerEngine']))
            else:
                locations[loc_name] = 1

    def get_widget(self):
        self.widget = CloudProvWidget(self)
        return self.widget


class BlazeMeterClientEmul(BlazeMeterClient):
    def __init__(self, parent_logger):
        super(BlazeMeterClientEmul, self).__init__(parent_logger)
        self.results = []

    def _request(self, url, data=None, headers=None, checker=None, method=None):
        self.log.debug("Request %s: %s", url, data)
        res = self.results.pop(0)
        self.log.debug("Response: %s", res)
        return res


class ResultsFromBZA(ResultsProvider):
    """
    :type client: BlazeMeterClient
    """

    def __init__(self, client):
        super(ResultsFromBZA, self).__init__()
        self.client = client
        self.master_id = None  # must be set afterwards
        self.min_ts = 0
        self.log = logging.getLogger('')

    def _calculate_datapoints(self, final_pass=False):
        if self.master_id is None:
            return

        try:
            data = self.client.get_kpis(self.master_id, self.min_ts)
        except URLError:
            self.log.warning("Failed to get result KPIs, will retry in %s seconds...", self.client.timeout)
            self.log.debug("Full exception: %s", traceback.format_exc())
            time.sleep(self.client.timeout)
            data = self.client.get_kpis(self.master_id, self.min_ts)
            self.log.info("Succeeded with retry")

        for label in data:
            if label['kpis']:
                label['kpis'].pop(-1)  # never take last second since it could be incomplete

        timestamps = []
        for label in data:
            if label['label'] == 'ALL':
                timestamps.extend([kpi['ts'] for kpi in label['kpis']])

        for tstmp in timestamps:
            point = DataPoint(tstmp)
            for label in data:
                for kpi in label['kpis']:
                    if kpi['ts'] != tstmp:
                        continue

                    kpiset = KPISet()
                    kpiset[KPISet.FAILURES] = kpi['ec']
                    kpiset[KPISet.CONCURRENCY] = kpi['na']
                    kpiset[KPISet.SAMPLE_COUNT] = kpi['n']
                    kpiset.sum_rt += kpi['t_avg'] * kpi['n'] / 1000.0
                    kpiset.sum_lt += kpi['lt_avg'] * kpi['n'] / 1000.0
                    point[DataPoint.CURRENT]['' if label['label'] == 'ALL' else label['label']] = kpiset

            point.recalculate()
            self.min_ts = point[DataPoint.TIMESTAMP] + 1
            yield point


class CloudProvWidget(Pile):
    def __init__(self, prov):
        """
        :type prov: CloudProvisioning
        """
        self.prov = prov
        self.text = Text("")
        self._sessions = None
        super(CloudProvWidget, self).__init__([self.text])

    def update(self):
        if not self._sessions:
            self._sessions = self.prov.client.get_master_sessions(self.prov.client.active_session_id)
            if not self._sessions:
                return

        mapping = BetterDict()
        cnt = 0
        for session in self._sessions:
            try:
                cnt += 1
                name_split = session['name'].split('/')
                location = session['configuration']['location']
                count = session['configuration']['serversCount']
                mapping.get(name_split[0]).get(name_split[1])[location] = count
            except KeyError:
                self._sessions = None

        txt = "%s #%s\n" % (self.prov.test_name, self.prov.client.active_session_id)
        for executor, scenarios in iteritems(mapping):
            txt += " %s" % executor
            for scenario, locations in iteritems(scenarios):
                txt += " %s:\n" % scenario
                for location, count in iteritems(locations):
                    txt += "  Agents in %s: %s\n" % (location, count)

        self.text.set_text(txt)

# This locust test script example will simulate a user
# browsing the Locust documentation on http://docs.locust.io

import random

from locust import HttpLocust, TaskSet, task
from pyquery import PyQuery


class BrowseDocumentation(TaskSet):
    def on_start(self):
        # assume all users arrive at the index page
        self.index_page()
        self.urls_on_current_page = self.toc_urls

    @task(10)
    def index_page(self):
        r = self.client.get("/")
        pq = PyQuery(r.content)
        link_elements = pq(".toctree-wrapper a.internal")
        self.toc_urls = [l.attrib["href"] for l in link_elements]

    @task(50)
    def load_page(self, url=None):
        url = random.choice(self.toc_urls)
        r = self.client.get(url)
        pq = PyQuery(r.content)
        link_elements = pq("a.internal")
        self.urls_on_current_page = [l.attrib["href"] for l in link_elements]

    @task(30)
    def load_sub_page(self):
        url = random.choice(self.urls_on_current_page)
        r = self.client.get(url)


class AwesomeUser(HttpLocust):
    task_set = BrowseDocumentation
    host = "http://docs.locust.io/en/latest/"

    # we assume someone who is browsing the Locust docs,
    # generally has a quite long waiting time (between
    # 20 and 600 seconds), since there's a bunch of text
    # on each page
    min_wait = 2 * 1000
    max_wait = 6 * 1000

# Copyright (c) 2013 Blizzard Entertainment
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.

from decoders import *


# Decoding instructions for each protocol type.
typeinfos = [
    ('_int',[(0,7)]),  #0
    ('_int',[(0,4)]),  #1
    ('_int',[(0,5)]),  #2
    ('_int',[(0,6)]),  #3
    ('_int',[(0,14)]),  #4
    ('_int',[(0,22)]),  #5
    ('_int',[(0,32)]),  #6
    ('_choice',[(0,2),{0:('m_uint6',3),1:('m_uint14',4),2:('m_uint22',5),3:('m_uint32',6)}]),  #7
    ('_struct',[[('m_userId',2,-1)]]),  #8
    ('_blob',[(0,8)]),  #9
    ('_int',[(0,8)]),  #10
    ('_struct',[[('m_flags',10,0),('m_major',10,1),('m_minor',10,2),('m_revision',10,3),('m_build',6,4),('m_baseBuild',6,5)]]),  #11
    ('_int',[(0,3)]),  #12
    ('_bool',[]),  #13
    ('_struct',[[('m_signature',9,0),('m_version',11,1),('m_type',12,2),('m_elapsedGameLoops',6,3),('m_useScaledTime',13,4)]]),  #14
    ('_fourcc',[]),  #15
    ('_blob',[(0,7)]),  #16
    ('_int',[(0,64)]),  #17
    ('_struct',[[('m_region',10,0),('m_programId',15,1),('m_realm',6,2),('m_name',16,3),('m_id',17,4)]]),  #18
    ('_struct',[[('m_a',10,0),('m_r',10,1),('m_g',10,2),('m_b',10,3)]]),  #19
    ('_int',[(0,2)]),  #20
    ('_optional',[10]),  #21
    ('_struct',[[('m_name',9,0),('m_toon',18,1),('m_race',9,2),('m_color',19,3),('m_control',10,4),('m_teamId',1,5),('m_handicap',0,6),('m_observe',20,7),('m_result',20,8),('m_workingSetSlotId',21,9)]]),  #22
    ('_array',[(0,5),22]),  #23
    ('_optional',[23]),  #24
    ('_blob',[(0,10)]),  #25
    ('_blob',[(0,11)]),  #26
    ('_struct',[[('m_file',26,0)]]),  #27
    ('_optional',[13]),  #28
    ('_int',[(-9223372036854775808,64)]),  #29
    ('_blob',[(0,12)]),  #30
    ('_blob',[(40,0)]),  #31
    ('_array',[(0,6),31]),  #32
    ('_optional',[32]),  #33
    ('_array',[(0,6),26]),  #34
    ('_optional',[34]),  #35
    ('_struct',[[('m_playerList',24,0),('m_title',25,1),('m_difficulty',9,2),('m_thumbnail',27,3),('m_isBlizzardMap',13,4),('m_restartAsTransitionMap',28,16),('m_timeUTC',29,5),('m_timeLocalOffset',29,6),('m_description',30,7),('m_imageFilePath',26,8),('m_campaignIndex',10,15),('m_mapFileName',26,9),('m_cacheHandles',33,10),('m_miniSave',13,11),('m_gameSpeed',12,12),('m_defaultDifficulty',3,13),('m_modPaths',35,14)]]),  #36
    ('_optional',[9]),  #37
    ('_optional',[31]),  #38
    ('_optional',[6]),  #39
    ('_struct',[[('m_race',21,-1)]]),  #40
    ('_struct',[[('m_team',21,-1)]]),  #41
    ('_struct',[[('m_name',9,-13),('m_clanTag',37,-12),('m_clanLogo',38,-11),('m_highestLeague',21,-10),('m_combinedRaceLevels',39,-9),('m_randomSeed',6,-8),('m_racePreference',40,-7),('m_teamPreference',41,-6),('m_testMap',13,-5),('m_testAuto',13,-4),('m_examine',13,-3),('m_customInterface',13,-2),('m_observe',20,-1)]]),  #42
    ('_array',[(0,5),42]),  #43
    ('_struct',[[('m_lockTeams',13,-12),('m_teamsTogether',13,-11),('m_advancedSharedControl',13,-10),('m_randomRaces',13,-9),('m_battleNet',13,-8),('m_amm',13,-7),('m_competitive',13,-6),('m_noVictoryOrDefeat',13,-5),('m_fog',20,-4),('m_observers',20,-3),('m_userDifficulty',20,-2),('m_clientDebugFlags',17,-1)]]),  #44
    ('_int',[(1,4)]),  #45
    ('_int',[(1,8)]),  #46
    ('_bitarray',[(0,6)]),  #47
    ('_bitarray',[(0,8)]),  #48
    ('_bitarray',[(0,2)]),  #49
    ('_bitarray',[(0,7)]),  #50
    ('_struct',[[('m_allowedColors',47,-6),('m_allowedRaces',48,-5),('m_allowedDifficulty',47,-4),('m_allowedControls',48,-3),('m_allowedObserveTypes',49,-2),('m_allowedAIBuilds',50,-1)]]),  #51
    ('_array',[(0,5),51]),  #52
    ('_struct',[[('m_randomValue',6,-26),('m_gameCacheName',25,-25),('m_gameOptions',44,-24),('m_gameSpeed',12,-23),('m_gameType',12,-22),('m_maxUsers',2,-21),('m_maxObservers',2,-20),('m_maxPlayers',2,-19),('m_maxTeams',45,-18),('m_maxColors',3,-17),('m_maxRaces',46,-16),('m_maxControls',10,-15),('m_mapSizeX',10,-14),('m_mapSizeY',10,-13),('m_mapFileSyncChecksum',6,-12),('m_mapFileName',26,-11),('m_mapAuthorName',9,-10),('m_modFileSyncChecksum',6,-9),('m_slotDescriptions',52,-8),('m_defaultDifficulty',3,-7),('m_defaultAIBuild',0,-6),('m_cacheHandles',32,-5),('m_hasExtensionMod',13,-4),('m_isBlizzardMap',13,-3),('m_isPremadeFFA',13,-2),('m_isCoopMode',13,-1)]]),  #53
    ('_optional',[1]),  #54
    ('_optional',[2]),  #55
    ('_struct',[[('m_color',55,-1)]]),  #56
    ('_array',[(0,6),6]),  #57
    ('_array',[(0,9),6]),  #58
    ('_struct',[[('m_control',10,-13),('m_userId',54,-12),('m_teamId',1,-11),('m_colorPref',56,-10),('m_racePref',40,-9),('m_difficulty',3,-8),('m_aiBuild',0,-7),('m_handicap',0,-6),('m_observe',20,-5),('m_workingSetSlotId',21,-4),('m_rewards',57,-3),('m_toonHandle',16,-2),('m_licenses',58,-1)]]),  #59
    ('_array',[(0,5),59]),  #60
    ('_struct',[[('m_phase',12,-10),('m_maxUsers',2,-9),('m_maxObservers',2,-8),('m_slots',60,-7),('m_randomSeed',6,-6),('m_hostUserId',54,-5),('m_isSinglePlayer',13,-4),('m_gameDuration',6,-3),('m_defaultDifficulty',3,-2),('m_defaultAIBuild',0,-1)]]),  #61
    ('_struct',[[('m_userInitialData',43,-3),('m_gameDescription',53,-2),('m_lobbyState',61,-1)]]),  #62
    ('_struct',[[('m_syncLobbyState',62,-1)]]),  #63
    ('_struct',[[('m_name',16,-1)]]),  #64
    ('_blob',[(0,6)]),  #65
    ('_struct',[[('m_name',65,-1)]]),  #66
    ('_struct',[[('m_name',65,-3),('m_type',6,-2),('m_data',16,-1)]]),  #67
    ('_struct',[[('m_type',6,-3),('m_name',65,-2),('m_data',30,-1)]]),  #68
    ('_array',[(0,5),10]),  #69
    ('_struct',[[('m_signature',69,-2),('m_toonHandle',16,-1)]]),  #70
    ('_struct',[[('m_gameFullyDownloaded',13,-8),('m_developmentCheatsEnabled',13,-7),('m_multiplayerCheatsEnabled',13,-6),('m_syncChecksummingEnabled',13,-5),('m_isMapToMapTransition',13,-4),('m_startingRally',13,-3),('m_debugPauseEnabled',13,-2),('m_baseBuildNum',6,-1)]]),  #71
    ('_struct',[[]]),  #72
    ('_int',[(0,16)]),  #73
    ('_struct',[[('x',73,-2),('y',73,-1)]]),  #74
    ('_struct',[[('m_which',12,-2),('m_target',74,-1)]]),  #75
    ('_struct',[[('m_fileName',26,-5),('m_automatic',13,-4),('m_overwrite',13,-3),('m_name',9,-2),('m_description',25,-1)]]),  #76
    ('_int',[(-2147483648,32)]),  #77
    ('_struct',[[('x',77,-2),('y',77,-1)]]),  #78
    ('_struct',[[('m_point',78,-4),('m_time',77,-3),('m_verb',25,-2),('m_arguments',25,-1)]]),  #79
    ('_struct',[[('m_data',79,-1)]]),  #80
    ('_int',[(0,20)]),  #81
    ('_struct',[[('m_abilLink',73,-3),('m_abilCmdIndex',2,-2),('m_abilCmdData',21,-1)]]),  #82
    ('_optional',[82]),  #83
    ('_null',[]),  #84
    ('_struct',[[('x',81,-3),('y',81,-2),('z',77,-1)]]),  #85
    ('_struct',[[('m_targetUnitFlags',10,-7),('m_timer',10,-6),('m_tag',6,-5),('m_snapshotUnitLink',73,-4),('m_snapshotControlPlayerId',54,-3),('m_snapshotUpkeepPlayerId',54,-2),('m_snapshotPoint',85,-1)]]),  #86
    ('_choice',[(0,2),{0:('None',84),1:('TargetPoint',85),2:('TargetUnit',86),3:('Data',6)}]),  #87
    ('_struct',[[('m_cmdFlags',81,-4),('m_abil',83,-3),('m_data',87,-2),('m_otherUnit',39,-1)]]),  #88
    ('_int',[(0,9)]),  #89
    ('_bitarray',[(0,9)]),  #90
    ('_array',[(0,9),89]),  #91
    ('_choice',[(0,2),{0:('None',84),1:('Mask',90),2:('OneIndices',91),3:('ZeroIndices',91)}]),  #92
    ('_struct',[[('m_unitLink',73,-4),('m_subgroupPriority',10,-3),('m_intraSubgroupPriority',10,-2),('m_count',89,-1)]]),  #93
    ('_array',[(0,9),93]),  #94
    ('_struct',[[('m_subgroupIndex',89,-4),('m_removeMask',92,-3),('m_addSubgroups',94,-2),('m_addUnitTags',58,-1)]]),  #95
    ('_struct',[[('m_controlGroupId',1,-2),('m_delta',95,-1)]]),  #96
    ('_struct',[[('m_controlGroupIndex',1,-3),('m_controlGroupUpdate',20,-2),('m_mask',92,-1)]]),  #97
    ('_struct',[[('m_count',89,-6),('m_subgroupCount',89,-5),('m_activeSubgroupIndex',89,-4),('m_unitTagsChecksum',6,-3),('m_subgroupIndicesChecksum',6,-2),('m_subgroupsChecksum',6,-1)]]),  #98
    ('_struct',[[('m_controlGroupId',1,-2),('m_selectionSyncData',98,-1)]]),  #99
    ('_array',[(0,3),77]),  #100
    ('_struct',[[('m_recipientId',1,-2),('m_resources',100,-1)]]),  #101
    ('_struct',[[('m_chatMessage',25,-1)]]),  #102
    ('_int',[(-128,8)]),  #103
    ('_struct',[[('x',77,-3),('y',77,-2),('z',77,-1)]]),  #104
    ('_struct',[[('m_beacon',103,-9),('m_ally',103,-8),('m_flags',103,-7),('m_build',103,-6),('m_targetUnitTag',6,-5),('m_targetUnitSnapshotUnitLink',73,-4),('m_targetUnitSnapshotUpkeepPlayerId',103,-3),('m_targetUnitSnapshotControlPlayerId',103,-2),('m_targetPoint',104,-1)]]),  #105
    ('_struct',[[('m_speed',12,-1)]]),  #106
    ('_struct',[[('m_delta',103,-1)]]),  #107
    ('_struct',[[('m_point',78,-3),('m_unit',6,-2),('m_pingedMinimap',13,-1)]]),  #108
    ('_struct',[[('m_verb',25,-2),('m_arguments',25,-1)]]),  #109
    ('_struct',[[('m_alliance',6,-2),('m_control',6,-1)]]),  #110
    ('_struct',[[('m_unitTag',6,-1)]]),  #111
    ('_struct',[[('m_unitTag',6,-2),('m_flags',10,-1)]]),  #112
    ('_struct',[[('m_conversationId',77,-2),('m_replyId',77,-1)]]),  #113
    ('_optional',[16]),  #114
    ('_struct',[[('m_gameUserId',1,-6),('m_observe',20,-5),('m_name',9,-4),('m_toonHandle',114,-3),('m_clanTag',37,-2),('m_clanLogo',38,-1)]]),  #115
    ('_array',[(0,5),115]),  #116
    ('_int',[(0,1)]),  #117
    ('_struct',[[('m_userInfos',116,-2),('m_method',117,-1)]]),  #118
    ('_struct',[[('m_purchaseItemId',77,-1)]]),  #119
    ('_struct',[[('m_difficultyLevel',77,-1)]]),  #120
    ('_choice',[(0,3),{0:('None',84),1:('Checked',13),2:('ValueChanged',6),3:('SelectionChanged',77),4:('TextChanged',26),5:('MouseButton',6)}]),  #121
    ('_struct',[[('m_controlId',77,-3),('m_eventType',77,-2),('m_eventData',121,-1)]]),  #122
    ('_struct',[[('m_soundHash',6,-2),('m_length',6,-1)]]),  #123
    ('_array',[(0,7),6]),  #124
    ('_struct',[[('m_soundHash',124,-2),('m_length',124,-1)]]),  #125
    ('_struct',[[('m_syncInfo',125,-1)]]),  #126
    ('_struct',[[('m_sound',6,-1)]]),  #127
    ('_struct',[[('m_transmissionId',77,-2),('m_thread',6,-1)]]),  #128
    ('_struct',[[('m_transmissionId',77,-1)]]),  #129
    ('_optional',[74]),  #130
    ('_optional',[73]),  #131
    ('_optional',[103]),  #132
    ('_struct',[[('m_target',130,-5),('m_distance',131,-4),('m_pitch',131,-3),('m_yaw',131,-2),('m_reason',132,-1)]]),  #133
    ('_struct',[[('m_skipType',117,-1)]]),  #134
    ('_int',[(0,11)]),  #135
    ('_struct',[[('x',135,-2),('y',135,-1)]]),  #136
    ('_struct',[[('m_button',6,-5),('m_down',13,-4),('m_posUI',136,-3),('m_posWorld',85,-2),('m_flags',103,-1)]]),  #137
    ('_struct',[[('m_posUI',136,-3),('m_posWorld',85,-2),('m_flags',103,-1)]]),  #138
    ('_struct',[[('m_achievementLink',73,-1)]]),  #139
    ('_struct',[[('m_abilLink',73,-3),('m_abilCmdIndex',2,-2),('m_state',103,-1)]]),  #140
    ('_struct',[[('m_soundtrack',6,-1)]]),  #141
    ('_struct',[[('m_planetId',77,-1)]]),  #142
    ('_struct',[[('m_key',103,-2),('m_flags',103,-1)]]),  #143
    ('_struct',[[('m_resources',100,-1)]]),  #144
    ('_struct',[[('m_fulfillRequestId',77,-1)]]),  #145
    ('_struct',[[('m_cancelRequestId',77,-1)]]),  #146
    ('_struct',[[('m_researchItemId',77,-1)]]),  #147
    ('_struct',[[('m_mercenaryId',77,-1)]]),  #148
    ('_struct',[[('m_battleReportId',77,-2),('m_difficultyLevel',77,-1)]]),  #149
    ('_struct',[[('m_battleReportId',77,-1)]]),  #150
    ('_int',[(0,19)]),  #151
    ('_struct',[[('m_decrementMs',151,-1)]]),  #152
    ('_struct',[[('m_portraitId',77,-1)]]),  #153
    ('_struct',[[('m_functionName',16,-1)]]),  #154
    ('_struct',[[('m_result',77,-1)]]),  #155
    ('_struct',[[('m_gameMenuItemIndex',77,-1)]]),  #156
    ('_struct',[[('m_purchaseCategoryId',77,-1)]]),  #157
    ('_struct',[[('m_button',73,-1)]]),  #158
    ('_struct',[[('m_cutsceneId',77,-2),('m_bookmarkName',16,-1)]]),  #159
    ('_struct',[[('m_cutsceneId',77,-1)]]),  #160
    ('_struct',[[('m_cutsceneId',77,-3),('m_conversationLine',16,-2),('m_altConversationLine',16,-1)]]),  #161
    ('_struct',[[('m_cutsceneId',77,-2),('m_conversationLine',16,-1)]]),  #162
    ('_struct',[[('m_observe',20,-5),('m_name',9,-4),('m_toonHandle',114,-3),('m_clanTag',37,-2),('m_clanLogo',38,-1)]]),  #163
    ('_struct',[[('m_recipient',12,-2),('m_string',26,-1)]]),  #164
    ('_struct',[[('m_recipient',12,-2),('m_point',78,-1)]]),  #165
    ('_struct',[[('m_progress',77,-1)]]),  #166
    ('_struct',[[('m_scoreValueMineralsCurrent',77,0),('m_scoreValueVespeneCurrent',77,1),('m_scoreValueMineralsCollectionRate',77,2),('m_scoreValueVespeneCollectionRate',77,3),('m_scoreValueWorkersActiveCount',77,4),('m_scoreValueMineralsUsedInProgressArmy',77,5),('m_scoreValueMineralsUsedInProgressEconomy',77,6),('m_scoreValueMineralsUsedInProgressTechnology',77,7),('m_scoreValueVespeneUsedInProgressArmy',77,8),('m_scoreValueVespeneUsedInProgressEconomy',77,9),('m_scoreValueVespeneUsedInProgressTechnology',77,10),('m_scoreValueMineralsUsedCurrentArmy',77,11),('m_scoreValueMineralsUsedCurrentEconomy',77,12),('m_scoreValueMineralsUsedCurrentTechnology',77,13),('m_scoreValueVespeneUsedCurrentArmy',77,14),('m_scoreValueVespeneUsedCurrentEconomy',77,15),('m_scoreValueVespeneUsedCurrentTechnology',77,16),('m_scoreValueMineralsLostArmy',77,17),('m_scoreValueMineralsLostEconomy',77,18),('m_scoreValueMineralsLostTechnology',77,19),('m_scoreValueVespeneLostArmy',77,20),('m_scoreValueVespeneLostEconomy',77,21),('m_scoreValueVespeneLostTechnology',77,22),('m_scoreValueMineralsKilledArmy',77,23),('m_scoreValueMineralsKilledEconomy',77,24),('m_scoreValueMineralsKilledTechnology',77,25),('m_scoreValueVespeneKilledArmy',77,26),('m_scoreValueVespeneKilledEconomy',77,27),('m_scoreValueVespeneKilledTechnology',77,28),('m_scoreValueFoodUsed',77,29),('m_scoreValueFoodMade',77,30),('m_scoreValueMineralsUsedActiveForces',77,31),('m_scoreValueVespeneUsedActiveForces',77,32),('m_scoreValueMineralsFriendlyFireArmy',77,33),('m_scoreValueMineralsFriendlyFireEconomy',77,34),('m_scoreValueMineralsFriendlyFireTechnology',77,35),('m_scoreValueVespeneFriendlyFireArmy',77,36),('m_scoreValueVespeneFriendlyFireEconomy',77,37),('m_scoreValueVespeneFriendlyFireTechnology',77,38)]]),  #167
    ('_struct',[[('m_playerId',1,0),('m_stats',167,1)]]),  #168
    ('_struct',[[('m_unitTagIndex',6,0),('m_unitTagRecycle',6,1),('m_unitTypeName',25,2),('m_controlPlayerId',1,3),('m_upkeepPlayerId',1,4),('m_x',10,5),('m_y',10,6)]]),  #169
    ('_struct',[[('m_unitTagIndex',6,0),('m_unitTagRecycle',6,1),('m_killerPlayerId',54,2),('m_x',10,3),('m_y',10,4),('m_killerUnitTagIndex',39,5),('m_killerUnitTagRecycle',39,6)]]),  #170
    ('_struct',[[('m_unitTagIndex',6,0),('m_unitTagRecycle',6,1),('m_controlPlayerId',1,2),('m_upkeepPlayerId',1,3)]]),  #171
    ('_struct',[[('m_unitTagIndex',6,0),('m_unitTagRecycle',6,1),('m_unitTypeName',25,2)]]),  #172
    ('_struct',[[('m_playerId',1,0),('m_upgradeTypeName',25,1),('m_count',77,2)]]),  #173
    ('_struct',[[('m_unitTagIndex',6,0),('m_unitTagRecycle',6,1)]]),  #174
    ('_array',[(0,10),77]),  #175
    ('_struct',[[('m_firstUnitIndex',6,0),('m_items',175,1)]]),  #176
    ('_struct',[[('m_playerId',1,0),('m_type',6,1),('m_userId',39,2),('m_slotId',39,3)]]),  #177
]

# Map from protocol NNet.Game.*Event eventid to (typeid, name)
game_event_types = {
    5: (72, 'NNet.Game.SUserFinishedLoadingSyncEvent'),
    7: (71, 'NNet.Game.SUserOptionsEvent'),
    9: (64, 'NNet.Game.SBankFileEvent'),
    10: (66, 'NNet.Game.SBankSectionEvent'),
    11: (67, 'NNet.Game.SBankKeyEvent'),
    12: (68, 'NNet.Game.SBankValueEvent'),
    13: (70, 'NNet.Game.SBankSignatureEvent'),
    14: (75, 'NNet.Game.SCameraSaveEvent'),
    21: (76, 'NNet.Game.SSaveGameEvent'),
    22: (72, 'NNet.Game.SSaveGameDoneEvent'),
    23: (72, 'NNet.Game.SLoadGameDoneEvent'),
    26: (80, 'NNet.Game.SGameCheatEvent'),
    27: (88, 'NNet.Game.SCmdEvent'),
    28: (96, 'NNet.Game.SSelectionDeltaEvent'),
    29: (97, 'NNet.Game.SControlGroupUpdateEvent'),
    30: (99, 'NNet.Game.SSelectionSyncCheckEvent'),
    31: (101, 'NNet.Game.SResourceTradeEvent'),
    32: (102, 'NNet.Game.STriggerChatMessageEvent'),
    33: (105, 'NNet.Game.SAICommunicateEvent'),
    34: (106, 'NNet.Game.SSetAbsoluteGameSpeedEvent'),
    35: (107, 'NNet.Game.SAddAbsoluteGameSpeedEvent'),
    36: (108, 'NNet.Game.STriggerPingEvent'),
    37: (109, 'NNet.Game.SBroadcastCheatEvent'),
    38: (110, 'NNet.Game.SAllianceEvent'),
    39: (111, 'NNet.Game.SUnitClickEvent'),
    40: (112, 'NNet.Game.SUnitHighlightEvent'),
    41: (113, 'NNet.Game.STriggerReplySelectedEvent'),
    43: (118, 'NNet.Game.SHijackReplayGameEvent'),
    44: (72, 'NNet.Game.STriggerSkippedEvent'),
    45: (123, 'NNet.Game.STriggerSoundLengthQueryEvent'),
    46: (127, 'NNet.Game.STriggerSoundOffsetEvent'),
    47: (128, 'NNet.Game.STriggerTransmissionOffsetEvent'),
    48: (129, 'NNet.Game.STriggerTransmissionCompleteEvent'),
    49: (133, 'NNet.Game.SCameraUpdateEvent'),
    50: (72, 'NNet.Game.STriggerAbortMissionEvent'),
    51: (119, 'NNet.Game.STriggerPurchaseMadeEvent'),
    52: (72, 'NNet.Game.STriggerPurchaseExitEvent'),
    53: (120, 'NNet.Game.STriggerPlanetMissionLaunchedEvent'),
    54: (72, 'NNet.Game.STriggerPlanetPanelCanceledEvent'),
    55: (122, 'NNet.Game.STriggerDialogControlEvent'),
    56: (126, 'NNet.Game.STriggerSoundLengthSyncEvent'),
    57: (134, 'NNet.Game.STriggerConversationSkippedEvent'),
    58: (137, 'NNet.Game.STriggerMouseClickedEvent'),
    59: (138, 'NNet.Game.STriggerMouseMovedEvent'),
    60: (139, 'NNet.Game.SAchievementAwardedEvent'),
    62: (140, 'NNet.Game.STriggerTargetModeUpdateEvent'),
    63: (72, 'NNet.Game.STriggerPlanetPanelReplayEvent'),
    64: (141, 'NNet.Game.STriggerSoundtrackDoneEvent'),
    65: (142, 'NNet.Game.STriggerPlanetMissionSelectedEvent'),
    66: (143, 'NNet.Game.STriggerKeyPressedEvent'),
    67: (154, 'NNet.Game.STriggerMovieFunctionEvent'),
    68: (72, 'NNet.Game.STriggerPlanetPanelBirthCompleteEvent'),
    69: (72, 'NNet.Game.STriggerPlanetPanelDeathCompleteEvent'),
    70: (144, 'NNet.Game.SResourceRequestEvent'),
    71: (145, 'NNet.Game.SResourceRequestFulfillEvent'),
    72: (146, 'NNet.Game.SResourceRequestCancelEvent'),
    73: (72, 'NNet.Game.STriggerResearchPanelExitEvent'),
    74: (72, 'NNet.Game.STriggerResearchPanelPurchaseEvent'),
    75: (147, 'NNet.Game.STriggerResearchPanelSelectionChangedEvent'),
    77: (72, 'NNet.Game.STriggerMercenaryPanelExitEvent'),
    78: (72, 'NNet.Game.STriggerMercenaryPanelPurchaseEvent'),
    79: (148, 'NNet.Game.STriggerMercenaryPanelSelectionChangedEvent'),
    80: (72, 'NNet.Game.STriggerVictoryPanelExitEvent'),
    81: (72, 'NNet.Game.STriggerBattleReportPanelExitEvent'),
    82: (149, 'NNet.Game.STriggerBattleReportPanelPlayMissionEvent'),
    83: (150, 'NNet.Game.STriggerBattleReportPanelPlaySceneEvent'),
    84: (150, 'NNet.Game.STriggerBattleReportPanelSelectionChangedEvent'),
    85: (120, 'NNet.Game.STriggerVictoryPanelPlayMissionAgainEvent'),
    86: (72, 'NNet.Game.STriggerMovieStartedEvent'),
    87: (72, 'NNet.Game.STriggerMovieFinishedEvent'),
    88: (152, 'NNet.Game.SDecrementGameTimeRemainingEvent'),
    89: (153, 'NNet.Game.STriggerPortraitLoadedEvent'),
    90: (155, 'NNet.Game.STriggerCustomDialogDismissedEvent'),
    91: (156, 'NNet.Game.STriggerGameMenuItemSelectedEvent'),
    93: (119, 'NNet.Game.STriggerPurchasePanelSelectedPurchaseItemChangedEvent'),
    94: (157, 'NNet.Game.STriggerPurchasePanelSelectedPurchaseCategoryChangedEvent'),
    95: (158, 'NNet.Game.STriggerButtonPressedEvent'),
    96: (72, 'NNet.Game.STriggerGameCreditsFinishedEvent'),
    97: (159, 'NNet.Game.STriggerCutsceneBookmarkFiredEvent'),
    98: (160, 'NNet.Game.STriggerCutsceneEndSceneFiredEvent'),
    99: (161, 'NNet.Game.STriggerCutsceneConversationLineEvent'),
    100: (162, 'NNet.Game.STriggerCutsceneConversationLineMissingEvent'),
    101: (72, 'NNet.Game.SGameUserLeaveEvent'),
    102: (163, 'NNet.Game.SGameUserJoinEvent'),
}

# The typeid of the NNet.Game.EEventId enum.
game_eventid_typeid = 0

# Map from protocol NNet.Game.*Message eventid to (typeid, name)
message_event_types = {
    0: (164, 'NNet.Game.SChatMessage'),
    1: (165, 'NNet.Game.SPingMessage'),
    2: (166, 'NNet.Game.SLoadingProgressMessage'),
    3: (72, 'NNet.Game.SServerPingMessage'),
}

# The typeid of the NNet.Game.EMessageId enum.
message_eventid_typeid = 1

# Map from protocol NNet.Replay.Tracker.*Event eventid to (typeid, name)
tracker_event_types = {
    0: (168, 'NNet.Replay.Tracker.SPlayerStatsEvent'),
    1: (169, 'NNet.Replay.Tracker.SUnitBornEvent'),
    2: (170, 'NNet.Replay.Tracker.SUnitDiedEvent'),
    3: (171, 'NNet.Replay.Tracker.SUnitOwnerChangeEvent'),
    4: (172, 'NNet.Replay.Tracker.SUnitTypeChangeEvent'),
    5: (173, 'NNet.Replay.Tracker.SUpgradeEvent'),
    6: (169, 'NNet.Replay.Tracker.SUnitInitEvent'),
    7: (174, 'NNet.Replay.Tracker.SUnitDoneEvent'),
    8: (176, 'NNet.Replay.Tracker.SUnitPositionsEvent'),
    9: (177, 'NNet.Replay.Tracker.SPlayerSetupEvent'),
}

# The typeid of the NNet.Replay.Tracker.EEventId enum.
tracker_eventid_typeid = 2

# The typeid of NNet.SVarUint32 (the type used to encode gameloop deltas).
svaruint32_typeid = 7

# The typeid of NNet.Replay.SGameUserId (the type used to encode player ids).
replay_userid_typeid = 8

# The typeid of NNet.Replay.SHeader (the type used to store replay game version and length).
replay_header_typeid = 14

# The typeid of NNet.Game.SDetails (the type used to store overall replay details).
game_details_typeid = 36

# The typeid of NNet.Replay.SInitData (the type used to store the inital lobby).
replay_initdata_typeid = 63


def _varuint32_value(value):
    # Returns the numeric value from a SVarUint32 instance.
    for k,v in value.iteritems():
        return v
    return 0


def _decode_event_stream(decoder, eventid_typeid, event_types, decode_user_id):
    # Decodes events prefixed with a gameloop and possibly userid
    gameloop = 0
    while not decoder.done():
        start_bits = decoder.used_bits()

        # decode the gameloop delta before each event
        delta = _varuint32_value(decoder.instance(svaruint32_typeid))
        gameloop += delta

        # decode the userid before each event
        if decode_user_id:
            userid = decoder.instance(replay_userid_typeid)

        # decode the event id
        eventid = decoder.instance(eventid_typeid)
        typeid, typename = event_types.get(eventid, (None, None))
        if typeid is None:
            raise CorruptedError('eventid(%d) at %s' % (eventid, decoder))

        # decode the event struct instance
        event = decoder.instance(typeid)
        event['_event'] = typename
        event['_eventid'] = eventid

        #  insert gameloop and userid
        event['_gameloop'] = gameloop
        if decode_user_id:
            event['_userid'] = userid

        # the next event is byte aligned
        decoder.byte_align()

        # insert bits used in stream
        event['_bits'] = decoder.used_bits() - start_bits

        yield event


def decode_replay_game_events(contents):
    """Decodes and yields each game event from the contents byte string."""
    decoder = BitPackedDecoder(contents, typeinfos)
    for event in _decode_event_stream(decoder,
                                      game_eventid_typeid,
                                      game_event_types,
                                      decode_user_id=True):
        yield event


def decode_replay_message_events(contents):
    """Decodes and yields each message event from the contents byte string."""
    decoder = BitPackedDecoder(contents, typeinfos)
    for event in _decode_event_stream(decoder,
                                      message_eventid_typeid,
                                      message_event_types,
                                      decode_user_id=True):
        yield event


def decode_replay_tracker_events(contents):
    """Decodes and yields each tracker event from the contents byte string."""
    decoder = VersionedDecoder(contents, typeinfos)
    for event in _decode_event_stream(decoder,
                                      tracker_eventid_typeid,
                                      tracker_event_types,
                                      decode_user_id=False):
        yield event


def decode_replay_header(contents):
    """Decodes and return the replay header from the contents byte string."""
    decoder = VersionedDecoder(contents, typeinfos)
    return decoder.instance(replay_header_typeid)


def decode_replay_details(contents):
    """Decodes and returns the game details from the contents byte string."""
    decoder = VersionedDecoder(contents, typeinfos)
    return decoder.instance(game_details_typeid)


def decode_replay_initdata(contents):
    """Decodes and return the replay init data from the contents byte string."""
    decoder = BitPackedDecoder(contents, typeinfos)
    return decoder.instance(replay_initdata_typeid)


def decode_replay_attributes_events(contents):
    """Decodes and yields each attribute from the contents byte string."""
    buffer = BitPackedBuffer(contents, 'little')
    attributes = {}
    if not buffer.done():
        attributes['source'] = buffer.read_bits(8)
        attributes['mapNamespace'] = buffer.read_bits(32)
        count = buffer.read_bits(32)
        attributes['scopes'] = {}
        while not buffer.done():
            value = {}
            value['namespace'] = buffer.read_bits(32)
            value['attrid'] = attrid = buffer.read_bits(32)
            scope = buffer.read_bits(8)
            value['value'] = buffer.read_aligned_bytes(4)[::-1].strip('\x00')
            if not scope in attributes['scopes']:
                attributes['scopes'][scope] = {}
            if not attrid in attributes['scopes'][scope]:
                attributes['scopes'][scope][attrid] = []
            attributes['scopes'][scope][attrid].append(value)
    return attributes


def unit_tag(unitTagIndex, unitTagRecycle):
    return (unitTagIndex << 18) + unitTagRecycle


def unit_tag_index(unitTag):
    return (unitTag >> 18) & 0x00003fff


def unit_tag_recycle(unitTag):
    return (unitTag) & 0x0003ffff

from threading import local

_thread_locals = local()


def get_current_request():
    return getattr(_thread_locals, 'request', None)


class ThreadLocals(object):
    """
    Middleware that stores the request object in thread local storage.
    """
    def process_request(self, request):
        _thread_locals.request = request

import logging

import django
from django.contrib import auth
from django.contrib.sessions.backends.base import SessionBase, CreateError
from django.core.exceptions import SuspiciousOperation
from django.db import IntegrityError, transaction, router
from django.utils import timezone
from django.utils.encoding import force_text


class SessionStore(SessionBase):
    """
    Implements database session store.
    """
    def __init__(self, user_agent, ip, session_key=None):
        super(SessionStore, self).__init__(session_key)
        # Truncate user_agent string to max_length of the CharField
        self.user_agent = user_agent[:200] if user_agent else user_agent
        self.ip = ip
        self.user_id = None

    def __setitem__(self, key, value):
        if key == auth.SESSION_KEY:
            self.user_id = value
        super(SessionStore, self).__setitem__(key, value)

    def load(self):
        try:
            s = Session.objects.get(
                session_key=self.session_key,
                expire_date__gt=timezone.now()
            )
            self.user_id = s.user_id
            # do not overwrite user_agent/ip, as those might have been updated
            if self.user_agent != s.user_agent or self.ip != s.ip:
                self.modified = True
            return self.decode(s.session_data)
        except (Session.DoesNotExist, SuspiciousOperation) as e:
            if isinstance(e, SuspiciousOperation):
                logger = logging.getLogger('django.security.%s' %
                                           e.__class__.__name__)
                logger.warning(force_text(e))
            self.create()
            return {}

    def exists(self, session_key):
        return Session.objects.filter(session_key=session_key).exists()

    def create(self):
        while True:
            self._session_key = self._get_new_session_key()
            try:
                # Save immediately to ensure we have a unique entry in the
                # database.
                self.save(must_create=True)
            except CreateError:
                # Key wasn't unique. Try again.
                continue
            self.modified = True
            self._session_cache = {}
            return

    def save(self, must_create=False):
        """
        Saves the current session data to the database. If 'must_create' is
        True, a database error will be raised if the saving operation doesn't
        create a *new* entry (as opposed to possibly updating an existing
        entry).
        """
        obj = Session(
            session_key=self._get_or_create_session_key(),
            session_data=self.encode(self._get_session(no_load=must_create)),
            expire_date=self.get_expiry_date(),
            user_agent=self.user_agent,
            user_id=self.user_id,
            ip=self.ip,
        )
        using = router.db_for_write(Session, instance=obj)
        try:
            if django.VERSION >= (1, 6):
                with transaction.atomic(using):
                    obj.save(force_insert=must_create, using=using)
            else:
                with transaction.commit_on_success(using):
                    obj.save(force_insert=must_create, using=using)
        except IntegrityError as e:
            if must_create and 'session_key' in str(e):
                raise CreateError
            raise

    def clear(self):
        super(SessionStore, self).clear()
        self.user_id = None

    def delete(self, session_key=None):
        if session_key is None:
            if self.session_key is None:
                return
            session_key = self.session_key
        try:
            Session.objects.get(session_key=session_key).delete()
        except Session.DoesNotExist:
            pass

    @classmethod
    def clear_expired(cls):
        Session.objects.filter(expire_date__lt=timezone.now()).delete()


# At bottom to avoid circular import
from ..models import Session

# coding=utf-8

"""
Collect IO Stats

Note: You may need to artifically generate some IO load on a disk/partition
before graphite will generate the metrics.

 * http://www.kernel.org/doc/Documentation/iostats.txt

#### Dependencies

 * /proc/diskstats

"""

import diamond.collector
import diamond.convertor
import time
import os
import re

try:
    import psutil
except ImportError:
    psutil = None


class DiskUsageCollector(diamond.collector.Collector):

    MAX_VALUES = {
        'reads':                    4294967295,
        'reads_merged':             4294967295,
        'reads_milliseconds':       4294967295,
        'writes':                   4294967295,
        'writes_merged':            4294967295,
        'writes_milliseconds':      4294967295,
        'io_milliseconds':          4294967295,
        'io_milliseconds_weighted': 4294967295
    }

    LastCollectTime = None

    def get_default_config_help(self):
        config_help = super(DiskUsageCollector, self).get_default_config_help()
        config_help.update({
            'devices': "A regex of which devices to gather metrics for."
                       + " Defaults to md, sd, xvd, disk, and dm devices",
            'sector_size': 'The size to use to calculate sector usage',
            'send_zero': 'Send io data even when there is no io',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(DiskUsageCollector, self).get_default_config()
        config.update({
            'path':     'iostat',
            'devices':  ('PhysicalDrive[0-9]+$'
                         + '|md[0-9]+$'
                         + '|sd[a-z]+[0-9]*$'
                         + '|x?vd[a-z]+[0-9]*$'
                         + '|disk[0-9]+$'
                         + '|dm\-[0-9]+$'),
            'sector_size': 512,
            'send_zero': False,
        })
        return config

    def get_disk_statistics(self):
        """
        Create a map of disks in the machine.

        http://www.kernel.org/doc/Documentation/iostats.txt

        Returns:
          (major, minor) -> DiskStatistics(device, ...)
        """
        result = {}

        if os.access('/proc/diskstats', os.R_OK):
            self.proc_diskstats = True
            fp = open('/proc/diskstats')

            try:
                for line in fp:
                    try:
                        columns = line.split()
                        # On early linux v2.6 versions, partitions have only 4
                        # output fields not 11. From linux 2.6.25 partitions
                        # have the full stats set.
                        if len(columns) < 14:
                            continue
                        major = int(columns[0])
                        minor = int(columns[1])
                        device = columns[2]

                        if (device.startswith('ram')
                                or device.startswith('loop')):
                            continue

                        result[(major, minor)] = {
                            'device': device,
                            'reads': float(columns[3]),
                            'reads_merged': float(columns[4]),
                            'reads_sectors': float(columns[5]),
                            'reads_milliseconds': float(columns[6]),
                            'writes': float(columns[7]),
                            'writes_merged': float(columns[8]),
                            'writes_sectors': float(columns[9]),
                            'writes_milliseconds': float(columns[10]),
                            'io_in_progress': float(columns[11]),
                            'io_milliseconds': float(columns[12]),
                            'io_milliseconds_weighted': float(columns[13])
                        }
                    except ValueError:
                        continue
            finally:
                fp.close()
        else:
            self.proc_diskstats = False
            if not psutil:
                self.log.error('Unable to import psutil')
                return None

            disks = psutil.disk_io_counters(True)
            for disk in disks:
                    result[(0, len(result))] = {
                        'device': disk,
                        'reads': disks[disk].read_count,
                        'reads_sectors': (disks[disk].read_bytes
                                          / int(self.config['sector_size'])),
                        'reads_milliseconds': disks[disk].read_time,
                        'writes': disks[disk].write_count,
                        'writes_sectors': (disks[disk].write_bytes
                                           / int(self.config['sector_size'])),
                        'writes_milliseconds': disks[disk].write_time,
                        'io_milliseconds':
                        disks[disk].read_time + disks[disk].write_time,
                        'io_milliseconds_weighted':
                        disks[disk].read_time + disks[disk].write_time
                    }

        return result

    def collect(self):

        # Handle collection time intervals correctly
        CollectTime = time.time()
        time_delta = float(self.config['interval'])
        if self.LastCollectTime:
            time_delta = CollectTime - self.LastCollectTime
        if not time_delta:
            time_delta = float(self.config['interval'])
        self.LastCollectTime = CollectTime

        exp = self.config['devices']
        reg = re.compile(exp)

        results = self.get_disk_statistics()
        if not results:
            self.log.error('No diskspace metrics retrieved')
            return None

        for key, info in results.iteritems():
            metrics = {}
            name = info['device']
            if not reg.match(name):
                continue

            for key, value in info.iteritems():
                if key == 'device':
                    continue
                oldkey = key

                for unit in self.config['byte_unit']:
                    key = oldkey

                    if key.endswith('sectors'):
                        key = key.replace('sectors', unit)
                        value /= (1024 / int(self.config['sector_size']))
                        value = diamond.convertor.binary.convert(value=value,
                                                                 oldUnit='kB',
                                                                 newUnit=unit)
                        self.MAX_VALUES[key] = diamond.convertor.binary.convert(
                            value=diamond.collector.MAX_COUNTER,
                            oldUnit='byte',
                            newUnit=unit)

                    metric_name = '.'.join([info['device'], key])
                    # io_in_progress is a point in time counter, !derivative
                    if key != 'io_in_progress':
                        metric_value = self.derivative(
                            metric_name,
                            value,
                            self.MAX_VALUES[key],
                            time_delta=False)
                    else:
                        metric_value = value

                    metrics[key] = metric_value

            if self.proc_diskstats:
                metrics['read_requests_merged_per_second'] = (
                    metrics['reads_merged'] / time_delta)
                metrics['write_requests_merged_per_second'] = (
                    metrics['writes_merged'] / time_delta)

            metrics['reads_per_second'] = metrics['reads'] / time_delta
            metrics['writes_per_second'] = metrics['writes'] / time_delta

            for unit in self.config['byte_unit']:
                metric_name = 'read_%s_per_second' % unit
                key = 'reads_%s' % unit
                metrics[metric_name] = metrics[key] / time_delta

                metric_name = 'write_%s_per_second' % unit
                key = 'writes_%s' % unit
                metrics[metric_name] = metrics[key] / time_delta

                # Set to zero so the nodes are valid even if we have 0 io for
                # the metric duration
                metric_name = 'average_request_size_%s' % unit
                metrics[metric_name] = 0

            metrics['io'] = metrics['reads'] + metrics['writes']

            metrics['average_queue_length'] = (
                metrics['io_milliseconds_weighted']
                / time_delta
                / 1000.0)

            metrics['util_percentage'] = (metrics['io_milliseconds']
                                          / time_delta
                                          / 10.0)

            if metrics['reads'] > 0:
                metrics['read_await'] = (
                    metrics['reads_milliseconds'] / metrics['reads'])
            else:
                metrics['read_await'] = 0

            if metrics['writes'] > 0:
                metrics['write_await'] = (
                    metrics['writes_milliseconds'] / metrics['writes'])
            else:
                metrics['write_await'] = 0

            for unit in self.config['byte_unit']:
                rkey = 'reads_%s' % unit
                wkey = 'writes_%s' % unit
                metric_name = 'average_request_size_%s' % unit
                if (metrics['io'] > 0):
                    metrics[metric_name] = (
                        metrics[rkey] + metrics[wkey]) / metrics['io']
                else:
                    metrics[metric_name] = 0

            metrics['iops'] = metrics['io'] / time_delta

            if (metrics['io'] > 0):
                metrics['service_time'] = (
                    metrics['io_milliseconds'] / metrics['io'])
                metrics['await'] = (
                    metrics['reads_milliseconds']
                    + metrics['writes_milliseconds']) / metrics['io']
            else:
                metrics['service_time'] = 0
                metrics['await'] = 0

            # http://www.scribd.com/doc/15013525
            # Page 28
            metrics['concurrent_io'] = (metrics['reads_per_second']
                                        + metrics['writes_per_second']
                                        ) * (metrics['service_time']
                                             / 1000.0)

            # Only publish when we have io figures
            if (metrics['io'] > 0 or self.config['send_zero']):
                for key in metrics:
                    metric_name = '.'.join([info['device'], key]).replace(
                        '/', '_')
                    self.publish(metric_name, metrics[key])

#!/usr/bin/python
# coding=utf-8

from test import CollectorTestCase
from test import get_collector_config
from mock import patch
import os

from diamond.collector import Collector
from gridengine import GridEngineCollector


class TestGridEngineCollector(CollectorTestCase):
    """Set up the fixtures for the test
    """
    def setUp(self):
        config = get_collector_config('GridEngineCollector', {})
        self.collector = GridEngineCollector(config, None)
        self.fixtures_dir = os.path.abspath(os.path.join(
            os.path.dirname(__file__), 'fixtures'))

    def test_import(self):
        """Test that import succeeds
        """
        self.assertTrue(GridEngineCollector)

    @patch.object(GridEngineCollector, '_queue_stats_xml')
    @patch.object(Collector, 'publish')
    def test_queue_stats_should_work_with_real_data(
            self, publish_mock, xml_mock):
        """Test that fixtures are parsed correctly
        """
        xml_mock.return_value = self.getFixture('queue_stats.xml').getvalue()
        self.collector._collect_queue_stats()

        published_metrics = {
            'queues.hadoop.load': 0.00532,
            'queues.hadoop.used': 0,
            'queues.hadoop.resv': 0,
            'queues.hadoop.available': 0,
            'queues.hadoop.total': 36,
            'queues.hadoop.temp_disabled': 0,
            'queues.hadoop.manual_intervention': 36,
            'queues.primary_q.load': 0.20509,
            'queues.primary_q.used': 1024,
            'queues.primary_q.resv': 0,
            'queues.primary_q.available': 1152,
            'queues.primary_q.total': 2176,
            'queues.primary_q.temp_disabled': 0,
            'queues.primary_q.manual_intervention': 0,
            'queues.secondary_q.load': 0.00460,
            'queues.secondary_q.used': 145,
            'queues.secondary_q.resv': 0,
            'queues.secondary_q.available': 1007,
            'queues.secondary_q.total': 1121,
            'queues.secondary_q.temp_disabled': 1,
            'queues.secondary_q.manual_intervention': 0
        }
        self.assertPublishedMany(publish_mock, published_metrics)

    @patch.object(GridEngineCollector, '_queue_stats_xml')
    @patch.object(Collector, 'publish')
    def test_707(
            self, publish_mock, xml_mock):
        """Test that fixtures are parsed correctly
        """
        xml_mock.return_value = self.getFixture('707.xml').getvalue()
        self.collector._collect_queue_stats()

#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from test import run_only
from mock import MagicMock
from mock import patch
from mock import call

from diamond.collector import Collector
from mongodb import MongoDBCollector

################################################################################


def run_only_if_pymongo_is_available(func):
    try:
        import pymongo
    except ImportError:
        pymongo = None
    pred = lambda: pymongo is not None
    return run_only(func, pred)


class TestMongoDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MongoDBCollector', {
            'host': 'localhost:27017',
            'databases': '^db',
        })
        self.collector = MongoDBCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(MongoDBCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'more_keys.nested_key': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_once_with('dbStats')
        metrics = {
            'db_keys.db_nested_key': 1,
            'dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'more_keys': 1,
            'key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_called_once_with('serverStatus')
        self.connection['db1'].collection_names.assert_called_once_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


class TestMongoMultiHostDBCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('MongoDBCollector', {
            'hosts': ['localhost:27017', 'localhost:27057'],
            'databases': '^db',
        })
        self.collector = MongoDBCollector(config, None)
        self.connection = MagicMock()

    def test_import(self):
        self.assertTrue(MongoDBCollector)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_server_stats(self,
                                                         publish_mock,
                                                         connector_mock):
        data = {'more_keys': {'nested_key': 1}, 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys.nested_key': 1,
            'localhost_27057.more_keys.nested_key': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_nested_keys_for_db_stats(self,
                                                     publish_mock,
                                                     connector_mock):
        data = {'db_keys': {'db_nested_key': 1}, 'dbkey': 2, 'dbstring': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection['db1'].command.assert_called_with('dbStats')
        metrics = {
            'localhost_27017.db_keys.db_nested_key': 1,
            'localhost_27057.db_keys.db_nested_key': 1,
            'localhost_27017.dbkey': 2,
            'localhost_27057.dbkey': 2
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_publish_stats_with_long_type(self,
                                                 publish_mock,
                                                 connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.assertPublishedMany(publish_mock, {
            'localhost_27017.more_keys': 1,
            'localhost_27057.more_keys': 1,
            'localhost_27017.key': 2,
            'localhost_27057.key': 2
        })

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_databases(self,
                                              publish_mock,
                                              connector_mock):
        self._annotate_connection(connector_mock, {})

        self.collector.collect()

        assert call('baddb') not in self.connection.__getitem__.call_args_list

    @run_only_if_pymongo_is_available
    @patch('pymongo.Connection')
    @patch.object(Collector, 'publish')
    def test_should_ignore_unneeded_collections(self,
                                                publish_mock,
                                                connector_mock):
        data = {'more_keys': long(1), 'key': 2, 'string': 'str'}
        self._annotate_connection(connector_mock, data)

        self.connection['db1'].collection_names.return_value = ['collection1',
                                                                'tmp.mr.tmp1']
        self.connection['db1'].command.return_value = {'key': 2,
                                                       'string': 'str'}

        self.collector.collect()

        self.connection.db.command.assert_called_with('serverStatus')
        self.connection['db1'].collection_names.assert_called_with()
        self.connection['db1'].command.assert_any_call('dbStats')
        self.connection['db1'].command.assert_any_call('collstats',
                                                       'collection1')
        assert call('collstats', 'tmp.mr.tmp1') not in self.connection['db1'].command.call_args_list  # NOQA
        metrics = {
            'localhost_27017.databases.db1.collection1.key': 2,
            'localhost_27057.databases.db1.collection1.key': 2,
        }

        self.assertPublishedMany(publish_mock, metrics)

    def _annotate_connection(self, connector_mock, data):
        connector_mock.return_value = self.connection
        self.connection.db.command.return_value = data
        self.connection.database_names.return_value = ['db1', 'baddb']


################################################################################
if __name__ == "__main__":
    unittest.main()

# coding=utf-8

"""
The NfsdCollector collects nfsd utilization metrics using /proc/net/rpc/nfsd.

#### Dependencies

 * /proc/net/rpc/nfsd

"""

import diamond.collector
import os


class NfsdCollector(diamond.collector.Collector):

    PROC = '/proc/net/rpc/nfsd'

    def get_default_config_help(self):
        config_help = super(NfsdCollector, self).get_default_config_help()
        config_help.update({
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(NfsdCollector, self).get_default_config()
        config.update({
            'path':     'nfsd'
        })
        return config

    def collect(self):
        """
        Collect stats
        """
        if os.access(self.PROC, os.R_OK):

            results = {}
            # Open file
            file = open(self.PROC)

            for line in file:
                line = line.split()

                if line[0] == 'rc':
                    results['reply_cache.hits'] = line[1]
                    results['reply_cache.misses'] = line[2]
                    results['reply_cache.nocache'] = line[3]
                elif line[0] == 'fh':
                    results['filehandle.stale'] = line[1]
                    results['filehandle.total-lookups'] = line[2]
                    results['filehandle.anonlookups'] = line[3]
                    results['filehandle.dir-not-in-cache'] = line[4]
                    results['filehandle.nodir-not-in-cache'] = line[5]
                elif line[0] == 'io':
                    results['input_output.bytes-read'] = line[1]
                    results['input_output.bytes-written'] = line[2]
                elif line[0] == 'th':
                    results['threads.threads'] = line[1]
                    results['threads.fullcnt'] = line[2]
                    results['threads.10-20-pct'] = line[3]
                    results['threads.20-30-pct'] = line[4]
                    results['threads.30-40-pct'] = line[5]
                    results['threads.40-50-pct'] = line[6]
                    results['threads.50-60-pct'] = line[7]
                    results['threads.60-70-pct'] = line[8]
                    results['threads.70-80-pct'] = line[9]
                    results['threads.80-90-pct'] = line[10]
                    results['threads.90-100-pct'] = line[11]
                    results['threads.100-pct'] = line[12]
                elif line[0] == 'ra':
                    results['read-ahead.cache-size'] = line[1]
                    results['read-ahead.10-pct'] = line[2]
                    results['read-ahead.20-pct'] = line[3]
                    results['read-ahead.30-pct'] = line[4]
                    results['read-ahead.40-pct'] = line[5]
                    results['read-ahead.50-pct'] = line[6]
                    results['read-ahead.60-pct'] = line[7]
                    results['read-ahead.70-pct'] = line[8]
                    results['read-ahead.80-pct'] = line[9]
                    results['read-ahead.90-pct'] = line[10]
                    results['read-ahead.100-pct'] = line[11]
                    results['read-ahead.not-found'] = line[12]
                elif line[0] == 'net':
                    results['net.cnt'] = line[1]
                    results['net.udpcnt'] = line[2]
                    results['net.tcpcnt'] = line[3]
                    results['net.tcpconn'] = line[4]
                elif line[0] == 'rpc':
                    results['rpc.cnt'] = line[1]
                    results['rpc.badfmt'] = line[2]
                    results['rpc.badauth'] = line[3]
                    results['rpc.badclnt'] = line[4]
                elif line[0] == 'proc2':
                    results['v2.unknown'] = line[1]
                    results['v2.null'] = line[2]
                    results['v2.getattr'] = line[3]
                    results['v2.setattr'] = line[4]
                    results['v2.root'] = line[5]
                    results['v2.lookup'] = line[6]
                    results['v2.readlink'] = line[7]
                    results['v2.read'] = line[8]
                    results['v2.wrcache'] = line[9]
                    results['v2.write'] = line[10]
                    results['v2.create'] = line[11]
                    results['v2.remove'] = line[12]
                    results['v2.rename'] = line[13]
                    results['v2.link'] = line[14]
                    results['v2.symlink'] = line[15]
                    results['v2.mkdir'] = line[16]
                    results['v2.rmdir'] = line[17]
                    results['v2.readdir'] = line[18]
                    results['v2.fsstat'] = line[19]
                elif line[0] == 'proc3':
                    results['v3.unknown'] = line[1]
                    results['v3.null'] = line[2]
                    results['v3.getattr'] = line[3]
                    results['v3.setattr'] = line[4]
                    results['v3.lookup'] = line[5]
                    results['v3.access'] = line[6]
                    results['v3.readlink'] = line[7]
                    results['v3.read'] = line[8]
                    results['v3.write'] = line[9]
                    results['v3.create'] = line[10]
                    results['v3.mkdir'] = line[11]
                    results['v3.symlink'] = line[12]
                    results['v3.mknod'] = line[13]
                    results['v3.remove'] = line[14]
                    results['v3.rmdir'] = line[15]
                    results['v3.rename'] = line[16]
                    results['v3.link'] = line[17]
                    results['v3.readdir'] = line[18]
                    results['v3.readdirplus'] = line[19]
                    results['v3.fsstat'] = line[20]
                    results['v3.fsinfo'] = line[21]
                    results['v3.pathconf'] = line[22]
                    results['v3.commit'] = line[23]
                elif line[0] == 'proc4':
                    results['v4.unknown'] = line[1]
                    results['v4.null'] = line[2]
                    results['v4.compound'] = line[3]
                elif line[0] == 'proc4ops':
                    results['v4.ops.unknown'] = line[1]
                    results['v4.ops.op0-unused'] = line[2]
                    results['v4.ops.op1-unused'] = line[3]
                    results['v4.ops.op2-future'] = line[4]
                    results['v4.ops.access'] = line[5]
                    results['v4.ops.close'] = line[6]
                    results['v4.ops.commit'] = line[7]
                    results['v4.ops.create'] = line[8]
                    results['v4.ops.delegpurge'] = line[9]
                    results['v4.ops.delegreturn'] = line[10]
                    results['v4.ops.getattr'] = line[11]
                    results['v4.ops.getfh'] = line[12]
                    results['v4.ops.link'] = line[13]
                    results['v4.ops.lock'] = line[14]
                    results['v4.ops.lockt'] = line[15]
                    results['v4.ops.locku'] = line[16]
                    results['v4.ops.lookup'] = line[17]
                    results['v4.ops.lookup_root'] = line[18]
                    results['v4.ops.nverify'] = line[19]
                    results['v4.ops.open'] = line[20]
                    results['v4.ops.openattr'] = line[21]
                    results['v4.ops.open_conf'] = line[22]
                    results['v4.ops.open_dgrd'] = line[23]
                    results['v4.ops.putfh'] = line[24]
                    results['v4.ops.putpubfh'] = line[25]
                    results['v4.ops.putrootfh'] = line[26]
                    results['v4.ops.read'] = line[27]
                    results['v4.ops.readdir'] = line[28]
                    results['v4.ops.readlink'] = line[29]
                    results['v4.ops.remove'] = line[30]
                    results['v4.ops.rename'] = line[31]
                    results['v4.ops.renew'] = line[32]
                    results['v4.ops.restorefh'] = line[33]
                    results['v4.ops.savefh'] = line[34]
                    results['v4.ops.secinfo'] = line[35]
                    results['v4.ops.setattr'] = line[36]
                    results['v4.ops.setcltid'] = line[37]
                    results['v4.ops.setcltidconf'] = line[38]
                    results['v4.ops.verify'] = line[39]
                    results['v4.ops.write'] = line[40]
                    results['v4.ops.rellockowner'] = line[41]

            # Close File
            file.close()

            for stat in results.keys():
                metric_name = '.' + stat
                metric_value = long(float(results[stat]))
                metric_value = self.derivative(metric_name, metric_value)
                self.publish(metric_name, metric_value)
            return True

        return False

# coding=utf-8

"""
Collect stats from postfix-stats. postfix-stats is a simple threaded stats
aggregator for Postfix. When running as a syslog destination, it can be used to
get realtime cumulative stats.

#### Dependencies

 * socket
 * json (or simeplejson)
 * [postfix-stats](https://github.com/disqus/postfix-stats)

"""

import socket
import sys

try:
    import json
except ImportError:
    import simplejson as json

import diamond.collector

from diamond.collector import str_to_bool

if sys.version_info < (2, 6):
    from string import maketrans
    DOTS_TO_UNDERS = maketrans('.', '_')
else:
    DOTS_TO_UNDERS = {ord(u'.'): u'_'}


class PostfixCollector(diamond.collector.Collector):

    def get_default_config_help(self):
        config_help = super(PostfixCollector,
                            self).get_default_config_help()
        config_help.update({
            'host':             'Hostname to coonect to',
            'port':             'Port to connect to',
            'include_clients':  'Include client connection stats',
        })
        return config_help

    def get_default_config(self):
        """
        Returns the default collector settings
        """
        config = super(PostfixCollector, self).get_default_config()
        config.update({
            'path':             'postfix',
            'host':             'localhost',
            'port':             7777,
            'include_clients':  True,
        })
        return config

    def get_json(self):
        json_string = ''

        address = (self.config['host'], int(self.config['port']))

        s = None
        try:
            try:
                s = socket.create_connection(address, timeout=1)

                s.sendall('stats\n')

                while 1:
                    data = s.recv(4096)
                    if not data:
                        break
                    json_string += data
            except socket.error:
                self.log.exception("Error talking to postfix-stats")
                return '{}'
        finally:
            if s:
                s.close()

        return json_string or '{}'

    def get_data(self):
        json_string = self.get_json()

        try:
            data = json.loads(json_string)
        except (ValueError, TypeError):
            self.log.exception("Error parsing json from postfix-stats")
            return None

        return data

    def collect(self):
        data = self.get_data()

        if not data:
            return

        if str_to_bool(self.config['include_clients']) and u'clients' in data:
            for client, value in data['clients'].iteritems():
                # translate dots to underscores in client names
                metric = u'.'.join(['clients',
                                    client.translate(DOTS_TO_UNDERS)])

                dvalue = self.derivative(metric, value)

                self.publish(metric, dvalue)

        for action in (u'in', u'recv', u'send'):
            if action not in data:
                continue

            for sect, stats in data[action].iteritems():
                for status, value in stats.iteritems():
                    metric = '.'.join([action,
                                       sect,
                                       status.translate(DOTS_TO_UNDERS)])

                    dvalue = self.derivative(metric, value)

                    self.publish(metric, dvalue)

        if u'local' in data:
            for key, value in data[u'local'].iteritems():
                metric = '.'.join(['local', key])

                dvalue = self.derivative(metric, value)

                self.publish(metric, dvalue)

#!/usr/bin/python
# coding=utf-8
################################################################################

from test import CollectorTestCase
from test import get_collector_config
from test import unittest
from mock import Mock
from mock import patch

from diamond.collector import Collector

from unbound import UnboundCollector

################################################################################


class TestUnboundCollector(CollectorTestCase):
    def setUp(self):
        config = get_collector_config('UnboundCollector', {})

        self.collector = UnboundCollector(config, None)

    def test_import(self):
        self.assertTrue(UnboundCollector)

    @patch.object(Collector, 'publish')
    def test_should_work_wtih_real_data(self, publish_mock):
        fixture_data = self.getFixture('unbound_stats').getvalue()
        collector_mock = patch.object(UnboundCollector,
                                      'run_command',
                                      Mock(return_value=[fixture_data, '']))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'thread0.num.queries': 10028,
            'thread0.num.cachehits': 10021,
            'thread0.num.cachemiss': 7,
            'thread0.num.prefetch': 1,
            'thread0.num.recursivereplies': 9,
            'thread0.requestlist.avg': 1.25,
            'thread0.requestlist.max': 2,
            'thread0.requestlist.overwritten': 0,
            'thread0.requestlist.exceeded': 0,
            'thread0.requestlist.current.all': 1,
            'thread0.requestlist.current.user': 1,
            'thread0.recursion.time.avg': 9.914812,
            'thread0.recursion.time.median': 0.08192,
            'total.num.queries': 125609,
            'total.num.cachehits': 125483,
            'total.num.cachemiss': 126,
            'total.num.prefetch': 16,
            'total.num.recursivereplies': 136,
            'total.requestlist.avg': 5.07746,
            'total.requestlist.max': 10,
            'total.requestlist.overwritten': 0,
            'total.requestlist.exceeded': 0,
            'total.requestlist.current.all': 23,
            'total.requestlist.current.user': 23,
            'total.recursion.time.avg': 13.045485,
            'total.recursion.time.median': 0.06016,
            'time.now': 1361926066.384873,
            'time.up': 3006293.632453,
            'time.elapsed': 9.981882,
            'mem.total.sbrk': 26767360,
            'mem.cache.rrset': 142606276,
            'mem.cache.message': 71303005,
            'mem.mod.iterator': 16532,
            'mem.mod.validator': 1114579,
            'num.query.type.A': 25596,
            'num.query.type.PTR': 39,
            'num.query.type.MX': 91,
            'num.query.type.AAAA': 99883,
            'num.query.class.IN': 125609,
            'num.query.opcode.QUERY': 125609,
            'num.query.tcp': 0,
            'num.query.ipv6': 0,
            'num.query.flags.QR': 0,
            'num.query.flags.AA': 0,
            'num.query.flags.TC': 0,
            'num.query.flags.RD': 125609,
            'num.query.flags.RA': 0,
            'num.query.flags.Z': 0,
            'num.query.flags.AD': 0,
            'num.query.flags.CD': 62,
            'num.query.edns.present': 62,
            'num.query.edns.DO': 62,
            'num.answer.rcode.NOERROR': 46989,
            'num.answer.rcode.SERVFAIL': 55,
            'num.answer.rcode.NXDOMAIN': 78575,
            'num.answer.rcode.nodata': 20566,
            'num.answer.secure': 0,
            'num.answer.bogus': 0,
            'num.rrset.bogus': 0,
            'unwanted.queries': 0,
            'unwanted.replies': 0,
            'histogram.16s+': 0.0,
            'histogram.256ms+': 3.0,
            'histogram.4s+': 1.0,
            'histogram.2s+': 0.0,
            'histogram.1s+': 0.0,
            'histogram.2ms+': 0.0,
            'histogram.1ms': 39.0,
            'histogram.32ms+': 18.0,
            'histogram.4ms+': 0.0,
            'histogram.16ms+': 10.0,
            'histogram.1ms+': 5.0,
            'histogram.32s+': 3.0,
            'histogram.512ms+': 6.0,
            'histogram.128ms+': 19.0,
            'histogram.64ms+': 20.0,
            'histogram.8ms+': 3.0,
            'histogram.64s+': 9.0,
            'histogram.8s+': 0.0,
        }

        self.setDocExample(collector=self.collector.__class__.__name__,
                           metrics=metrics,
                           defaultpath=self.collector.config['path'])
        self.assertPublishedMany(publish_mock, metrics)

    @patch.object(Collector, 'publish')
    def test_should_fail_gracefully(self, publish_mock):
        collector_mock = patch.object(UnboundCollector,
                                      'run_command',
                                      Mock(return_value=None))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        self.assertPublishedMany(publish_mock, {})

    @patch.object(Collector, 'publish')
    def test_exclude_histogram(self, publish_mock):
        self.collector.config['histogram'] = False

        fixture_data = self.getFixture('unbound_stats').getvalue()
        collector_mock = patch.object(UnboundCollector,
                                      'run_command',
                                      Mock(return_value=[fixture_data, '']))
        collector_mock.start()
        self.collector.collect()
        collector_mock.stop()

        metrics = {
            'thread0.num.queries': 10028,
            'thread0.num.cachehits': 10021,
            'thread0.num.cachemiss': 7,
            'thread0.num.prefetch': 1,
            'thread0.num.recursivereplies': 9,
            'thread0.requestlist.avg': 1.25,
            'thread0.requestlist.max': 2,
            'thread0.requestlist.overwritten': 0,
            'thread0.requestlist.exceeded': 0,
            'thread0.requestlist.current.all': 1,
            'thread0.requestlist.current.user': 1,
            'thread0.recursion.time.avg': 9.914812,
            'thread0.recursion.time.median': 0.08192,
            'total.num.queries': 125609,
            'total.num.cachehits': 125483,
            'total.num.cachemiss': 126,
            'total.num.prefetch': 16,
            'total.num.recursivereplies': 136,
            'total.requestlist.avg': 5.07746,
            'total.requestlist.max': 10,
            'total.requestlist.overwritten': 0,
            'total.requestlist.exceeded': 0,
            'total.requestlist.current.all': 23,
            'total.requestlist.current.user': 23,
            'total.recursion.time.avg': 13.045485,
            'total.recursion.time.median': 0.06016,
            'time.now': 1361926066.384873,
            'time.up': 3006293.632453,
            'time.elapsed': 9.981882,
            'mem.total.sbrk': 26767360,
            'mem.cache.rrset': 142606276,
            'mem.cache.message': 71303005,
            'mem.mod.iterator': 16532,
            'mem.mod.validator': 1114579,
            'num.query.type.A': 25596,
            'num.query.type.PTR': 39,
            'num.query.type.MX': 91,
            'num.query.type.AAAA': 99883,
            'num.query.class.IN': 125609,
            'num.query.opcode.QUERY': 125609,
            'num.query.tcp': 0,
            'num.query.ipv6': 0,
            'num.query.flags.QR': 0,
            'num.query.flags.AA': 0,
            'num.query.flags.TC': 0,
            'num.query.flags.RD': 125609,
            'num.query.flags.RA': 0,
            'num.query.flags.Z': 0,
            'num.query.flags.AD': 0,
            'num.query.flags.CD': 62,
            'num.query.edns.present': 62,
            'num.query.edns.DO': 62,
            'num.answer.rcode.NOERROR': 46989,
            'num.answer.rcode.SERVFAIL': 55,
            'num.answer.rcode.NXDOMAIN': 78575,
            'num.answer.rcode.nodata': 20566,
            'num.answer.secure': 0,
            'num.answer.bogus': 0,
            'num.rrset.bogus': 0,
            'unwanted.queries': 0,
            'unwanted.replies': 0,
        }

        histogram = {
            'histogram.16s+': 0.0,
            'histogram.256ms+': 3.0,
            'histogram.4s+': 1.0,
            'histogram.2s+': 0.0,
            'histogram.1s+': 0.0,
            'histogram.2ms+': 0.0,
            'histogram.1ms': 39.0,
            'histogram.32ms+': 18.0,
            'histogram.4ms+': 0.0,
            'histogram.16ms+': 10.0,
            'histogram.1ms+': 5.0,
            'histogram.32s+': 3.0,
            'histogram.512ms+': 6.0,
            'histogram.128ms+': 19.0,
            'histogram.64ms+': 20.0,
            'histogram.8ms+': 3.0,
            'histogram.64s+': 9.0,
            'histogram.8s+': 0.0,
        }

        self.assertPublishedMany(publish_mock, metrics)
        self.assertUnpublishedMany(publish_mock, histogram)

################################################################################
if __name__ == "__main__":
    unittest.main()

#!/usr/bin/env python
# coding=utf-8

# This is the MIT License
# http://www.opensource.org/licenses/mit-license.php
#
# Copyright (c) 2007,2008 Nick Galbreath
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
#

#
# Version 1.0 - 21-Apr-2007
#   initial
# Version 2.0 - 16-Nov-2008
#   made class Gmetric thread safe
#   made gmetrix xdr writers _and readers_
#   Now this only works for gmond 2.X packets, not tested with 3.X
#
# Version 3.0 - 09-Jan-2011 Author: Vladimir Vuksan
#   Made it work with the Ganglia 3.1 data format


from xdrlib import Packer, Unpacker
import socket

slope_str2int = {'zero': 0,
                 'positive': 1,
                 'negative': 2,
                 'both': 3,
                 'unspecified': 4}

# could be autogenerated from previous but whatever
slope_int2str = {0: 'zero',
                 1: 'positive',
                 2: 'negative',
                 3: 'both',
                 4: 'unspecified'}


class Gmetric:
    """
    Class to send gmetric/gmond 2.X packets

    Thread safe
    """

    type = ('', 'string', 'uint16', 'int16', 'uint32', 'int32', 'float',
            'double', 'timestamp')
    protocol = ('udp', 'multicast')

    def __init__(self, host, port, protocol):
        if protocol not in self.protocol:
            raise ValueError("Protocol must be one of: " + str(self.protocol))

        self.socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        if protocol == 'multicast':
            self.socket.setsockopt(socket.IPPROTO_IP,
                                   socket.IP_MULTICAST_TTL, 20)
        self.hostport = (host, int(port))
        #self.socket.connect(self.hostport)

    def send(self, NAME, VAL, TYPE='', UNITS='', SLOPE='both',
             TMAX=60, DMAX=0, GROUP=""):
        if SLOPE not in slope_str2int:
            raise ValueError("Slope must be one of: " + str(self.slope.keys()))
        if TYPE not in self.type:
            raise ValueError("Type must be one of: " + str(self.type))
        if len(NAME) == 0:
            raise ValueError("Name must be non-empty")

        (meta_msg, data_msg) = gmetric_write(NAME,
                                             VAL,
                                             TYPE,
                                             UNITS,
                                             SLOPE,
                                             TMAX,
                                             DMAX,
                                             GROUP)
        # print msg

        self.socket.sendto(meta_msg, self.hostport)
        self.socket.sendto(data_msg, self.hostport)


def gmetric_write(NAME, VAL, TYPE, UNITS, SLOPE, TMAX, DMAX, GROUP):
    """
    Arguments are in all upper-case to match XML
    """
    packer = Packer()
    HOSTNAME = "test"
    SPOOF = 0
    # Meta data about a metric
    packer.pack_int(128)
    packer.pack_string(HOSTNAME)
    packer.pack_string(NAME)
    packer.pack_int(SPOOF)
    packer.pack_string(TYPE)
    packer.pack_string(NAME)
    packer.pack_string(UNITS)
    packer.pack_int(slope_str2int[SLOPE])  # map slope string to int
    packer.pack_uint(int(TMAX))
    packer.pack_uint(int(DMAX))
    # Magic number. Indicates number of entries to follow. Put in 1 for GROUP
    if GROUP == "":
        packer.pack_int(0)
    else:
        packer.pack_int(1)
        packer.pack_string("GROUP")
        packer.pack_string(GROUP)

    # Actual data sent in a separate packet
    data = Packer()
    data.pack_int(128 + 5)
    data.pack_string(HOSTNAME)
    data.pack_string(NAME)
    data.pack_int(SPOOF)
    data.pack_string("%s")
    data.pack_string(str(VAL))

    return packer.get_buffer(),  data.get_buffer()


def gmetric_read(msg):
    unpacker = Unpacker(msg)
    values = dict()
    unpacker.unpack_int()
    values['TYPE'] = unpacker.unpack_string()
    values['NAME'] = unpacker.unpack_string()
    values['VAL'] = unpacker.unpack_string()
    values['UNITS'] = unpacker.unpack_string()
    values['SLOPE'] = slope_int2str[unpacker.unpack_int()]
    values['TMAX'] = unpacker.unpack_uint()
    values['DMAX'] = unpacker.unpack_uint()
    unpacker.done()
    return values


if __name__ == '__main__':
    import optparse
    parser = optparse.OptionParser()
    parser.add_option("", "--protocol", dest="protocol", default="udp",
                      help="The gmetric internet protocol, either udp or"
                          + "multicast, default udp")
    parser.add_option("", "--host",  dest="host",  default="127.0.0.1",
                      help="GMond aggregator hostname to send data to")
    parser.add_option("", "--port",  dest="port",  default="8649",
                      help="GMond aggregator port to send data to")
    parser.add_option("", "--name",  dest="name",  default="",
                      help="The name of the metric")
    parser.add_option("", "--value", dest="value", default="",
                      help="The value of the metric")
    parser.add_option("", "--units", dest="units", default="",
                      help="The units for the value, e.g. 'kb/sec'")
    parser.add_option("", "--slope", dest="slope", default="both",
                      help="The sign of the derivative of the value over time,"
                          + "one of zero, positive, negative, both (default)")
    parser.add_option("", "--type",  dest="type",  default="",
                      help="The value data type, one of string, int8, uint8,"
                          + "int16, uint16, int32, uint32, float, double")
    parser.add_option("", "--tmax",  dest="tmax",  default="60",
                      help="The maximum time in seconds between gmetric calls,"
                          + "default 60")
    parser.add_option("", "--dmax",  dest="dmax",  default="0",
                      help="The lifetime in seconds of this metric, default=0,"
                          + "meaning unlimited")
    parser.add_option("", "--group",  dest="group",  default="",
                      help="Group metric belongs to. If not specified Ganglia"
                          + "will show it as no_group")
    (options, args) = parser.parse_args()

    g = Gmetric(options.host, options.port, options.protocol)
    g.send(options.name, options.value, options.type, options.units,
           options.slope, options.tmax, options.dmax, options.group)

#!/usr/bin/python
# coding=utf-8
################################################################################

from test import unittest
from test import run_only
import configobj

from diamond.handler.riemann import RiemannHandler
from diamond.metric import Metric


def run_only_if_bernhard_is_available(func):
    try:
        import bernhard
    except ImportError:
        bernhard = None
    pred = lambda: bernhard is not None
    return run_only(func, pred)


class TestRiemannHandler(unittest.TestCase):

    @run_only_if_bernhard_is_available
    def test_metric_to_riemann_event(self):
        config = configobj.ConfigObj()
        config['host'] = 'localhost'
        config['port'] = 5555

        handler = RiemannHandler(config)
        metric = Metric('servers.com.example.www.cpu.total.idle',
                        0,
                        timestamp=1234567,
                        host='com.example.www')

        event = handler._metric_to_riemann_event(metric)

        self.assertEqual(event, {
            'host': 'com.example.www',
            'service': 'servers.cpu.total.idle',
            'time': 1234567,
            'metric': 0.0,
            'ttl': None
        })


from django.core.management.base import AppCommand, CommandError
from drf_generators.generators import *
from optparse import make_option
import django


class Command(AppCommand):
    help = 'Generates DRF API Views and Serializers for a Django app'

    args = "[appname ...]"

    base_options = (
        make_option('-f', '--format', dest='format', default='viewset',
                    help='view format (default: viewset)'),

        make_option('-d', '--depth', dest='depth', default=0,
                    help='serialization depth'),

        make_option('--force', dest='force', action='store_true',
                    help='force overwrite files'),

        make_option('--serializers', dest='serializers', action='store_true',
                    help='generate serializers only'),

        make_option('--views', dest='views', action='store_true',
                    help='generate views only'),

        make_option('--urls', dest='urls', action='store_true',
                    help='generate urls only'),
    )

    option_list = AppCommand.option_list + base_options

    def handle_app_config(self, app_config, **options):
        if app_config.models_module is None:
            raise CommandError('You must provide an app to generate an API')

        if django.VERSION[1] == 7:
            force = options['force'] if 'force' in options else False
            format = options['format'] if 'format' in options else None
            depth = options['depth'] if 'depth' in format else 0
            if 'serializers' in options:
                serializers = options['serializers']
            else:
                serializers = False
            views = options['views'] if 'views' in options else False
            urls = options['urls'] if 'urls' in options else False

        elif django.VERSION[1] >= 8:
            force = options['force']
            format = options['format']
            depth = options['depth']
            serializers = options['serializers']
            views = options['views']
            urls = options['urls']
        else:
            raise CommandError('You must be using Django 1.7, 1.8 or 1.9')

        if format == 'viewset':
            generator = ViewSetGenerator(app_config, force)
        elif format == 'apiview':
            generator = APIViewGenerator(app_config, force)
        elif format == 'function':
            generator = FunctionViewGenerator(app_config, force)
        elif format == 'modelviewset':
            generator = ModelViewSetGenerator(app_config, force)
        else:
            message = '\'%s\' is not a valid format. ' % options['format']
            message += '(viewset, modelviewset, apiview, function)'
            raise CommandError(message)

        if serializers:
            result = generator.generate_serializers(depth)
        elif views:
            result = generator.generate_views()
        elif urls:
            result = generator.generate_urls()
        else:
            result = generator.generate_serializers(depth) + '\n'
            result += generator.generate_views() + '\n'
            result += generator.generate_urls()

        print(result)

"""Define the menu contents, hotkeys, and event bindings.

There is additional configuration information in the EditorWindow class (and
subclasses): the menus are created there based on the menu_specs (class)
variable, and menus not created are silently skipped in the code here.  This
makes it possible, for example, to define a Debug menu which is only present in
the PythonShell window, and a Format menu which is only present in the Editor
windows.

"""
import sys
from configHandler import idleConf
import macosxSupport

menudefs = [
 # underscore prefixes character to underscore
 ('file', [
   ('_New Window', '<<open-new-window>>'),
   ('_Open...', '<<open-window-from-file>>'),
   ('Open _Module...', '<<open-module>>'),
   ('Class _Browser', '<<open-class-browser>>'),
   ('_Path Browser', '<<open-path-browser>>'),
   None,
   ('_Save', '<<save-window>>'),
   ('Save _As...', '<<save-window-as-file>>'),
   ('Save Cop_y As...', '<<save-copy-of-window-as-file>>'),
   None,
   ('Prin_t Window', '<<print-window>>'),
   None,
   ('_Close', '<<close-window>>'),
   ('E_xit', '<<close-all-windows>>'),
  ]),
 ('edit', [
   ('_Undo', '<<undo>>'),
   ('_Redo', '<<redo>>'),
   None,
   ('Cu_t', '<<cut>>'),
   ('_Copy', '<<copy>>'),
   ('_Paste', '<<paste>>'),
   ('Select _All', '<<select-all>>'),
   None,
   ('_Find...', '<<find>>'),
   ('Find A_gain', '<<find-again>>'),
   ('Find _Selection', '<<find-selection>>'),
   ('Find in Files...', '<<find-in-files>>'),
   ('R_eplace...', '<<replace>>'),
   ('Go to _Line', '<<goto-line>>'),
  ]),
('format', [
   ('_Indent Region', '<<indent-region>>'),
   ('_Dedent Region', '<<dedent-region>>'),
   ('Comment _Out Region', '<<comment-region>>'),
   ('U_ncomment Region', '<<uncomment-region>>'),
   ('Tabify Region', '<<tabify-region>>'),
   ('Untabify Region', '<<untabify-region>>'),
   ('Toggle Tabs', '<<toggle-tabs>>'),
   ('New Indent Width', '<<change-indentwidth>>'),
   ]),
 ('run', [
   ('Python Shell', '<<open-python-shell>>'),
   ]),
 ('shell', [
   ('_View Last Restart', '<<view-restart>>'),
   ('_Restart Shell', '<<restart-shell>>'),
   ]),
 ('debug', [
   ('_Go to File/Line', '<<goto-file-line>>'),
   ('!_Debugger', '<<toggle-debugger>>'),
   ('_Stack Viewer', '<<open-stack-viewer>>'),
   ('!_Auto-open Stack Viewer', '<<toggle-jit-stack-viewer>>'),
   ]),
 ('options', [
   ('_Configure IDLE...', '<<open-config-dialog>>'),
   None,
   ]),
 ('help', [
   ('_About IDLE', '<<about-idle>>'),
   None,
   ('_IDLE Help', '<<help>>'),
   ('Python _Docs', '<<python-docs>>'),
   ]),
]

if macosxSupport.runningAsOSXApp():
    # Running as a proper MacOS application bundle. This block restructures
    # the menus a little to make them conform better to the HIG.

    quitItem = menudefs[0][1][-1]
    closeItem = menudefs[0][1][-2]

    # Remove the last 3 items of the file menu: a separator, close window and
    # quit. Close window will be reinserted just above the save item, where
    # it should be according to the HIG. Quit is in the application menu.
    del menudefs[0][1][-3:]
    menudefs[0][1].insert(6, closeItem)

    # Remove the 'About' entry from the help menu, it is in the application
    # menu
    del menudefs[-1][1][0:2]

    menudefs.insert(0,
            ('application', [
                ('About IDLE', '<<about-idle>>'),
                None,
                ('_Preferences....', '<<open-config-dialog>>'),
            ]))


default_keydefs = idleConf.GetCurrentKeySet()

del sys

from Tkinter import *
import SearchEngine
from SearchDialogBase import SearchDialogBase

def replace(text):
    root = text._root()
    engine = SearchEngine.get(root)
    if not hasattr(engine, "_replacedialog"):
        engine._replacedialog = ReplaceDialog(root, engine)
    dialog = engine._replacedialog
    dialog.open(text)

class ReplaceDialog(SearchDialogBase):

    title = "Replace Dialog"
    icon = "Replace"

    def __init__(self, root, engine):
        SearchDialogBase.__init__(self, root, engine)
        self.replvar = StringVar(root)

    def open(self, text):
        SearchDialogBase.open(self, text)
        try:
            first = text.index("sel.first")
        except TclError:
            first = None
        try:
            last = text.index("sel.last")
        except TclError:
            last = None
        first = first or text.index("insert")
        last = last or first
        self.show_hit(first, last)
        self.ok = 1

    def create_entries(self):
        SearchDialogBase.create_entries(self)
        self.replent = self.make_entry("Replace with:", self.replvar)

    def create_command_buttons(self):
        SearchDialogBase.create_command_buttons(self)
        self.make_button("Find", self.find_it)
        self.make_button("Replace", self.replace_it)
        self.make_button("Replace+Find", self.default_command, 1)
        self.make_button("Replace All", self.replace_all)

    def find_it(self, event=None):
        self.do_find(0)

    def replace_it(self, event=None):
        if self.do_find(self.ok):
            self.do_replace()

    def default_command(self, event=None):
        if self.do_find(self.ok):
            self.do_replace()
            self.do_find(0)

    def replace_all(self, event=None):
        prog = self.engine.getprog()
        if not prog:
            return
        repl = self.replvar.get()
        text = self.text
        res = self.engine.search_text(text, prog)
        if not res:
            text.bell()
            return
        text.tag_remove("sel", "1.0", "end")
        text.tag_remove("hit", "1.0", "end")
        line = res[0]
        col = res[1].start()
        if self.engine.iswrap():
            line = 1
            col = 0
        ok = 1
        first = last = None
        # XXX ought to replace circular instead of top-to-bottom when wrapping
        text.undo_block_start()
        while 1:
            res = self.engine.search_forward(text, prog, line, col, 0, ok)
            if not res:
                break
            line, m = res
            chars = text.get("%d.0" % line, "%d.0" % (line+1))
            orig = m.group()
            new = m.expand(repl)
            i, j = m.span()
            first = "%d.%d" % (line, i)
            last = "%d.%d" % (line, j)
            if new == orig:
                text.mark_set("insert", last)
            else:
                text.mark_set("insert", first)
                if first != last:
                    text.delete(first, last)
                if new:
                    text.insert(first, new)
            col = i + len(new)
            ok = 0
        text.undo_block_stop()
        if first and last:
            self.show_hit(first, last)
        self.close()

    def do_find(self, ok=0):
        if not self.engine.getprog():
            return False
        text = self.text
        res = self.engine.search_text(text, None, ok)
        if not res:
            text.bell()
            return False
        line, m = res
        i, j = m.span()
        first = "%d.%d" % (line, i)
        last = "%d.%d" % (line, j)
        self.show_hit(first, last)
        self.ok = 1
        return True

    def do_replace(self):
        prog = self.engine.getprog()
        if not prog:
            return False
        text = self.text
        try:
            first = pos = text.index("sel.first")
            last = text.index("sel.last")
        except TclError:
            pos = None
        if not pos:
            first = last = pos = text.index("insert")
        line, col = SearchEngine.get_line_col(pos)
        chars = text.get("%d.0" % line, "%d.0" % (line+1))
        m = prog.match(chars, col)
        if not prog:
            return False
        new = m.expand(self.replvar.get())
        text.mark_set("insert", first)
        text.undo_block_start()
        if m.group():
            text.delete(first, last)
        if new:
            text.insert(first, new)
        text.undo_block_stop()
        self.show_hit(first, text.index("insert"))
        self.ok = 0
        return True

    def show_hit(self, first, last):
        text = self.text
        text.mark_set("insert", first)
        text.tag_remove("sel", "1.0", "end")
        text.tag_add("sel", first, last)
        text.tag_remove("hit", "1.0", "end")
        if first == last:
            text.tag_add("hit", first)
        else:
            text.tag_add("hit", first, last)
        text.see("insert")
        text.update_idletasks()

    def close(self, event=None):
        SearchDialogBase.close(self, event)
        self.text.tag_remove("hit", "1.0", "end")

"""ParenMatch -- An IDLE extension for parenthesis matching.

When you hit a right paren, the cursor should move briefly to the left
paren.  Paren here is used generically; the matching applies to
parentheses, square brackets, and curly braces.
"""

from HyperParser import HyperParser
from configHandler import idleConf

_openers = {')':'(',']':'[','}':'{'}
CHECK_DELAY = 100 # miliseconds

class ParenMatch:
    """Highlight matching parentheses

    There are three supported style of paren matching, based loosely
    on the Emacs options.  The style is select based on the
    HILITE_STYLE attribute; it can be changed used the set_style
    method.

    The supported styles are:

    default -- When a right paren is typed, highlight the matching
        left paren for 1/2 sec.

    expression -- When a right paren is typed, highlight the entire
        expression from the left paren to the right paren.

    TODO:
        - extend IDLE with configuration dialog to change options
        - implement rest of Emacs highlight styles (see below)
        - print mismatch warning in IDLE status window

    Note: In Emacs, there are several styles of highlight where the
    matching paren is highlighted whenever the cursor is immediately
    to the right of a right paren.  I don't know how to do that in Tk,
    so I haven't bothered.
    """
    menudefs = [
        ('edit', [
            ("Show surrounding parens", "<<flash-paren>>"),
        ])
    ]
    STYLE = idleConf.GetOption('extensions','ParenMatch','style',
            default='expression')
    FLASH_DELAY = idleConf.GetOption('extensions','ParenMatch','flash-delay',
            type='int',default=500)
    HILITE_CONFIG = idleConf.GetHighlight(idleConf.CurrentTheme(),'hilite')
    BELL = idleConf.GetOption('extensions','ParenMatch','bell',
            type='bool',default=1)

    RESTORE_VIRTUAL_EVENT_NAME = "<<parenmatch-check-restore>>"
    # We want the restore event be called before the usual return and
    # backspace events.
    RESTORE_SEQUENCES = ("<KeyPress>", "<ButtonPress>",
                         "<Key-Return>", "<Key-BackSpace>")

    def __init__(self, editwin):
        self.editwin = editwin
        self.text = editwin.text
        # Bind the check-restore event to the function restore_event,
        # so that we can then use activate_restore (which calls event_add)
        # and deactivate_restore (which calls event_delete).
        editwin.text.bind(self.RESTORE_VIRTUAL_EVENT_NAME,
                          self.restore_event)
        self.counter = 0
        self.is_restore_active = 0
        self.set_style(self.STYLE)

    def activate_restore(self):
        if not self.is_restore_active:
            for seq in self.RESTORE_SEQUENCES:
                self.text.event_add(self.RESTORE_VIRTUAL_EVENT_NAME, seq)
            self.is_restore_active = True

    def deactivate_restore(self):
        if self.is_restore_active:
            for seq in self.RESTORE_SEQUENCES:
                self.text.event_delete(self.RESTORE_VIRTUAL_EVENT_NAME, seq)
            self.is_restore_active = False

    def set_style(self, style):
        self.STYLE = style
        if style == "default":
            self.create_tag = self.create_tag_default
            self.set_timeout = self.set_timeout_last
        elif style == "expression":
            self.create_tag = self.create_tag_expression
            self.set_timeout = self.set_timeout_none

    def flash_paren_event(self, event):
        indices = HyperParser(self.editwin, "insert").get_surrounding_brackets()
        if indices is None:
            self.warn_mismatched()
            return
        self.activate_restore()
        self.create_tag(indices)
        self.set_timeout_last()

    def paren_closed_event(self, event):
        # If it was a shortcut and not really a closing paren, quit.
        closer = self.text.get("insert-1c")
        if closer not in _openers:
            return
        hp = HyperParser(self.editwin, "insert-1c")
        if not hp.is_in_code():
            return
        indices = hp.get_surrounding_brackets(_openers[closer], True)
        if indices is None:
            self.warn_mismatched()
            return
        self.activate_restore()
        self.create_tag(indices)
        self.set_timeout()

    def restore_event(self, event=None):
        self.text.tag_delete("paren")
        self.deactivate_restore()
        self.counter += 1   # disable the last timer, if there is one.

    def handle_restore_timer(self, timer_count):
        if timer_count == self.counter:
            self.restore_event()

    def warn_mismatched(self):
        if self.BELL:
            self.text.bell()

    # any one of the create_tag_XXX methods can be used depending on
    # the style

    def create_tag_default(self, indices):
        """Highlight the single paren that matches"""
        self.text.tag_add("paren", indices[0])
        self.text.tag_config("paren", self.HILITE_CONFIG)

    def create_tag_expression(self, indices):
        """Highlight the entire expression"""
        if self.text.get(indices[1]) in (')', ']', '}'):
            rightindex = indices[1]+"+1c"
        else:
            rightindex = indices[1]
        self.text.tag_add("paren", indices[0], rightindex)
        self.text.tag_config("paren", self.HILITE_CONFIG)

    # any one of the set_timeout_XXX methods can be used depending on
    # the style

    def set_timeout_none(self):
        """Highlight will remain until user input turns it off
        or the insert has moved"""
        # After CHECK_DELAY, call a function which disables the "paren" tag
        # if the event is for the most recent timer and the insert has changed,
        # or schedules another call for itself.
        self.counter += 1
        def callme(callme, self=self, c=self.counter,
                   index=self.text.index("insert")):
            if index != self.text.index("insert"):
                self.handle_restore_timer(c)
            else:
                self.editwin.text_frame.after(CHECK_DELAY, callme, callme)
        self.editwin.text_frame.after(CHECK_DELAY, callme, callme)

    def set_timeout_last(self):
        """The last highlight created will be removed after .5 sec"""
        # associate a counter with an event; only disable the "paren"
        # tag if the event is for the most recent timer.
        self.counter += 1
        self.editwin.text_frame.after(self.FLASH_DELAY,
                                      lambda self=self, c=self.counter: \
                                      self.handle_restore_timer(c))

"""
Dialog for building Tkinter accelerator key bindings
"""
from tkinter import *
import tkinter.messagebox as tkMessageBox
import string
from . import macosxSupport

class GetKeysDialog(Toplevel):
    def __init__(self,parent,title,action,currentKeySequences):
        """
        action - string, the name of the virtual event these keys will be
                 mapped to
        currentKeys - list, a list of all key sequence lists currently mapped
                 to virtual events, for overlap checking
        """
        Toplevel.__init__(self, parent)
        self.configure(borderwidth=5)
        self.resizable(height=FALSE,width=FALSE)
        self.title(title)
        self.transient(parent)
        self.grab_set()
        self.protocol("WM_DELETE_WINDOW", self.Cancel)
        self.parent = parent
        self.action=action
        self.currentKeySequences=currentKeySequences
        self.result=''
        self.keyString=StringVar(self)
        self.keyString.set('')
        self.SetModifiersForPlatform() # set self.modifiers, self.modifier_label
        self.modifier_vars = []
        for modifier in self.modifiers:
            variable = StringVar(self)
            variable.set('')
            self.modifier_vars.append(variable)
        self.advanced = False
        self.CreateWidgets()
        self.LoadFinalKeyList()
        self.withdraw() #hide while setting geometry
        self.update_idletasks()
        self.geometry("+%d+%d" %
            ((parent.winfo_rootx()+((parent.winfo_width()/2)
                -(self.winfo_reqwidth()/2)),
              parent.winfo_rooty()+((parent.winfo_height()/2)
                -(self.winfo_reqheight()/2)) )) ) #centre dialog over parent
        self.deiconify() #geometry set, unhide
        self.wait_window()

    def CreateWidgets(self):
        frameMain = Frame(self,borderwidth=2,relief=SUNKEN)
        frameMain.pack(side=TOP,expand=TRUE,fill=BOTH)
        frameButtons=Frame(self)
        frameButtons.pack(side=BOTTOM,fill=X)
        self.buttonOK = Button(frameButtons,text='OK',
                width=8,command=self.OK)
        self.buttonOK.grid(row=0,column=0,padx=5,pady=5)
        self.buttonCancel = Button(frameButtons,text='Cancel',
                width=8,command=self.Cancel)
        self.buttonCancel.grid(row=0,column=1,padx=5,pady=5)
        self.frameKeySeqBasic = Frame(frameMain)
        self.frameKeySeqAdvanced = Frame(frameMain)
        self.frameControlsBasic = Frame(frameMain)
        self.frameHelpAdvanced = Frame(frameMain)
        self.frameKeySeqAdvanced.grid(row=0,column=0,sticky=NSEW,padx=5,pady=5)
        self.frameKeySeqBasic.grid(row=0,column=0,sticky=NSEW,padx=5,pady=5)
        self.frameKeySeqBasic.lift()
        self.frameHelpAdvanced.grid(row=1,column=0,sticky=NSEW,padx=5)
        self.frameControlsBasic.grid(row=1,column=0,sticky=NSEW,padx=5)
        self.frameControlsBasic.lift()
        self.buttonLevel = Button(frameMain,command=self.ToggleLevel,
                text='Advanced Key Binding Entry >>')
        self.buttonLevel.grid(row=2,column=0,stick=EW,padx=5,pady=5)
        labelTitleBasic = Label(self.frameKeySeqBasic,
                text="New keys for  '"+self.action+"' :")
        labelTitleBasic.pack(anchor=W)
        labelKeysBasic = Label(self.frameKeySeqBasic,justify=LEFT,
                textvariable=self.keyString,relief=GROOVE,borderwidth=2)
        labelKeysBasic.pack(ipadx=5,ipady=5,fill=X)
        self.modifier_checkbuttons = {}
        column = 0
        for modifier, variable in zip(self.modifiers, self.modifier_vars):
            label = self.modifier_label.get(modifier, modifier)
            check=Checkbutton(self.frameControlsBasic,
                command=self.BuildKeyString,
                text=label,variable=variable,onvalue=modifier,offvalue='')
            check.grid(row=0,column=column,padx=2,sticky=W)
            self.modifier_checkbuttons[modifier] = check
            column += 1
        labelFnAdvice=Label(self.frameControlsBasic,justify=LEFT,
                            text=\
                            "Select the desired modifier keys\n"+
                            "above, and the final key from the\n"+
                            "list on the right.\n\n" +
                            "Use upper case Symbols when using\n" +
                            "the Shift modifier.  (Letters will be\n" +
                            "converted automatically.)")
        labelFnAdvice.grid(row=1,column=0,columnspan=4,padx=2,sticky=W)
        self.listKeysFinal=Listbox(self.frameControlsBasic,width=15,height=10,
                selectmode=SINGLE)
        self.listKeysFinal.bind('<ButtonRelease-1>',self.FinalKeySelected)
        self.listKeysFinal.grid(row=0,column=4,rowspan=4,sticky=NS)
        scrollKeysFinal=Scrollbar(self.frameControlsBasic,orient=VERTICAL,
                command=self.listKeysFinal.yview)
        self.listKeysFinal.config(yscrollcommand=scrollKeysFinal.set)
        scrollKeysFinal.grid(row=0,column=5,rowspan=4,sticky=NS)
        self.buttonClear=Button(self.frameControlsBasic,
                text='Clear Keys',command=self.ClearKeySeq)
        self.buttonClear.grid(row=2,column=0,columnspan=4)
        labelTitleAdvanced = Label(self.frameKeySeqAdvanced,justify=LEFT,
                text="Enter new binding(s) for  '"+self.action+"' :\n"+
                "(These bindings will not be checked for validity!)")
        labelTitleAdvanced.pack(anchor=W)
        self.entryKeysAdvanced=Entry(self.frameKeySeqAdvanced,
                textvariable=self.keyString)
        self.entryKeysAdvanced.pack(fill=X)
        labelHelpAdvanced=Label(self.frameHelpAdvanced,justify=LEFT,
            text="Key bindings are specified using Tkinter keysyms as\n"+
                 "in these samples: <Control-f>, <Shift-F2>, <F12>,\n"
                 "<Control-space>, <Meta-less>, <Control-Alt-Shift-X>.\n"
                 "Upper case is used when the Shift modifier is present!\n\n" +
                 "'Emacs style' multi-keystroke bindings are specified as\n" +
                 "follows: <Control-x><Control-y>, where the first key\n" +
                 "is the 'do-nothing' keybinding.\n\n" +
                 "Multiple separate bindings for one action should be\n"+
                 "separated by a space, eg., <Alt-v> <Meta-v>." )
        labelHelpAdvanced.grid(row=0,column=0,sticky=NSEW)

    def SetModifiersForPlatform(self):
        """Determine list of names of key modifiers for this platform.

        The names are used to build Tk bindings -- it doesn't matter if the
        keyboard has these keys, it matters if Tk understands them. The
        order is also important: key binding equality depends on it, so
        config-keys.def must use the same ordering.
        """
        import sys
        if macosxSupport.runningAsOSXApp():
            self.modifiers = ['Shift', 'Control', 'Option', 'Command']
        else:
            self.modifiers = ['Control', 'Alt', 'Shift']
        self.modifier_label = {'Control': 'Ctrl'} # short name

    def ToggleLevel(self):
        if  self.buttonLevel.cget('text')[:8]=='Advanced':
            self.ClearKeySeq()
            self.buttonLevel.config(text='<< Basic Key Binding Entry')
            self.frameKeySeqAdvanced.lift()
            self.frameHelpAdvanced.lift()
            self.entryKeysAdvanced.focus_set()
            self.advanced = True
        else:
            self.ClearKeySeq()
            self.buttonLevel.config(text='Advanced Key Binding Entry >>')
            self.frameKeySeqBasic.lift()
            self.frameControlsBasic.lift()
            self.advanced = False

    def FinalKeySelected(self,event):
        self.BuildKeyString()

    def BuildKeyString(self):
        keyList = modifiers = self.GetModifiers()
        finalKey = self.listKeysFinal.get(ANCHOR)
        if finalKey:
            finalKey = self.TranslateKey(finalKey, modifiers)
            keyList.append(finalKey)
        self.keyString.set('<' + '-'.join(keyList) + '>')

    def GetModifiers(self):
        modList = [variable.get() for variable in self.modifier_vars]
        return [mod for mod in modList if mod]

    def ClearKeySeq(self):
        self.listKeysFinal.select_clear(0,END)
        self.listKeysFinal.yview(MOVETO, '0.0')
        for variable in self.modifier_vars:
            variable.set('')
        self.keyString.set('')

    def LoadFinalKeyList(self):
        #these tuples are also available for use in validity checks
        self.functionKeys=('F1','F2','F2','F4','F5','F6','F7','F8','F9',
                'F10','F11','F12')
        self.alphanumKeys=tuple(string.ascii_lowercase+string.digits)
        self.punctuationKeys=tuple('~!@#%^&*()_-+={}[]|;:,.<>/?')
        self.whitespaceKeys=('Tab','Space','Return')
        self.editKeys=('BackSpace','Delete','Insert')
        self.moveKeys=('Home','End','Page Up','Page Down','Left Arrow',
                'Right Arrow','Up Arrow','Down Arrow')
        #make a tuple of most of the useful common 'final' keys
        keys=(self.alphanumKeys+self.punctuationKeys+self.functionKeys+
                self.whitespaceKeys+self.editKeys+self.moveKeys)
        self.listKeysFinal.insert(END, *keys)

    def TranslateKey(self, key, modifiers):
        "Translate from keycap symbol to the Tkinter keysym"
        translateDict = {'Space':'space',
                '~':'asciitilde','!':'exclam','@':'at','#':'numbersign',
                '%':'percent','^':'asciicircum','&':'ampersand','*':'asterisk',
                '(':'parenleft',')':'parenright','_':'underscore','-':'minus',
                '+':'plus','=':'equal','{':'braceleft','}':'braceright',
                '[':'bracketleft',']':'bracketright','|':'bar',';':'semicolon',
                ':':'colon',',':'comma','.':'period','<':'less','>':'greater',
                '/':'slash','?':'question','Page Up':'Prior','Page Down':'Next',
                'Left Arrow':'Left','Right Arrow':'Right','Up Arrow':'Up',
                'Down Arrow': 'Down', 'Tab':'Tab'}
        if key in translateDict:
            key = translateDict[key]
        if 'Shift' in modifiers and key in string.ascii_lowercase:
            key = key.upper()
        key = 'Key-' + key
        return key

    def OK(self, event=None):
        if self.advanced or self.KeysOK():  # doesn't check advanced string yet
            self.result=self.keyString.get()
            self.destroy()

    def Cancel(self, event=None):
        self.result=''
        self.destroy()

    def KeysOK(self):
        '''Validity check on user's 'basic' keybinding selection.

        Doesn't check the string produced by the advanced dialog because
        'modifiers' isn't set.

        '''
        keys = self.keyString.get()
        keys.strip()
        finalKey = self.listKeysFinal.get(ANCHOR)
        modifiers = self.GetModifiers()
        # create a key sequence list for overlap check:
        keySequence = keys.split()
        keysOK = False
        title = 'Key Sequence Error'
        if not keys:
            tkMessageBox.showerror(title=title, parent=self,
                                   message='No keys specified.')
        elif not keys.endswith('>'):
            tkMessageBox.showerror(title=title, parent=self,
                                   message='Missing the final Key')
        elif (not modifiers
              and finalKey not in self.functionKeys + self.moveKeys):
            tkMessageBox.showerror(title=title, parent=self,
                                   message='No modifier key(s) specified.')
        elif (modifiers == ['Shift']) \
                 and (finalKey not in
                      self.functionKeys + self.moveKeys + ('Tab', 'Space')):
            msg = 'The shift modifier by itself may not be used with'\
                  ' this key symbol.'
            tkMessageBox.showerror(title=title, parent=self, message=msg)
        elif keySequence in self.currentKeySequences:
            msg = 'This key combination is already in use.'
            tkMessageBox.showerror(title=title, parent=self, message=msg)
        else:
            keysOK = True
        return keysOK

if __name__ == '__main__':
    #test the dialog
    root=Tk()
    def run():
        keySeq=''
        dlg=GetKeysDialog(root,'Get Keys','find-again',[])
        print(dlg.result)
    Button(root,text='Dialog',command=run).pack()
    root.mainloop()

from visual import *
# Kadir Haldenbilen, February 2011

print ("Click to pause or restart.")

scene.autocenter = True
scene.width = 1024
scene.height = 768

mfrm = frame(axis=(0,0,1))                  # Motor Frame
rfrm = frame(frame=mfrm)                    # Rotor Frame

# Create contactor
# First, draw the outer circle
g1 = shapes.circle(radius=1.2)
ns=24
# We will have 24 contactor surfaces, 2 per each rotor wiring
for i in range(ns):
    # Second, subtract rectangular pieces to get a slice for each contact surface
    t = shapes.rectangle(pos=(1.2*cos(i*2*pi/ns),1.2*sin(i*2*pi/ns)),
                         width=2.1, height=0.05, rotate=i*2*pi/ns)
    g1 = g1 - t

g1 = g1 - shapes.circle(radius=0.5)     # Last, subtract rotor shaft
cl = 2.0
# Now, extrude to get "cylindrical" contactor surfaces
ge1 = extrusion(pos=[(0,0,0),(0,0,cl)], shape=g1, color=(1,0.5,0.3),
                material=materials.rough, frame=rfrm)

# Create contactor soldering tips, same as above
g2 = shapes.circle(radius=1.4)
ns=24
sphs = []
for i in range(ns):
    t = shapes.rectangle(pos=(1.2*cos(i*2*pi/ns),1.2*sin(i*2*pi/ns)),
                         width=2.1, height=0.2, rotate=i*2*pi/ns)
    g2 = g2 - t
    sldr = sphere(frame=rfrm, pos=(1.195*cos(i*2*pi/ns+pi/ns),1.195*sin(i*2*pi/ns+pi/ns),2.2),
                  radius=0.1, material=materials.shiny)     # add solders
    sphs.append(sldr)

g2 = g2 - shapes.circle(radius=0.6)     # Subtract a wider circle to enable soldering

# Finally extrude to get soldering surfaces
ge2 = extrusion(pos=[(0,0,2),(0,0,2.4)], shape=g2, color=(1,0.5,0.3),
                material=materials.rough, frame=rfrm)

# Add shaft insulator material
# Define a circular ring of thickness=0.05
sc = shapes.circle(radius=0.5, thickness=0.05)
# Extrude the ring to get a thin hollow cylinder insulator over the shaft
sce = extrusion(pos=[(0,0,-0.5),(0,0,9.5)], shape=sc, color=(1,0,0),
                material=materials.plastic, frame=rfrm)

# The Rotor Shaft, defined by a simple cylinder
shaft = cylinder(frame=rfrm, pos=(0,0,-1.5), axis=(0,0,12), radius=0.495,
                 material=materials.blazed)
# Add a piece of gear at one end of the shaft
# Use the gear shape to define the shape, note radius, addendum, dedendum sizes
gr = shapes.gear(n=9, radius=0.455, addendum=0.04, dedendum=0.06, fradius=0.01)
# Extrude the gear shape appending it to the shaft end
gre = extrusion(frame=rfrm, pos=[(0,0,-1.5),(0,0,-3)], shape=gr,
                material=materials.blazed)

# Define Rotor Core
# Normally the core should have been built of many very thin sheets
# For performance reasons, a single block is built
# First define the outer circle
g3 = shapes.circle(radius=3.0)

ns=12
# We will have 12 wiring sections on the rotor core
for i in range(ns):
    # First define the vertical channels
    t1 = shapes.rectangle(pos=(3*cos(i*2*pi/ns),3*sin(i*2*pi/ns)),
                         width=1.1, height=0.3, rotate=i*2*pi/ns)
    # Then define winding hollow as a trapezoidal area
    t2 = shapes.trapezoid(pos=(2.*cos(i*2*pi/ns),2.*sin(i*2*pi/ns)),
                         width=1.2, top=0.5, height=1.4, roundness=0.1,
                          rotate=i*2*pi/ns+pi/2, )
    g3 = g3 - t2 - t1   # From the circle, subtract wiring areas

# Obtain rotor core profile
g3 = g3 - shapes.circle(radius=0.495)       # Subtract rotor shaft area

# Define rotor core body sizes
ps = 5.5
dlt = 0.05
thk = 5.04
nl = 1      # nl = 100
cf = frame(frame=rfrm, pos=(0,0, thk/2.+cl/2.0))
for i in range(nl):
    # Extrude rotor core profile to get the full core body
    ge3 = extrusion(pos=[(0,0,i*dlt),(0,0,i*dlt+thk)], shape=g3,
                    color=(0.7,0.7,0.705), twist=0.0, frame=cf)

# Do the core wire windings
# Here is a trick to build a saw-teeth profile, to represent many single windings
N = 20 # coils
vright = vector(.3,1.3)
r = mag(vright)/(2*N)
vright = norm(vright)
# S is the cross sectional profile of "winding block"
S = Polygon([(-.1,-.65), (0,-.65), (.3,.65), (-.1,.65)])
for n in range(N):
    right = vector(0,-.65)+(r+n*2*r)*vright
    # Add saw-teeth on the block to represent wires
    S += shapes.circle(pos=(right.x,right.y), radius=r, np=4)

# Define the winding path as a rounded rectangle
P = shapes.rectangle(width=.5, height=thk)
P += shapes.circle(pos=(0,-thk/2), radius=.25, np=10)
P += shapes.circle(pos=(0,+thk/2), radius=.25, np=10)
wrfs = []
for i in range(ns):
    # We need a separate frame for individiual winding section
    wrf = frame(frame=cf, pos=(0,2,thk/2.))
    wrfs.append(wrf)
    # Extrude the winding block per winding path
    wre = extrusion(frame=wrf, pos=P, shape=S,
                    color=(.7,.5,.15), material=materials.rough)
    # Make angular corrections to position on the rotor core
    wrf.rotate(axis=(0,0,1), angle=(i*2*pi/ns+pi/ns), origin=(0,0,0))

# Connect contactor surfaces to windings with cables
for i in range(ns):
    # Connect every other contactor to one end of windings (somewhere!)
    curve(frame=rfrm, pos=[sphs[i*2].pos, cf.pos+wrfs[i].pos], radius=0.05,
          color=(0.4,0.2,0))
    # Connect remaining ones to the other end of windings (somewhere!)
    curve(frame=rfrm, pos=[sphs[i*2+1].pos, cf.pos+wrfs[i].pos], radius=0.05,
        color=(0,0,1))

# Create Brushes
# From a rectangular cross section, subtract rotor contactor circle, leaving us two
# brushes on each sides of the contactor, with circular profile
br = shapes.rectangle(width=5, height=0.4) - shapes.circle(radius=1.21)
# Extrude the brush profile to get the brushes in 3D
bre = extrusion(frame=mfrm, pos=[(0,0,0.4),(0,0,1.6)], color=(0.1,0.1,0.15),
                material=materials.rough, shape=br)

# Create Brush Housings
# Define a rectangular frame, with a thickness of 0.1
bh = shapes.rectangle(width=1.3, height=0.5, thickness=0.1)
# Extrude the rectangular frame to obtain hollow boxes for each housing
bhe1 = extrusion(frame=mfrm, pos=[(1.4,0,1),(2.9,0,1)], shape=bh, color=(0.9,1,0.8),
                 material=materials.rough)
bhe2 = extrusion(frame=mfrm, pos=[(-1.4,0,1),(-2.9,0,1)], shape=bh, color=(0.9,1,0.8),
                 material=materials.rough)

# Place a screw on each housing to fix the power cables
# Create a screw head profile by subtracting a cross from a circle
scrh = shapes.circle(radius=1) - shapes.cross()
scrh.scale(0.15,0.15)
# Extrude a little to get the screw head
scrhw1 = extrusion(frame=mfrm, pos=[(2.7,0.2,1),(2.7,0.3,1)], shape=scrh, color=(1,1,0.8),
                 material=materials.rough)
scrhw2 = extrusion(frame=mfrm, pos=[(-2.7,0.2,1),(-2.7,0.3,1)], shape=scrh, color=(1,1,0.8),
                 material=materials.rough)

# Create the screw bodies
# Use a square to create the body with teeth
scrb = shapes.rectangle(scale=0.1)
# Extrude the square with twist parameter to get the teeth of the screw
scrbe1 = extrusion(frame=mfrm, pos=paths.line(start=(2.7,0.2,1), end=(2.7,-0.3,1),
                    np=20), shape=scrb, twist=0.4, color=(1,1,0.9),
                 material=materials.rough)
scrbe2 = extrusion(frame=mfrm, pos=paths.line(start=(-2.7,0.2,1), end=(-2.7,-0.3,1),
                    np=20), shape=scrb, twist=0.4, color=(1,1,0.9),
                 material=materials.rough)

# Place the brush system on a craddle
# Use a rectangular block, subtract rotor circle to allow space for the rotor
crdl = (shapes.rectangle(pos=(0,-0.9), width=5.8, height=1.4) -
        shapes.circle(radius=1.21) - shapes.circle(pos=(-2.2,-0.9), radius=0.1))
# Extrude the block to get the craddle
crdle = extrusion(frame=mfrm, pos=[(0,-0.05,0.2),(0,-0.05,1.8)],
                material=materials.plastic, shape=crdl)

# Connect power cables to the brushes
# Use simple curves to define cables
cbl1i = curve(frame=mfrm, pos=[scrhw1.pos[-2], scrhw1.pos[-2]- vector(-2,0,0)],
            radius=0.03, color=ge1.color)
cbl1o = curve(frame=mfrm, pos=[scrhw1.pos[-2], scrhw1.pos[-2]- vector(-1.5,0,0)],
            radius=0.05, color=(0,0,1))

cbl2i = curve(frame=mfrm, pos=[scrhw2.pos[-2], scrhw2.pos[-2]+ vector(-0.5,0,0)],
            radius=0.03, color=ge1.color)
cbl2i.append(pos=cbl2i.pos[-1]+(0,-2,0))
cbl2i.append(pos=cbl2i.pos[-1]+(7,0,0))
cbl2o = curve(frame=mfrm, pos=cbl2i.pos, radius=0.05, color=(1,0,0))
cbl2o.pos[-1]-= (0.5,0,0)

# Add ball-bearings at both ends of the shaft
# First create the cross section of the bearing
# From a rectangular shape subtract the circles for the balls, and then
# subtract another rectangle for the shaft
br = shapes.rectangle(width=0.54, height=0.75) - shapes.circle(radius=0.25) - shapes.rectangle(width=0.30,height=0.76)
b1f = frame(frame=rfrm, pos=(0,0,-0.75))
# Extrude the cross section along a full circle to get a ball bearing
br1 = extrusion(frame=b1f, pos=paths.circle(radius=0.75), shape=br,
                material=materials.blazed)
b1f.rotate(angle=pi/2)
b2f = frame(frame=rfrm, pos=(0,0,10.))
br2 = extrusion(frame=b2f, pos=paths.circle(radius=0.75), shape=br,
                material=materials.blazed)
b2f.rotate(angle=pi/2)

# Do not forget to add the balls
bbrs1 = []
bbrs2 = []
for i in range(7):
    bbrs1.append(sphere(frame=rfrm, pos=(0.75*cos(i*2*pi/7.0), 0.75*sin(i*2*pi/7.0), -0.75),
                        radius=0.25, material=materials.rough))
    bbrs2.append(sphere(frame=rfrm, pos=(0.75*cos(i*2*pi/7.0), 0.75*sin(i*2*pi/7.0), 10.),
                        radius=0.25, material=materials.rough))

# Define the stator core - again defined as a single block
# We did not include all stator parts here for better visualisation
# Use a rounded rectangle for the stator base.
# Subtract a large circle in the middle to allow for the rotor
# Subtract circular holes to place the stator windings
# Subtract some more holes for fixing the stator core on the motor body
stb = (shapes.rectangle(pos=(0,-2.25), width=6, height=3, roundness=0.5) -
       shapes.rectangle(width=8.5, height=4.6) - shapes.circle(radius=3.1) -
       shapes.circle(pos=(2.6,-2.1), radius=0.3) -
       shapes.circle(pos=(2.0,-3.4), radius=0.15) -
       shapes.circle(pos=(-2.6,-2.1), radius=0.3) -
       shapes.circle(pos=(-2.0,-3.4), radius=0.15))

# Extrude the stator profile to get the stator core
stbe = extrusion(frame=mfrm, pos=[(0,0,thk/2.+cl/2.0), (0,0,thk/2.+cl/2.0+thk)], shape=stb)


# Here is a complex path definition for stator winding, which is not planar.
# The path is made up of an arc in YZ + line in ZX + arc in ZY + line in XZ

tp = []
# Define the arc path
pp = shapes.arc(angle1=-pi/3.5, angle2=pi/3.5, radius=3.4)
cp = pp.contour(0)[0:len(pp.contour(0))//2]
for p in cp:
    tp.append((0,-p[0],p[1]))
# Create the reverse arc path
tmp = []
tmp.extend(tp)
tmp.reverse()
tp.append(vector(tp[-1])-vector(thk+0.7,0,0))
for p in tmp:
    # We are in 3D, not in 2D
    tp.append(vector(p)-vector(thk+0.7,0,0))
tp.append(vector(tp[-1])+vector(thk+0.7,0,0))
# Just a simple winding cross section for the whole of the stator winding
sts = shapes.circle(radius=0.3)
sfrm = frame(frame=mfrm, pos=(0,0,thk+cl*2-0.15))
# Extrude the winding profile along the complex stator path
stse = extrusion(frame=sfrm, pos=tp,
                 shape=sts, color=(1,0,0))
sfrm.rotate(axis=(0,1,0), angle=-pi/2)

# Create the motor cover as a rotational extrusion along the mootor
# Add two rounded rectangles which will cover all the rotor and stator.
# Leave the tips of shaft outside the cover
cvr = (shapes.rectangle(width=3, height=11.4, roundness=0.1) +
       shapes.rectangle(width=9, height=10, roundness=0.1))
cvrc = Polygon(cvr.contour(0))
# Create a scaled down copy of the same profile
cvrc.scale(0.95,0.95)
# Subtract the smaller one to get a thin "skin" to represent the cover
# We do not need the full profile, take out the lower half, allowing some
# space for the ball bearings
cvr = (cvr - cvrc -  shapes.rectangle(width=1.8, height=12) -
       shapes.rectangle(pos=(-4,0), width=7, height=12))
cfrm = frame(pos=(-4.6,0,0))
# Rotate the profile around the shaft along an arc to get the cover.
# Do not use full circle, so that we can see the inside of the motor
cvre = extrusion(frame=cfrm, pos=paths.arc(angle1=-pi/4, angle2=pi, radius=0.1),
                 shape=cvr, color=(0,0.6,0.3), material=materials.rough)
# Place the cover correctly
cfrm.rotate(angle=pi/2)
cfrm.rotate(axis=(0,1,0), angle=pi/2)
cfrm.rotate(axis=(1,0,0), angle=-pi/2)

# Connect power cables
angl = pi/400
run = True
# Turn on the motor
while True:
    rate(100)
    if run:
        rfrm.rotate(angle=angl, axis=(0,0,1))
    if scene.mouse.events:
        m = scene.mouse.getevent()
        if m.click == 'left':
            run = not run
            

## 3-D tictactoe   Ruth Chabay 2000/05

from visual import *
from tictacdat import *

scene.width=600
scene.height=600
scene.title="3D TicTacToe: 4 in a row"

# draw board
gray = (1,1,1)
yo=2.       
base=grid (n=4, ds=1, gridcolor=gray)
base.pos=base.pos+vector(-0.5, -2., -0.5)

second=grid(n=4, ds=1, gridcolor=gray)
second.pos=second.pos+vector(-0.5, -1., -0.5)
third=grid(n=4, ds=1, gridcolor=gray)
third.pos=third.pos+vector(-0.5, 0, -0.5)
top=grid(n=4, ds=1, gridcolor=gray)
top.pos=top.pos+vector(-0.5, 1., -0.5)

# get list of winning combinations
wins=win()

print("****************************************")
print("Drag ball up starting from bottom grid.")
print("Release to deposit ball in a square.")
print("****************************************")
print("  ")

# make sliders
bars={}
balls={}
form = '{0[0]} {0[1]} {0[2]}'
for x in arange(-2, 2,1):
    for z in arange(-2, 2,1):
        cyl=cylinder(pos=(x,-2,z), axis=(0,3,0), radius=0.05, visible=0)
        loc = (int(round(x)),int(round(-yo)),int(round(z)))
        bars[form.format(loc)]=cyl

# set reasonable viewing angle
scene.center=(-.5,-.5,-.5)
scene.forward = (0,-0.05,-1)
scene.autoscale=0

nballs=0
visbar=None
red=(1,0,0)
blue=(.3,.3,1)
bcolor=red
point=None
won=None

while len(balls) < 4*4*4:
    while True:
        rate(100)
        if scene.mouse.events:
            p = scene.mouse.getevent()
            if p.drag:
                point=p.project(normal=vector(0,1,0),d=-yo)   # 'None' if not in plane
                break

    if point == None: continue
    
    # chose valid square
    point=(int(round(point[0])), int(round(point[1])), int(round(point[2])))
    if not (visbar==None): visbar.visible=0
    lookup = form.format(point)
    if not (lookup in bars):
        continue
    visbar=bars[lookup]
    visbar.visible=1   
    nballs=nballs+1
    b=sphere(pos=point, radius=0.3, color=bcolor)
    while not scene.mouse.events:
        rate(100)
        y=scene.mouse.pos.y
        if y > 1.: y=1.
        if y < -yo: y=-yo
        b.y=y
    scene.mouse.getevent()  # get rid of drop depositing ball
    bpoint=(int(round(b.x)), int(round(b.y)), int(round(b.z)))
    lookup = form.format(bpoint)
    if not(form.format(lookup) in balls): # not already a ball there
        b.pos=bpoint
        balls[lookup]=b
        if bcolor==red: bcolor=blue
        else:bcolor=red
    else:               ## already a ball there, so abort
        b.visible=0  
    visbar.visible=0
    visbar=None
    # check for four in a row
    for a in wins:
        a0=a[0] in balls
        a1=a[1] in balls
        a2=a[2] in balls
        a3=a[3] in balls
        if a0 and a1 and a2 and a3:
            ccolor=balls[a[0]].color
            if balls[a[1]].color==balls[a[2]].color==balls[a[3]].color==ccolor:
                won=ccolor
                print(" ")
                if ccolor==red:
                    print("***********")
                    print(" Red wins!")
                    print("***********")
                else:
                    print("***********")
                    print(" Blue wins!")
                    print("***********")
                for flash in range(5):
                    sleep(0.1)
                    balls[a[0]].color=(1,1,1)
                    balls[a[1]].color=(1,1,1)
                    balls[a[2]].color=(1,1,1)
                    balls[a[3]].color=(1,1,1)
                    sleep(0.1)
                    balls[a[0]].color=ccolor
                    balls[a[1]].color=ccolor
                    balls[a[2]].color=ccolor
                    balls[a[3]].color=ccolor
                    rate(10)
                break
    if not (won==None):
        break

print("game over")

from reding.managers import ObjectSubjectsManager, SubjectObjectsManager, ObjectsManager
from reding.settings import KEY_CONFIG
from reding.settings import PAGINATION_DEFAULT_OFFSET as OFFSET
from reding.settings import PAGINATION_DEFAULT_SIZE as SIZE

from flask.ext.restful import reqparse, fields, marshal_with, abort
from flask.ext import restful

from time import time
from six import text_type


def get_user_object_reply(object_id, user_id, vote, when, review):
    return {
        'object_id': object_id,
        'user_id': user_id,
        'vote': vote,
        'when': when,
        'review': review
    }


object_resource_fields = {
    'votes_no': fields.Integer,
    'amount': fields.Integer,
    'average': fields.Float,
    'object_id': fields.String,
}

user_object_resource_fields = {
    'vote': fields.Integer,
    'review': fields.Raw,
    'object_id': fields.String,
    'user_id': fields.String,
    'when': fields.DateTime
}


class RedingResource(restful.Resource):
    parser_cls = reqparse.RequestParser

    def __init__(self):
        super(RedingResource, self).__init__()
        self.parser = self.parser_cls()
        self.configure()

    def configure(self):
        for key in KEY_CONFIG:
            self.parser.add_argument(key, type=str)


class VotedListResource(RedingResource):
    def configure(self):
        super(VotedListResource, self).configure()
        self.parser.add_argument('object_id', type=str, action='append')
        self.parser.add_argument('sort', type=str, default='+')
        self.parser.add_argument('offset', type=int, default=OFFSET)
        self.parser.add_argument('size', type=int, default=SIZE)

    @marshal_with(object_resource_fields)
    def get(self):
        args = self.parser.parse_args()

        amounts = ObjectsManager(**args).scoredrange(
            offset=args['offset'],
            size=args['size'],
            reverse=args['sort'] == '-',
        )

        reply = []
        osmanager = ObjectSubjectsManager(**args)
        for object_id, amount in amounts:
            votes_no = osmanager.count(object_id=object_id)
            if votes_no:  # skipping objects with no votes
                reply.append(
                    dict(
                        votes_no=votes_no,
                        average=amount / votes_no,
                        amount=amount,
                        object_id=object_id,
                    )
                )
        return reply

    def post(self):
        """
        It sorts a list of 'object_id' with their amount of votes and returns it,
        objects not rated are at the end of the list
        :return: list
        """
        args = self.parser.parse_args()

        return ObjectsManager(**args).filtered(
            objects=args['object_id'],
            now=int(time()),
            reverse=args['sort'] == '-',
        )


class VotedSummaryResource(RedingResource):
    def configure(self):
        super(VotedSummaryResource, self).configure()
        self.parser.add_argument('vote', type=int, default=0)

    @marshal_with(object_resource_fields)
    def get(self, object_id):
        args = self.parser.parse_args()

        vote = args['vote']

        amount = ObjectsManager(**args).score(object_id=object_id) or 0

        votes_no = ObjectSubjectsManager(**args).count(
            object_id=object_id,
            min_vote=vote or '-inf',
            max_vote=vote or '+inf',
        )

        if not votes_no:
            average = 0
            amount = 0
        elif vote:
            average = vote
            amount = vote * votes_no
        else:
            average = amount / votes_no

        return (
            dict(
                votes_no=votes_no,
                average=average,
                amount=amount,
                object_id=object_id,
            )
        )


class VotingUserListResource(RedingResource):
    def configure(self):
        super(VotingUserListResource, self).configure()
        self.parser.add_argument('sort', type=str, default='+')
        self.parser.add_argument('offset', type=int, default=OFFSET)
        self.parser.add_argument('size', type=int, default=SIZE)
        self.parser.add_argument('vote', type=int, default=0)

    @marshal_with(user_object_resource_fields)
    def get(self, object_id):
        args = self.parser.parse_args()

        osmanager = ObjectSubjectsManager(**args)
        somanager = SubjectObjectsManager(**args)

        votes = osmanager.scoredrange(
            object_id=object_id,
            offset=args['offset'],
            size=args['size'],
            min_vote=args['vote'] or '-inf',
            max_vote=args['vote'] or '+inf',
            reverse=args['sort'] == '-',
        )

        if not votes:
            return []

        reviews = osmanager.reviews(object_id, *[user_id for user_id, _ in votes])

        reply = [
            get_user_object_reply(
                object_id=object_id,
                user_id=user_id,
                vote=vote,
                when=somanager.score(user_id=user_id, object_id=object_id),
                review=reviews[user_id],
            ) for user_id, vote in votes
        ]
        return reply


class UserSummaryResource(RedingResource):
    def configure(self):
        super(UserSummaryResource, self).configure()
        self.parser.add_argument('sort', type=str, default='+')
        self.parser.add_argument('offset', type=int, default=OFFSET)
        self.parser.add_argument('size', type=int, default=SIZE)

    @marshal_with(user_object_resource_fields)
    def get(self, user_id):
        args = self.parser.parse_args()

        osmanager = ObjectSubjectsManager(**args)
        somanager = SubjectObjectsManager(**args)

        votetimes = somanager.scoredrange(
            user_id=user_id,
            offset=args['offset'],
            size=args['size'],
            reverse=args['sort'] == '-',
        )
        reply = [
            get_user_object_reply(
                object_id=object_id,
                user_id=user_id,
                vote=osmanager.score(object_id=object_id, user_id=user_id),
                review=osmanager.review(object_id=object_id, user_id=user_id),
                when=when,
            ) for object_id, when in votetimes
        ]

        return reply


class VoteSummaryResource(RedingResource):
    @marshal_with(user_object_resource_fields)
    def get(self, object_id, user_id):
        args = self.parser.parse_args()

        osmanager = ObjectSubjectsManager(**args)
        somanager = SubjectObjectsManager(**args)

        vote = osmanager.score(object_id=object_id, user_id=user_id)
        when = somanager.score(user_id=user_id, object_id=object_id)

        if not (vote and when):
            message = "No vote on {object_id} by {user_id}.".format(
                object_id=object_id,
                user_id=user_id
            )
            abort(404, message=message)

        return get_user_object_reply(
            object_id=object_id,
            user_id=user_id,
            vote=vote,
            when=when,
            review=osmanager.review(object_id=object_id, user_id=user_id),
        )

    def post(self, object_id, user_id):
        return self.put(object_id, user_id)

    @marshal_with(user_object_resource_fields)
    def put(self, object_id, user_id):
        self.parser.add_argument('vote', type=int, required=True)
        self.parser.add_argument('review', type=text_type)
        args = self.parser.parse_args()

        osmanager = ObjectSubjectsManager(**args)
        somanager = SubjectObjectsManager(**args)

        self._perform_correction(object_id, user_id, args['vote'], args)
        osmanager.create(object_id=object_id, user_id=user_id, vote=args['vote'], review=args['review'])
        somanager.create(user_id=user_id, object_id=object_id, timestamp=time())

        return get_user_object_reply(
            object_id=object_id,
            user_id=user_id,
            vote=osmanager.score(object_id=object_id, user_id=user_id),
            when=somanager.score(user_id=user_id, object_id=object_id),
            review=osmanager.review(object_id=object_id, user_id=user_id),
        )

    def delete(self, object_id, user_id):
        args = self.parser.parse_args()

        self._perform_correction(object_id, user_id, 0, args)
        SubjectObjectsManager(**args).remove(user_id=user_id, object_id=object_id)
        ObjectSubjectsManager(**args).remove(object_id=object_id, user_id=user_id)

        return '', 204

    def _perform_correction(self, object_id, user_id, next_vote, args):
        prev_vote = ObjectSubjectsManager(**args).score(object_id=object_id, user_id=user_id) or 0
        correction = next_vote - prev_vote
        omanager = ObjectsManager(**args)
        omanager.incrby(object_id=object_id, delta=correction)
        amount = omanager.score(object_id=object_id)

        if amount == 0:
            omanager.remove(object_id=object_id)

__all__ = (
    'VotedSummaryResource',
    'VotedListResource',
    'VotingUserListResource',
    'VoteSummaryResource',
    'UserSummaryResource',
)

#!/usr/bin/env python
import pandas as pd
import datetime
import pdfplumber
from pdfplumber.utils import within_bbox, collate_chars
import sys, os

COLUMNS = [
    "month",
    "state",
    "permit",
    "permit_recheck",
    "handgun",
    "long_gun",
    "other",
    "multiple",
    "admin",
    "prepawn_handgun",
    "prepawn_long_gun",
    "prepawn_other",
    "redemption_handgun",
    "redemption_long_gun",
    "redemption_other",
    "returned_handgun",
    "returned_long_gun",
    "returned_other",
    "rentals_handgun",
    "rentals_long_gun",
    "private_sale_handgun",
    "private_sale_long_gun",
    "private_sale_other",
    "return_to_seller_handgun",
    "return_to_seller_long_gun",
    "return_to_seller_other",
    "totals"
]

# Where, in pixels from the top,
# the data starts and ends on each page.
DATA_START_TOP = 80
DATA_END_TOP = 474

def parse_field(text):
    if text == None: return None
    if text[0] in "0123456789":
        return int(text.replace(",", ""))
    return text

def parse_month(month_str):
    d = datetime.datetime.strptime(month_str, "%B - %Y")
    return d.strftime("%Y-%m")

def validate_data(checks):
    try:
        assert(len(checks) > 0)
    except:
        raise Exception("No data found.")

    ## Test vertical totals
    # [2:] because first two columns are month and state name
    for c in COLUMNS[2:]:
        v_total = checks[c].iloc[-1]
        v_colsum = checks[c].sum()
        try:
            assert(v_colsum == (v_total * 2))
        except:
            raise Exception("Vertical totals don't match on {0}.".format(c))

    ## Test horizontal totals
    h_colsums = checks.fillna(0).sum(axis=1)
    h_totals = checks["totals"].fillna(0)
    zipped = zip(checks["state"], h_colsums, h_totals)
    for state, h_colsum, h_total in zipped:
        try:
            assert(h_colsum == (h_total * 2))
        except:
            raise Exception("Horizontal totals don't match on {0}.".format(state))

def parse_value(x):
    if pd.isnull(x): return None
    return int(x.replace(",", ""))

def parse_page(page):

    month_crop = page.crop((0, 35, page.width, 65), strict=True)
    month_text = month_crop.extract_text(x_tolerance=2)
    month = parse_month(month_text)
    sys.stderr.write("\r" + month)

    table_crop = page.crop((0, 80, page.width, 485))
    _table = table_crop.extract_table(h="gutters",
        x_tolerance=5,
        y_tolerance=5,
        gutter_min_height=5)
    
    table = pd.DataFrame([ [ month ] + row for row in _table ])

    table.columns = COLUMNS
    table[table.columns[2:]] = table[table.columns[2:]].applymap(parse_value)

    table.loc[(table["state"] == "llinois"), "state"] = "Illinois"
    try: validate_data(table)
    except: raise Exception("Invalid data for " + month)

    return table

def parse_pdf(file_obj):
    pdf = pdfplumber.load(file_obj)

    checks = pd.concat(list(map(parse_page, pdf.pages)))\
        .reset_index(drop=True)

    return checks[checks["state"] != "Totals"]

if __name__ == "__main__":
    buf = getattr(sys.stdin, 'buffer', sys.stdin)
    checks = parse_pdf(buf)
    checks.to_csv(sys.stdout, index=False, float_format="%.0f")
    sys.stderr.write("\r\n")

from __future__ import unicode_literals
from six import PY2, PY3
from six.moves import filter, map, range

cal1 = """
BEGIN:VCALENDAR
METHOD:PUBLISH
VERSION:2.0
X-WR-CALNAME:plop
PRODID:-//Apple Inc.//Mac OS X 10.9//EN
X-APPLE-CALENDAR-COLOR:#882F00
X-WR-TIMEZONE:Europe/Brussels
CALSCALE:GREGORIAN
BEGIN:VTIMEZONE
TZID:Europe/Brussels
BEGIN:DAYLIGHT
TZOFFSETFROM:+0100
RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU
DTSTART:19810329T020000
TZNAME:UTC+2
TZOFFSETTO:+0200
END:DAYLIGHT
BEGIN:STANDARD
TZOFFSETFROM:+0200
RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU
DTSTART:19961027T030000
TZNAME:UTC+1
TZOFFSETTO:+0100
END:STANDARD
END:VTIMEZONE
BEGIN:VEVENT
CREATED:20131024T204716Z
UID:ABBF2903-092F-4202-98B6-F757437A5B28
DTEND;TZID=Europe/Brussels:20131029T113000
TRANSP:OPAQUE
SUMMARY:dfqsdfjqkshflqsjdfhqs fqsfhlqs dfkqsldfkqsdfqsfqsfqsfs
DTSTART;TZID=Europe/Brussels:20131029T103000
DTSTAMP:20131024T204741Z
SEQUENCE:3
DESCRIPTION:Lorem ipsum dolor sit amet, consectetur adipiscing elit. Sed
 vitae facilisis enim. Morbi blandit et lectus venenatis tristique. Donec
 sit amet egestas lacus. Donec ullamcorper, mi vitae congue dictum, quam
 dolor luctus augue, id cursus purus justo vel lorem. Ut feugiat enim ips
 um, quis porta nibh ultricies congue. Pellentesque nisl mi, molestie id
 sem vel, vehicula nullam.
END:VEVENT
END:VCALENDAR
"""

cal2 = """
BEGIN:VCALENDAR
BEGIN:VEVENT
DTEND;TZID=Europe/Berlin:20120608T212500
SUMMARY:Name
DTSTART;TZID=Europe/Berlin:20120608T202500
LOCATION:MUC
SEQUENCE:0
BEGIN:VALARM
TRIGGER:-PT1H
DESCRIPTION:Event reminder
ACTION:DISPLAY
END:VALARM
END:VEVENT
END:VCALENDAR
"""


cal3 = """
BEGIN:VCALENDAR
END:VCALENDAR
"""

cal4 = """BEGIN:VCALENDAR"""

cal5 = """
BEGIN:VCALENDAR
VERSION:2.0
END:VCALENDAR
"""

cal6 = """
DESCRIPTION:a
 b
"""

cal7 = """
BEGIN:VCALENDAR

END:VCALENDAR
"""

cal8 = """
BEGIN:VCALENDAR
\t
END:VCALENDAR
"""

cal9 = """

BEGIN:VCALENDAR
END:VCALENDAR
"""

cal10 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN
BEGIN:VEVENT
DTEND;TZID=Europe/Berlin:20120608T212500
DTSTAMP:20131024T204741Z
SUMMARY:Name
DTSTART;TZID=Europe/Berlin:20120608T202500
LOCATION:MUC
SEQUENCE:0
BEGIN:VALARM
TRIGGER:-PT1H
DESCRIPTION:Event reminder
ACTION:DISPLAY
END:VALARM
END:VEVENT
END:VCALENDAR
"""

cal11 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN
END:VCAL
"""

cal12 = """
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.8//EN
BEGIN:VEVENT
SUMMARY:Name
DTSTART;TZID=Europe/Berlin:20120608T202500
DURATION:P1DT1H
LOCATION:MUC
SEQUENCE:0
BEGIN:VALARM
TRIGGER:-PT1H
DESCRIPTION:Event reminder
ACTION:DISPLAY
END:VALARM
END:VEVENT
END:VCALENDAR
"""

cal13 = """
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN
BEGIN:VEVENT
SUMMARY:Name
DTSTART;TZID=Europe/Berlin:20120608T202500
DTEND;TZID=Europe/Berlin:20120608T212500
DURATION:P1DT1H
LOCATION:MUC
SEQUENCE:0
BEGIN:VALARM
TRIGGER:-PT1H
DESCRIPTION:Event reminder
ACTION:DISPLAY
END:VALARM
END:VEVENT
END:VCALENDAR
"""

cal14 = u"""
BEGIN:VCALENDAR
VERSION:2.0;42
PRODID:-//Apple Inc.//Mac OS X 10.9//EN
END:VCALENDAR
"""

cal15 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN

BEGIN:VEVENT
SUMMARY:Hello, \\n World\\; This is a backslash : \\\\ and another new \\N line
DTSTART;TZID=Europe/Berlin:20120608T202500
DTEND;TZID=Europe/Berlin:20120608T212500
LOCATION:MUC
END:VEVENT

END:VCALENDAR
"""

# Event with URL
cal16 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN

BEGIN:VEVENT
SUMMARY:Hello, \\n World\\; This is a backslash : \\\\ and another new \\N line
DTSTART;TZID=Europe/Berlin:20120608T202500
DTEND;TZID=Europe/Berlin:20120608T212500
LOCATION:MUC
URL:http://example.com/pub/calendars/jsmith/mytime.ics
END:VEVENT

END:VCALENDAR
"""

cal17 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN

BEGIN:VEVENT
SUMMARY:Some special \\; chars
DTSTART;TZID=Europe/Berlin:20130608T202501
DTEND;TZID=Europe/Berlin:20130608T212501
LOCATION:In\\, every text field
DESCRIPTION:Yes\\, all of them\\;
END:VEVENT
END:VCALENDAR
"""


# long event which is not all_day
cal18 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:ownCloud Calendar 0.7.3
X-WR-CALNAME:test
BEGIN:VEVENT
UID:3912dcd3d4
DTSTAMP:20151113T004809Z
CREATED:20151113T004809Z
LAST-MODIFIED:20151113T004809Z
SUMMARY:long event
DTSTART;TZID=Europe/Berlin:20151113T140000
DTEND;TZID=Europe/Berlin:20151124T080000
LOCATION:
DESCRIPTION:
CATEGORIES:
END:VEVENT
END:VCALENDAR
"""

# Event with TRANSP
cal19 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:-//Apple Inc.//Mac OS X 10.9//EN

BEGIN:VEVENT
SUMMARY:Hello, \\n World\\; This is a backslash : \\\\ and another new \\N line
DTSTART;TZID=Europe/Berlin:20120608T202500
DTEND;TZID=Europe/Berlin:20120608T212500
LOCATION:MUC
TRANSP:OPAQUE
END:VEVENT
END:VCALENDAR
"""

# 3 days all-day event including end date
cal20 = u"""
BEGIN:VCALENDAR
VERSION:2.0
PRODID:manually crafted from an ownCloud 8.0 ics
BEGIN:VEVENT
SUMMARY:3 days party
DTSTART;VALUE=DATE:20151114
DTEND;VALUE=DATE:20151116
END:VEVENT
END:VCALENDAR
"""

unfolded_cal2 = [
    'BEGIN:VCALENDAR',
    'BEGIN:VEVENT',
    'DTEND;TZID=Europe/Berlin:20120608T212500',
    'SUMMARY:Name',
    'DTSTART;TZID=Europe/Berlin:20120608T202500',
    'LOCATION:MUC',
    'SEQUENCE:0',
    'BEGIN:VALARM',
    'TRIGGER:-PT1H',
    'DESCRIPTION:Event reminder',
    'ACTION:DISPLAY',
    'END:VALARM',
    'END:VEVENT',
    'END:VCALENDAR',
]

unfolded_cal1 = [
    'BEGIN:VCALENDAR',
    'METHOD:PUBLISH',
    'VERSION:2.0',
    'X-WR-CALNAME:plop',
    'PRODID:-//Apple Inc.//Mac OS X 10.9//EN',
    'X-APPLE-CALENDAR-COLOR:#882F00',
    'X-WR-TIMEZONE:Europe/Brussels',
    'CALSCALE:GREGORIAN',
    'BEGIN:VTIMEZONE',
    'TZID:Europe/Brussels',
    'BEGIN:DAYLIGHT',
    'TZOFFSETFROM:+0100',
    'RRULE:FREQ=YEARLY;BYMONTH=3;BYDAY=-1SU',
    'DTSTART:19810329T020000',
    'TZNAME:UTC+2',
    'TZOFFSETTO:+0200',
    'END:DAYLIGHT',
    'BEGIN:STANDARD',
    'TZOFFSETFROM:+0200',
    'RRULE:FREQ=YEARLY;BYMONTH=10;BYDAY=-1SU',
    'DTSTART:19961027T030000',
    'TZNAME:UTC+1',
    'TZOFFSETTO:+0100',
    'END:STANDARD',
    'END:VTIMEZONE',
    'BEGIN:VEVENT',
    'CREATED:20131024T204716Z',
    'UID:ABBF2903-092F-4202-98B6-F757437A5B28',
    'DTEND;TZID=Europe/Brussels:20131029T113000',
    'TRANSP:OPAQUE',
    'SUMMARY:dfqsdfjqkshflqsjdfhqs fqsfhlqs dfkqsldfkqsdfqsfqsfqsfs',
    'DTSTART;TZID=Europe/Brussels:20131029T103000',
    'DTSTAMP:20131024T204741Z',
    'SEQUENCE:3',
    'DESCRIPTION:Lorem ipsum dolor sit amet, consectetur adipiscing elit. \
Sedvitae facilisis enim. Morbi blandit et lectus venenatis tristique. \
Donecsit amet egestas lacus. Donec ullamcorper, mi vitae congue dictum, \
quamdolor luctus augue, id cursus purus justo vel lorem. \
Ut feugiat enim ipsum, quis porta nibh ultricies congue. \
Pellentesque nisl mi, molestie idsem vel, vehicula nullam.',
    'END:VEVENT',
    'END:VCALENDAR',
]

unfolded_cal6 = ['DESCRIPTION:ab']

'''
Created on Dec 9, 2013

@author: adh
'''
import os
import shutil

import subprocess

from ..build_base import Build
from ..errors import BuildError

basedir = os.path.dirname(__file__)


class WindowsBuild(Build):
    PLATFORM = 'windows'
    LICENSE_FILE = 'COPYING.txt'

    def prune(self):
        super(self.__class__, self).prune()

        # prune everything in certfuzz/analysis except drillresults
        cfadir = os.path.join(self.export_path, 'certfuzz', 'analysis')
        p_to_del = []
        if os.path.exists(cfadir):
            p_to_del.extend([os.path.join(cfadir, x) for x in os.listdir(cfadir) if x != "drillresults"])

        # prune these dirs too
        for x in ['certfuzz/analyzers',
                  'certfuzz/campaign/config/bff_config.py',
                  'certfuzz/debuggers/crashwrangler.py',
                  'certfuzz/debuggers/gdb.py',
                  'certfuzz/debuggers/mr_crash_hash.py',
                  'certfuzz/debuggers/nulldebugger.py',
                  'certfuzz/debuggers/templates',
                  'build',
                  'installer',
                  'test',
                  ]:
            p_to_del.append(os.path.join(self.export_path, x))

        for p in p_to_del:
            if os.path.isfile(p):
                os.remove(p)
            elif os.path.isdir(p):
                shutil.rmtree(p)

            if os.path.exists(p):
                raise BuildError("Unable to remove %s" % p)

    def package(self):
        '''
        Builds a Windows Installer
        '''
        from .nsis import buildnsi

        # Copy files required by nsis
        for f in ['cert.ico', 'EnvVarUpdate.nsh', 'vmwarning.txt']:
            src = os.path.join(basedir, 'nsis', f)
            shutil.copy(src, self.build_dir)
#        shutil.copy('dist/windows/nsis/cert.ico', self.build_dir)
#        shutil.copy('dist/windows/nsis/EnvVarUpdate.nsh', self.build_dir)

        nsifile = os.path.join(self.build_dir, 'foe2.nsi')

        # generate the nsi file
        buildnsi.main(svn_rev=self.svn_rev, outfile=nsifile, build_dir=self.build_dir)
#        subprocess.call(args, stdout=open(nsifile, 'w'))

        # invoke makensis on the file we just made
        subprocess.call(['makensis', nsifile])

from .errors import AndroidEmulatorManagerError, AvdMgrError
from .errors import AvdClonerError, OrphanedProcessError
from .cloner import AvdCloner, clone_avd
from .orphan_catcher import OrphanCatcher
from ..api import AndroidEmulator, AndroidEmulatorError

'''
Created on Aug 1, 2013

@organization: cert.org
'''
import logging

logger = logging.getLogger(__name__)


class IterationBase(object):
    def __init__(self):
        pass

    def __enter__(self):
        pass

    def __exit__(self, etype, value, traceback):
        pass

    def keep_crash(self, crash):
        pass

    def _create_minimizer_cfg(self):
        pass

    def minimize(self, crash):
        pass

    def _copy_seedfile(self):
        pass

    def copy_files(self, crash):
        pass

    def record_success(self):
        pass

    def record_failure(self):
        pass

    def _process_crash(self, crash):
        pass

    def _log_crash(self, crash):
        pass

    def _build_crash(self, fuzzer, cmdlist, dbg_opts, fuzzed_file):
        pass

    def _fuzz_and_run(self):
        pass

    def go(self):
            logger.info('Iteration: %d File: %s', self.current_seed, self.sf.path)
            self._fuzz_and_run()

            # process all the crashes
            for c in self.crashes:
                self._process_crash(c)



"""
"""
from . import Fuzzer
from . import FuzzerError
from . import FuzzerExhaustedError
import logging
from random import getrandbits

logger = logging.getLogger(__name__)

class InsertFuzzerError(FuzzerError):
    pass

class InsertFuzzer(Fuzzer):
    '''
    This fuzzer module iterates through an input file, inserting a random byte
    between every byte position as it goes. E.g. insert before byte 0, before
    byte 1, etc.
    '''
    def _fuzz(self):
        '''
        Insert individual bytes of input and put output in self.fuzzed
        '''

        # TODO: add range list support to insert fuzzer
#        if self.options.get('use_range_list'):
#            bytes_to_fuzz = []
#            for (start, end) in self.options['range_list']:
#                    bytes_to_fuzz.extend(xrange(start, end + 1))
#        else:
#            bytes_to_fuzz = xrange(len(byte_buffer))
        bytes_to_fuzz = xrange(len(self.input))

        # we can calculate the byte to insert on based on the number of tries
        # on this seed file
        byte_pos = self.sf.tries
        byte_to_insert = getrandbits(8)

        if byte_pos < len(bytes_to_fuzz):
            self.input.insert(byte_pos, byte_to_insert)
        else:
            #indicate we didn't fuzz the file for this iteration
            raise FuzzerExhaustedError('Iteration exceeds available values')

        logger.debug('%s - inserted byte 0x%02x at 0x%02x', self.sf.basename,
                     byte_to_insert, byte_pos)

        self.fuzzed = self.input

_fuzzer_class = InsertFuzzer

'''
Created on Jul 9, 2013

@organization: cert.org
'''
class MinimizerError(Exception):
    pass

class WindowsMinimizerError(MinimizerError):
    pass

'''
Created on Aug 8, 2011

@organization: cert.org
'''

import unittest
from certfuzz.analyzers import Analyzer

class MockObj(object):
    def __init__(self, **kwargs):
        for (kw, arg) in kwargs:
            self.__setattr__(kw, arg)

class MockCfg(MockObj):

    def get_command_list(self, *args):
        pass

class MockCrash(MockObj):
    def __init__(self):
        self.fuzzedfile = MockFile()
        self.killprocname = 'killprocname'

class MockFile(MockObj):
    def __init__(self):
        self.dirname = 'dirname'
        self.path = 'path'

class Test(unittest.TestCase):

    def setUp(self):
        cfg = MockCfg()
        crash = MockCrash()
        self.analyzer = Analyzer(cfg, crash, timeout=0)
        self.assertTrue(self.analyzer, 'Analyzer does not exist')

    def tearDown(self):
        pass

    def testName(self):
        pass

if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()

'''
Created on Apr 10, 2012

@organization: cert.org
'''

import unittest
import tempfile
import shutil
import yaml
import os
from certfuzz.campaign import config
import pprint

_count = 0
def _counter():
    global _count
    _count += 1

class Test(unittest.TestCase):

    def setUp(self):
        self.tempdir = tempfile.mkdtemp()

    def tearDown(self):
        shutil.rmtree(self.tempdir)

    def _write_yaml(self, thing=None):
        if thing is None:
            thing = dict(a=1, b=2, c=3, d=4)
        fd, f = tempfile.mkstemp(suffix='yaml', dir=self.tempdir)
        os.close(fd)
        with open(f, 'wb') as fd:
            yaml.dump(thing, fd)

        return thing, f

    def test_parse_yaml(self):
        thing, f = self._write_yaml()

        self.assertTrue(os.path.exists(f))
        self.assertTrue(os.path.getsize(f) > 0)

        from_yaml = config.parse_yaml(f)
        self.assertEqual(thing, from_yaml)

    def test_config_init(self):
        thing, f = self._write_yaml()
        c = config.Config(f)
        self.assertEqual(f, c.file)
        self.assertEqual(thing, c.config)

    def test_validate(self):
        dummy, f = self._write_yaml()
        c = config.Config(f)
        # add some validations
        c.validations.append(_counter)
        c.validations.append(_counter)
        c.validations.append(_counter)

        # confirm that each validation got run
        self.assertEqual(0, _count)
        c.validate()
        self.assertEqual(3, _count)

    def test_load(self):
        dummy, f = self._write_yaml()
        os.remove(f)
        c = config.Config(f)
        self.assertEqual(None, c.config)
        c.load()
        # nothing should have happened
        self.assertEqual(None, c.config)

        # write another yaml file
        thing, f = self._write_yaml()
        # sub the new file name
        c.file = f
        # load it
        c.load()
        # we should get the thing back again
        self.assertEqual(thing, c.config)

        # load should add each of the things as
        # config attributes
        for k, v in thing.iteritems():
            self.assertTrue(hasattr(c, k))
            self.assertEqual(c.__getattribute__(k), v)

if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()

import os
import tempfile
from certfuzz.fuzztools.zzuflog import ZzufLog
'''
Created on Apr 8, 2011

@organization: cert.org
'''

import unittest

class Test(unittest.TestCase):
    def delete_file(self, f):
        if os.path.exists(f):
            os.remove(f)
        self.assertFalse(os.path.exists(f))

    def tearDown(self):
        self.delete_file(self.infile)
        self.delete_file(self.outfile)

    def setUp(self):
        (fd1, f1) = tempfile.mkstemp(text=True)
        os.close(fd1)
        self.infile = f1

        (fd2, f2) = tempfile.mkstemp(text=True)
        os.close(fd2)
        self.outfile = f2

        self.log = ZzufLog(self.infile, self.outfile)

    def test_get_last_line(self):
        open(self.infile, 'w')
        self.assertEqual(self.log._get_last_line(), '')

        (fd, f) = tempfile.mkstemp(text=True)
        os.write(fd, "firstline\n")
        os.write(fd, "secondline\n")
        os.write(fd, "thirdline\n")
        os.close(fd)

        log = ZzufLog(f, self.outfile)
        # log.line gets the result of _get_last_line before the infile is wiped out
        self.assertEqual(log.line, 'thirdline')
        self.delete_file(f)

    def test_set_exitcode(self):
        self.log.result = "blah"
        self.log._set_exitcode()
        self.assertEqual(self.log.exitcode, '')

        self.log.result = "exit 1701"
        self.log._set_exitcode()
        self.assertEqual(self.log.exitcode, 1701)

    def test_set_signal(self):
        self.log.result = "blah"
        self.log._set_signal()
        self.assertEqual(self.log.signal, '')

        self.log.result = "signal 17938"
        self.log._set_signal()
        self.assertEqual(self.log.signal, '17938')

    def test_parse_line(self):
        self.log.line = "blah"
        self.assertEqual(self.log._parse_line(), (False, False, ''))
        self.log.line = "zzuf[s=99,r=foo]: Welcome to Jurassic Park"
        self.assertEqual(self.log._parse_line(), (99, 'foo', 'Welcome to Jurassic Park'))

    def test_was_out_of_memory(self):
        # should be true
        self.log.result = "signal 15"
        self.assertTrue(self.log._was_out_of_memory())
        self.log.result = "exit 143"
        self.assertTrue(self.log._was_out_of_memory())

        # should be false
        self.log.result = "signal 8"
        self.assertFalse(self.log._was_out_of_memory())
        self.log.result = "exit 18"
        self.assertFalse(self.log._was_out_of_memory())

    def test_was_killed(self):
        # should be true
        self.log.result = "signal 9"
        self.assertTrue(self.log._was_killed())
        self.log.result = "exit 137"
        self.assertTrue(self.log._was_killed())

        # should be false
        self.log.result = "signal 8"
        self.assertFalse(self.log._was_killed())
        self.log.result = "exit 18"
        self.assertFalse(self.log._was_killed())

    def test_read_zzuf_log(self):
        (fd, f) = tempfile.mkstemp(text=True)
        line = "zzuf[s=%d,r=%s]: %s\n"
        os.write(fd, line % (10, "0.1-0.2", "foo"))
        os.write(fd, line % (85, "0.01-0.02", "bar"))
        os.close(fd)

        log = ZzufLog(f, self.outfile)

        self.assertEqual(log.seed, 85)
        self.assertEqual(log.range, "0.01-0.02")
        self.assertEqual(log.result, "bar")
        self.assertEqual(log.line, (line % (85, "0.01-0.02", "bar")).strip())

        # cleanup
        self.delete_file(f)

    def test_crash_logged(self):
        self.log.result = "a"
        self.log._set_exitcode()
        self.assertFalse(self.log.crash_logged(False))

        # _was_killed => true
        # should be false
        self.log.result = "signal 9"
        self.log._set_exitcode()
        self.assertFalse(self.log.crash_logged(False))

        # _was_out_of_memory => true
        # should be false
        self.log.result = "signal 15"
        self.log._set_exitcode()
        self.assertFalse(self.log.crash_logged(False))

        # should be false since infile is empty
        self.log.result = "a"
        self.log._set_exitcode()
        self.assertFalse(self.log.parsed)
        self.assertFalse(self.log.crash_logged(False))

        # should be true
        self.log.result = "a"
        self.log._set_exitcode()
        self.log.parsed = True # have to fake it since infile is empty
        self.assertTrue(self.log.crash_logged(False))

#    def test_crash_exit(self):
#        crash_exit_code_list = [77, 88, 99]
#
#        self.log.result = "exit 77"
#        self.log._set_exitcode()
#        self.assertTrue(self.log._crash_exit(crash_exit_code_list))
#
#        self.log.result = "exit 88"
#        self.log._set_exitcode()
#        self.assertTrue(self.log._crash_exit(crash_exit_code_list))
#
#        self.log.result = "exit 99"
#        self.log._set_exitcode()
#        self.assertTrue(self.log._crash_exit(crash_exit_code_list))
#
#        self.log.result = "exit 1"
#        self.log._set_exitcode()
#        self.assertFalse(self.log._crash_exit(crash_exit_code_list))
#
#        self.log.result = "exit 2"
#        self.log._set_exitcode()
#        self.assertFalse(self.log._crash_exit(crash_exit_code_list))
#
#        self.log.result = "exit 3"
#        self.log._set_exitcode()
#        self.assertFalse(self.log._crash_exit(crash_exit_code_list))

if __name__ == "__main__":
    #import sys;sys.argv = ['', 'Test.testName']
    unittest.main()

'''
Timeseries.py - functions associated with analysis of time series data
======================================================================
:Author: Mike Morgan
:Release: $Id$
:Date: |today|
:Tags: Python

Functions are split into::
 * clustering assessment
 * data transformation and normalisation
 * differential expression analysis
 * clustering and distance metric functions
'''
import sklearn.metrics.cluster.supervised as supervised
from math import log
import CGAT.Experiment as E
import numpy as np
import pandas as pd
import itertools
import os
import sys
import math
from rpy2.robjects import pandas2ri
from rpy2.robjects.packages import importr
from rpy2.robjects import r as R
import rpy2.robjects as ro
import random

import cmetrics as c2m


def get_r_path():
    """return path of R support functions.
    """
    return os.path.dirname(__file__)

#################################
# Clustering assessment functions
#################################


def get_label_map(labels):
    '''
    return a dictionary with integer:string mapping
    '''
    label_set = set()
    map_dict = {}
    for val in labels:
        label_set.update(val)
    for lab, integer in enumerate(label_set):
        map_dict[integer] = lab

    return map_dict


def make_mapped_matrix(map_dict, input_frame):
    '''
    return a matrix with integer labels from mapping
    '''

    frame_index = input_frame.index.tolist()
    nindex = len(frame_index)
    ncols = len(input_frame.columns)
    integer_matrix = np.ndarray((nindex, ncols),
                                dtype=np.int32)

    E.info("mapping cluster labels")
    matrix_idx = [h for h, g in enumerate(frame_index)]
    for idx in matrix_idx:
        for col in range(ncols):
            mod = input_frame.iloc[idx][col+1]
            integer_matrix[idx][col] = map_dict[mod]

    return integer_matrix


def randIndexes(clustering_results):
    '''
    Calculate Rand index and adjusted Rand index over pairwise
    clustering comparisons.
    Use cythonised function to calculate indices
    '''

    # reassign module and gene labels with integer ids, integer comparison is
    # much faster than string comparison
    cluster_labels = clustering_results.values
    map_dict = get_label_map(cluster_labels)

    gene_map = {}
    for r, gene in enumerate(clustering_results.index):
        gene_map[gene] = r
    E.info("mapping gene ids")

    integer_matrix = make_mapped_matrix(map_dict, clustering_results)
    # take a small slice of the matrix for testing 5 genes, 3 clusterings

    E.info("counting clustering consensus")
    # use cythonized function to return rand index matrix
    cy_rand = c2m.consensus_metrics(integer_matrix)
    E.info("Rand Index calculated for all clusterings")

    return cy_rand


def unravel_arrays(metric_array):
    '''
    Unravel a numpy array such that only one half of the symmetrical
    matrix is output.  Do not output diagonal values.
    '''

    dim = metric_array.shape[0]
    flat_array = []

    for indx in itertools.combinations(range(0, dim), r=2):
        if indx[0] != indx[1]:
            flat_array.append(metric_array[indx[1], indx[0]])
        else:
            pass
    return flat_array


def mutualInformation(cluster1, cluster2):
    '''
    Calculate the mutual information for a given pair of clusterings.
    Assume clustering1 represents the reference ground truth.
    Code from scikit-learn.

    The mutual information of two clusterings, U and V is given by:

    MI(U, V) = sum(R)sum(C)P(i, j) [log(P(i, j)/(p(i)p'(j)))]

    This is the similarity of two clustering labels, where P(i) is
    the probability of a random sample in clustering Ui, and P'(j)
    the probability of a random sample in clustering Vj.
    '''

    cont = contingency(cluster1, cluster2)
    cont_sum = np.sum(cont)
    pi = np.sum(cont, axis=1)
    pj = np.sum(cont, axis=0)
    outer = np.outer(pi, pj)
    nnz = cont != 0

    cont_nm = cont[nnz]
    log_cont_nm = np.log(cont_nm)
    cont_nm /= cont_sum
    log_outer = -np.log(outer[nnz]) + log(pi.sum()) + log(pj.sum())
    mi = (cont_nm * (log_cont_nm - log(cont_sum)) + (cont_nm * log_outer))

    return mi.sum()


def contingency(cluster1, cluster2):
    '''
    Return a n x m matrix of clustering overlaps, where n
    is the number of clusters in clustering1 and m is the
    number of clusters in clustering2.  Return an np array.
    '''
    cont = pd.DataFrame(columns=cluster1.keys(), index=cluster2.keys())
    cont = cont.fillna(0.0)

    for x in itertools.product(cluster1.keys(), cluster2.keys()):
        set1 = cluster1[x[0]]
        set2 = cluster2[x[1]]
        intersect = len(set1.intersection(set2))
        cont[x[0]][x[1]] = intersect

    cont = cont.as_matrix()
    return cont


def entropy(cluster_labels):
    '''
    Calculate the entropy of a clustering.
    Entropy(X):
    H(X) = sum(p(i)log(1/(pi)))
    '''

    if len(cluster_labels) == 0:
        return 1.0
    else:
        pass

    cluster_prob = [len(cluster_labels[x]) for x in cluster_labels.keys()]
    pi = np.array(cluster_prob).astype(np.float)
    pi = pi[pi > 0]
    pi_sum = np.sum(pi)

    entropy = -np.sum((pi / pi_sum) * (np.log(pi) - log(pi_sum)))

    return entropy


def adjustedMutualInformation(cluster1, cluster2):
    '''
    Using the scikit-learn algorithms, calculate the adjusted mutual
    information for two clusterings.  Assume cluster1 is the
    reference/ground truth clustering.
    The adjusted MI accounts for higher scores by chance, particularly
    in the case where a larger number of clusters leads to a higher MI.

    AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [max(H(U), H(V)) - E(MI(U, V))]
    where E(MI(U, V)) is the expected mutual information given the
    number of clusters and H(U), H(V) are the entropies of clusterings
    U and V.
    '''

    cont = contingency(cluster1, cluster2)
    mi = mutualInformation(cluster1, cluster2)
    sample_size = float(sum([len(cluster1[x]) for x in cluster1.keys()]))

    # Given the number of samples, what is the expected number
    # of overlaps that would occur by chance?
    emi = supervised.expected_mutual_information(cont, sample_size)

    # calculate the entropy for each clustering
    h_clust1, h_clust2 = entropy(cluster1), entropy(cluster2)

    if abs(h_clust1) == 0.0:
        h_clust1 = 0.0
    else:
        pass
    if abs(h_clust2) == 0.0:
        h_clust2 = 0.0
    else:
        pass

    ami = (mi - emi) / (max(h_clust1, h_clust2) - emi)

    # bug: entropy will return -0 in some instances
    # make sure this is actually 0 else ami will return None
    # instead of 0.0

    if np.isnan(ami):
        ami = np.nan_to_num(ami)
    else:
        pass

    return ami


#################################################
# Data transformation and normalisation functions
#################################################


def deseqNormalize(infile,
                   time_points,
                   reps,
                   conditions=None):
    '''
    Library size normalisation and variance stabilizing transformation of
    timeseries RNA-seq data

    :param infile: count table from NGS-seq experiment
    :type infile: str
    :param time_points: time point labels
    :type time_points: str list
    :param reps: replicates labels
    :type reps: str list
    :param conditions: if  multiple experimental conditions
    are to be normalised at the same time
    :type conditions: str list
    '''
    # MM: NB - this should be split into separate library size
    # normalisation and VST transformations
    # maybe add in different transformation options.

    pandas2ri.activate()
    reps = reps

    # load library
    R('''suppressMessages(library("DESeq"))''')

    # generates a lists for the design data frame
    # of the proper length
    # these need to be rpy2 objects to be parsed
    # properly in the string formatting

    E.info("converting to pandas dataframe object")

    if infile.split(".")[-1] == "gz":
        comp = "gzip"
    else:
        comp = None

    data_frame = pd.read_table(infile,
                               index_col=0,
                               header=0,
                               sep="\t",
                               compression=comp)
    # py2ri requires activation
    pandas2ri.activate()
    rdf = pandas2ri.py2ri(data_frame)

    if not conditions:
        time_rep_comb = [x for x in itertools.product(time_points, reps)]
        time_cond = ro.StrVector([x[0] for x in time_rep_comb])
        rep_cond = ro.StrVector([x[1] for x in time_rep_comb])

        R.assign('countsTable', rdf)
        R('''design <- data.frame(row.names=colnames(countsTable),'''
          '''times=%s, replicates=%s)''' % (time_cond.r_repr(),
                                            rep_cond.r_repr()))
    elif conditions:
        design_dict = {}
        for x in data_frame.columns.values:
            sample_dict = {}
            sample_dict['condition'] = str(x).split(".")[0]
            sample_dict['times'] = int(str(x).split(".")[1])
            sample_dict['replicates'] = str(x).split(".")[2]
            design_dict[x] = sample_dict
            design_frame = pd.DataFrame(design_dict)
            design_frame = design_frame.T

        des_cond = design_frame['condition'].values.tolist()
        des_time = design_frame['times'].values.tolist()
        des_reps = design_frame['replicates'].values.tolist()

        cond_cond = ro.StrVector([x for x in des_cond])
        time_cond = ro.StrVector([x for x in des_time])
        rep_cond = ro.StrVector([x for x in des_reps])

        R.assign('countsTable', rdf)
        R.assign('design', design_frame)

    # create the count data set and normalize to library size
    # transform with variance stabilizing transformation
    # only select genes with an average of ten reads mapping

    E.info("calculating size factors and dispersion")
    R('''notZero <- (rowMeans(countsTable) > 1)''')
    R('''cds <- newCountDataSet(countsTable[notZero, ], design)''')
    R('''cds_size <- estimateSizeFactors(cds)''')
    R('''cds_disp <- estimateDispersions(cds_size, method="blind")''')

    E.info("applying variance stabilizing transformation")

    R('''vst <- varianceStabilizingTransformation(cds_disp)''')

    # format data set to long format with condition and replicate labels
    # convert to a numpy array

    R('''replicates <- c(%s)''' % rep_cond.r_repr())
    R('''times <- c(%s)''' % time_cond.r_repr())
    if conditions:
        R('''conditions <- c(%s)''' % cond_cond.r_repr())
        R('''trans_vst = data.frame(t(exprs(vst)), '''
          '''times, replicates, conditions)''')
    else:
        R('''trans_vst = data.frame(t(exprs(vst)), times, replicates)''')

    # load data and convert to pandas object
    data_file = pandas2ri.ri2py(R["trans_vst"])
    
    return data_file


def avTimeExpression(infile):
    '''
    Calculate average expression over replicates at each time point
    Requires genes as columns with 'replicates' and 'times' as additional
    columns
    '''

    # check file compression
    if infile.split(".")[-1] == "gz":
        comp = "gzip"
    else:
        comp = None

    df = pd.read_table(infile, sep="\t",
                       header=0, index_col=0,
                       compression=comp)

    # average over replicates at each time point
    # then recombine
    df_groups = df.groupby(by='times')
    data_frame = pd.DataFrame(index=df.columns,
                              columns=None)
    for names, groups in df_groups:
        _df = groups.drop(['times', 'replicates'], axis=1)
        _df = _df.apply(np.mean, axis=0)
        data_frame[names] = _df

    # check no extraneous columns remaining
    try:
        data_frame.drop(['replicates', 'times'],
                        inplace=True,
                        axis=0)
    except KeyError:
        pass

    return data_frame


def covarFilter(infile,
                time_points,
                replicates,
                quantile):
    '''
    Filter gene list based on the distribution of the
    sums of the covariance of each gene.  This is highly
    recommended to reduce the total number of genes used
    in the dynamic time warping clustering to reduce the
    computational time.  The threshold is placed at the
    intersection of the expected and observed value
    for the given quantile.
    '''

    time_points.sort()
    time_rep_comb = [x for x in itertools.product(time_points, replicates)]
    time_cond = ro.StrVector([x[0] for x in time_rep_comb])
    rep_cond = ro.StrVector([x[1] for x in time_rep_comb])
    df = pd.read_table(infile, sep="\t", header=0, index_col=0)

    df.drop(['replicates'], inplace=True, axis=1)
    df.drop(['times'], inplace=True, axis=1)
    df = df.fillna(0.0)

    # convert data frame and import into R namespace
    # py2ri requires activation
    pandas2ri.activate()
    R.assign('diff_data', pandas2ri.py2ri(df))

    E.info("loading data frame")

    # need to be careful about column headers and transposing data frames

    R('''trans_data <- data.frame(diff_data)''')
    R('''times <- c(%s)''' % time_cond.r_repr())
    R('''replicates <- c(%s)''' % rep_cond.r_repr())

    # calculate the covariance matrix for all genes
    # sum each gene's covariance vector

    E.info("calculating sum of covariance of expression")

    R('''covar.mat <- abs(cov(trans_data))''')
    R('''sum.covar <- rowSums(covar.mat)''')
    R('''exp.covar <- abs(qnorm(ppoints(sum.covar),'''
      '''mean=mean(sum.covar), sd=sd(sum.covar)))''')
    R('''sum.covar.quant <- quantile(sum.covar)''')
    R('''exp.covar.quant <- quantile(exp.covar)''')

    E.info("filter on quantile")

    R('''filtered_genes <- names(sum.covar[sum.covar > '''
      '''sum.covar.quant[%(quantile)i]'''
      ''' & sum.covar > exp.covar.quant[%(quantile)i]])''' % locals())
    R('''filtered_frame <- data.frame(diff_data[, filtered_genes],'''
      '''times, replicates)''')

    # load data and convert to pandas object
    filtered_frame = pandas2ri.ri2py(R["filtered_frame"]).T

    return filtered_frame


def clusterPCA(infile,
               cluster_file,
               image_dir):
    '''
    PCA for each module within an experimental condition across
    the time series.
    Take PC1 as the module eigengene and return the loadings and proportion
    of variance explained for the eigengene.
    The eigengene expression across the time series is taken to be the
    value for PC1 at each timepoint as a vector.
    This is basically what WGCNA moduleEigengenes does but it
    does not recover the PC loadings.

    Warning: this script will error if there is only one
    cluster. Make sure you have more than one cluster before
    trying to perform uninformative analyses.

    Parameters:

    rpath : string
        Path of R support libraries

    '''

    header = cluster_file.split("/")[-1].split("-")[0]

    # reshape data
    R('''sink(file='sink_file.txt')''')
    R('''suppressMessages(library("reshape2"))''')
    R('''suppressMessages(library("WGCNA"))''')

    # AH: these were hard-coded paths, parameterized them to point to the
    # directory of this module's location
    R('''source("%s")''' % os.path.join(get_r_path(), "summarySE.R"))
    R('''source("%s")''' % os.path.join(get_r_path(), "clusterEigengenes.R"))
    R('''cluster_match <- read.table('%(cluster_file)s', h=T, '''
      '''row.names=1)''' % locals())
    R('''express_data <- read.table('%(infile)s', '''
      '''h=T, row.names=1, stringsAsFactors=F)''' % locals())
    R('''sink(file=NULL)''')
    R('''colnames(cluster_match) <- c("genes", "cluster")''')
    R('''express_data <- data.frame(t(express_data))''')
    R('''express_data$times <- as.numeric(as.character(express_data$times))''')
    R('''data_melt <- melt(express_data, '''
      '''id.vars=c("times", "replicates"))''')

    # sometimes data is read in as a factor/string.
    # Explicitly convert to numeric

    R('''data_melt$value <- as.numeric(as.character(data_melt$value))''')
    R('''data_sum <- summarySE(data_melt, measurevar="value", '''
      '''groupvars=c("times", "variable"))''')
    R('''data_mod <- data.frame(data_sum$times,'''
      ''' data_sum$variable, data_sum$value)''')
    R('''colnames(data_mod) <- c("times", "gene", "value")''')
    R('''data_wide <- dcast(data_mod, gene ~ times, value.var="value")''')
    R('''rownames(data_wide) <- data_wide$gene''')
    R('''times <- as.numeric(as.character(unique(express_data$times)))''')
    R('''data_wide <- data.frame(data_wide[,-1])''')
    R('''colnames(data_wide) <- times''')

    # derive module eigengenes - return a dataframe of eigengene expression
    R('''eigen_clustered <- clusterPCA(cluster_frame=cluster_match, '''
      '''expression_frame=data_wide, n=times)''')
    R('''eigen_frame <- eigenExpress(eigen_clustered, n=times)''')

    # generate loadings plot for each eigengene
    R('''eigenLoad(clusterPCA(cluster_frame=cluster_match, '''
      '''expression_frame=data_wide, n=times), image.dir="%(image_dir)s", '''
      '''condition="%(header)s")''' % locals())

    # generate expression profile plots for all eigengenes
    R('''eigenPlot(eigen_frame, image.dir="%(image_dir)s", '''
      '''condition="%(header)s")''' % locals())

    eigen_frame = pandas2ri.ri2py(R["eigen_frame"])
    eigen_frame.index = eigen_frame['cluster']
    eigen_frame.drop(['cluster'], inplace=True, axis=1)

    return eigen_frame


#########################
# Differential expression
#########################


def conditionDESeq2(data_frame, header, alpha, res_dir):
    '''
    Perform DESeq2-based analysis of condition:time interaction
    dependent differential expression
    '''

    E.info("Differential expression testing for %s" % header)
    cols = data_frame.columns

    # py2ri requires activation
    pandas2ri.activate()
    counts = pandas2ri.py2ri(data_frame)

    des_times = ro.IntVector([x.split(".")[1] for x in cols])
    des_reps = ro.StrVector([x.split(".")[2] for x in cols])
    des_cond = ro.StrVector([x.split(".")[0] for x in cols])
    genes = ro.StrVector([x for x in data_frame.index])

    # setup counts table and design frame

    R('''suppressPackageStartupMessages(library("DESeq2"))''')
    R('''sink(file="/dev/null")''')
    R('''times <- as.factor(%s)''' % des_times.r_repr())
    R('''reps <- c(%s)''' % des_reps.r_repr())
    R('''condition <- c(%s)''' % des_cond.r_repr())
    R('''design <- data.frame(times, reps, condition)''')
    R('''counts <- data.frame(%s)''' % counts.r_repr())
    R('''genes <- c(%s)''' % genes.r_repr())
    R('''rownames(counts) <- genes''')
    R('''rownames(design) <- colnames(counts)''')

    # use DESeq() with LRT and reduced formula.  Use effect
    # size moderation

    R('''dds <- DESeqDataSetFromMatrix(countData=counts, '''
      '''colData=design, '''
      '''design=~reps + times + condition + times:condition)''')
    R('''dds <- DESeq(dds, test="LRT", '''
      '''reduced=~reps + times + condition, betaPrior=T)''')
    R('''res <- results(dds)[order(results(dds)$padj, na.last=T), ]''')
    R('''res.df <- data.frame(res)''')

    # generate dispersion and MA plots
    R('''png("%s/%s-dispersions.png")''' % (res_dir,
                                            header))
    R('''plotDispEsts(dds)''')
    R('''dev.off()''')

    R('''png("%s/%s-MAplot.png")''' % (res_dir,
                                       header))
    R('''plotMA(res, alpha=%0.3f, ylim=c(-5,5))''' % alpha)
    R('''dev.off()''')
    R('''sink(file=NULL)''')

    df = pandas2ri.ri2py(R['res.df'])

    return df


def timepointDESeq2(data_frame, header, alpha, res_dir):
    '''
    Perform DESeq2-based differential expression analysis of condition:time
    interaction
    '''

    E.info("Differential expression testing for %s" % header)
    cols = data_frame.columns

    # py2ri requires activation
    pandas2ri.activate()
    counts = pandas2ri.py2ri(data_frame)

    des_times = ro.IntVector([x.split(".")[1] for x in cols])
    des_reps = ro.StrVector([x.split(".")[2] for x in cols])
    genes = ro.StrVector([x for x in data_frame.index])

    # setup counts table and design frame

    R('''suppressPackageStartupMessages(library("DESeq2"))''')
    R('''sink(file="/dev/null")''')
    R('''times <- as.factor(%s)''' % des_times.r_repr())
    R('''reps <- c(%s)''' % des_reps.r_repr())
    R('''design <- data.frame(times, reps)''')
    R('''counts <- data.frame(%s)''' % counts.r_repr())
    R('''genes <- c(%s)''' % genes.r_repr())
    R('''rownames(counts) <- genes''')
    R('''rownames(design) <- colnames(counts)''')

    # use DESeq() with LRT and reduced formula.  Use effect
    # size moderation

    R('''dds <- DESeqDataSetFromMatrix(countData=counts, '''
      '''colData=design, '''
      '''design=~reps + times )''')
    R('''dds <- DESeq(dds, betaPrior=T)''')
    R('''res <- results(dds)[order(results(dds)$padj, na.last=T), ]''')
    R('''res.df <- data.frame(res)''')

    # generate dispersion and MA plots
    R('''png("%s/%s-dispersions.png")''' % (res_dir,
                                            header))
    R('''plotDispEsts(dds)''')
    R('''dev.off()''')

    R('''png("%s/%s-MAplot.png")''' % (res_dir,
                                       header))
    R('''plotMA(res, alpha=%0.3f, ylim=c(-5,5))''' % alpha)
    R('''dev.off()''')
    R('''sink(file=NULL)''')

    df = pandas2ri.ri2py(R['res.df'])

    return df


def genSigGenes(file_list, alpha, out_dir):
    '''
    With a list of input files (results from DESeq2) generate a
    dictionary of genes that are statistically significantly differentially
    expressed between conditions and/or time points.
    '''

    alpha = float(alpha)
    deg_dict = {}
    for infle in file_list:
        if infle.split("/")[0].split(".")[0] == "diff_condition":
            header = infle.split("/")[1].split(".")[1]
            header = header.rstrip("-diff-cond.tsv")
            header = "%s" % header

        elif infle.split("/")[0].split(".")[0] == "diff_timepoints":
            header = infle.split("/")[1].split("-")[0]
            header = "%s_%s" % (header.split("_")[0],
                                header.split("_")[2])

        in_df = pd.read_table(infle, sep="\t", header=0, index_col=0)
        sig_genes = in_df[in_df['padj'] <= alpha]
        deg_dict[header] = sig_genes.index.tolist()

    if file_list[0].split("/")[0].split(".")[0] == "diff_condition":
        condition = file_list[0].split("/")[1].split(".")[0]
        condition = "%s-condition" % condition

    elif file_list[0].split("/")[0].split(".")[0] == "diff_timepoints":
        condition = file_list[0].split("/")[1].split("_")[0]
        condition = "%s-time" % condition

    drawVennDiagram(deg_dict, condition, out_dir)


def drawVennDiagram(deg_dict, header, out_dir):
    '''
    Take a dictionary of gene IDs, with keys corresponding
    to timepoints/differential expression analyses and
    generate a Venn diagram.  Maximum of 5 overlapping sets
    possible using R package:: VennDiagram.
    '''

    keys = deg_dict.keys()
    try:
        keys = sorted(keys, key=lambda x: int(x.split("_")[1].rstrip("-time")))
    except IndexError:
        pass

    venn_size = len(keys)
    R('''suppressPackageStartupMessages(library("VennDiagram"))''')
    n1 = set(deg_dict[keys[0]])
    n2 = set(deg_dict[keys[1]])
    area1 = len(n1)
    area2 = len(n2)
    n12 = len(n1.intersection(n2))

    # for Venn > 2 sets
    if venn_size == 3:
        n3 = set(deg_dict[keys[2]])
        area3 = len(n3)
        n13 = len(n1.intersection(n3))
        n23 = len(n2.intersection(n3))
        n123 = len((n1.intersection(n2)).intersection(n3))
        cat1, cat2, cat3 = keys

        R('''png("%(out_dir)s/%(header)s-venn.png", '''
          '''width=1.8, height=1.8, res=90, units="in")''' % locals())
        R('''draw.triple.venn(%(area1)d, %(area2)d, %(area3)d, '''
          '''%(n12)d, %(n23)d, %(n13)d, %(n123)d, '''
          '''c('%(cat1)s', '%(cat2)s', '%(cat3)s'), '''
          '''col=c('red', 'yellow', 'skyblue'), '''
          '''fill=c('red', 'yellow', 'skyblue'), '''
          '''margin=0.05, alpha=0.5)''' % locals())
        R('''dev.off()''')

    elif venn_size == 4:
        n3 = set(deg_dict[keys[2]])
        area3 = len(n3)
        n13 = len(n1.intersection(n3))
        n23 = len(n2.intersection(n3))
        n123 = len((n1.intersection(n2)).intersection(n3))

        n4 = set(deg_dict[keys[3]])
        area4 = len(n4)
        n14 = len(n1.intersection(n4))
        n24 = len(n2.intersection(n4))
        n34 = len(n3.intersection(n4))
        n124 = len((n1.intersection(n2)).intersection(n4))
        n134 = len((n1.intersection(n3)).intersection(n4))
        n234 = len((n2.intersection(n3)).intersection(n4))
        n1234 = len(((n1.intersection(n2)).intersection(n3)).intersection(n4))
        cat1, cat2, cat3, cat4 = keys

        R('''png("%(out_dir)s/%(header)s-venn.png",'''
          '''width=2.3, height=2.3, res=300, units="in")''' % locals())
        R('''draw.quad.venn(%(area1)d, %(area2)d, %(area3)d, %(area4)d,'''
          '''%(n12)d, %(n13)d, %(n14)d, %(n23)d, %(n24)d, %(n34)d,'''
          '''%(n123)d, %(n124)d, %(n134)d, %(n234)d, %(n1234)d,'''
          '''c('%(cat1)s', '%(cat2)s', '%(cat3)s', '%(cat4)s'), '''
          '''col=c("red", "yellow", "skyblue", "orange"), '''
          '''fill=c("red", "yellow", "skyblue", "orange"), '''
          '''margin=0.05, alpha=0.5)''' % locals())
        R('''dev.off()''')

    elif venn_size == 5:
        n3 = set(deg_dict[keys[2]])
        area3 = len(n3)
        n13 = len(n1.intersection(n3))
        n23 = len(n2.intersection(n3))
        n123 = len((n1.intersection(n2)).intersection(n3))

        n4 = set(deg_dict[keys[3]])
        area4 = len(n4)
        n14 = len(n1.intersection(n4))
        n24 = len(n2.intersection(n4))
        n34 = len(n3.intersection(n4))
        n124 = len((n1.intersection(n2)).intersection(n4))
        n134 = len((n1.intersection(n3)).intersection(n4))
        n234 = len((n2.intersection(n3)).intersection(n4))
        n1234 = len(((n1.intersection(n2)).intersection(n3)).intersection(n4))

        n5 = set(deg_dict[keys[4]])
        area5 = len(n5)
        n15 = len(n1.intersection(n5))
        n25 = len(n2.intersection(n5))
        n35 = len(n3.intersection(n5))
        n45 = len(n4.intersection(n5))
        n125 = len((n1.intersection(n2)).intersection(n5))
        n135 = len((n1.intersection(n3)).intersection(n5))
        n145 = len((n1.intersection(n4)).intersection(n5))
        n235 = len((n2.intersection(n3)).intersection(n5))
        n245 = len((n2.intersection(n4)).intersection(n5))
        n345 = len((n3.intersection(n4)).intersection(n5))
        n1235 = len(((n1.intersection(n2)).intersection(n3)).intersection(n5))
        n1245 = len(((n1.intersection(n2)).intersection(n4)).intersection(n5))
        n1345 = len(((n1.intersection(n3)).intersection(n4)).intersection(n5))
        n2345 = len(((n2.intersection(n3)).intersection(n4)).intersection(n5))
        nstep = ((n1.intersection(n2)).intersection(n3))
        n12345 = len((nstep.intersection(n4)).intersection(n5))
        cat1, cat2, cat3, cat4, cat5 = keys

        R('''png("%(out_dir)s/%(header)s-venn.png", '''
          '''height=1.8, width=1.8, res=90, units="in")''' % locals())
        R('''draw.quintuple.venn(%(area1)d, %(area2)d, %(area3)d, '''
          '''%(area4)d, %(area5)d, %(n12)d, %(n13)d, %(n14)d,'''
          '''%(n15)d, %(n23)d, %(n24)d, %(n25)d, %(n34)d, %(n35)d,'''
          '''%(n45)d, %(n123)d, %(n124)d, %(n125)d, %(n134)d,'''
          '''%(n135)d, %(n145)d, %(n234)d, %(n235)d, %(n245)d,'''
          '''%(n345)d, %(n1234)d, %(n1235)d, %(n1245)d, %(n1345)d,'''
          '''%(n2345)d, %(n12345)d, '''
          '''c('%(cat1)s', '%(cat2)s', '%(cat3)s', '%(cat4)s', '%(cat5)s'),'''
          '''col=c("red", "yellow", "skyblue", "orange", "purple"),'''
          '''fill=c("red", "yellow", "skyblue", "orange", "purple"),'''
          '''alpha=0.05, margin=0.05, cex=rep(0.8, 31))''' % locals())
        R('''dev.off()''')

    elif venn_size > 5:
        raise ValueError("Illegal venn diagram size, must be <= 5")


def maSigPro(infile,
             order_terms=1,
             fdr=0.01,
             adjust="BH",
             stepwise="backward",
             include_p=0.01,
             rsq=0.2,
             var_group="all"):
    '''
    Generate differentially expressed genes for each experimental
    condition across a time series.  Uses the bioconductor
    package maSigPro to derive a set of genes of interest.
    '''

    ref_gtf = str(infile).split("-")[1]
    data_frame = pd.read_table(infile, sep="\t", index_col=0, header=0)
    design_dict = {}

    for x in data_frame.index.values:
        sample_dict = {}
        condition = str(x).split(".")[0]
        sample_dict[condition] = 1
        sample_dict['times'] = int(str(x).split(".")[1])
        sample_dict['replicates'] = str(x).split(".")[2]
        design_dict[x] = sample_dict

    design_frame = pd.DataFrame(design_dict)
    design_frame = design_frame.T
    cols = ['times', 'replicates', condition]
    design_frame = design_frame[cols]
    design_file = "deseq.dir/%s-%s-design.tsv" % (condition, ref_gtf)
    design_frame.to_csv(design_file, sep="\t")
    data_file = "deseq.dir/%s-%s-data.tsv" % (condition, ref_gtf)
    results_file = "deseq.dir/%s-%s-maSigPro.tsv" % (condition, ref_gtf)

    # data frame columns must be in the order time-replicate-condition
    # for maSigPro
    # define the numnber of higher-order terms included in the models

    masigpro_out = "deseq.dir/maSigPro.out"

    R('''suppressMessages(library("maSigPro"))''')
    R('''input_data <- read.table('%(infile)s', sep="\t", '''
      '''h=T, row.names=1)''' % locals())
    R('''input_data <- t(input_data[0:(length(input_data)-2)])''')

    E.info("constructing experimental design matrix")

    R('''input_design <- data.matrix(read.table('%(design_file)s', '''
      '''sep="\t", h=T, row.names=1))''' % locals())
    R('''%(condition)s_mat <- make.design.matrix(input_design, '''
      '''degree = %(order_terms)i )''' % locals())
    R('''sink(file = '%(masigpro_out)s')''' % locals())

    E.info("fitting linear model for each gene with "
           "%i polynomial terms" % order_terms)

    R('''%(condition)s_fit <- p.vector(input_data, %(condition)s_mat, '''
      '''Q = %(fdr)f, MT.adjust = '%(adjust)s')''' % locals())

    # fit a linear model to each of the genes called as
    # differentially expressed
    # report genes with model R-squared > threshold
    # maSigPro gives an un-suppressable output to stdout
    # therefore sink is used to shunt this to a temporary file 'maSigPro.out'

    R('''%(condition)s_step <- T.fit(%(condition)s_fit, '''
      '''step.method='%(stepwise)s', alfa=%(include_p)f)''' % locals())

    E.info("selecting significantly differentially "
           "expressed genes at FDR=%0.3f" % fdr)

    R('''sink(file=NULL)''')
    R('''%(condition)s_sigs <- get.siggenes(%(condition)s_step, '''
      '''rsq=%(rsq)f, vars='%(var_group)s')''' % locals())
    R('''write.table(%(condition)s_sigs$sig.genes$%(condition)s$group.coeffs'''
      ''',file="deseq.dir/%(condition)s-%(ref_gtf)s-coefficients.tsv", '''
      '''sep="\t")''' % locals())
    R('''write.table(%(condition)s_sigs$sig.genes$%(condition)s$sig.pvalues,'''
      '''file="deseq.dir/%(condition)s-%(ref_gtf)s-pvalues.tsv",'''
      ''' sep="\t")''' % locals())
    R('''write.table(%(condition)s_sigs$summary, '''
      '''file='deseq.dir/%(condition)s-%(ref_gtf)s-geneids.tsv', '''
      '''sep="\t")''' % locals())
    # merge the p-value and coefficient results into a single file
    p_file = "deseq.dir/%(condition)s-%(ref_gtf)s-pvalues.tsv" % locals()
    coef_file = "deseq.dir/%s-%s-coefficients.tsv" % (condition,
                                                      ref_gtf)
    p_frame = pd.read_table(p_file, sep="\t")
    coef_frame = pd.read_table(coef_file, sep="\t")
    results_frame = pd.merge(coef_frame, p_frame,
                             how='right',
                             left_index=True,
                             right_index=True)

    results_frame.to_csv(results_file, sep="\t")

    R('''diff_genes <- data.frame(%(condition)s_fit$SELEC)''' % locals())
    diff_genes = pandas2ri.ri2py[R'diff_genes']

    return diff_genes


##########################################
# Clustering and distance metric functions
##########################################


def splitReplicates(infile,
                    axis,
                    group_var,
                    outdir):
    '''
    Group data by replicates, new flat file for
    each.
    '''

    if axis == "row":
        axis = 1
    elif axis == "column":
        axis = 0

    inf_prefix = infile.split("/")[1].split("-")
    inf_prefix = inf_prefix[0] + "-" + inf_prefix[1]

    df = pd.read_table(infile,
                       sep="\t",
                       header=0,
                       index_col=0).T

    rep_groups = df.groupby(by=group_var,
                            axis=axis)
    for name, groups in rep_groups:
        outfile = outdir + "/" + inf_prefix + "-%s-expression.tsv" % name
        _df = groups.T
        _df.columns = _df.loc['times']
        _df.drop(['times'], axis=axis, inplace=True)
        _df.drop(['replicates'], axis=axis, inplace=True)
        _df.to_csv(outfile, sep="\t", index_label="gene_id")


def genResampleData(data_frame,
                    multiple_index,
                    replicates,
                    sample_reps,
                    times,
                    condition,
                    ref_gtf,
                    out_dir,
                    seed):
    '''
    Resample the data n-times with replacement - store
    in an sql database for ease of access
    '''

    vst_long = data_frame.T
    vst_long.index = multiple_index

    reps_dict = {}
    random.seed(seed)
    for it in range(1, replicates + 1):
        df = pd.DataFrame()
        df_dict = {}
        for i in times:
            k = str(random.randint(1,
                                   len(sample_reps)))
            series = vst_long.loc[str(i), 'R%s' % k]
            df_dict[str(i)] = series
        df = pd.DataFrame(df_dict)
        cols = df.columns.tolist()
        cols = [int(x) for x in cols]
        cols.sort()
        cols = [str(x) for x in cols]
        df = df[cols]
        reps_dict[str(it)] = df
        table = "%s-%s-resample_%i-expression" % (condition,
                                                  ref_gtf,
                                                  it)
        seg_file = "%s/%s.tsv" % (out_dir, table)
        df.to_csv(seg_file, sep="\t")

    sys.stdout.write("%i replicate datasets generated\n" % replicates)


def temporalCorrelate(series1, series2):
    '''
    Calculate the temporal correlation according to Chouakira & Nagabhushan
    Assumes both time series are of the same length
    '''

    series1 = list(series1)
    series2 = list(series2)

    sum_prod = []
    sum_usq = []
    sum_vsq = []
    for i in range(len(series1)-1):
        u = float(series1[i+1]) - float(series1[i])
        v = float(series2[i+1]) - float(series2[i])
        prod = u * v
        sum_prod.append(prod)
        sq_u = u**2
        sq_v = v**2
        sum_usq.append(sq_u)
        sum_vsq.append(sq_v)

    nume = sum(sum_prod)
    denom = math.sqrt(sum(sum_usq)) * math.sqrt(sum(sum_vsq))

    if denom != 0:
        return(nume/float(denom))
    else:
        return 0


def crossCorrelate(t, s, lag=0):
    '''
    Calculate the cross-correlation of two timeseries, s and t.
    Return the normalized correlation value at lag=n.
    Uses numpy.correlate; default is to return lag=0.
    TODO: return multiple lags?
    '''

    t_mean = np.mean(t)
    s_mean = np.mean(s)
    t_std = np.std(t)
    s_std = np.std(s)
    len_t = len(t)

    t_norm = [((x - t_mean)/(t_std * len_t)) for x in t]
    s_norm = [((y - s_mean)/s_std) for y in s]

    if lag == 0:
        xcorr = np.correlate(t_norm, s_norm)
    elif lag != 0:
        xcorr = np.correlate(t_norm, s_norm, mode=2)[len_t - 1 + lag]

    return xcorr


def adaptiveTune(value, k):
    '''
    Calculate the adaptive tuning function from Chouakira & Nagabhushan
    '''

    if k == 0:
        return 1.0
    else:
        return (2/(1 + math.exp(k*abs(value))))


def dtwWrapper(data, rows, columns, k):
    '''
    wrapper function for dynamic time warping.
    includes use of exponential adaptive tuning function
    with temporal correlation if k > 0
    '''

    # not explicitly called, but needs to be in R environment
    DTW = importr("dtw")

    # create a data frame of zeros of size number of ids x number of ids
    # fill it with the calculated distance metric for each pair wise comparison

    df_ = pd.DataFrame(index=rows,
                       columns=columns)
    df_ = df_.fillna(0.0).astype(np.float64)

    # fill the array with dtw-distance values
    pandas2ri.activate()

    for i in rows:
        E.info("DTW %s" % i)
        for j in columns:
            series1 = data.loc[i].values.tolist()
            series2 = data.loc[j].values.tolist()
            DTW_value = (R.dtw(series1,
                               series2)).rx('distance')[0][0]
            cort_value = temporalCorrelate(series1, series2)
            tuned_value = adaptiveTune(cort_value, k)
            time_dist = DTW_value * tuned_value
            df_.loc[i][j] = float(time_dist)
            df_[j][i] = float(time_dist)

    return df_


def correlateDistanceMetric(data, rows, columns, method, lag=0):
    '''
    wrapper for correlation coefficients as distance metrics
    for time-series clustering.
    Use either temporal correlation (analagous to template matching)
    or normalised cross correlation.
    '''

    # create blank (all 0's) dataframe to fill with correlation values

    df_ = pd.DataFrame(index=rows,
                       columns=columns)
    df_ = df_.fillna(0.0)

    if method == "cross-correlate":
        for i in rows:
            E.info("cross-correlation %s" % i)
            for j in columns:
                series1 = data.loc[i].values.tolist()
                series2 = data.loc[j].values.tolist()
                corr = crossCorrelate(series1, series2, lag=lag)
                df_.loc[i][j] = 1.0 - abs(corr)
                df_[j][i] = 1.0 - abs(corr)

    elif method == "temporal-correlate":
        for i in rows:
            E.info("temporal correlation %s" % i)
            for j in columns:
                series1 = data.loc[i].tolist()
                series2 = data.loc[j].tolist()
                corr = temporalCorrelate(series1, series2)
                df_.loc[i][j] = 1.0 - abs(corr)
                df_[j][i] = 1.0 - abs(corr)

    return df_


def splitFiles(infile, nchunks, out_dir):
    '''
    Give files names based on splitting into an arbitrary number of chunks
    '''

    df = pd.read_table(infile, sep="\t", header=0, index_col=0)
    total = len(df.index.tolist())

    # split into aribitrary number of chunks, or arbitrary chunk size?
    # small n bad for large input size, large n bad for small input size
    # set min/max chunk size, e.g. 100 genes minimum, 500 maximum?

    if total/nchunks < 100:
        step = 100
        E.warn("too few genes in each chunk, resetting to 100 genes per chunk")
    elif total/nchunks > 500:
        step = 500
        E.warn("too many genes per chunk, resetting to 500 genes per chunk")
    else:
        step = total/nchunks
        E.info("chunking input file into %i chunks" % step)

    file_pattern = infile.split("/")[1].rstrip("-expression.tsv")
    idx = 0
    for i in range(step, total, step):
        start = "%s" % idx
        end = "%s" % i
        file_name = "%s/%s-%s_%s-split.tsv" % (out_dir,
                                               file_pattern,
                                               start,
                                               end)
        with open(file_name, "w") as file_handle:
            file_handle.write(file_name + "\n")
        idx = i

    # final file
    start = "%s" % idx
    end = "%s" % total
    file_name = "%s/%s-%s_%s-split.tsv" % (out_dir,
                                           file_pattern,
                                           start,
                                           end)
    with open(file_name, "w") as file_handle:
        file_handle.write(file_name + "\n")


def mergeFiles(file_list, outfile):
    '''
    Merge files after split-transform operations
    '''

    # sort list by starting index of file
    res_list = sorted(file_list,
                      key=lambda x: int(x.split("/")[-1].split("-")[3].split("_")[0]))

    # merge files using pandas data frame merge method
    full_frame = pd.read_table(res_list[0], sep="\t", index_col=0, header=0)
    res_list.remove(res_list[0])
    for fle in res_list:
        df = pd.read_table(fle, sep="\t", index_col=0, header=0)
        full_frame = pd.merge(left=full_frame,
                              right=df,
                              how='inner',
                              left_index=True,
                              right_index=True)

    full_frame.to_csv(outfile, sep="\t")


def treeCutting(infile,
                expression_file,
                cluster_file,
                cluster_algorithm,
                deepsplit=False):
    '''
    Use dynamic tree cutting to derive clusters for each
    resampled distance matrix
    '''
    wgcna_out = "/dev/null"

    E.info("loading distance matrix")

    df = pd.read_table(infile, sep="\t",
                       header=0, index_col=0)
    df = df.fillna(0.0)
    genes = df.index
    genes_r = ro.StrVector([g for g in genes])

    # py2ri requires activation
    pandas2ri.activate()
    rdf = pandas2ri.py2ri(df)

    R.assign("distance_data", rdf)
    R.assign("gene_ids", genes_r)

    R('''sink(file='%(wgcna_out)s')''' % locals())
    R('''suppressPackageStartupMessages(library("WGCNA"))''')
    R('''suppressPackageStartupMessages(library("flashClust"))''')
    E.info("clustering data by %s linkage" % cluster_algorithm)
    R('''rownames(distance_data) <- gene_ids''')
    R('''clustering <- flashClust(as.dist(distance_data),'''
      ''' method='%(cluster_algorithm)s')''' % locals())
    if deepsplit:
        R('''cluster_cut <- cutreeDynamic(dendro=clustering, '''
          '''minClusterSize=50, deepSplit=T)''')
    else:
        R('''cluster_cut <- cutreeDynamic(dendro=clustering, '''
          '''minClusterSize=50, deepSplit=F)''')

    R('''color_cut <- labels2colors(cluster_cut)''')
    R('''write.table(color_cut, file = '%(cluster_file)s','''
      '''sep="\t")''' % locals())
    R('''cluster_matched <- data.frame(cbind(rownames(distance_data),'''
      '''color_cut))''')
    R('''colnames(cluster_matched) = c("gene_id", "cluster")''')
    R('''cluster_matched <- data.frame(cluster_matched$gene_id,'''
      '''cluster_matched$cluster)''')
    R('''sink(file=NULL)''')

    cluster_frame = pandas2ri.ri2py(R["cluster_matched"])
    cluster_frame.columns = ['gene_id', 'cluster']
    cluster_frame.index = cluster_frame['gene_id']
    cluster_frame.drop(['gene_id'], inplace=True, axis=1)

    return cluster_frame


def clusterAverage(file_list):
    '''
    Average distance measures across replicates
    '''

    # map replicate number on to dataframe as identifier
    # used to aggregate over replicates for each gene id
    # assumes filename is condition-reference-replicate

    df_dict = {}
    for fle in file_list:
        f = fle.split("/")[-1]
        rep = f.split("-")[2]
        _df = pd.read_table(fle, sep="\t",
                            header=0, index_col=0)
        df_dict[rep] = _df

    # group concatenated dataframes by gene_id
    concat_df = pd.concat(df_dict)
    group_df = concat_df.groupby(level=1)

    # aggregate/summarise distance metric over replicates for each gene_id
    agg_dict = {}
    for names, groups in group_df:
        agg_dict[names] = np.mean(groups, axis=0)

    agg_df = pd.DataFrame(agg_dict)
    return agg_df


def clusterAgreement(infile):
    '''
    calculate co-occurence of genes within resampled clusters
    '''
    # read in aggregated cluster assignment file, genes as rows,
    # iterations as columns

    df = pd.read_table(infile, sep="\t", header=0, index_col=0)
    genes = df.index.values

    # instantiate an empy matrix to count the number of times each
    # gene appears with others at each iteration

    dmat = pd.DataFrame(index=genes,
                        columns=genes)
    dmat = dmat.fillna(0)

    # generate all pair-wise combinations of genes, index into dmat using these
    reps = df.columns.values

    # alternative, faster code for consensus clustering - might be able to
    # improve it by using less nested for loops

    # generate a set for each cluster that contains the genes belonging to
    # each cluster count the number of times two genes occur in a cluster
    # and add to the dataframe repeat for each resampling iteration
    # cluster sets are generated at every iteration - time improvement over
    # directly accessing and storing as two dataframes is a factor of 7-8

    for i in reps:
        # from the input dataframe generate a list of sets, one set for each
        # cluster in that resampling iteration.
        # repeat for every resampling iteration.

        clusters = set(df[i].values.tolist())
        cluster_dict = {}
        for col in clusters:
            cluster_dict[col] = []
        for gene in genes:
            k_gene = df[i][gene]
            cluster_dict[k_gene].append(gene)
        rep_list = []

        # for each cluster add all the genes with that cluster ID to the set
        # add all of the cluster sets to a list container

        for col in clusters:
            col_set = set()
            clust_col = cluster_dict[col]
            gene_members = itertools.combinations_with_replacement(clust_col,
                                                                   2)
            col_set.add(gene_members)
            rep_list.append(col_set)

        # count if two genes occur in the same cluster, adding to both sides
        # of the symmetrical dataframe

        for cluster_set in rep_list:
            for combs in cluster_set:
                for x in combs:
                    if x[0] == x[1]:
                        dmat[x[0]][x[1]] += 1
                    else:
                        dmat[x[0]][x[1]] += 1
                        dmat[x[1]][x[0]] += 1

    # calculate the proportion of co-occurences

    prob = lambda x: x/float(len(reps))

    probs_df = dmat.applymap(prob)

    return probs_df


def consensusClustering(infile,
                        cutHeight,
                        cluster_algorithm,
                        min_size=30,
                        deepsplit=False):
    '''
    hierachichal clustering based on gene-cluster correlation across
    resampled datasets.  cut tree based with dynamic tree cut
    TODO: change this to cutHeight?  i.e. 0.2 = 80% clustering
    agreement OR use dynamic tree cut without deepsplit.
    '''
    condition = infile.split("/")[1].split("-")[0]
    wgcna_out = "tmp.dir/consensus-WGCNA.out"

    R('''sink(file='%(wgcna_out)s')''' % locals())
    R('''suppressMessages(library("WGCNA"))''')
    R('''suppressMessages(library("flashClust"))''')

    E.info("loading distance matrix")

    df = pd.read_table(infile, sep="\t", header=0, index_col=0)
    labels = df.index.tolist()
    labels_r = ro.StrVector([l for l in labels])

    # py2ri requires activation
    pandas2ri.activate()
    df_r = pandas2ri.py2ri(df)

    R.assign("distance.frame", df_r)
    R.assign("labels", labels_r)

    # large matricies/distance objects may need more
    # memory - allocate 1GB
    R('''memory.limit(10000)''')
    R('''rownames(distance.frame) <- labels''')
    R('''distance_data <- data.matrix(distance.frame)''')

    E.info("clustering data by %s linkage" % cluster_algorithm)

    R('''clustering <- flashClust(as.dist(1-distance_data),'''
      '''method='%(cluster_algorithm)s')''' % locals())

    if cutHeight > float(0.01):
        R('''cluster_cut <- cutreeStatic(dendro=clustering, '''
          '''minSize=%(min_size)i, cutHeight=%(cutHeight)s)''' % locals())

    elif deepsplit:
        R('''cluster_cut <- cutreeDynamic(dendro=clustering, '''
          '''deepSplit=T, minClusterSize=%(min_size)i)''' % locals())
    else:
        R('''cluster_cut <- cutreeDynamic(dendro=clustering, '''
          '''deepSplit=F, minClusterSize=%(min_size)i)''' % locals())

    R('''color_cut <- labels2colors(cluster_cut)''')
    R('''cluster_matched <- data.frame(cbind(rownames(distance_data),'''
      '''color_cut))''')
    R('''colnames(cluster_matched) = c("gene_id", "cluster")''')
    R('''cluster_matched <- data.frame(cluster_matched$gene_id,'''
      '''cluster_matched$cluster)''')

    # plot and save dendrogram of clustering

    # AH: disabled, requires plots.dir to exist which might not be the case
    # AH: and thus causes this method to fail. Path names need to be parameterizable.
    # R('''png("plots.dir/%(condition)s-dendrogram-consensus_clustering.png")'''
    #   % locals())
    # R('''plotDendroAndColors(dendro=clustering, colors=color_cut,'''
    #   '''groupLabels="Dynamic tree cut",'''
    #   '''dendroLabels=F, addGuide=T, guideHang=0.05, '''
    #   '''hang=0.03, main="%(condition)s")''' % locals())
    # R('''dev.off()''')
    # R('''sink(file=NULL)''')
    cluster_frame = pandas2ri.ri2py(R["cluster_matched"])

    return cluster_frame

################################################################################
#
#   MRC FGU Computational Genomics Group
#
#   $Id: pipeline_cpg.py 2900 2011-05-24 14:38:00Z david $
#
#   Copyright (C) 2009 Andreas Heger
#
#   This program is free software; you can redistribute it and/or
#   modify it under the terms of the GNU General Public License
#   as published by the Free Software Foundation; either version 2
#   of the License, or (at your option) any later version.
#
#   This program is distributed in the hope that it will be useful,
#   but WITHOUT ANY WARRANTY; without even the implied warranty of
#   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#   GNU General Public License for more details.
#
#   You should have received a copy of the GNU General Public License
#   along with this program; if not, write to the Free Software
#   Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA  02111-1307, USA.
#################################################################################
"""
==========================================
Read Mapping parameter titration pipeline
==========================================

:Author: David Sims 
:Release: $Id: mapping_titration.py 2900 2011-05-24 14:38:00Z david $
:Date: |today|
:Tags: Python

   * align reads to the genome using a range of different parameters
   * calculate alignment statistics

Requirements
------------

On top of the default CGAT setup, the pipeline requires the following software to be in the 
path:

+--------------------+-------------------+------------------------------------------------+
|*Program*           |*Version*          |*Purpose*                                       |
+--------------------+-------------------+------------------------------------------------+
|bowtie_             |>=0.12.7           |read mapping                                    |
+--------------------+-------------------+------------------------------------------------+


Pipline Output
==============

The results of the computation are all stored in an sqlite relational
database :file:`csvdb`.

Glossary
========

.. glossary::

   bowtie
      bowtie_ - a read mapper

.. _bowtie: http://bowtie-bio.sourceforge.net/index.shtml

Code
====

"""
import sys
import tempfile
import optparse
import shutil
import itertools
import csv
import math
import random
import re
import glob
import os
import shutil
import collections
import CGAT.Experiment as E
import logging as L
from ruffus import *
import csv
import sqlite3
import CGAT.IndexedFasta as IndexedFasta
import CGAT.IndexedGenome as IndexedGenome
import CGAT.FastaIterator as FastaIterator
import CGAT.Genomics as Genomics
import CGAT.IOTools as IOTools
import CGAT.MAST as MAST
import CGAT.GTF as GTF
import CGAT.GFF as GFF
import CGAT.Bed as Bed
import cStringIO
import pysam
import numpy
import gzip
import fileinput
import CGATPipelines.PipelineTracks as PipelineTracks
import CGATPipelines.PipelineMapping as PipelineMapping
from bein.util import *

USECLUSTER = True

###################################################
###################################################
###################################################
## Pipeline configuration
###################################################
import CGAT.Pipeline as P
P.getParameters(  ["%s.ini" % __file__[:-len(".py")],  "../pipeline.ini", "pipeline.ini" ] )
PARAMS = P.PARAMS

bowtie_options = {'n0m1':"-n 0 -a --best --strata -m 1 -3 1",'n1m1':"-n 1 -a --best --strata -m 1 -3 1",'n2m1':"-n 2 -a --best --strata -m 1 -3 1",'n3m1':"-n 3 -a --best --strata -m 1 -3 1",
                  'n0m2':"-n 0 -a --best --strata -m 2 -3 1",'n1m2':"-n 1 -a --best --strata -m 2 -3 1",'n2m2':"-n 2 -a --best --strata -m 2 -3 1",'n3m2':"-n 3 -a --best --strata -m 2 -3 1",
                  'n0m3':"-n 0 -a --best --strata -m 3 -3 1",'n1m3':"-n 1 -a --best --strata -m 3 -3 1",'n2m3':"-n 2 -a --best --strata -m 3 -3 1",'n3m3':"-n 3 -a --best --strata -m 3 -3 1",
                  'n0m4':"-n 0 -a --best --strata -m 4 -3 1",'n1m4':"-n 1 -a --best --strata -m 4 -3 1",'n2m4':"-n 2 -a --best --strata -m 4 -3 1",'n3m4':"-n 3 -a --best --strata -m 4 -3 1",
                  'n0m5':"-n 0 -a --best --strata -m 5 -3 1",'n1m5':"-n 1 -a --best --strata -m 5 -3 1",'n2m5':"-n 2 -a --best --strata -m 5 -3 1",'n3m5':"-n 3 -a --best --strata -m 5 -3 1",
                  'v0m1':"-v 0 -a --best --strata -m 1 -3 1",'v1m1':"-v 1 -a --best --strata -m 1 -3 1",'v2m1':"-v 2 -a --best --strata -m 1 -3 1",'v3m1':"-v 3 -a --best --strata -m 1 -3 1",
                  'v0m2':"-v 0 -a --best --strata -m 2 -3 1",'v1m2':"-v 1 -a --best --strata -m 2 -3 1",'v2m2':"-v 2 -a --best --strata -m 2 -3 1",'v3m2':"-v 3 -a --best --strata -m 2 -3 1",
                  'v0m3':"-v 0 -a --best --strata -m 3 -3 1",'v1m3':"-v 1 -a --best --strata -m 3 -3 1",'v2m3':"-v 2 -a --best --strata -m 3 -3 1",'v3m3':"-v 3 -a --best --strata -m 3 -3 1",
                  'v0m4':"-v 0 -a --best --strata -m 4 -3 1",'v1m4':"-v 1 -a --best --strata -m 4 -3 1",'v2m4':"-v 2 -a --best --strata -m 4 -3 1",'v3m4':"-v 3 -a --best --strata -m 4 -3 1",
                  'v0m5':"-v 0 -a --best --strata -m 5 -3 1",'v1m5':"-v 1 -a --best --strata -m 5 -3 1",'v2m5':"-v 2 -a --best --strata -m 5 -3 1",'v3m5':"-v 3 -a --best --strata -m 5 -3 1"}

###################################################################
###################################################################
###################################################################
## MAP READS
@files( [ (PARAMS["test_file"], "%s.bam" % x, bowtie_options.get(x))  for x in bowtie_options.keys() ] )
def buildBAM( infile, outfile, options ):
    '''map reads with bowtie'''
    to_cluster = True
    job_options= "-pe dedicated %i -R y" % PARAMS["bowtie_threads"]
    m = PipelineMapping.Bowtie()
    reffile = PARAMS["samtools_genome"]
    bowtie_options = options
    statement = m.build( (infile,), outfile ) 
    #print(statement)
    P.run()

#########################################################################
@transform( buildBAM, 
            regex(r"(\S+).bam"),
            r"\1.nsrt.bam" )
def sortByName( infile, outfile ):
    '''Add number of hits tags to sam file'''
    to_cluster = USECLUSTER
    track = P.snip(outfile, ".bam")
    statement = '''samtools sort -n %(infile)s %(track)s;'''
    P.run()

#########################################################################
@transform( sortByName, 
            regex(r"(\S+).nsrt.bam"),
            r"\1.nh.bam" )
def addNHTag( infile, outfile ):
    '''Add number of hits tags to sam file'''
    to_cluster = USECLUSTER

    inf = pysam.Samfile(infile, "rb")
    outf = pysam.Samfile(outfile, "wb", template=inf)
    for readset in read_sets(inf,keep_unmapped=True):
        nh = len(readset)
        for read in readset:
            if (read.is_unmapped):
                nh = 0
            read.tags = read.tags+[("NH",nh)]
            outf.write(read)
    inf.close()
    outf.close()

#########################################################################
@transform( addNHTag, 
            regex(r"(\S+).bam"),
            r"\1.srt.bam" )
def sortByPosition( infile, outfile ):
    '''Add number of hits tags to sam file'''
    to_cluster = USECLUSTER
    track = P.snip(outfile, ".bam")
    statement = '''samtools sort %(infile)s %(track)s;'''
    P.run()

#########################################################################
@transform( sortByPosition,
            regex( r"(\S+).nh.srt.bam"),
            r"\1.dedup.bam")
def dedup(infiles, outfile):
        '''Remove duplicate alignments from BAM files.'''
        to_cluster = USECLUSTER
        track = P.snip( outfile, ".bam" )
        statement = '''MarkDuplicates INPUT=%(infiles)s  ASSUME_SORTED=true OUTPUT=%(outfile)s METRICS_FILE=%(track)s.dupstats VALIDATION_STRINGENCY=SILENT; ''' % locals()
        statement += '''samtools index %(outfile)s; ''' % locals()
        #print statement
        P.run()

#########################################################################
@merge( dedup, "picard_duplicate_stats.load" )
def loadPicardDuplicateStats( infiles, outfile ):
    '''Merge Picard duplicate stats into single table and load into SQLite.'''

    tablename = P.toTable( outfile )

    outf = open('dupstats.txt','w')

    first = True
    for f in infiles:
        track = P.snip( os.path.basename(f), ".dedup.bam" )
        statfile = P.snip(f, ".bam" )  + ".dupstats"
        if not os.path.exists( statfile ): 
            E.warn( "File %s missing" % statfile )
            continue
        lines = [ x for x in open( statfile, "r").readlines() if not x.startswith("#") and x.strip() ]
        if first: outf.write( "%s\t%s" % ("track", lines[0] ) )
        first = False
        outf.write( "%s\t%s" % (track,lines[1] ))

        
    outf.close()
    tmpfilename = outf.name

    statement = '''cat %(tmpfilename)s
                | python %(scriptsdir)s/csv2db.py
                      --index=track
                      --table=%(tablename)s 
                > %(outfile)s
               '''
    P.run()

#########################################################################
@transform( dedup, 
            regex(r"(\S+).dedup.bam"),
            r"\1.readstats" )
def buildBAMStats( infile, outfile ):
    '''Count number of reads mapped, duplicates, etc. '''
    to_cluster = USECLUSTER
    scriptsdir = PARAMS["general_scriptsdir"]
    statement = '''python %(scriptsdir)s/bam2stats.py --force 
                   --output-filename-pattern=%(outfile)s.%%s < %(infile)s > %(outfile)s'''
    P.run()

#########################################################################
@merge( buildBAMStats, "bam_stats.load" )
def loadBAMStats( infiles, outfile ):
    '''Import bam statistics into SQLite'''

    scriptsdir = PARAMS["general_scriptsdir"]
    header = ",".join( [P.snip( os.path.basename(x), ".readstats") for x in infiles] )
    filenames = " ".join( [ "<( cut -f 1,2 < %s)" % x for x in infiles ] )
    tablename = P.toTable( outfile )
    E.info( "loading bam stats - summary" )
    statement = """python %(scriptsdir)s/combine_tables.py
                      --headers=%(header)s
                      --missing=0
                      --ignore-empty
                   %(filenames)s
                | perl -p -e "s/bin/track/"
                | perl -p -e "s/unique/unique_alignments/"
                | python %(scriptsdir)s/table2table.py --transpose
                | python %(scriptsdir)s/csv2db.py
                      --allow-empty
                      --index=track
                      --table=%(tablename)s 
                > %(outfile)s"""
    P.run()

    for suffix in ("nm", "nh"):
        E.info( "loading bam stats - %s" % suffix )
        filenames = " ".join( [ "%s.%s" % (x, suffix) for x in infiles ] )
        tname = "%s_%s" % (tablename, suffix)
        
        statement = """python %(scriptsdir)s/combine_tables.py
                      --header=%(header)s
                      --skip-titles
                      --missing=0
                      --ignore-empty
                   %(filenames)s
                | perl -p -e "s/bin/%(suffix)s/"
                | python %(scriptsdir)s/csv2db.py
                      --table=%(tname)s 
                      --allow-empty
                >> %(outfile)s """
        P.run()



#########################################################################
@transform( dedup, 
            regex( r"(\S+)/bam/(\S+).bam"),
            r"\1/bam/\2.alignstats" )
def buildPicardAlignStats( infile, outfile ):
    '''Gather BAM file alignment statistics using Picard '''
    to_cluster = USECLUSTER
    track = P.snip( os.path.basename(infile), ".bam" )
    statement = '''CollectAlignmentSummaryMetrics INPUT=%(infile)s REFERENCE_SEQUENCE=%%(samtools_genome)s ASSUME_SORTED=true OUTPUT=%(outfile)s VALIDATION_STRINGENCY=SILENT ''' % locals()
    P.run()

############################################################
@merge( buildPicardAlignStats, "picard_align_stats.load" )
def loadPicardAlignStats( infiles, outfile ):
    '''Merge Picard alignment stats into single table and load into SQLite.'''

    tablename = P.toTable( outfile )

    outf = P.getTempFile()

    first = True
    for f in infiles:
        track = P.snip( os.path.basename(f), ".dedup.alignstats" )
        if not os.path.exists( f ): 
            E.warn( "File %s missing" % f )
            continue
        lines = [ x for x in open( f, "r").readlines() if not x.startswith("#") and x.strip() ]
        if first: outf.write( "%s\t%s" % ("track", lines[0] ) )
        first = False
        for i in range(1, len(lines)):
            outf.write( "%s\t%s" % (track,lines[i] ))

        
    outf.close()
    tmpfilename = outf.name

    statement = '''cat %(tmpfilename)s
                | python %(scriptsdir)s/csv2db.py
                      --index=track
                      --table=%(tablename)s 
                > %(outfile)s
               '''
    P.run()

    os.unlink( tmpfilename )


############################################################
############################################################
############################################################
## Pipeline organisation
@follows( buildBAM, sortByName, addNHTag, sortByPosition, dedup, 
          loadPicardDuplicateStats, buildBAMStats, loadBAMStats)
def mapReads():
    '''Align reads to target genome.'''
    pass

@follows( mapReads )
def full():
    '''run the full pipeline.'''
    pass

############################################################
############################################################
############################################################
## REPORTS
@follows( mkdir( "report" ) )
def build_report():
    '''build report from scratch.'''

    E.info( "starting documentation build process from scratch" )
    P.run_report( clean = True )

@follows( mkdir( "report" ) )
def update_report():
    '''update report.'''

    E.info( "updating documentation" )
    P.run_report( clean = False )


if __name__== "__main__":
    sys.exit( P.main(sys.argv) )




# Make a tree from a matrix of RPKM values giving all genes equal weight 
# (requiring at least one lane to be above a certain minimum RPKM cutoff)

from math import log
from commands import getoutput
from string import letters
from random import choice

import os, sys, re, optparse

import Experiment as E

def main( argv = None ):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if not argv: argv = sys.argv

    # setup command line parser
    parser = optparse.OptionParser( version = "%prog version: $Id: script_template.py 2871 2010-03-03 10:20:44Z andreas $", 
                                    usage = globals()["__doc__"] )

    ## add common options (-h/--help, ...) and parse command line 
    (options, args) = E.Start( parser, argv = argv )

    infile = open(args[0], 'r')
    min_rpkm = float(args[1])
    outfile = open(args[2] + '.distance', 'w')

    # try to detect if relative or absolute path
    if argv[1][0] == '/' or argv[1][0] == '~':
        ABS_PATH = True
    else:
        ABS_PATH = False

    # build output matrix
    for line in infile:
        if line.startswith("#"): continue
        if line.startswith("gene_id"): 
            header = line[:-1].split('\t')[2::]
            num_samples=len(header)
            outfile.write("   %s\n" % num_samples)

            # initialize output matrix
            the_matrix=[]
            for i in range(num_samples):
                the_matrix.append([0.0]*num_samples)
            continue

        la = map(float, line.rstrip('\n').split('\t')[2::])
        if max(la) < min_rpkm:
            continue
        la = map(lambda x: x + 0.01, la)    # to handle any zero values, add 0.01 to every RPKM
        avg_rpkm = float(sum(la))/len(la)
        ratios = map(lambda x: log(x/avg_rpkm, 2), la)
        for i in range(num_samples):
            for j in range(num_samples):
                the_matrix[i][j] += abs( ratios[i] - ratios[j] )

    # write distance matrix
    for i in range(num_samples):
        outfile.write( "%-10s" % header[i] )
        for j in range(num_samples):
            outfile.write( ' ' + str( the_matrix[i][j] ) )
        outfile.write( '\n' )
    infile.close(); outfile.close()

    # create tmp directory & work there - different syntax though if absolute vs relative path
    # make commands file for fitch & run
    commands_file = open( argv[3] + '.commands', 'w')
    TMP_DIR = "".join([choice(letters) for x in xrange(10)]); getoutput('mkdir %s' % TMP_DIR)
    if ABS_PATH:
        commands_file.write( '%s\nG\nJ\n23\n5000\nP\n0\n2\nY\n' % (argv[3] + '.distance') )
        commands_file.close()
        getoutput('cd %s; fitch < %s; rm outfile; mv outtree %s; cd ..' % ( TMP_DIR, argv[3] + '.commands', argv[3] ) )
    else:
        commands_file.write( '../%s\nG\nJ\n23\n5000\nP\n0\n2\nY\n' % (argv[3] + '.distance') )
        commands_file.close()
        getoutput('cd %s; fitch < ../%s; rm outfile; mv outtree ../%s; cd ..' % ( TMP_DIR, argv[3] + '.commands', argv[3] ) )
    getoutput('rmdir %s' % TMP_DIR )


    ## write footer and output benchmark information.
    E.Stop()

if __name__ == "__main__":
    sys.exit( main( sys.argv) )


import random
import sys
import gzip
import CGAT.Pipeline as P
import CGAT.Experiment as E

def write_random_records(fqa, fqb, outfa, outfb, N):
    """ get N random headers from a fastq file without reading the
    whole thing into memory"""

    records = sum(1 for _ in gzip.open(fqa)) / 4
    rand_records = sorted([random.randint(0, records - 1) for _ in xrange(N)])

    suba, subb = gzip.open(outfa, "w"), gzip.open(outfb, "w")
    fha, fhb = gzip.open(fqa),  gzip.open(fqb)
    rec_no = - 1
    
    for rr in rand_records:

        while rec_no < rr:
            rec_no += 1       
            for i in range(4): fha.readline()
            for i in range(4): fhb.readline()
        for i in range(4):
            suba.write(fha.readline())
            subb.write(fhb.readline())
        rec_no += 1 # (thanks @anderwo)

    print >>sys.stderr, "wrote to %s, %s" % (suba.name, subb.name)

if __name__ == "__main__":
    assert int(sys.argv[5]), "not a valid number to subsample"
    write_random_records(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], int(sys.argv[5]))

'''bam2stats.py - compute stats from a bam-file
===============================================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Genomics NGS Summary BAM

Purpose
-------

This script takes a bam file as input and computes a few metrics by
iterating over the file. The metrics output are:

+------------------------+------------------------------------------+
|*Category*              |*Content*                                 |
+------------------------+------------------------------------------+
|total                   |total number of alignments in bam file    |
+------------------------+------------------------------------------+
|alignments_mapped       |alignments mapped to a chromosome (bam    |
|                        |flag)                                     |
+------------------------+------------------------------------------+
|alignments_unmapped     |alignments unmapped (bam flag)            |
+------------------------+------------------------------------------+
|qc_fail                 |alignments failing QC (bam flag)          |
+------------------------+------------------------------------------+
|mate_unmapped           |alignments in which the mate is unmapped  |
|                        |(bam flag)                                |
+------------------------+------------------------------------------+
|reverse                 |alignments in which read maps to reverse  |
|                        |strand (bam flag)                         |
+------------------------+------------------------------------------+
|mate_reverse            |alignments in which mate maps to reverse  |
|                        |strand (bam flag)                         |
+------------------------+------------------------------------------+
|proper_pair             |alignments in which both pairs have been  |
|                        |mapped properly (according to the mapper) |
|                        |(bam flag)                                |
+------------------------+------------------------------------------+
|read1                   |alignments for 1st read of pair (bam flag)|
+------------------------+------------------------------------------+
|paired                  |alignments of reads that are paired (bam  |
|                        |flag)                                     |
+------------------------+------------------------------------------+
|duplicate               |read is PCR or optical duplicate (bam     |
|                        |flag)                                     |
+------------------------+------------------------------------------+
|read2                   |alignment is for 2nd read of pair (bam    |
|                        |flag)                                     |
+------------------------+------------------------------------------+
|secondary               |alignment is not primary alignment        |
+------------------------+------------------------------------------+
|alignments_rna          |alignments mapping to regions specified in|
|                        |a .gff file                               |
+------------------------+------------------------------------------+
|alignments_no_rna       |alignments not mapping to regions in a    |
|                        |.gff file (if --ignore-masked-reads has   |
|                        |been set, otherwise equal to mapped)      |
+------------------------+------------------------------------------+
|alignments_duplicates   |number of alignments mapping to the same  |
|                        |location                                  |
+------------------------+------------------------------------------+
|alignments_unique       |number of alignments mapping to unique    |
|                        |locations                                 |
+------------------------+------------------------------------------+
|reads_total             |number of reads in file. Either given via |
|                        |--num-reads or deduc ed as the sum of     |
|                        |mappend and unmapped reads                |
+------------------------+------------------------------------------+
|reads_mapped            |number of reads mapping in file. Derived  |
|                        |from the total number o f alignments and  |
|                        |removing counts for multiple              |
|                        |matches. Requires the NH flag to be set   |
|                        |correctly.                                |
+------------------------+------------------------------------------+
|reads_unmapped          |number of reads unmapped in file. Assumes |
|                        |that there is only one                    |
|                        |entry per unmapped read.                  |
+------------------------+------------------------------------------+
|reads_missing           |number of reads missing, if number of     |
|                        |reads given by --input-rea ds. Otherwise  |
|                        |0.                                        |
+------------------------+------------------------------------------+
|reads_norna             |reads not mapping to repetetive RNA       |
|                        |regions.                                  |
+------------------------+------------------------------------------+
|pairs_total             |number of total pairs - this is the number|
|                        |of reads_total divided by two. If there   |
|                        |were no pairs, pairs_total will be 0.     |
+------------------------+------------------------------------------+
|pairs_mapped            |number of mapped pairs - this is the same |
|                        |as the number of proper pairs.            |
+------------------------+------------------------------------------+

Additionally, the script outputs histograms for the following tags and
scores.

* NM: number of mismatches in alignments.
* NH: number of hits of reads.
* mapq: mapping quality of alignments.

Supplying a fastq file
++++++++++++++++++++++

If a fastq file is supplied (``--fastq-file``), the script will
compute some additional summary statistics. However, as it builds a dictionary
of all sequences, it will also require a good  amount of memory. The additional
metrics output are:

+-----------------------------+----------------------------------------+
|*Category*                   |*Content*                               |
+-----------------------------+----------------------------------------+
|pairs_total                  |total number of pairs in input data     |
+-----------------------------+----------------------------------------+
|pairs_mapped                 |pairs in which both reads map           |
+-----------------------------+----------------------------------------+
|pairs_unmapped               |pairs in which neither read maps        |
+-----------------------------+----------------------------------------+
|pairs_proper_unique          |pairs which are proper and map uniquely.|
+-----------------------------+----------------------------------------+
|pairs_incomplete_unique      |pairs in which one of the reads maps    |
|                             |uniquely, but the other does not map.   |
+-----------------------------+----------------------------------------+
|pairs_incomplete_multimapping|pairs in which one of the reads maps    |
|                             |uniquely, but the other maps to multiple|
|                             |locations.                              |
+-----------------------------+----------------------------------------+
|pairs_proper_duplicate       |pairs which are proper and unique, but  |
|                             |marked as duplicates.                   |
+-----------------------------+----------------------------------------+
|pairs_proper_multimapping    |pairs which are proper, but map to      |
|                             |multiple locations.                     |
+-----------------------------+----------------------------------------+
|pairs_not_proper_unique      |pairs mapping uniquely, but not flagged |
|                             |as proper                               |
+-----------------------------+----------------------------------------+
|pairs_other                  |pairs not in any of the above categories|
+-----------------------------+----------------------------------------+

Note that for paired-end data, any ``\1`` or ``/2`` suffixes will be
removed from the read name in the assumption that these have been removed
in the bam file as well.

Usage
-----

Example::

   python bam2stats.py in.bam

This command will generate various statistics based on the supplied
BAM file, such as percentage reads mapped and percentage reads mapped
in pairs. The output looks like this:

+-----------------------------+------+-------+-----------------+
|category                     |counts|percent|of               |
+-----------------------------+------+-------+-----------------+
|alignments_total             |32018 |100.00 |alignments_total |
+-----------------------------+------+-------+-----------------+
|alignments_mapped            |32018 |100.00 |alignments_total |
+-----------------------------+------+-------+-----------------+
|alignments_unmapped          |0     | 0.00  |alignments_total |
+-----------------------------+------+-------+-----------------+
|alignments_qc_fail           |0     | 0.00  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_mate_unmapped     |241   | 0.75  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_reverse           |16016 |50.02  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_mate_reverse      |15893 |49.64  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_proper_pair       |30865 |96.40  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_read1             |16057 |50.15  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_paired            |32018 |100.00 |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_duplicate         |0     | 0.00  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_read2             |15961 |49.85  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_secondary         |0     | 0.00  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_rna               |68    | 0.21  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_no_rna            |31950 |99.79  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|alignments_filtered          |31950 |99.79  |alignments_mapped|
+-----------------------------+------+-------+-----------------+
|reads_total                  |34250 |100.00 |reads_total      |
+-----------------------------+------+-------+-----------------+
|reads_unmapped               |0     | 0.00  |reads_total      |
+-----------------------------+------+-------+-----------------+
|reads_mapped                 |32018 |93.48  |reads_total      |
+-----------------------------+------+-------+-----------------+
|reads_missing                |2232  | 6.52  |reads_total      |
+-----------------------------+------+-------+-----------------+
|reads_mapped_unique          |32018 |100.00 |reads_mapped     |
+-----------------------------+------+-------+-----------------+
|reads_multimapping           |0     | 0.00  |reads_mapped     |
+-----------------------------+------+-------+-----------------+
|pairs_total                  |17125 |100.00 |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_mapped                 |17125 |100.00 |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_unmapped               |0     | 0.00  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_proper_unique          |14880 |86.89  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_incomplete_unique      |2232  |13.03  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_incomplete_multimapping|0     | 0.00  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_proper_duplicate       |0     | 0.00  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_proper_multimapping    |0     | 0.00  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_not_proper_unique      |13    | 0.08  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|pairs_other                  |0     | 0.00  |pairs_total      |
+-----------------------------+------+-------+-----------------+
|read1_total                  |17125 |100.00 |read1_total      |
+-----------------------------+------+-------+-----------------+
|read1_unmapped               |0     | 0.00  |read1_total      |
+-----------------------------+------+-------+-----------------+
|read1_mapped                 |16057 |93.76  |read1_total      |
+-----------------------------+------+-------+-----------------+
|read1_mapped_unique          |16057 |100.00 |read1_mapped     |
+-----------------------------+------+-------+-----------------+
|reads_multimapping           |0     | 0.00  |read1_mapped     |
+-----------------------------+------+-------+-----------------+
|read1_missing                |1068  | 6.65  |read1_total      |
+-----------------------------+------+-------+-----------------+
|read2_total                  |17125 |100.00 |read2_total      |
+-----------------------------+------+-------+-----------------+
|read2_unmapped               |0     | 0.00  |read2_total      |
+-----------------------------+------+-------+-----------------+
|read2_mapped                 |15961 |93.20  |read2_total      |
+-----------------------------+------+-------+-----------------+
|read2_mapped_unique          |15961 |100.00 |read2_mapped     |
+-----------------------------+------+-------+-----------------+
|reads_multimapping           |0     | 0.00  |read2_mapped     |
+-----------------------------+------+-------+-----------------+
|read2_missing                |1164  | 7.29  |read2_total      |
+-----------------------------+------+-------+-----------------+

The first column contains the caterogy, the second the number of
counts and the third a percentage. The fourth column denotes the
denomiminator that was used to compute the percentage. In the table
above, wee see that 16,057 first reads in a pair map and 15,961
second reads in pair map, resulting in 14,880 proper uniquely mapped
pairs.

Type::

   cgat bam2stats --help

for command line help.

Bam2stats can read from standard input::

   cat in.bam | python bam2stats.py -


Documentation
-------------

Reads are not counted via read name, but making use of NH and HI flags
when present.  To recap, NH is the number of reported alignments that
contain the query in the current record, while HI is the hit index and
ranges from 0 to NH-1.

Unfortunately, not all aligners follow this convention. For example,
gsnap seems to set NH to the number of reportable alignments, while
the actual number of reported alignments in the file is less. Thus, if
the HI flag is present, the maximum HI is used to correct the NH
flag. The assumption is, that the same reporting threshold has been
used for all alignments.

If no NH flag is present, it is assumed that all reads have only been
reported once.

Multi-matching counts after filtering are really guesswork. Basically,
the assumption is that filtering is consistent and will tend to remove
all alignments of a query.

Command line options
--------------------

'''

import os
import sys
import CGAT.Experiment as E
import CGAT.IOTools as IOTools
import CGAT.GTF as GTF
import pysam

try:
    import pyximport
    pyximport.install(build_in_temp=False)
    import _bam2stats
except ImportError:
    import CGAT._bam2stats as _bam2stats

FLAGS = {
    1: 'paired',
    2: 'proper_pair',
    4: 'unmapped',
    8: 'mate_unmapped',
    16: 'reverse',
    32: 'mate_reverse',
    64: 'read1',
    128: 'read2',
    256: 'secondary',
    512: 'qc_fail',
    1024: 'duplicate',
}


def computeMappedReadsFromAlignments(total_alignments, nh, max_hi):
    '''compute number of reads alignment from total number of alignments.
    '''
    nreads_mapped = total_alignments
    if len(nh) > 0:
        max_nh = max(nh.keys())
        if max_hi > 0:
            for x in xrange(2, min(max_nh + 1, max_hi)):
                nreads_mapped -= (nh[x] / x) * (x - 1)
            for x in xrange(max_hi, max_nh + 1):
                nreads_mapped -= (nh[x] / max_hi) * (max_hi - 1)
        else:
            for x in xrange(2, max(nh.keys()) + 1):
                nreads_mapped -= (nh[x] / x) * (x - 1)

    return nreads_mapped


def writeNH(outfile, nh, max_hi):
    '''output nh array, correcting for max_hi if less than nh'''

    # need to remove double counting
    # one read matching to 2 positions is only 2

    max_nh = max(nh.keys())
    if max_hi > 0:
        for x in xrange(1, min(max_nh + 1, max_hi)):
            if nh[x] == 0:
                continue
            outfile.write("%i\t%i\n" % (x, nh[x] / x))
        for x in xrange(max_hi, max_nh + 1):
            if nh[x] == 0:
                continue
            outfile.write("%i\t%i\n" % (x, nh[x] / max_hi))
    else:
        for x in xrange(1, max_nh + 1):
            if nh[x] == 0:
                continue
            outfile.write("%i\t%i\n" % (x, nh[x] / x))


def main(argv=None):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if not argv:
        argv = sys.argv

    # setup command line parser
    parser = E.OptionParser(version="%prog version: $Id$",
                            usage=globals()["__doc__"])

    parser.add_option(
        "-r", "--mask-bed-file", dest="filename_rna", type="string",
        metavar='GFF',
        help="gff formatted file with masking locations. The number of "
        "reads overlapping the intervals in the given file will be "
        "computed. Note that the computation currently does not take "
        "into account indels, so it is an approximate count only. "
        "[%default]")

    parser.add_option(
        "-f", "--ignore-masked-reads", dest="remove_rna", action="store_true",
        help="as well as counting reads in the file given by --mask-bed-file, "
        "also remove these reads for duplicate and match statistics. "
        "[%default]")

    parser.add_option(
        "-i", "--num-reads", dest="input_reads", type="int",
        help="the number of reads - if given, used to provide percentages "
        "[%default]")

    parser.add_option(
        "-d", "--output-details", dest="output_details", action="store_true",
        help="output per-read details into a separate file. Read names are "
        "md5/base64 encoded [%default]")

    parser.add_option(
        "-q", "--fastq-file", dest="filename_fastq",
        help="filename with sequences and quality scores. This file is only "
        "used to collect sequence identifiers. Thus, for paired end data a "
        "single file is sufficient [%default]")

    parser.set_defaults(
        filename_rna=None,
        remove_rna=False,
        input_reads=0,
        force_output=False,
        filename_fastq=None,
        output_details=False,
    )

    # add common options (-h/--help, ...) and parse command line
    (options, args) = E.Start(parser, argv=argv, add_output_options=True)

    if options.filename_rna:
        rna = GTF.readAndIndex(
            GTF.iterator(IOTools.openFile(options.filename_rna)))
    else:
        rna = None

    if len(args) > 0:
        pysam_in = pysam.AlignmentFile(args[0], "rb")
    elif options.stdin == sys.stdin:
        pysam_in = pysam.AlignmentFile("-", "rb")
    else:
        pysam_in = pysam.AlignmentFile(options.stdin, "rb")

    if options.output_details:
        outfile_details = E.openOutputFile("details", "w")
    else:
        outfile_details = None

    if options.filename_fastq and not os.path.exists(options.filename_fastq):
        raise IOError("file %s does not exist" % options.filename_fastq)

    (counter, flags_counts, nh_filtered, nh_all,
     nm_filtered, nm_all, mapq, mapq_all, max_hi) = \
        _bam2stats.count(pysam_in,
                         options.remove_rna,
                         rna,
                         filename_fastq=options.filename_fastq,
                         outfile_details=outfile_details)

    if max_hi > 0 and max_hi != max(nh_all.keys()):
        E.warn("max_hi(%i) is inconsistent with max_nh (%i) "
               "- counts will be corrected"
               % (max_hi, max(nh_all.keys())))

    outs = options.stdout
    outs.write("category\tcounts\tpercent\tof\n")

    def _write(outs, text, numerator, denominator, base):
        percent = IOTools.prettyPercent(numerator, denominator)
        outs.write('%s\t%i\t%s\t%s\n' % (text,
                                         numerator,
                                         percent,
                                         base))

    ###############################
    ###############################
    ###############################
    # Output alignment information
    ###############################
    nalignments_unmapped = flags_counts["unmapped"]
    nalignments_mapped = counter.alignments_input - nalignments_unmapped

    _write(outs,
           "alignments_total",
           counter.alignments_input,
           counter.alignments_input,
           "alignments_total")

    if counter.alignments_input == 0:
        E.warn("no alignments in BAM file - no further output")
        E.Stop()
        return

    _write(outs,
           "alignments_mapped",
           nalignments_mapped,
           counter.alignments_input,
           'alignments_total')
    _write(outs,
           "alignments_unmapped",
           nalignments_unmapped,
           counter.alignments_input,
           'alignments_total')

    if nalignments_mapped == 0:
        E.warn("no mapped alignments - no further output")
        E.Stop()
        return

    for flag, counts in flags_counts.iteritems():
        if flag == "unmapped":
            continue
        _write(outs,
               'alignments_' + flag,
               counts,
               nalignments_mapped,
               'alignments_mapped')

    if options.filename_rna:
        _write(outs,
               "alignments_rna",
               counter.alignments_rna,
               nalignments_mapped,
               'alignments_mapped')
        _write(outs,
               "alignments_no_rna",
               counter.alignments_no_rna,
               nalignments_mapped,
               'alignments_mapped')

    _write(outs,
           "alignments_filtered",
           counter.alignments_filtered,
           nalignments_mapped,
           "alignments_mapped")

    if counter.filtered == nalignments_mapped:
        normby = "alignments_mapped"
    else:
        normby = "alignments_filtered"

    if counter.filtered > 0:
        _write(outs,
               "alignments_duplicates",
               counter.alignments_duplicates,
               counter.alignments_filtered,
               normby)
        _write(outs,
               "alignments_unique",
               counter.aligmnments_filtered - counter.alignments_duplicates,
               counter.alignments_filtered,
               normby)

    ###############################
    ###############################
    ###############################
    # Output read based information
    ###############################

    # derive the number of mapped reads in file from alignment counts
    if options.filename_fastq:
        nreads_total = counter.total_read
        _write(outs,
               "reads_total",
               counter.total_read,
               nreads_total,
               'reads_total')
        _write(outs,
               "reads_unmapped",
               counter.total_read_is_unmapped,
               nreads_total,
               'reads_total')
        _write(outs,
               "reads_mapped",
               counter.total_read_is_mapped,
               nreads_total,
               'reads_total')
        _write(outs,
               "reads_missing",
               counter.total_read_is_missing,
               nreads_total,
               'reads_total')
        _write(outs,
               "reads_mapped_unique",
               counter.total_read_is_mapped_uniq,
               counter.total_read_is_mapped,
               'reads_mapped')
        _write(outs,
               "reads_multimapping",
               counter.total_read_is_mmap,
               counter.total_read_is_mapped,
               'reads_mapped')
    else:
        E.warn('inferring read counts from alignments and NH tags')
        nreads_unmapped = flags_counts["unmapped"]
        nreads_mapped = computeMappedReadsFromAlignments(nalignments_mapped,
                                                         nh_all, max_hi)

        nreads_missing = 0
        if options.input_reads:
            nreads_total = options.input_reads
            # unmapped reads in bam file?
            if nreads_unmapped:
                nreads_missing = nreads_total - nreads_unmapped - nreads_mapped
            else:
                nreads_unmapped = nreads_total - nreads_mapped

        elif nreads_unmapped:
            # if unmapped reads are in bam file, take those
            nreads_total = nreads_mapped + nreads_unmapped
        else:
            # otherwise normalize by mapped reads
            nreads_unmapped = 0
            nreads_total = nreads_mapped

        outs.write("reads_total\t%i\t%5.2f\treads_total\n" %
                   (nreads_total, 100.0))
        outs.write("reads_mapped\t%i\t%5.2f\treads_total\n" %
                   (nreads_mapped, 100.0 * nreads_mapped / nreads_total))
        outs.write("reads_unmapped\t%i\t%5.2f\treads_total\n" %
                   (nreads_unmapped, 100.0 * nreads_unmapped / nreads_total))
        outs.write("reads_missing\t%i\t%5.2f\treads_total\n" %
                   (nreads_missing, 100.0 * nreads_missing / nreads_total))

        if len(nh_all) > 1:
            outs.write("reads_unique\t%i\t%5.2f\treads_mapped\n" %
                       (nh_all[1], 100.0 * nh_all[1] / nreads_mapped))

        # compute after filtering
        # not that these are rough guesses
        if options.filename_rna:
            nreads_norna = computeMappedReadsFromAlignments(
                counter.filtered, nh_filtered, max_hi)
            _write(outs,
                   "reads_norna",
                   nreads_norna,
                   nreads_mapped,
                   "reads_mapped")
            if len(nh_filtered) > 1:
                _write(outs,
                       "reads_norna_unique",
                       nh_filtered[1],
                       nreads_norna,
                       "reads_mapped")

    pysam_in.close()

    ###############################
    ###############################
    ###############################
    # Output pair information
    ###############################
    if flags_counts["read2"] > 0:
        if options.filename_fastq:
            pairs_mapped = counter.total_pair_is_mapped

            # sanity check
            assert counter.total_pair_is_mapped == \
                (counter.total_pair_is_proper_uniq +
                 counter.total_pair_is_incomplete_uniq +
                 counter.total_pair_is_incomplete_mmap +
                 counter.total_pair_is_proper_duplicate +
                 counter.total_pair_is_proper_mmap +
                 counter.total_pair_not_proper_uniq +
                 counter.total_pair_is_other)

            outs.write("pairs_total\t%i\t%5.2f\tpairs_total\n" %
                       (counter.total_pairs,
                        100.0 * counter.total_pairs / counter.total_pairs))
            outs.write("pairs_mapped\t%i\t%5.2f\tpairs_total\n" %
                       (pairs_mapped,
                        100.0 * pairs_mapped / counter.total_pairs))
            outs.write(
                "pairs_unmapped\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_unmapped,
                 100.0 * counter.total_pair_is_unmapped / counter.total_pairs))
            outs.write(
                "pairs_proper_unique\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_proper_uniq,
                 100.0 * counter.total_pair_is_proper_uniq /
                 counter.total_pairs))
            outs.write(
                "pairs_incomplete_unique\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_incomplete_uniq,
                 100.0 * counter.total_pair_is_incomplete_uniq /
                 counter.total_pairs))
            outs.write(
                "pairs_incomplete_multimapping\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_incomplete_mmap,
                 100.0 * counter.total_pair_is_incomplete_mmap /
                 counter.total_pairs))
            outs.write(
                "pairs_proper_duplicate\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_proper_duplicate,
                 100.0 * counter.total_pair_is_proper_duplicate /
                 counter.total_pairs))
            outs.write(
                "pairs_proper_multimapping\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_proper_mmap,
                 100.0 * counter.total_pair_is_proper_mmap /
                 counter.total_pairs))
            outs.write(
                "pairs_not_proper_unique\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_not_proper_uniq,
                 100.0 * counter.total_pair_not_proper_uniq /
                 counter.total_pairs))
            outs.write(
                "pairs_other\t%i\t%5.2f\tpairs_total\n" %
                (counter.total_pair_is_other,
                 100.0 * counter.total_pair_is_other /
                 counter.total_pairs))

            nread1_total = counter.total_read1
            _write(outs,
                   "read1_total",
                   counter.total_read1,
                   nread1_total,
                   'read1_total')
            _write(outs,
                   "read1_unmapped",
                   counter.total_read1_is_unmapped,
                   nread1_total,
                   'read1_total')
            _write(outs,
                   "read1_mapped",
                   counter.total_read1_is_mapped,
                   nread1_total,
                   'read1_total')
            _write(outs,
                   "read1_mapped_unique",
                   counter.total_read1_is_mapped_uniq,
                   counter.total_read1_is_mapped,
                   'read1_mapped')
            _write(outs,
                   "reads_multimapping",
                   counter.total_read1_is_mmap,
                   counter.total_read1_is_mapped,
                   'read1_mapped')
            _write(outs,
                   "read1_missing",
                   counter.total_read1_is_missing,
                   counter.total_read1_is_mapped,
                   'read1_total')

            nread2_total = counter.total_read2
            _write(outs,
                   "read2_total",
                   counter.total_read2,
                   nread2_total,
                   'read2_total')
            _write(outs,
                   "read2_unmapped",
                   counter.total_read2_is_unmapped,
                   nread2_total,
                   'read2_total')
            _write(outs,
                   "read2_mapped",
                   counter.total_read2_is_mapped,
                   nread2_total,
                   'read2_total')
            _write(outs,
                   "read2_mapped_unique",
                   counter.total_read2_is_mapped_uniq,
                   counter.total_read2_is_mapped,
                   'read2_mapped')
            _write(outs,
                   "reads_multimapping",
                   counter.total_read2_is_mmap,
                   counter.total_read2_is_mapped,
                   'read2_mapped')
            _write(outs,
                   "read2_missing",
                   counter.total_read2_is_missing,
                   counter.total_read2_is_mapped,
                   'read2_total')

        else:
            # approximate counts
            pairs_total = nreads_total // 2
            pairs_mapped = flags_counts["proper_pair"] // 2
            _write(outs,
                   "pairs_total",
                   pairs_total,
                   pairs_total,
                   "pairs_total")
            _write(outs,
                   "pairs_mapped",
                   pairs_mapped,
                   pairs_total,
                   "pairs_total")
    else:
        # no paired end data
        pairs_total = pairs_mapped = 0
        outs.write("pairs_total\t%i\t%5.2f\tpairs_total\n" %
                   (pairs_total, 0.0))
        outs.write("pairs_mapped\t%i\t%5.2f\tpairs_total\n" %
                   (pairs_mapped, 0.0))

    if options.force_output or len(nm_filtered) > 0:
        outfile = E.openOutputFile("nm", "w")
        outfile.write("NM\talignments\n")
        if len(nm_filtered) > 0:
            for x in xrange(0, max(nm_filtered.keys()) + 1):
                outfile.write("%i\t%i\n" % (x, nm_filtered[x]))
        else:
            outfile.write("0\t%i\n" % (counter.filtered))
        outfile.close()

    if options.force_output or len(nh_all) > 1:
        outfile = E.openOutputFile("nh_all", "w")
        outfile.write("NH\treads\n")
        if len(nh_all) > 0:
            writeNH(outfile, nh_all, max_hi)
        else:
            # assume all are unique if NH flag not set
            outfile.write("1\t%i\n" % (counter.mapped_reads))
        outfile.close()

    if options.force_output or len(nh_filtered) > 1:
        outfile = E.openOutputFile("nh", "w")
        outfile.write("NH\treads\n")
        if len(nh_filtered) > 0:
            writeNH(outfile, nh_filtered, max_hi)
        else:
            # assume all are unique if NH flag not set
            outfile.write("1\t%i\n" % (counter.filtered))
        outfile.close()

    if options.force_output or len(mapq_all) > 1:
        outfile = E.openOutputFile("mapq", "w")
        outfile.write("mapq\tall_reads\tfiltered_reads\n")
        for x in xrange(0, max(mapq_all.keys()) + 1):
            outfile.write("%i\t%i\t%i\n" % (x, mapq_all[x], mapq[x]))
        outfile.close()

    # write footer and output benchmark information.
    E.Stop()

if __name__ == "__main__":
    sys.exit(main(sys.argv))

'''
cgat_fasta2cDNA.py - template for CGAT scripts
====================================================

:Author:
:Release: $Id$
:Date: |today|
:Tags: Python

Purpose
-------

.. Mike transcript processing - converting multi-fasta of exon
features into a multi-fasta of spliced cDNAs/RNAs

Usage
-----

.. Example use case

Example::

   python cgat_fasta2cDNA.py

Type::

   python cgat_fasta2cDNA.py --help

for command line help.

Command line options
--------------------

'''

import sys
import CGAT.Experiment as E
import CGAT.IOTools as IOTools


def makeSplicedFasta(infile):
    '''
    Merge fasta sequences together into a single
    spliced transcript sequence
    '''

    fasta_dict = {}
    with IOTools.openFile(infile, "rb") as fafile:
        for line in fafile.readlines():
            if line[0] == '>':
                header = line.rstrip("\n")
                fasta_dict[header] = ''
            else:
                fasta_dict[header] += line.rstrip("\n")

    for key, value in fasta_dict.items():
        yield "%s\n%s\n" % (key, value)


def main(argv=None):
    """script main.
    parses command line options in sys.argv, unless *argv* is given.
    """

    if argv is None:
        argv = sys.argv

    # setup command line parser
    parser = E.OptionParser(version="%prog version: $Id$",
                            usage=globals()["__doc__"])

    # add common options (-h/--help, ...) and parse command line
    (options, args) = E.Start(parser, argv=argv)

    infile = argv[-1]
    for record in makeSplicedFasta(infile):
        options.stdout.write(record)

    # write footer and output benchmark information.
    E.Stop()

if __name__ == "__main__":
    sys.exit(main(sys.argv))

'''
fasta2distances.py - analyze pairs of sequences
===============================================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

Purpose
-------

This script computes various distances between sequences.

Usage
-----

Example::

   python fasta2distances.py --help

Type::

   python fasta2distances.py --help

for command line help.

Command line options
--------------------

'''
import sys
import math
import CGAT.Experiment as E
import CGAT.Genomics as Genomics
import CGAT.FastaIterator as FastaIterator


def FilterAlignedPairForPositions(seq1, seq2, method):
    """given the method, return a set of aligned sequences
    only containing certain positions.

    Available filters:
    all:        do nothing.
    codon1,codon2,codon3: return 1st, 2nd, 3rd codon positions only.
    d4: only changes within fourfold-degenerate sites
    """

    l1 = len(seq1)
    l2 = len(seq2)

    if method == "all":
        return seq1, seq2
    elif method == "codon1":
        return ("".join([seq1[x] for x in range(0, l1, 3)]),
                "".join([seq2[x] for x in range(0, l2, 3)]))
    elif method == "codon2":
        return ("".join([seq1[x] for x in range(1, l1, 3)]),
                "".join([seq2[x] for x in range(1, l2, 3)]))
    elif method == "codon3":
        return ("".join([seq1[x] for x in range(2, l1, 3)]),
                "".join([seq2[x] for x in range(2, l2, 3)]))
    elif method == "d4":
        s1 = []
        s2 = []
        for x in range(0, l1, 3):
            codon1 = seq1[x:x + 3]
            codon2 = seq2[x:x + 3]
            try:
                aa1, deg11, deg12, deg13 = Genomics.GetDegeneracy(codon1)
                aa2, deg11, deg22, deg23 = Genomics.GetDegeneracy(codon2)
            except KeyError:
                continue
            if aa1 == aa2 and deg13 == 4 and deg23 == 4:
                s1.append(codon1[2])
                s2.append(codon2[2])
        return "".join(s1), "".join(s2)


# ------------------------------------------------------------------------
def CalculateDistanceJC69(info, do_gamma=False, alpha=None):
    """return Jukes-Cantor distance.
    """
    try:
        p = float(info.mNDifferent) / info.mNAligned

        if do_gamma:
            # not done yet
            distance = 0.75 * alpha * (pow(1 - 4 * p / 3, -1 / alpha) - 1)
            variance = p * (1 - p) / (pow(1 - 4 * p / 3, -2 / (alpha + 1)) * L)
        else:
            distance = -0.75 * math.log(1.0 - 4.0 * p / 3.0)
            variance = p * (1.0 - p) / \
                (math.pow(1.0 - 4.0 * p / 3, 2.0) * info.mNAligned)
    except:
        raise ValueError

    return distance, variance

# ------------------------------------------------------------------------


def CalculateDistanceT92(info):
    """
    P,Q: transitions, transversions frequencies
    q: G+C content

    d = -2q(1 - q)loge(1 - P/[2q(1 - q)] - Q) -[1 -2q(1 -q)]loge(1 - 2Q)/2,(4.18)
    V(d) = [c12P + c32Q - (c1P + c3Q)2]/n,(4.19)
    where c1 = 1/(1 - P/[2q(1 - q)] - Q), c2 = 1/(1 - 2Q), c3 = 2q(1 - q)(c1 - c2) + c2, and q is the G+C content

    Note: result is undefined if
        the number of transversions is >= 0.5
        the G+C content is 0

    raises ValueErrors for undefined results
    """
    gc = info.getGCContent()

    # if there are no GC or no AT pairs: result is undefined
    if gc == 0 or gc == 1:
        raise ValueError

    wg = 2.0 * gc * (1.0 - gc)

    P = float(info.mNTransitions) / info.mNAligned
    Q = float(info.mNTransversions) / info.mNAligned

    a1 = 1.0 - P / wg - Q
    if a1 <= 0:
        raise ValueError

    a2 = 1.0 - 2.0 * Q
    if a2 <= 0:
        raise ValueError

    # print a1, a2, wg, gc, "p=", P, "q=", Q, str(info)

    distance = -wg * math.log(a1) - 0.5 * (1.0 - wg) * math.log(a2)

    c1 = 1 / a1
    c2 = 1 / a2
    c3 = wg * (c1 - c2) + c2

    variance = (
        c1 * c1 * P + c3 * c3 * Q - math.pow(c1 * P + c3 * Q, 2.0)) / info.mNAligned

    return distance, variance

# ------------------------------------------------------------------------


def main(argv=None):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if argv is None:
        argv = sys.argv

    parser = E.OptionParser(
        version="%prog version: $Id: fasta2distances.py 2781 2009-09-10 11:33:14Z andreas $")

    parser.add_option("--filters", dest="filters", type="string",
                      help="Filters to use for filtering sequences [all|codon1|codon2|codon3|d4].")
    parser.add_option("--fields", dest="fields", type="string",
                      help="Fields to output [aligned|nunaligned1|nunaligned2|identical|transitions|transversions|jc69|t92].")

    parser.set_defaults(
        filename_map=None,
        filters="all,codon1,codon2,codon3,d4",
        gap_char="-",
        fields="aligned,unaligned1,unaligned2,identical,transitions,transversions,jc69,t92",
    )

    (options, args) = E.Start(parser, add_pipe_options=True)

    options.filters = options.filters.split(",")
    options.fields = options.fields.split(",")

    iterator = FastaIterator.FastaIterator(options.stdin)

    headers = ["id1", "id2"]
    for f in options.filters:
        headers += list(map(lambda x: "%s_%s" % (f, x), options.fields))

    options.stdout.write("\t".join(headers) + "\n")

    while 1:
        try:
            cur_record = iterator.next()
            if cur_record is None:
                break
            first_record = cur_record
            cur_record = iterator.next()
            if cur_record is None:
                break
            second_record = cur_record

        except StopIteration:
            break

        if len(first_record.sequence) != len(second_record.sequence):
            raise "sequences %s and %s of unequal length" % (
                first_record.title, second_record.title)

        if len(first_record.sequence) % 3 != 0:
            raise "sequence %s not multiple of 3" % first_record.title

        # old: Bio.Alphabet.IUPAC.extended_dna.letters
        alphabet = "ACGT" + options.gap_char

        result = []
        for f in options.filters:

            s1, s2 = FilterAlignedPairForPositions(first_record.sequence,
                                                   second_record.sequence,
                                                   f)

            info = Genomics.CalculatePairIndices(s1, s2, options.gap_char)

            for field in options.fields:

                if field == "aligned":
                    c = "%i" % info.mNAligned
                elif field == "unaligned1":
                    c = "%i" % info.mNUnaligned1
                elif field == "unaligned2":
                    c = "%i" % info.mNUnaligned2
                elif field == "transversions":
                    c = "%i" % info.mNTransversions
                elif field == "transitions":
                    c = "%i" % info.mNTransitions
                elif field == "identical":
                    c = "%i" % info.mNIdentical
                elif field == "jc69":
                    try:
                        c = "%6.4f" % CalculateDistanceJC69(info)[0]
                    except ValueError:
                        c = "nan"
                elif field == "t92":
                    try:
                        c = "%6.4f" % CalculateDistanceT92(info)[0]
                    except ValueError:
                        c = "nan"
                else:
                    raise "Unknown field %s" % field

                result.append(c)

        options.stdout.write("%s\t%s\t%s\n" % (first_record.title,
                                               second_record.title,
                                               "\t".join(result)))

    E.Stop()

if __name__ == "__main__":
    sys.exit(main(sys.argv))

'''
gff2fasta.py - output sequences from genomic features
=====================================================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Genomics Intervals Sequences GFF Fasta Transformation

Purpose
-------

This script outputs the genomic sequences for intervals within
a :term:`gff` or :term: `gtf` formatted file.

The ouput can be optionally masked and filtered.

Usage
-----

If you want to convert a ``features.gff`` file with intervals information
into a :term:`fasta` file containing the sequence of each interval, use this
script as follows::

   python gff2fasta.py --genome-file=hg19 < features.gff > features.fasta

The input can also be a :term:`gtf` formatted file. In that case, use the
``--is-gtf`` option::

   python gff2fasta.py --genome-file=hg19 --is-gtf < features.gtf >\
 features.fasta

If you want to add a polyA tail onto each transcript you can use the `extend`
options:

   python gff2fasta.py --genome-file=hg19 --is-gtf
   --extend-at=3 --extend-by=125 --extend-with=A
   < features.gtf > features.fasta

If you want to merge the sequence of similar features together, please use
``--merge-overlapping``::

   python gff2fasta.py --genome-file=hg19 --merge-overlapping < features.gff >\
 features.fasta

It is possible to filter the output by selecting a minimum or maximum number
of nucleotides in the resultant fasta sequence with ``--max-length`` or
``--min-interval-length`` respectively::

   python gff2fasta.py --genome-file=hg19 --max-length=100\
 < features.gff > features.fasta

Or you can also filter the output by features name with the ``--feature``
option::

   python gff2fasta.py --genome-file=hg19 --feature=exon < features.gff\
 > features.fasta

On the other hand, low-complexity regions can be masked with the ``--masker``
option and a given :term:`gff` formatted file::

   python gff2fasta.py --genome-file=hg19 --masker=dust\
 --maskregions-bed-file=intervals.gff < features.gff > features.fasta

where ``--masker`` can take the following values: ``dust``, ``dustmasker``,
and ``softmask``.

Options
-------

``--is-gtf``
  Tells the script to expect a :term:`gtf` format file

``--genome-file``
  PATH to Fasta file of genome build to use

``--merge-overlapping``
  Merge features in :term:`gtf`/:term:`gff` file that are adjacent and share
  attributes

``--method=filter --filter-method``
  Filter on a :term:`gff` feature such as ``exon`` or ``CDS``

``--maskregions-bed-file``
  Mask sequences in intervals in :term:`gff` file

``--remove-masked-regions``
  Remove sequences in intervals in :term:`gff` file rather than masking them

``--min-interval-length``
  Minimum output sequence length

``--max-length``
  Maximum output sequence length

``--extend-at``
  Extend sequence at 3', 5' or both end.  Optionally '3only' or '5only' will
  return only the 3' or 5' extended sequence

``--extend-by``
  Used in conjunction with ``--extend-at``, the number of nucleotides to extend
  by

``--extend-with``
  Optional. Used in conjunction with ``--extend-at`` and ``--extend-by``.
  Instead of extending by the genomic sequence, extend by this string repeated
  n times, where n is --entend-by


``--masker``
  Masker type to use: dust, dustmasker, soft or none

``--fold-at``
  Fold the fasta sequence every n bases

``--naming-attribute``
  Use this attribute to name the fasta entries

Command line options
--------------------
'''

import sys
import CGAT.Experiment as E
import CGAT.GTF as GTF
import CGAT.Genomics as Genomics
import CGAT.IndexedFasta as IndexedFasta
import CGAT.Intervals as Intervals
import CGAT.Masker as Masker
import bx.intervals.intersection


def main(argv=None):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if argv is None:
        argv = sys.argv

    parser = E.OptionParser(
        version="%prog version: $Id$",
        usage=globals()["__doc__"])

    parser.add_option("--is-gtf", dest="is_gtf", action="store_true",
                      help="input is gtf instead of gff.")

    parser.add_option("-g", "--genome-file", dest="genome_file", type="string",
                      help="filename with genome [default=%default].")

    parser.add_option(
        "-m", "--merge-adjacent", dest="merge", action="store_true",
        help="merge adjacent intervals with the same attributes."
        " [default=%default]")

    parser.add_option(
        "-e", "--feature", dest="feature", type="string",
        help="filter by a feature, for example 'exon', 'CDS'."
        " If set to the empty string, all entries are output "
        "[%default].")

    parser.add_option(
        "-f", "--maskregions-bed-file", dest="filename_masks",
        type="string", metavar="gff",
        help="mask sequences with regions given in gff file "
        "[%default].")

    parser.add_option(
        "--remove-masked-regions", dest="remove_masked_regions",
        action="store_true",
        help="remove regions instead of masking [%default].")

    parser.add_option(
        "--min-interval-length", dest="min_length", type="int",
        help="set minimum length for sequences output "
        "[%default]")

    parser.add_option(
        "--max-length", dest="max_length", type="int",
        help="set maximum length for sequences output "
        "[%default]")

    parser.add_option(
        "--extend-at", dest="extend_at", type="choice",
        choices=("none", "3", "5", "both", "3only", "5only"),
        help="extend at no end, 3', 5' or both ends. If "
        "3only or 5only are set, only the added sequence "
        "is returned [default=%default]")

    parser.add_option(
        "--extend-by", dest="extend_by", type="int",
        help="extend by # bases [default=%default]")

    parser.add_option(
        "--extend-with", dest="extend_with", type="string",
        help="extend using base [default=%default]")

    parser.add_option(
        "--masker", dest="masker", type="choice",
        choices=("dust", "dustmasker", "softmask", "none"),
        help="apply masker [%default].")

    parser.add_option(
        "--fold-at", dest="fold_at", type="int",
        help="fold sequence every n bases[%default].")

    parser.add_option(
        "--fasta-name-attribute", dest="naming_attribute", type="string",
        help="use attribute to name fasta entry. Currently only compatable"
        " with gff format [%default].")

    parser.set_defaults(
        is_gtf=False,
        genome_file=None,
        merge=False,
        feature=None,
        filename_masks=None,
        remove_masked_regions=False,
        min_length=0,
        max_length=0,
        extend_at=None,
        extend_by=100,
        extend_with=None,
        masker=None,
        fold_at=None,
        naming_attribute=False
    )

    (options, args) = E.Start(parser)

    if options.genome_file:
        fasta = IndexedFasta.IndexedFasta(options.genome_file)
        contigs = fasta.getContigSizes()

    if options.is_gtf:
        iterator = GTF.transcript_iterator(GTF.iterator(options.stdin))
    else:
        gffs = GTF.iterator(options.stdin)
        if options.merge:
            iterator = GTF.joined_iterator(gffs)
        else:
            iterator = GTF.chunk_iterator(gffs)

    masks = None
    if options.filename_masks:
        masks = {}
        with open(options.filename_masks, "r") as infile:
            e = GTF.readAsIntervals(GTF.iterator(infile))

        # convert intervals to intersectors
        for contig in e.keys():
            intersector = bx.intervals.intersection.Intersecter()
            for start, end in e[contig]:
                intersector.add_interval(bx.intervals.Interval(start, end))
            masks[contig] = intersector

    ninput, noutput, nmasked, nskipped_masked = 0, 0, 0, 0
    nskipped_length = 0
    nskipped_noexons = 0

    feature = options.feature

#    for item in iterator:
# print len(item) # 3, 2
#	for i in item:
# print len(i) # 9, 9, 9, 9, 9
#	   print i.contig
#	   print i.strand
#	   print i.transcript_id

    # iterator is a list containing groups (lists) of features.
    # Each group of features have in common the same transcript ID, in case of
    # GTF files.
    for ichunk in iterator:

        ninput += 1

        if feature:
            chunk = filter(lambda x: x.feature == feature, ichunk)
        else:
            chunk = ichunk

        if len(chunk) == 0:
            nskipped_noexons += 1
            E.info("no features in entry from "
                   "%s:%i..%i - %s" % (ichunk[0].contig,
                                       ichunk[0].start,
                                       ichunk[0].end,
                                       str(ichunk[0])))
            continue

        contig, strand = chunk[0].contig, chunk[0].strand
        if options.is_gtf:
            name = chunk[0].transcript_id
        else:
            if options.naming_attribute:
                attr_dict = {x.split("=")[0]: x.split("=")[1]
                             for x in chunk[0].attributes.split(";")}
                name = attr_dict[options.naming_attribute]
            else:
                name = str(chunk[0].attributes)

        lcontig = contigs[contig]
        positive = Genomics.IsPositiveStrand(strand)
        intervals = [(x.start, x.end) for x in chunk]
        intervals.sort()

        if masks:
            if contig in masks:
                masked_regions = []
                for start, end in intervals:
                    masked_regions += [(x.start, x.end)
                                       for x in masks[contig].find(start, end)]

                masked_regions = Intervals.combine(masked_regions)
                if len(masked_regions):
                    nmasked += 1

                if options.remove_masked_regions:
                    intervals = Intervals.truncate(intervals, masked_regions)
                else:
                    raise "unimplemented"

                if len(intervals) == 0:
                    nskipped_masked += 1
                    if options.loglevel >= 1:
                        options.stdlog.write("# skipped because fully masked: "
                                             "%s: regions=%s masks=%s\n" %
                                             (name,
                                              str([(x.start,
                                                    x.end) for x in chunk]),
                                              masked_regions))
                    continue

        out = intervals

        if options.extend_at and not options.extend_with:
            if options.extend_at == "5only":
                intervals = [(max(0, intervals[0][0] - options.extend_by),
                              intervals[0][0])]
            elif options.extend_at == "3only":
                intervals = [(intervals[-1][1],
                              min(lcontig,
                                  intervals[-1][1] + options.extend_by))]
            else:
                if options.extend_at in ("5", "both"):
                    intervals[0] = (max(0,
                                        intervals[0][0] - options.extend_by),
                                    intervals[0][1])
                if options.extend_at in ("3", "both"):
                    intervals[-1] = (intervals[-1][0],
                                     min(lcontig,
                                         intervals[-1][1] + options.extend_by))

        if not positive:
            intervals = [(lcontig - x[1], lcontig - x[0])
                         for x in intervals[::-1]]
            out.reverse()

        s = [fasta.getSequence(contig, strand, start, end)
             for start, end in intervals]
        # IMS: allow for masking of sequences
        s = Masker.maskSequences(s, options.masker)
        l = sum([len(x) for x in s])
        if (l < options.min_length or
                (options.max_length and l > options.max_length)):
            nskipped_length += 1
            if options.loglevel >= 1:
                options.stdlog.write("# skipped because length out of bounds "
                                     "%s: regions=%s len=%i\n" %
                                     (name, str(intervals), l))
                continue

        if options.extend_at and options.extend_with:
            extension = "".join((options.extend_with,) * options.extend_by)

            if options.extend_at in ("5", "both"):
                s[1] = extension + s[1]
            if options.extend_at in ("3", "both"):
                s[-1] = s[-1] + extension

        if options.fold_at:
            n = options.fold_at
            s = "".join(s)
            seq = "\n".join([s[i:i+n] for i in range(0, len(s), n)])
        else:
            seq = "\n".join(s)

        options.stdout.write(">%s %s:%s:%s\n%s\n" % (name,
                                                     contig,
                                                     strand,
                                                     ";".join(
                                                         ["%i-%i" %
                                                          x for x in out]),
                                                     seq))

        noutput += 1

    E.info("ninput=%i, noutput=%i, nmasked=%i, nskipped_noexons=%i, "
           "nskipped_masked=%i, nskipped_length=%i" %
           (ninput, noutput, nmasked, nskipped_noexons,
            nskipped_masked, nskipped_length))

    E.Stop()

if __name__ == "__main__":
    sys.exit(main(sys.argv))


'''
psl2map.py - build a mappping from blat alignments
==================================================

:Author: Andreas Heger
:Release: $Id$
:Date: |today|
:Tags: Python

Purpose
-------

This scripts reads :term:`psl` formatted alignments and builds a map of queries 
to targets. The mapping can be restricted by different measures of uniqueness.

If polyA processing is turned on, the non-overlapping terminus of each read will 
be checked if they are mostly A. If they are, the query coverage will be adjusted
appropriately and the read flagged in the section polyA.

Usage
-----

Example::

   python <script_name>.py --help

Type::

   python <script_name>.py --help

for command line help.

Command line options
--------------------

'''

import sys
import CGAT.Intervals as Intervals
import CGAT.Experiment as E
import CGAT.Histogram as Histogram
import CGAT.Blat as Blat
import CGAT.GTF as GTF
import CGAT.IndexedFasta as IndexedFasta


def printHistogram(values, section, options, min_value=0, increment=1.0):

    if len(values) == 0:
        if options.loglevel >= 1:
            options.stdlog.write(
                "# no histogram data for section %s\n" % (section))
        return

    outfile = open(options.output_filename_pattern % section, "w")
    h = Histogram.Calculate(
        values, no_empty_bins=True, min_value=0, increment=1.0)

    outfile.write("bin\t%s\n" % section)
    for bin, val in h:
        outfile.write("%5.2f\t%i\n" % (bin, val))
    outfile.close()


def printMatched(query_ids, section, options):

    outfile = open(options.output_filename_pattern % section, "w")

    for query_id in query_ids:
        outfile.write("%s\n" % (query_id))
    outfile.close()


def detectPolyA(query_id, matches, options, queries_fasta=None):
    """detect PolyA tails and adjust coverage.

    polyA detection. If there are ambiguous matches, the location
    of the polyA tail is not straight-forward as suboptimal matches
    might be declared to be a tail.

    1. collect all matches with polyA tail - the remainder is unchanged
    2. declare polyA tail by the shortest! unaligned segment and compute
       coverage for each match appropriately. 

    The method checks whether the tails are consistent (always at the same end).
    If not, an AssertionError is thrown
    """
    max_total = matches[0].mQueryLength
    best_start, best_end, best_pA, best_pT, best_tail = 0, 0, 0, 0, ""

    tail_matches = []
    new_matches = []
    for match in matches:
        missing_start = match.mQueryFrom
        missing_end = match.mQueryLength - match.mQueryTo
        if missing_start < missing_end:
            smaller = missing_start
            larger = missing_end
            start, end = match.mQueryTo, match.mQueryLength
        else:
            smaller = missing_end
            larger = missing_start
            start, end = 0, match.mQueryFrom

        # check if tail is at least polyA_min_aligned and at most polyA_max_unaligned
        # are missing from the other end.
        if not(smaller < options.polyA_max_unaligned and larger > options.polyA_min_unaligned):
            new_matches.append(match)
            continue

        tail = queries_fasta.getSequence(query_id)[start:end]

        counts = {"A": 0, "T": 0, "N": 0}
        for c in tail.upper():
            counts[c] = counts.get(c, 0) + 1
        total = end - start
        pA = 100.0 * (counts["A"] + counts["N"]) / total
        pT = 100.0 * (counts["T"] + counts["N"]) / total

        if options.loglevel >= 5:
            options.stdlog.write(
                "# polyA detection: %s:%i-%i pA=%5.2f pT=%5.2f tail=%s\n" % (query_id, start, end, pA, pT, tail))

        if max(pA, pT) < options.polyA_min_percent:
            new_matches.append(match)
            continue

        if total < max_total:
            max_total = total
            best_start, best_end, best_pA, best_pT, best_tail = start, end, pA, pT, tail

        if not(best_start == start or best_end == end):
            if options.loglevel >= 1:
                options.stdlog.write("# inconsistent polyA tails for %s: %i-%i, %i-%i - analysis skipped\n" %
                                     (query_id, best_start, best_end, start, end))
            return matches

        tail_matches.append(match)

    if tail_matches:
        for match in tail_matches:
            match.mQueryCoverage += 100.0 * \
                float(len(best_tail)) / match.mQueryLength
            assert match.mQueryCoverage <= 100.0, "%s: coverage %f > 100.0: incr=%f\n%s" % (
                query_id, match.mQueryCoverage, float(len(best_tail)) / match.mQueryLength, str(match))
            new_matches.append(match)

        options.outfile_polyA.write("%s\t%i\t%i\t%i\t%5.2f\t%5.2f\t%s\n" %
                                    (query_id,
                                     len(tail_matches),
                                     best_start, best_end,
                                     best_pA, best_pT, best_tail))

    assert len(new_matches) == len(matches)

    return new_matches


def selectMatches(query_id, matches, options, queries_fasta=None):
    """find the best match."""

    if options.loglevel >= 2:
        options.stdlog.write(
            "# attempting to select best match for %s\n" % query_id)

        if options.loglevel >= 3:
            for match in matches:
                options.stdlog.write("# match=%s\n" % str(match))

    new_matches = []

    if options.polyA:
        matches = detectPolyA(query_id, matches, options, queries_fasta)

    if options.matching_mode == "all":
        return matches, None

    elif options.matching_mode in ("best-coverage", "best-query-coverage", "best-sbjct-coverage",
                                   "best-pid",
                                   "best-covpid", "best-query-covpid", "best-sbjct-covpid",
                                   "best-min-covpid", "best-query-min-covpid", "best-sbjct-min-covpid",):
        if options.matching_mode == "best-coverage":
            f = lambda match: min(match.mQueryCoverage, match.mSbjctCoverage)
        elif options.matching_mode == "best-query-coverage":
            f = lambda match: match.mQueryCoverage
        elif options.matching_mode == "best-sbjct-coverage":
            f = lambda match: match.mSbjctCoverage
        elif options.matching_mode == "best-pid":
            f = lambda match: match.mPid
        elif options.matching_mode == "best-covpid":
            f = lambda match: min(
                match.mQueryCoverage, match.mSbjctCoverage) * match.mPid
        elif options.matching_mode == "best-query-covpid":
            f = lambda match: match.mQueryCoverage * match.mPid
        elif options.matching_mode == "best-sbjct-covpid":
            f = lambda match: match.mSbjctCoverage * match.mPid
        elif options.matching_mode == "best-min-covpid":
            f = lambda match: min(
                (match.mQueryCoverage, match.mSbjctCoverage, match.mPid))
        elif options.matching_mode == "best-query-min-covpid":
            f = lambda match: min(match.mQueryCoverage, match.mPid)
        elif options.matching_mode == "best-sbjct-min-covpid":
            f = lambda match: min(match.mSbjctCoverage, match.mPid)

        for match in matches:
            match.mMatchScore = f(match)

        # collect "significant" matches
        # this filter removes matches out of contention, i.e., matches
        # with very low score are not considered when assessing the uniqueness
        # of the highest scoring match
        matches.sort(lambda x, y: cmp(x.mMatchScore, y.mMatchScore))
        matches.reverse()
        best_score = min(matches[0].mMatchScore * options.collection_threshold,
                         matches[0].mMatchScore - options.collection_distance)

        for match in matches:
            # stop when matchscore drops below best score
            if match.mMatchScore < best_score:
                break
            new_matches.append(match)

        if not options.keep_all_best:

            if len(new_matches) > 1:

                if len(new_matches) == 2:
                    # accept matches against chrX and chrX_random (or vice
                    # versa)
                    if new_matches[0].mSbjctId == "%s_random" % new_matches[1].mSbjctId:
                        return new_matches[1:], None
                    elif new_matches[1].mSbjctId == "%s_random" % new_matches[0].mSbjctId:
                        return new_matches[:1], None
                    # or against chrUn or chrU.
                    else:
                        new_matches = [x for x in new_matches if not(
                            x.mSbjctId.endswith("Un") or x.mSbjctId.endswith("chrU"))]
                        if len(new_matches) == 1:
                            return new_matches, None

                if options.ignore_all_random:
                    new_matches = [x for x in new_matches if not(x.mSbjctId.endswith(
                        "_random") or x.mSbjctId.endswith("Un") or x.mSbjctId.endswith("chrU"))]
                    if len(new_matches) == 1:
                        return new_matches, None

                return [], "not unique: %s" % (" ".join(map(lambda x: str(x.mMatchScore), matches)))

    elif options.matching_mode == "unique":
        # only return matches if they are "unique", i.e. no other match
        if len(matches) == 1:
            new_matches.append(matches[0])
        else:
            return [], "not unique: %s" % (" ".join(map(lambda x: str(x.mMatchScore), matches)))

    matches = new_matches

    if options.best_per_sbjct:
        new_matches = []
        sbjcts = set()
        for match in matches:
            if match.mSbjctId in sbjcts:
                continue
            new_matches.append(match)
            sbjcts.add(match.mSbjctId)

        matches = new_matches

    return matches, None


def main(argv=None):
    """script main.

    parses command line options in sys.argv, unless *argv* is given.
    """

    if argv is None:
        argv = sys.argv

    parser = E.OptionParser(
        version="%prog version: $Id: psl2map.py 2781 2009-09-10 11:33:14Z andreas $", usage=globals()["__doc__"])

    parser.add_option("--queries-tsv-file", dest="input_filename_queries", type="string",
                      help="fasta filename with queries - required for polyA analysis [%default].")

    parser.add_option("--polyA", dest="polyA", action="store_true",
                      help="detect polyA tails [%default].")

    parser.add_option("-p", "--output-filename-pattern", dest="output_filename_pattern", type="string",
                      help="OUTPUT filename with histogram information on aggregate coverages [%default].")

    parser.add_option("--output-filename-empty", dest="output_filename_empty", type="string",
                      help="OUTPUT filename with queries for which all matches have been discarded [%default].")

    parser.add_option("-o", "--output-format", dest="output_format", type="choice",
                      choices=("map", "psl"),
                      help="output format to choose [%default].")

    parser.add_option("-z", "--from-zipped", dest="from_zipped", action="store_true",
                      help="input is zipped.")

    parser.add_option("--threshold-min-pid", dest="threshold_min_pid", type="float",
                      help="minimum thresholds for pid [%default].")

    parser.add_option("--threshold-min-matches", dest="threshold_min_matches", type="int",
                      help="minimum threshold for number of matching residues [%default].")

    parser.add_option("--threshold-max-error-rate", dest="threshold_max_error_rate", type="float",
                      help="maximum threshold for error of aligned part [%default].")

    parser.add_option("--threshold-good-query-coverage", dest="threshold_good_query_coverage", type="float",
                      help="minimum query coverage for segments to be counted as good [%default].")

    parser.add_option("--threshold-min-query-coverage", dest="threshold_min_query_coverage", type="float",
                      help="minimum query coverage for segments to be accepted [%default].")

    parser.add_option("--threshold-max-query-gapchars", dest="threshold_max_query_gapchars", type="int",
                      help="maximum number of gap characters  in query[%default].")

    parser.add_option("--threshold-max-query-gaps", dest="threshold_max_query_gaps", type="int",
                      help="maximum number of gaps  in query[%default].")

    parser.add_option("--threshold-max-sbjct-gapchars", dest="threshold_max_sbjct_gapchars", type="int",
                      help="maximum number of gap characters  in sbjct[%default].")

    parser.add_option("--keep-unique-matches", dest="keep_unique_matches", action="store_true",
                      help="ignore filters for unique matches [%default].")

    parser.add_option("--keep-all-best", dest="keep_all_best", action="store_true",
                      help="when sorting matches, keep all matches within the collection threshold [%default].")

    parser.add_option("--output-best-per-subject", dest="best_per_sbjct", action="store_true",
                      help="keep only the best entry per sbjct (for transcript mapping) [%default].")

    parser.add_option("--threshold-max-sbjct-gaps", dest="threshold_max_sbjct_gaps", type="int",
                      help="maximum number of gaps  in sbjct[%default].")

    parser.add_option("--test", dest="test", type="int",
                      help="test - stop after # rows of parsing[%default].")

    parser.add_option("-m", "--matching-mode", dest="matching_mode", type="choice",
                      choices=("best-coverage", "best-query-coverage", "best-sbjct-coverage",
                               "best-pid", "best-covpid", "best-query-covpid", "best-sbjct-covpid",
                               "best-min-covpid", "best-query-min-covpid", "best-sbjct-min-covpid",
                               "unique", "all"),
                      help="determines how to selecte the best match [%default].")

    parser.add_option("--subjctfilter-tsv-file", dest="filename_filter_sbjct", type="string",
                      help="gff file for filtering sbjct matches. Matches overlapping these regions are discarded, but see --keep-forbidden [%default].")

    parser.add_option("--keep-forbidden", dest="keep_forbidden", action="store_true",
                      help="if set, keep only matches that overlap the regions supplied with --subjctfilter-tsv-file [%default].")

    parser.add_option("--query-forward-coordinates", dest="query_forward_coordinates", action="store_true",
                      help="use forward coordinates for query, strand will refer to sbjct [%default].")

    parser.add_option("--ignore-all-random", dest="ignore_all_random", action="store_true",
                      help="if there are multiple best matches, ignore all those to chrUn and _random [%default].")

    parser.add_option("--collection-threshold", dest="collection_threshold", type="float",
                      help="threshold for collecting matches, percent of best score [%default].")

    parser.add_option("--collection-distance", dest="collection_distance", type="float",
                      help="threshold for collecting matches, difference to best score [%default].")

    parser.set_defaults(input_filename_domains=None,
                        input_filename_queries=None,
                        threshold_good_query_coverage=90.0,
                        threshold_min_pid=30.0,
                        threshold_min_matches=0,
                        threshold_max_error_rate=None,
                        output_filename_pattern="%s",
                        keep_unique_matches=False,
                        output_format="map",
                        print_matched=["full", "partial", "good"],
                        from_zipped=False,
                        combine_overlaps=True,
                        min_length_domain=30,
                        threshold_min_query_coverage=50,
                        min_length_singletons=30,
                        new_family_id=10000000,
                        add_singletons=False,
                        matching_mode="best-coverage",
                        best_per_sbjct=False,
                        threshold_max_query_gapchars=None,
                        threshold_max_query_gaps=None,
                        threshold_max_sbjct_gapchars=None,
                        threshold_max_sbjct_gaps=None,
                        filename_filter_sbjct=None,
                        keep_forbidden=False,
                        keep_all_best=False,
                        test=None,
                        query_forward_coordinates=False,
                        output_filename_empty=None,
                        collection_threshold=1.0,
                        collection_distance=0,
                        polyA=False,
                        # max residues missing from non polyA end
                        polyA_max_unaligned=3,
                        # min residues in tail
                        polyA_min_unaligned=10,
                        # min percent residues that are A/T in tail
                        polyA_min_percent=70.0,
                        # ignore duplicate matches if they are on Un or
                        # _random
                        ignore_all_random=False,
                        )

    (options, args) = E.Start(parser, add_pipe_options=True)

    if len(args) == 1:
        if options.from_zipped or args[0][-3:] == ".gz":
            import gzip
            infile = gzip.open(args[0], "r")
        else:
            infile = open(args[0], "r")
    else:
        infile = sys.stdin

    if options.input_filename_queries:
        queries_fasta = IndexedFasta.IndexedFasta(
            options.input_filename_queries)
    else:
        queries_fasta = None

    if options.filename_filter_sbjct:

        try:
            import bx.intervals.intersection
        except ImportError:
            raise "filtering for intervals requires the bx tools."

        intervals = GTF.readGFFFromFileAsIntervals(
            open(options.filename_filter_sbjct, "r"))

        intersectors = {}

        for contig, values in intervals.items():
            intersector = bx.intervals.intersection.Intersecter()
            for start, end in values:
                intersector.add_interval(bx.intervals.Interval(start, end))
            intersectors[contig] = intersector

        if options.loglevel >= 1:
            options.stdlog.write("# read %i intervals for %i contigs.\n" %
                                 (sum([len(x) for x in intervals.values()]),
                                  len(intersectors)))
    else:
        intersectors = None

    ################################################
    ################################################
    ################################################
    # processing of a chunk (matches of same query)
    ################################################
    ninput, noutput, nskipped = 0, 0, 0

    # number of sequences with full/partial/good matches
    nfull_matches, npartial_matches, ngood_matches = 0, 0, 0
    # number of sequences which are fully/good/partially matched
    # i.e., after combining all aligned regions
    nfully_matched, npartially_matched, nwell_matched = 0, 0, 0

    nremoved_pid, nremoved_query_coverage, nempty = 0, 0, 0
    nremoved_gaps, nremoved_nmatches = 0, 0
    nremoved_regions = 0
    nqueries_removed_region = 0

    aggregate_coverages = []
    mapped_coverages = []
    fully_matched = []
    well_matched = []
    partially_matched = []
    new_family_id = options.new_family_id

    if options.output_filename_empty:
        outfile_empty = open(options.output_filename_empty, "w")
        outfile_empty.write("read_id\tcomment\n")
    else:
        outfile_empty = None

    if options.polyA:
        options.outfile_polyA = open(
            options.output_filename_pattern % "polyA", "w")
        options.outfile_polyA.write("query_id\tstart\tend\tpA+N\tpT+N\ttail\n")

    def processChunk(query_id, matches):
        """process a set of matches from query_id"""

        global ninput, noutput, nskipped
        global nfull_matches, npartial_matches, ngood_matches
        global nremoved_pid, nremoved_query_coverage, nempty, nremoved_gaps, nremoved_nmatches
        global nremoved_regions, nqueries_removed_region
        global outfile_empty
        ninput += 1

        full_matches = []
        good_matches = []
        partial_matches = []

        x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches = 0, 0, 0, 0
        nmatches = len(matches)

        new_matches = []

        # absolute filters applicable to non-fragmentory matches

        for match in matches:

            if match.mPid < options.threshold_min_pid:
                nremoved_pid += 1
                continue

            if match.mNMatches < options.threshold_min_matches:
                nremoved_nmatches += 1
                continue

            if options.threshold_max_error_rate:
                r = 100.0 * \
                    math.power(
                        options.threshold_max_error_rate, match.mNMatches + match.mNMismatches)
                if match.mPid < r:
                    nremoved_pid += 1
                    x_nremoved_pid += 1
                    continue

            new_matches.append(match)

        matches = new_matches

        # filter matches
        if len(matches) == 0:
            if outfile_empty:
                outfile_empty.write("%s\tall matches removed after applying thresholds: before=%i, npid=%i, nqcoverage=%i, ngaps=%i, nmatches=%i\n" %
                                    (query_id, nmatches, x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches))
            nskipped += 1
            return

        if options.keep_unique_matches and len(matches) == 1:
            pass
        else:
            new_matches = []

            for match in matches:

                if match.mQueryCoverage < options.threshold_min_query_coverage:
                    nremoved_query_coverage += 1
                    x_nquery_coverage += 1
                    continue

                if options.threshold_max_query_gaps and options.threshold_max_query_gaps > match.mQueryNGapsCounts:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if options.threshold_max_query_gapchars and options.threshold_max_query_gapchars > match.mQueryNGapsBases:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if options.threshold_max_sbjct_gaps and options.threshold_max_sbjct_gaps > match.mSbjctNGapsCounts:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                if options.threshold_max_sbjct_gapchars and options.threshold_max_sbjct_gapchars > match.mSbjctNGapsBases:
                    nremoved_gaps += 1
                    x_nremoved_gaps += 1
                    continue

                new_matches.append(match)
            matches = new_matches

        if len(matches) == 0:
            if outfile_empty:
                outfile_empty.write("%s\tall matches removed after applying thresholds: before=%i, npid=%i, nqcoverage=%i, ngaps=%i, nmatches=%i\n" %
                                    (query_id, nmatches, x_nremoved_pid, x_nquery_coverage, x_nremoved_gaps, x_nremoved_nmatches))
            nskipped += 1
            return

        # Remove queries matching to a forbidden region. This section
        # will remove the full query if any of its matches matches in a
        # forbidden region.
        keep = True
        for match in matches:
            if intersectors and match.mSbjctId in intersectors:
                found = intersectors[match.mSbjctId].find(
                    match.mSbjctFrom, match.mSbjctTo)
                if found and not options.keep_forbidden or (found and not options.keep_forbidden):
                    nremoved_regions += 1
                    keep = False
                    continue

        if not keep:
            nqueries_removed_region += 1
            if outfile_empty:
                outfile_empty.write(
                    "%s\toverlap with forbidden region\n" % query_id)
            return

        # check for full length matches
        for match in matches:
            if match.mQueryCoverage >= 99.9:
                full_matches.append(match)
            if match.mQueryCoverage > options.threshold_good_query_coverage:
                good_matches.append(match)
            else:
                partial_matches.append(match)

        if full_matches:
            nfull_matches += 1
        elif good_matches:
            ngood_matches += 1
        elif partial_matches:
            npartial_matches += 1

        # compute coverage of sequence with matches
        intervals = []
        for match in full_matches + good_matches + partial_matches:
            intervals.append((match.mQueryFrom, match.mQueryTo))

        rest = Intervals.complement(intervals, 0, match.mQueryLength)

        query_coverage = 100.0 * \
            (match.mQueryLength -
             sum(map(lambda x: x[1] - x[0], rest))) / match.mQueryLength

        if query_coverage >= 99.9:
            fully_matched.append(query_id)
        elif query_coverage > options.threshold_good_query_coverage:
            well_matched.append(query_id)
        else:
            partially_matched.append(query_id)

        aggregate_coverages.append(query_coverage)

        # select matches to output
        matches, msg = selectMatches(query_id, matches, options, queries_fasta)

        if len(matches) > 0:
            for match in matches:
                if options.query_forward_coordinates:
                    match.convertCoordinates()

                if options.output_format == "map":
                    options.stdout.write("%s\n" %
                                         "\t".join(map(str, (
                                             match.mQueryId, match.mSbjctId,
                                             match.strand,
                                             "%5.2f" % match.mQueryCoverage,
                                             "%5.2f" % match.mSbjctCoverage,
                                             "%5.2f" % match.mPid,
                                             match.mQueryLength,
                                             match.mSbjctLength,
                                             match.mQueryFrom, match.mQueryTo,
                                             match.mSbjctFrom, match.mSbjctTo,
                                             ",".join(
                                                 map(str, match.mBlockSizes)),
                                             ",".join(
                                                 map(str, match.mQueryBlockStarts)),
                                             ",".join(
                                                 map(str, match.mSbjctBlockStarts)),
                                         ))))
                elif options.output_format == "psl":
                    options.stdout.write(str(match) + "\n")

            noutput += 1
        else:
            if outfile_empty:
                outfile_empty.write(
                    "%s\tno matches selected: %s\n" % (query_id, msg))
            nempty += 1

    if options.output_format == "map":
        options.stdout.write("\t".join(("query_id", "sbjct_id", "sstrand", "qcoverage", "scoverage",
                                        "pid", "qlen", "slen", "qfrom", "qto", "sfrom", "sto", "blocks", "qstarts", "sstarts")) + "\n")
    elif options.output_format == "psl":
        options.stdout.write(Blat.Match().getHeader() + "\n")

    ################################################
    ################################################
    ################################################
    # main loop
    ################################################
    nfully_covered = None
    matches = []
    last_query_id = None
    is_complete = True
    ninput_lines = 0

    skip = 0

    iterator = Blat.BlatIterator(infile)

    while 1:

        try:
            match = iterator.next()
        except Blat.ParsingError:
            iterator = Blat.BlatIterator(infile)
            continue

        if match is None:
            break

        ninput_lines += 1

        if options.test and ninput_lines > options.test:
            break

        if match.mQueryId != last_query_id:
            if last_query_id:
                processChunk(last_query_id, matches)
            matches = []
            last_query_id = match.mQueryId

        matches.append(match)

    processChunk(last_query_id, matches)

    printHistogram(aggregate_coverages, "aggregate", options)

    printHistogram(mapped_coverages, "mapped", options)

    if "full" in options.print_matched:
        printMatched(fully_matched, "full", options)

    if "good" in options.print_matched:
        printMatched(well_matched, "good", options)

    if "partial" in options.print_matched:
        printMatched(partially_matched, "partial", options)

    if options.loglevel >= 1:
        options.stdlog.write(
            "# alignments: ninput=%i, is_complete=%s\n" % (ninput_lines, str(is_complete)))
        options.stdlog.write(
            "# queries: ninput=%i, noutput=%i\n" % (ninput, noutput))
        options.stdlog.write("# individual coverage: full=%i, good=%i, partial=%i\n" % (
            nfull_matches, ngood_matches, npartial_matches))
        options.stdlog.write("# aggregate  coverage: full=%i, good=%i, partial=%i\n" % (
            len(fully_matched), len(well_matched), len(partially_matched)))
        options.stdlog.write("# omitted queries: total=%i, thresholds=%i, regions=%i, selection=%i\n" %
                             (nskipped + nqueries_removed_region + nempty,
                              nskipped, nqueries_removed_region, nempty))
        options.stdlog.write("# omitted matches: pid=%i, query_coverage=%i, gaps=%i, regions=%i, nmatches=%i\n" % (
            nremoved_pid, nremoved_query_coverage, nremoved_gaps, nremoved_regions, nremoved_nmatches))

    E.Stop()

if __name__ == "__main__":
    sys.exit(main(sys.argv))

import os
import platform

__all__ = ['settings']


class InsufficientParametersException(Exception):
    pass


class CacheBrowserSettings(dict):
    def __init__(self, *args, **kwargs):
        super(CacheBrowserSettings, self).__init__(*args, **kwargs)

        if platform.system() == 'Windows':
            self.data_dir = os.path.join(os.environ['ALLUSERSPROFILE'], 'CacheBrowser')
        else:
            self.data_dir = '/tmp/'

        # Set defaults
        self['host'] = '0.0.0.0'
        self['port'] = 9876
        self['database'] = os.path.join(self.data_dir, 'cachebrowser.db')

        # Use attributes instead of dictionary values
        self.host = '0.0.0.0'
        self.port = 9876
        self.database = os.path.join(self.data_dir, 'cachebrowser.db')

        self.default_bootstrap_sources = [
            {
                'type': 'local',
                'path': 'data/local_bootstrap.yaml'
            },
            {
                'type': 'remote',
                'url': 'https://www.cachebrowser.info/bootstrap'
            }
        ]

        self.bootstrap_sources = []

    def get_or_error(self, key):
        if self.get(key, None):
            return self[key]
        raise InsufficientParametersException("Missing parameter %s" % key)

    def update_from_args(self, args):
        self.host = self._read_arg(args, 'host', self.host)
        self.port = self._read_arg(args, 'port', self.port)
        self.database = self._read_arg(args, 'database', self.database)

        self.read_bootstrap_sources(args)

    def read_bootstrap_sources(self, args):
        local_sources = args.get('local_bootstrap') or []
        for source in local_sources:
            self.bootstrap_sources.append({
                'type': 'local',
                'path': source
            })

    @staticmethod
    def _read_arg(args, key, default):
        try:
            return args[key]
        except KeyError:
            return default


settings = CacheBrowserSettings()
"""Implements the HintProtocol, which generates hints for students
that are stuck on a coding question. The protocol uses analytics
to determine whether a hint should be given and then
obtains them from the hint generation server. Free response questions
are posed before and after hints are provided.
"""

from client.sources.common import core
from client.sources.common import models as sources_models
from client.protocols.common import models as protocol_models
from client.utils import auth
from client.utils import format

import json
import logging
import os
import pickle
import random
import re
import urllib.error
import urllib.request

log = logging.getLogger(__name__)

#####################
# Hinting Mechanism #
#####################

class HintingProtocol(protocol_models.Protocol):
    """A protocol that provides rubber duck debugging and hints if applicable.
    """

    HINT_SERVER = "https://hinting.cs61a.org/"
    HINT_ENDPOINT = 'api/hints'
    SMALL_EFFORT = 5
    LARGE_EFFORT = 8
    WAIT_ATTEMPTS = 5

    def run(self, messages):
        """Determine if a student is elgible to recieve a hint. Based on their
        state, poses reflection questions.

        After more attempts, ask if students would like hints. If so, query
        the server.
        """
        if self.args.local:
            return

        if 'analytics' not in messages:
            log.info('Analytics Protocol is required for hint generation')
            return
        if 'file_contents' not in messages:
            log.info('File Contents needed to generate hints')
            return

        if self.args.no_hints:
            messages['hinting'] = {'disabled': 'user'}
            return

        messages['hinting'] = {}
        history = messages['analytics'].get('history', {})
        questions = history.get('questions', [])
        current_q = history.get('question', {})


        for question in current_q:
            if question not in questions:
                continue
            stats = questions[question]
            messages['hinting'][question] = {'prompts': {}, 'reflection': {}}
            hint_info = messages['hinting'][question]

            # Determine a users elgibility for a prompt

            # If the user just solved this question, provide a reflection prompt
            if stats['solved'] and stats['attempts'] > self.SMALL_EFFORT:
                hint_info['elgible'] = False
                if self.args.question:
                    # Only prompt for reflection with question specified.
                    log.info('Giving reflection response on %s', question)
                    reflection = random.choice(SOLVE_SUCCESS_MSG)
                    if not confirm("Nice work! Could you answer a quick question"
                                   " about how you approached this question?"):
                        hint_info['reflection']['accept'] = False
                    else:
                        hint_info['reflection']['accept'] = True
                        prompt_user(reflection, hint_info)
            elif stats['attempts'] < self.SMALL_EFFORT:
                log.info("Question %s is not elgible: Attempts: %s, Solved: %s",
                         question, stats['attempts'], stats['solved'])
                hint_info['elgible'] = False
            else:
                # Only prompt every WAIT_ATTEMPTS attempts to avoid annoying user
                if stats['attempts'] % self.WAIT_ATTEMPTS != 0:
                    hint_info['disabled'] = 'timer'
                    hint_info['elgible'] = False
                    log.info('Waiting for %d more attempts before prompting',
                             stats['attempts'] % self.WAIT_ATTEMPTS)
                else:
                    hint_info['elgible'] = not stats['solved']

            if not hint_info['elgible']:
                continue

            log.info('Prompting for hint on %s', question)

            if confirm("Check for hints on {}?".format(question)):
                hint_info['accept'] = True
                print("Thinking... (could take up to 15 seconds)")
                try:
                    response = self.query_server(messages, question)
                    hint_info['response'] = response

                    hint = response['message']
                    pre_prompt = response['pre-prompt']
                    post_prompt = response['post-prompt']
                    log.info("Hint server response: {}".format(response))
                    if not hint and not pre_prompt:
                        print("Sorry. No hints found for {}".format(question))
                        continue

                    if pre_prompt:
                        print("-- Before the hint, respond to this question."
                              " When you are done typing, press Enter. --")
                        if not prompt_user(pre_prompt, hint_info):
                            # Do not provide hint, if no response from user
                            continue

                    # Provide padding for the the hint
                    print("\n{}\n".format(hint))

                    if post_prompt:
                        prompt_user(post_prompt, hint_info)

                except urllib.error.URLError:
                    log.debug("Network error while fetching hint")
                    hint_info['fetch_error'] = True
                    print("\r\nCould not get a hint.")
            else:
                log.info('Declined Hints for %s', question)
                hint_info['accept'] = False

    def query_server(self, messages, test):
        access_token, _, _ = auth.get_storage()
        user = auth.get_student_email(access_token) or access_token
        if user:
            # The hinting server should not recieve identifying information
            user = hash(user)
        data = {
            'assignment': self.assignment.endpoint,
            'test': test,
            'messages': messages,
            'user': user
        }

        serialized_data = json.dumps(data).encode(encoding='utf-8')

        address = self.HINT_SERVER + self.HINT_ENDPOINT

        log.info('Sending hint request to %s', address)
        request = urllib.request.Request(address)
        request.add_header("Content-Type", "application/json")

        response = urllib.request.urlopen(request, serialized_data, 10)
        return json.loads(response.read().decode('utf-8'))

def prompt_user(query, results):
    try:
        response = None
        short_respones = 0
        while not response:
            response = input("{}\nYour Response: ".format(query))
            if not response or len(response) < 5:
                short_respones += 1
                # Do not ask more than twice to avoid annoying the user
                if short_respones > 2:
                    break
                print("Please enter at least a sentence.")
        results['prompts'][query] = response
        return response
    except KeyboardInterrupt:
        # Hack for windows:
        results['prompts'][query] = 'KeyboardInterrupt'
        try:
            print("Exiting Hint") # Second I/O will get KeyboardInterrupt
            return ''
        except KeyboardInterrupt:
            return ''

def confirm(message):
    response = input("{} [yes/no]: ".format(message))
    return response.lower() == "yes" or response.lower() == "y"

SOLVE_SUCCESS_MSG = [
    "If another student had the same error on this question, what advice would you give them?",
    "What did you learn from writing this program about things that you'll continue to do in the future?",
    "What difficulties did you encounter in understanding the problem?",
    "What difficulties did you encounter in designing the program?",
]

protocol = HintingProtocol

"""This module contains code related to controlling and writing to stdout."""

import os
import sys

class _OutputLogger(object):
    """Custom logger for capturing and suppressing standard output."""
    # TODO(albert): logger should fully implement output stream.

    def __init__(self):
        self._current_stream = self._stdout = sys.stdout
        self._devnull = open(os.devnull, 'w')
        self._logs = {}
        self._num_logs = 0

    def on(self):
        """Allows print statements to emit to standard output."""
        self._current_stream = self._stdout

    def off(self):
        """Prevents print statements from emitting to standard out."""
        self._current_stream = self._devnull

    def new_log(self):
        """Registers a new log so that calls to write will append to the log.

        RETURN:
        int; a unique ID to reference the log.
        """
        log_id = self._num_logs
        self._logs[log_id] = []
        self._num_logs += 1
        return log_id

    def get_log(self, log_id):
        assert log_id in self._logs
        return self._logs[log_id]

    def remove_log(self, log_id):
        assert log_id in self._logs, 'Log id {} not found'.format(log_id)
        del self._logs[log_id]

    def remove_all_logs(self):
        self._logs = {}

    def is_on(self):
        return self._current_stream == self._stdout

    def write(self, msg):
        """Writes msg to the current output stream (either standard
        out or dev/null). If a log has been registered, append msg
        to the log.

        PARAMTERS:
        msg -- str
        """
        self._current_stream.write(msg)
        for log in self._logs.values():
            log.append(msg)

    def flush(self):
        self._current_stream.flush()

    # TODO(albert): rewrite this to be cleaner.
    def __getattr__(self, attr):
        return getattr(self._current_stream, attr)

_logger = sys.stdout = _OutputLogger()

def on():
    _logger.on()

def off():
    _logger.off()

def get_log(log_id):
    return _logger.get_log(log_id)

def new_log():
    return _logger.new_log()

def remove_log(log_id):
    _logger.remove_log(log_id)

def remove_all_logs():
    _logger.remove_all_logs()

from client import exceptions as ex
from client.sources.ok_test import concept
import unittest

class ConceptSuiteTest(unittest.TestCase):
    TEST_NAME = 'A'
    SUITE_NUMBER = 0

    def makeTest(self, cases):
        return concept.ConceptSuite(False, False, type='concept', cases=cases)

    def testConstructor_noCases(self):
        try:
            self.makeTest([])
        except TypeError:
            self.fail()

    def testConstructor_validTestCase(self):
        try:
            self.makeTest([
                {
                    'question': 'Question 1',
                    'answer': 'Answer',
                },
                {
                    'question': 'Question 1',
                    'answer': 'Answer',
                },
            ])
        except TypeError:
            self.fail()

    def testConstructor_missingQuestion(self):
        self.assertRaises(ex.SerializeException, self.makeTest, [
                {
                    'answer': 'Answer',
                },
                {
                    'question': 'Question 1',
                    'answer': 'Answer',
                },
            ])

    def testConstructor_missingAnswer(self):
        self.assertRaises(ex.SerializeException, self.makeTest, [
                {
                    'question': 'Question 1',
                    'answer': 'Answer',
                },
                {
                    'question': 'Question 1',
                },
            ])

    def testRun_noCases(self):
        test = self.makeTest([])
        self.assertEqual({
            'passed': 0,
            'failed': 0,
            'locked': 0,
        }, test.run(self.TEST_NAME, self.SUITE_NUMBER))

    def testRun_lockedCases(self):
        test = self.makeTest([
            {
                'question': 'Question 1',
                'answer': 'Answer',
                'locked': True,
            },
            {
                'question': 'Question 1',
                'answer': 'Answer',
            },
        ])
        self.assertEqual({
            'passed': 0,
            'failed': 0,
            'locked': 2,    # Can't continue if preceding test is locked.
        }, test.run(self.TEST_NAME, self.SUITE_NUMBER))

    def testRun_noLockedCases(self):
        test = self.makeTest([
            {
                'question': 'Question 1',
                'answer': 'Answer',
            },
            {
                'question': 'Question 1',
                'answer': 'Answer',
            },
        ])
        self.assertEqual({
            'passed': 2,
            'failed': 0,
            'locked': 0,
        }, test.run(self.TEST_NAME, self.SUITE_NUMBER))


# Copyright (c) Metakernel Development Team.
# Distributed under the terms of the Modified BSD License.

from metakernel import Magic

class GetMagic(Magic):
    def line_get(self, variable):
        """
        %get VARIABLE - get a variable from the kernel in a Python-type.

        This line magic is used to get a variable.

        Examples:
            %get x 
        """
        self.retval = self.kernel.get_variable(variable)

    def post_process(self, retval):
        return self.retval


def register_magics(kernel):
   kernel.register_magics(GetMagic)


from metakernel.tests.utils import (get_kernel, get_log_text, 
                                    clear_log_text, EvalKernel)
import os

def test_download_magic():
    kernel = get_kernel(EvalKernel)
    kernel.do_execute("%download --filename TEST.txt https://raw.githubusercontent.com/calysto/metakernel/master/LICENSE.txt")
    text = get_log_text(kernel)
    assert "Downloaded 'TEST.txt'" in text, text
    assert os.path.isfile("TEST.txt"), "File does not exist: TEST.txt"

    clear_log_text(kernel)

    kernel.do_execute("%download https://raw.githubusercontent.com/calysto/metakernel/master/LICENSE.txt")
    text = get_log_text(kernel)
    assert "Downloaded 'LICENSE.txt'" in text, text
    assert os.path.isfile("LICENSE.txt"), "File does not exist: LICENSE.txt"


def teardown():
    for fname in ['TEST.txt', 'LICENSE.txt']:
        try:
            os.remove(fname)
        except:
            pass

"""
==========
tutormagic
==========

Magics to display pythontutor.com in the notebook.
"""

# Based on Kiko Correoso's IPython magic:
# https://github.com/kikocorreoso/tutormagic
# and Doug Blank's
# http://jupyter.cs.brynmawr.edu/hub/dblank/public/Examples/Online%20Python%20Tutor.ipynb
#-----------------------------------------------------------------------------
# Copyright (C) 2015 Kiko Correoso and the pythontutor.com developers
#
# Distributed under the terms of the MIT License. The full license is in
# the file LICENSE, distributed as part of this software.
#
# Contributors:
#   kikocorreoso
#-----------------------------------------------------------------------------

from metakernel import Magic, option
from IPython.display import IFrame
import sys
if sys.version_info.major == 2 and sys.version_info.minor == 7:
    from urllib import quote
elif sys.version_info.major == 3 and sys.version_info.minor >= 3:
    from urllib.parse import quote
    
class TutorMagic(Magic):

    @option(
        '-l', '--language', action='store', nargs = 1,
        help=("Possible languages to be displayed within the iframe. " +
              "Possible values are: python, python2, python3, java, javascript")
    )
    def cell_tutor(self, language=None):
        """
        %%tutor [--language=LANGUAGE] - show cell with 
        Online Python Tutor.

        Defaults to use the language of the current kernel.
        'python' is an alias for 'python3'.

        Examples:
           %%tutor -l python3
           a = 1
           b = 1
           a + b

           [You will see an iframe with the pythontutor.com page 
           including the code above.]

           %%tutor --language=java
           
           public class Test {
               public Test() {
               }
               public static void main(String[] args) {
                   int x = 1;
                   System.out.println("Hi");
               }
           }
        """
        if language is None:
            language = self.kernel.language_info["name"]
        if language not in ['python', 'python2', 'python3', 'java', 'javascript']:
            raise ValueError("{} not supported. Only the following options are allowed: "
                             "'python2', 'python3', 'java', 'javascript'".format(language))
        
        url = "https://pythontutor.com/iframe-embed.html#code="
        url += quote(self.code)
        url += "&origin=opt-frontend.js&cumulative=false&heapPrimitives=false"
        url += "&textReferences=false&"
        if language in ["python3", "python"]:
            url += "py=3&rawInputLstJSON=%5B%5D&curInstr=0&codeDivWidth=350&codeDivHeight=400"
        elif language == "python2":
            url += "py=2&rawInputLstJSON=%5B%5D&curInstr=0&codeDivWidth=350&codeDivHeight=400"
        elif language == "java":
            url += "py=java&rawInputLstJSON=%5B%5D&curInstr=0&codeDivWidth=350&codeDivHeight=400"
        elif language == "javascript":
            url += "py=js&rawInputLstJSON=%5B%5D&curInstr=0&codeDivWidth=350&codeDivHeight=400"

        # Display the results in the output area
        self.kernel.Display(IFrame(url, height=500, width="100%"))
        self.evaluate = False

def register_magics(kernel):
    kernel.register_magics(TutorMagic)

def register_ipython_magics():
    from metakernel import IPythonKernel
    from IPython.core.magic import register_cell_magic
    kernel = IPythonKernel()
    magic = TutorMagic(kernel)

    @register_cell_magic
    def tutor(line, cell):
        magic.code = cell
        magic.cell_tutor(language="python3")

# nice plotting functions.

# %pyplot, %plt, %plot
@register_line_magic('plt')
@register_line_magic('plot') #this is just alias: if I type plot, I better plot. 
@register_line_magic('pyplot')
@register_line_magic('pyploy') #common mispelling I make
def _pyplot(line):
    _ip.run_line_magic('matplotlib', line)
    _ip.run_code("""from matplotlib import pyplot as plt""")

    # use Bayesian Methods for Hackers plotting style
    _ip.run_code("""plt.style.use('bmh')""")

    # better hists
    def hist_(*args, **kwargs):
        kwargs.pop('alpha', None)
        kwargs.pop('histtype', None)
        kwargs.pop('normed', None)
        return plt.hist(*args, histtype='stepfilled', alpha=0.85, normed=True, **kwargs)

    # <3 figsize
    def figsize(sizex, sizey):
        """Set the default figure size to be [sizex, sizey].
        This is just an easy to remember, convenience wrapper that sets::
          matplotlib.rcParams['figure.figsize'] = [sizex, sizey]
        """
        import matplotlib
        matplotlib.rcParams['figure.figsize'] = [sizex, sizey]

    # aliases
    _ip.user_ns['hist_'] = hist_
    _ip.user_ns['figsize'] = figsize
    _ip.user_ns['plot'] = plt.plot
    _ip.user_ns['subplot'] = plt.subplot

del _pyplot

# test_generate_datasets.py
from __future__ import print_function
import os

import pytest
import matplotlib.pyplot as plt

from lifelines.estimation import KaplanMeierFitter, NelsonAalenFitter
from lifelines.generate_datasets import exponential_survival_data


def test_exponential_data_sets_correct_censor():
    print(os.environ)
    N = 20000
    censorship = 0.2
    T, C = exponential_survival_data(N, censorship, scale=10)
    assert abs(C.mean() - (1 - censorship)) < 0.02


@pytest.mark.skipif("DISPLAY" not in os.environ, reason="requires display")
def test_exponential_data_sets_fit():
    N = 20000
    T, C = exponential_survival_data(N, 0.2, scale=10)
    naf = NelsonAalenFitter()
    naf.fit(T, C).plot()
    plt.title("Should be a linear with slope = 0.1")


@pytest.mark.skipif("DISPLAY" not in os.environ, reason="requires display")
def test_kmf_minimum_observation_bias():
    N = 250
    kmf = KaplanMeierFitter()
    T, C = exponential_survival_data(N, 0.1, scale=10)
    B = 0.01 * T
    kmf.fit(T, C, entry=B)
    kmf.plot()
    plt.title("Should have larger variances in the tails")

#!/usr/bin/env python
# -*- coding: utf-8 -*-

##
# IbPy package root.
#
##

# these values substituted during release build.
api = "0"
version = "0"
revision = "r0"



#!/usr/bin/env python
""" generated source for module UnderComp """
#
# Original file copyright original author(s).
# This file copyright Troy Melhase, troy@gci.net.
#
# WARNING: all changes to this file will be lost.

# 
#  * UnderComp.java
#  *
#  
# package: com.ib.client
class UnderComp(object):
    """ generated source for class UnderComp """
    m_conId = 0
    m_delta = float()
    m_price = float()

    def __init__(self):
        """ generated source for method __init__ """
        self.m_conId = 0
        self.m_delta = 0
        self.m_price = 0

    def __eq__(self, p_other):
        """ generated source for method equals """
        if self is p_other:
            return True
        if p_other is None or not (isinstance(p_other, (UnderComp, ))):
            return False
        l_theOther = p_other
        if self.m_conId != l_theOther.m_conId:
            return False
        if self.m_delta != l_theOther.m_delta:
            return False
        if self.m_price != l_theOther.m_price:
            return False
        return True


# coding=utf-8
"""treecheckboxdialog.py - tree checkbox dialog for selection of tree branches
"""

import wx


class TreeCheckboxDialog(wx.Dialog):
    """A dialog for "selecting" items on a tree by checking them"""

    def __init__(self, parent, d, *args, **kwargs):
        """Initialize the dialog

        d - dictionary representing the tree.
            Keys form the dictionary labels, values are dictionaries of subtrees
            A leaf is marked with a dictionary entry whose key is None and
            whose value is True or False, depending on whether it is
            initially selected or not.
        """
        wx.Dialog.__init__(self, parent, *args, **kwargs)

        self.bitmaps = []
        self.parent_reflects_child = True
        sizer = wx.BoxSizer(wx.VERTICAL)
        self.SetSizer(sizer)
        tree_style = wx.TR_DEFAULT_STYLE
        self.tree_ctrl = wx.TreeCtrl(self,
                                     style=tree_style)
        sizer.Add(self.tree_ctrl, 1, wx.EXPAND | wx.ALL, 5)

        image_list = wx.ImageList(16, 16)
        for i, state_flag in enumerate(
                (0, wx.CONTROL_CHECKED, wx.CONTROL_UNDETERMINED)):
            for j, selection_flag in enumerate((0, wx.CONTROL_CURRENT)):
                idx = image_list.Add(
                        self.get_checkbox_bitmap(state_flag | selection_flag,
                                                 16, 16))
        self.tree_ctrl.SetImageList(image_list)
        self.image_list = image_list
        image_index, selected_image_index = self.img_idx(d)
        root_id = self.tree_ctrl.AddRoot("All", image_index,
                                         selected_image_index,
                                         wx.TreeItemData(d))
        self.tree_ctrl.SetItemImage(root_id, image_index,
                                    wx.TreeItemIcon_Normal)
        self.tree_ctrl.SetItemImage(root_id, selected_image_index,
                                    wx.TreeItemIcon_Selected)
        self.tree_ctrl.SetItemImage(root_id, image_index,
                                    wx.TreeItemIcon_Expanded)
        self.tree_ctrl.SetItemImage(root_id, image_index,
                                    wx.TreeItemIcon_SelectedExpanded)
        self.root_id = root_id
        self.tree_ctrl.SetItemHasChildren(root_id, len(d) > 1)
        self.Bind(wx.EVT_TREE_ITEM_EXPANDING, self.on_expanding, self.tree_ctrl)
        self.tree_ctrl.Bind(wx.EVT_LEFT_DOWN, self.on_left_down)
        self.tree_ctrl.Expand(root_id)
        table_sizer = wx.GridBagSizer()
        sizer.Add(table_sizer, 0, wx.EXPAND)
        table_sizer.Add(wx.StaticText(self, label='Key:'), (0, 0), flag=wx.LEFT | wx.RIGHT, border=3)
        for i, (bitmap, description) in enumerate((
                (image_list.GetBitmap(0), "No subitems selected / not selected"),
                (image_list.GetBitmap(2), "All subitems selected / selected"),
                (image_list.GetBitmap(4), "Some subitems selected. Open tree to see selections."))):
            bitmap_ctrl = wx.StaticBitmap(self)
            bitmap_ctrl.SetBitmap(bitmap)
            table_sizer.Add(bitmap_ctrl, (i, 1), flag=wx.RIGHT, border=5)
            table_sizer.Add(wx.StaticText(self, label=description), (i, 2))
        table_sizer.AddGrowableCol(2)
        sizer.Add(self.CreateStdDialogButtonSizer(wx.CANCEL | wx.OK),
                  flag=wx.CENTER)
        self.Layout()

    def set_parent_reflects_child(self, value):
        """Set the "parent_reflects_child" flag

        If you uncheck all of a parent's children, maybe that means
        that the parent should be unchecked too. But imagine the case
        where the user is checking and unchecking subdirectories. Perhaps
        they want the files in the parent, but not in the child. Set this
        to False to make the parent state be "None" if all children are False.
        This drives the parent to None instead of False, indicating that
        files should be picked up from the currenet directory, but not kids."""
        self.parent_reflects_child = value

    @staticmethod
    def img_idx(d):
        if d[None] is False:
            return 0, 1
        elif d[None] is True:
            return 2, 3
        else:
            return 4, 5

    def get_item_data(self, item_id):
        x = self.tree_ctrl.GetItemData(item_id)
        d = x.GetData()
        return d

    def on_expanding(self, event):
        """Populate subitems on expansion"""
        item_id = event.GetItem()
        d = self.get_item_data(item_id)
        if len(d) > 1:
            self.populate(item_id)

    def populate(self, item_id):
        """Populate the subitems of a tree"""
        try:
            d = self.get_item_data(item_id)
            assert len(d) > 1
            if self.tree_ctrl.GetChildrenCount(item_id, False) == 0:
                for key in sorted([x for x in d.keys() if x is not None]):
                    d1 = d[key]
                    if hasattr(d1, "__call__"):
                        # call function to get real value
                        self.SetCursor(wx.StockCursor(wx.CURSOR_WAIT))
                        d1 = d1()
                        d[key] = d1
                    image_index, selected_index = self.img_idx(d1)
                    sub_id = self.tree_ctrl.AppendItem(item_id, key, image_index,
                                                       selected_index,
                                                       wx.TreeItemData(d1))
                    self.tree_ctrl.SetItemImage(sub_id, image_index,
                                                wx.TreeItemIcon_Normal)
                    self.tree_ctrl.SetItemImage(sub_id, selected_index,
                                                wx.TreeItemIcon_Selected)
                    self.tree_ctrl.SetItemImage(sub_id, image_index,
                                                wx.TreeItemIcon_Expanded)
                    self.tree_ctrl.SetItemImage(sub_id, selected_index,
                                                wx.TreeItemIcon_SelectedExpanded)
                    self.tree_ctrl.SetItemHasChildren(sub_id, len(d1) > 1)
        finally:
            self.SetCursor(wx.NullCursor)

    def on_left_down(self, event):
        item_id, where = self.tree_ctrl.HitTest(event.Position)
        if where & wx.TREE_HITTEST_ONITEMICON == 0:
            event.Skip()
            return

        d = self.get_item_data(item_id)
        if d[None] is None or d[None] is False:
            state = True
        else:
            state = False
        self.set_item_state(item_id, state)
        self.set_parent_state(item_id)

    def set_parent_state(self, item_id):
        if item_id != self.root_id:
            parent_id = self.tree_ctrl.GetItemParent(item_id)
            d_parent = self.get_item_data(parent_id)
            child_id, _ = self.tree_ctrl.GetFirstChild(parent_id)
            state = self.get_item_data(child_id)[None]
            while True:
                if child_id == self.tree_ctrl.GetLastChild(parent_id):
                    break
                child_id = self.tree_ctrl.GetNextSibling(child_id)
                next_state = self.get_item_data(child_id)[None]
                if next_state != state:
                    state = None
                    break

            if d_parent[None] is not state:
                if state is False and not self.parent_reflects_child:
                    state = None
                d_parent[None] = state
                image_index, selected_index = self.img_idx(d_parent)
                self.tree_ctrl.SetItemImage(parent_id, image_index, wx.TreeItemIcon_Normal)
                self.tree_ctrl.SetItemImage(parent_id, selected_index, wx.TreeItemIcon_Selected)
                self.tree_ctrl.SetItemImage(parent_id, image_index, wx.TreeItemIcon_Expanded)
                self.tree_ctrl.SetItemImage(parent_id, selected_index, wx.TreeItemIcon_SelectedExpanded)
                self.set_parent_state(parent_id)

    def set_item_state(self, item_id, state):
        d = self.get_item_data(item_id)
        d[None] = state
        image_index, selected_index = self.img_idx(d)
        self.tree_ctrl.SetItemImage(item_id, image_index, wx.TreeItemIcon_Normal)
        self.tree_ctrl.SetItemImage(item_id, selected_index, wx.TreeItemIcon_Selected)
        self.tree_ctrl.SetItemImage(item_id, image_index, wx.TreeItemIcon_Expanded)
        self.tree_ctrl.SetItemImage(item_id, selected_index, wx.TreeItemIcon_SelectedExpanded)
        if len(d) > 1:
            if self.tree_ctrl.GetChildrenCount(item_id) == 0:
                self.populate(item_id)
            child_id, _ = self.tree_ctrl.GetFirstChild(item_id)
            while True:
                d1 = self.get_item_data(child_id)
                if d1[None] is not state:
                    self.set_item_state(child_id, state)
                if child_id == self.tree_ctrl.GetLastChild(item_id):
                    break
                child_id = self.tree_ctrl.GetNextSibling(child_id)

    def get_checkbox_bitmap(self, flags, width, height):
        """Return a bitmap with a checkbox drawn into it

        flags - rendering flags including CONTROL_CHECKED and CONTROL_UNDETERMINED
        width, height - size of bitmap to return
        """
        dc = wx.MemoryDC()
        bitmap = wx.EmptyBitmap(width, height)
        dc.SelectObject(bitmap)
        dc.SetBrush(wx.BLACK_BRUSH)
        dc.SetTextForeground(wx.BLACK)
        try:
            dc.Clear()
            render = wx.RendererNative.Get()
            render.DrawCheckBox(self, dc, (0, 0, width, height), flags)
        finally:
            dc.SelectObject(wx.NullBitmap)
        dc.Destroy()
        self.bitmaps.append(bitmap)
        return bitmap

'''<b>Overlay Outlines</b> places outlines produced by an 
<b>Identify</b> module over a desired image.
<hr>
This module places outlines (in a special format produced by an <b>Identify</b> module)
on any desired image (grayscale, color, or blank). The
resulting image can be saved using the <b>SaveImages</b> module.

See also <b>IdentifyPrimaryObjects, IdentifySecondaryObjects, IdentifyTertiaryObjects</b>.
'''

import centrosome.outline
import numpy as np
from scipy.ndimage import distance_transform_edt

import cellprofiler.cpimage as cpi
import cellprofiler.cpmodule as cpm
import cellprofiler.settings as cps
from cellprofiler.settings import YES, NO

WANTS_COLOR = "Color"
WANTS_GRAYSCALE = "Grayscale"

MAX_IMAGE = "Max of image"
MAX_POSSIBLE = "Max possible"

COLORS = {"White": (1, 1, 1),
          "Black": (0, 0, 0),
          "Red": (1, 0, 0),
          "Green": (0, 1, 0),
          "Blue": (0, 0, 1),
          "Yellow": (1, 1, 0)}

COLOR_ORDER = ["Red", "Green", "Blue", "Yellow", "White", "Black"]

FROM_IMAGES = "Image"
FROM_OBJECTS = "Objects"

NUM_FIXED_SETTINGS_V1 = 5
NUM_FIXED_SETTINGS_V2 = 6
NUM_FIXED_SETTINGS_V3 = 6
NUM_FIXED_SETTINGS = 6

NUM_OUTLINE_SETTINGS_V2 = 2
NUM_OUTLINE_SETTINGS_V3 = 4
NUM_OUTLINE_SETTINGS = 4


class OverlayOutlines(cpm.CPModule):
    module_name = 'OverlayOutlines'
    variable_revision_number = 3
    category = "Image Processing"

    def create_settings(self):
        self.blank_image = cps.Binary(
                "Display outlines on a blank image?",
                False, doc="""
            Select <i>%(YES)s</i> to produce an
            image of the outlines on a black background.
            <p>Select <i>%(NO)s</i>, the module will overlay the
            outlines on an image of your choosing.</p>""" % globals())

        self.image_name = cps.ImageNameSubscriber(
                "Select image on which to display outlines", cps.NONE, doc="""
            <i>(Used only when a blank image has not been selected)</i> <br>
            Choose the image to serve as the background for the outlines.
            You can choose from images that were loaded or created by modules
            previous to this one.""")

        self.line_width = cps.Float(
                "Width of outlines", "1", doc="""
            Enter the width, in pixels, of the
            outlines to be displayed on the image.""")

        self.output_image_name = cps.ImageNameProvider(
                "Name the output image", "OrigOverlay", doc="""
            Enter the name of the output image with the outlines overlaid.
            This image can be selected in later modules (for instance, <b>SaveImages</b>).""")

        self.wants_color = cps.Choice(
                "Outline display mode",
                [WANTS_COLOR, WANTS_GRAYSCALE], doc="""
            Specify how to display the outline contours around
            your objects. Color outlines produce a clearer display for
            images where the cell borders have a high intensity, but take
            up more space in memory. Grayscale outlines are displayed with
            either the highest possible intensity or the same intensity
            as the brightest pixel in the image.""")

        self.spacer = cps.Divider(line=False)

        self.max_type = cps.Choice(
                "Select method to determine brightness of outlines",
                [MAX_IMAGE, MAX_POSSIBLE], doc="""
            <i>(Used only when outline display mode is grayscale)</i> <br>
            The following options are possible for setting the intensity
            (brightness) of the outlines:
            <ul>
            <li><i>%(MAX_IMAGE)s:</i> Set the brighness to the
            the same as the brightest point in the image.</li>
            <li><i>%(MAX_POSSIBLE)s:</i> Set to the maximum
            possible value for this image format.</li>
            </ul>
            If your image is quite dim, then putting bright white lines
            onto it may not be useful. It may be preferable to make the
            outlines equal to the maximal brightness already occurring
            in the image.""" % globals())

        self.outlines = []
        self.add_outline(can_remove=False)
        self.add_outline_button = cps.DoSomething("", "Add another outline", self.add_outline)

    def add_outline(self, can_remove=True):
        group = cps.SettingsGroup()
        if can_remove:
            group.append("divider", cps.Divider(line=False))

        group.append("outline_choice", cps.Choice(
                "Load outlines from an image or objects?",
                [FROM_OBJECTS, FROM_IMAGES], doc="""
            This setting selects what source the outlines come from:
            <ul>
            <li><i>%(FROM_OBJECTS)s:</i> Create the image directly from the
            objects. This option will improve the functionality of the
            contrast options for this module's interactive display and will
            save memory.</li>
            <li><i>%(FROM_IMAGES)s:</i> Prior versions of <b>OverlayOutlines</b> would only
            display outline images which were optional outputs of the identify
            modules. For legacy pipelines or to continue using the outline
            images instead of objects, choose this option.</li>
            </ul>
            """ % globals()))

        group.append("objects_name", cps.ObjectNameSubscriber(
                "Select objects to display", cps.NONE,
                doc="""Choose the objects whose outlines you would like
        to display."""))
        group.append("outline_name", cps.OutlineNameSubscriber(
                "Select outlines to display",
                cps.NONE, doc="""
            Choose outlines to display, from a previous <b>Identify</b>
            module. Each of the <b>Identify</b> modules has a checkbox that
            determines whether the outlines are saved. If you have checked this,
            you were asked to supply a name for the outline; you
            can then select that name here.
            """))

        default_color = (COLOR_ORDER[len(self.outlines)]
                         if len(self.outlines) < len(COLOR_ORDER)
                         else COLOR_ORDER[0])
        group.append("color", cps.Color(
                "Select outline color", default_color))
        if can_remove:
            group.append("remover", cps.RemoveSettingButton("", "Remove this outline", self.outlines, group))

        self.outlines.append(group)

    def prepare_settings(self, setting_values):
        num_settings = \
            (len(setting_values) - NUM_FIXED_SETTINGS) / NUM_OUTLINE_SETTINGS
        if len(self.outlines) == 0:
            self.add_outline(False)
        elif len(self.outlines) > num_settings:
            del self.outlines[num_settings:]
        else:
            for i in range(len(self.outlines), num_settings):
                self.add_outline()

    def settings(self):
        result = [self.blank_image, self.image_name, self.output_image_name,
                  self.wants_color, self.max_type, self.line_width]
        for outline in self.outlines:
            result += [outline.outline_name, outline.color,
                       outline.outline_choice, outline.objects_name]
        return result

    def visible_settings(self):
        result = [self.blank_image]
        if not self.blank_image.value:
            result += [self.image_name]
        result += [self.output_image_name, self.wants_color,
                   self.line_width, self.spacer]
        if (self.wants_color.value == WANTS_GRAYSCALE and not
        self.blank_image.value):
            result += [self.max_type]
        for outline in self.outlines:
            result += [outline.outline_choice]
            if self.wants_color.value == WANTS_COLOR:
                result += [outline.color]
            if outline.outline_choice == FROM_IMAGES:
                result += [outline.outline_name]
            else:
                result += [outline.objects_name]
            if hasattr(outline, "remover"):
                result += [outline.remover]
        result += [self.add_outline_button]
        return result

    def run(self, workspace):
        if self.wants_color.value == WANTS_COLOR:
            pixel_data = self.run_color(workspace)
        else:
            pixel_data = self.run_bw(workspace)
        if self.blank_image.value:
            output_image = cpi.Image(pixel_data)
            workspace.image_set.add(self.output_image_name.value, output_image)
        else:
            image = workspace.image_set.get_image(self.image_name.value)
            output_image = cpi.Image(pixel_data, parent_image=image)
            workspace.image_set.add(self.output_image_name.value, output_image)
            workspace.display_data.image_pixel_data = image.pixel_data
        if self.__can_composite_objects() and self.show_window:
            workspace.display_data.labels = {}
            for outline in self.outlines:
                name = outline.objects_name.value
                objects = workspace.object_set.get_objects(name)
                workspace.display_data.labels[name] = \
                    [labels for labels, indexes in objects.get_labels()]

        workspace.display_data.pixel_data = pixel_data

    def __can_composite_objects(self):
        '''Return True if we can use object compositing during display'''
        for outline in self.outlines:
            if outline.outline_choice == FROM_IMAGES:
                return False
        return True

    def display(self, workspace, figure):
        from cellprofiler.gui.cpfigure import CPLD_LABELS, CPLD_NAME, \
            CPLD_OUTLINE_COLOR, CPLD_MODE, CPLDM_OUTLINES, CPLDM_ALPHA, \
            CPLDM_NONE, CPLD_LINE_WIDTH, CPLD_ALPHA_COLORMAP, CPLD_ALPHA_VALUE

        figure.set_subplots((1, 1))

        if self.__can_composite_objects():
            if self.blank_image:
                pixel_data = np.zeros(workspace.display_data.pixel_data.shape)
            else:
                pixel_data = workspace.display_data.image_pixel_data
            cplabels = []
            ldict = workspace.display_data.labels
            for outline in self.outlines:
                name = outline.objects_name.value
                if self.wants_color.value == WANTS_COLOR:
                    color = np.array(outline.color.to_rgb(), float)
                else:
                    color = np.ones(3) * 255.0
                d = {CPLD_NAME: name,
                     CPLD_LABELS: ldict[name],
                     CPLD_OUTLINE_COLOR: color,
                     CPLD_MODE: CPLDM_OUTLINES,
                     CPLD_LINE_WIDTH: self.line_width.value}
                cplabels.append(d)
        else:
            pixel_data = workspace.display_data.pixel_data
            cplabels = None
        if self.blank_image.value:
            if self.wants_color.value == WANTS_COLOR:
                figure.subplot_imshow(0, 0, pixel_data,
                                      self.output_image_name.value,
                                      cplabels=cplabels)
            else:
                figure.subplot_imshow_bw(0, 0, pixel_data,
                                         self.output_image_name.value,
                                         cplabels=cplabels)
        else:
            figure.set_subplots((2, 1))

            image_pixel_data = workspace.display_data.image_pixel_data
            if image_pixel_data.ndim == 2:
                figure.subplot_imshow_bw(0, 0, image_pixel_data,
                                         "Original: %s" %
                                         self.image_name.value)
            else:
                figure.subplot_imshow_color(0, 0, image_pixel_data,
                                            "Original: %s" %
                                            self.image_name.value)
            if self.wants_color.value == WANTS_COLOR:
                if cplabels is not None and pixel_data.ndim == 2:
                    fn = figure.subplot_imshow_grayscale
                else:
                    fn = figure.subplot_imshow
                fn(1, 0, pixel_data,
                   self.output_image_name.value,
                   sharexy=figure.subplot(0, 0),
                   cplabels=cplabels)
            else:
                figure.subplot_imshow_bw(1, 0, pixel_data,
                                         self.output_image_name.value,
                                         sharexy=figure.subplot(0, 0),
                                         cplabels=cplabels)

    def run_bw(self, workspace):
        image_set = workspace.image_set
        if self.blank_image.value:
            shape = self.get_outline(workspace, self.outlines[0]).shape[:2]
            pixel_data = np.zeros(shape)
            maximum = 1
        else:
            image = image_set.get_image(self.image_name.value,
                                        must_be_grayscale=True)
            pixel_data = image.pixel_data
            maximum = 1 if self.max_type == MAX_POSSIBLE else np.max(pixel_data)
            pixel_data = pixel_data.copy()
        for outline in self.outlines:
            mask = self.get_outline(workspace, outline)
            i_max = min(mask.shape[0], pixel_data.shape[0])
            j_max = min(mask.shape[1], pixel_data.shape[1])
            mask = mask[:i_max, :j_max]
            pixel_data[:i_max, :j_max][mask] = maximum
        return pixel_data

    def run_color(self, workspace):
        image_set = workspace.image_set
        if self.blank_image.value:
            pixel_data = None
            pdmax = 1
        else:
            image = image_set.get_image(self.image_name.value)
            pixel_data = image.pixel_data
            if pixel_data.ndim == 2:
                pixel_data = np.dstack((pixel_data, pixel_data, pixel_data))
            else:
                pixel_data = pixel_data.copy()
            pdmax = float(np.max(pixel_data))
            if pdmax <= 0:
                pdmax = 1
        for outline in self.outlines:
            outline_img = self.get_outline(workspace, outline)
            if pixel_data is None:
                pixel_data = np.zeros(list(outline_img.shape[:2]) + [3], np.float32)
            i_max = min(outline_img.shape[0], pixel_data.shape[0])
            j_max = min(outline_img.shape[1], pixel_data.shape[1])
            outline_img = outline_img[:i_max, :j_max, :]
            window = pixel_data[:i_max, :j_max, :]
            # Original:
            #   alpha = outline_img[:,:,3]
            #   pixel_data[:i_max, :j_max, :] = (
            #       window * (1 - alpha[:,:,np.newaxis]) +
            #       outline_img[:,:,:3] * alpha[:,:,np.newaxis] * pdmax)
            #
            # Memory reduced:
            alpha = outline_img[:, :, 3]
            outline_img[:, :, :3] *= pdmax
            outline_img[:, :, :3] *= alpha[:, :, np.newaxis]
            # window is a view on pixel_data
            window *= (1 - alpha)[:, :, np.newaxis]
            window += outline_img[:, :, :3]

        return pixel_data

    def get_outline(self, workspace, outline):
        '''Get outline, with aliasing and taking widths into account'''
        if outline.outline_choice == FROM_IMAGES:
            name = outline.outline_name.value
            pixel_data = workspace.image_set.get_image(name).pixel_data
        else:
            name = outline.objects_name.value
            objects = workspace.object_set.get_objects(name)
            pixel_data = np.zeros(objects.shape, bool)
            for labels, indexes in objects.get_labels():
                pixel_data = \
                    pixel_data | centrosome.outline.outline(labels)
        if self.wants_color == WANTS_GRAYSCALE:
            return pixel_data.astype(bool)
        color = np.array(outline.color.to_rgb(), float) / 255.0
        if pixel_data.ndim == 2:
            if len(color) == 3:
                color = np.hstack((color, [1]))
            pixel_data = pixel_data > 0
            output_image = color[np.newaxis, np.newaxis, :] * pixel_data[:, :, np.newaxis]
        else:
            output_image = np.dstack([pixel_data[:, :, i] for i in range(3)] +
                                     [np.sum(pixel_data, 2) > 0])
        # float16s are slower, but since we're potentially allocating an image
        # 4 times larger than our input, the tradeoff is worth it.
        if hasattr(np, 'float16'):
            output_image = output_image.astype(np.float16)
        if self.line_width.value > 1:
            half_line_width = float(self.line_width.value) / 2
            d, (i, j) = distance_transform_edt(output_image[:, :, 3] == 0,
                                               return_indices=True)
            mask = (d > 0) & (d <= half_line_width - .5)
            output_image[mask, :] = output_image[i[mask], j[mask], :]
            #
            # Do a little aliasing here using an alpha channel
            #
            mask = ((d > max(0, half_line_width - .5)) &
                    (d < half_line_width + .5))
            d = half_line_width + .5 - d
            output_image[mask, :3] = output_image[i[mask], j[mask], :3]
            output_image[mask, 3] = d[mask]

        return output_image

    def upgrade_settings(self, setting_values, variable_revision_number,
                         module_name, from_matlab):
        if from_matlab and variable_revision_number == 2:
            # Order is
            # image_name
            # outline name
            # max intensity
            # output_image_name
            # color
            setting_values = [cps.YES if setting_values[0] == "Blank" else cps.NO,
                              setting_values[0],
                              setting_values[3],
                              WANTS_COLOR,
                              setting_values[2],
                              setting_values[1],
                              setting_values[4]]
            from_matlab = False
            variable_revision_number = 1
        if (not from_matlab) and variable_revision_number == 1:
            #
            # Added line width
            #
            setting_values = setting_values[:NUM_FIXED_SETTINGS_V1] + \
                             ["1"] + setting_values[NUM_FIXED_SETTINGS_V1:]
            variable_revision_number = 2

        if (not from_matlab) and variable_revision_number == 2:
            #
            # Added overlay image / objects choice
            #
            new_setting_values = setting_values[:NUM_FIXED_SETTINGS_V2]
            for i in range(NUM_FIXED_SETTINGS_V2, len(setting_values),
                           NUM_OUTLINE_SETTINGS_V2):
                new_setting_values += \
                    setting_values[i:(i + NUM_OUTLINE_SETTINGS_V2)]
                new_setting_values += [FROM_IMAGES, cps.NONE]
            setting_values = new_setting_values
            variable_revision_number = 3
        return setting_values, variable_revision_number, from_matlab

"""test_correctilluminationcalculate.py - test the CorrectIlluminationCalculate module
"""

import base64
import sys
import unittest
import zlib
from StringIO import StringIO

import numpy as np

from cellprofiler.preferences import set_headless

set_headless()

import cellprofiler.pipeline as cpp
import cellprofiler.settings as cps
import cellprofiler.cpimage as cpi
import cellprofiler.workspace as cpw
import cellprofiler.objects as cpo
import cellprofiler.measurements as cpm
import cellprofiler.modules.injectimage as inj
import cellprofiler.modules.correctilluminationcalculate as calc

INPUT_IMAGE_NAME = "MyImage"
OUTPUT_IMAGE_NAME = "MyResult"
AVERAGE_IMAGE_NAME = "Ave"
DILATED_IMAGE_NAME = "Dilate"


class TestCorrectImage_Calculate(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        '''Backwards compatibility for Python 2.6 unittest'''
        if not hasattr(cls, "assertIn"):
            cls.assertIn = lambda self, x, y: self.assertTrue(x in y)
        if not hasattr(cls, "assertNotIn"):
            cls.assertNotIn = lambda self, x, y: self.assertFalse(x in y)

    def error_callback(self, calller, event):
        if isinstance(event, cpp.RunExceptionEvent):
            self.fail(event.error.message)

    def make_workspaces(self, images_and_masks):
        '''Make a workspace for each image set provided

        images_and_masks - a collection of two-tuples: image+mask

        returns a list of workspaces + the module
        '''
        image_set_list = cpi.ImageSetList()
        workspaces = []
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 1
        module.image_name.value = INPUT_IMAGE_NAME
        module.illumination_image_name.value = OUTPUT_IMAGE_NAME
        module.average_image_name.value = AVERAGE_IMAGE_NAME
        module.dilated_image_name.value = DILATED_IMAGE_NAME
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        measurements = cpm.Measurements()

        for i, (image, mask) in enumerate(images_and_masks):
            image_set = image_set_list.get_image_set(i)
            if mask is None:
                image = cpi.Image(image)
            else:
                image = cpi.Image(image, mask)
            image_set.add(INPUT_IMAGE_NAME, image)
            workspace = cpw.Workspace(
                    pipeline, module, image_set, cpo.ObjectSet(),
                    measurements, image_set_list)
            workspaces.append(workspace)
        return workspaces, module

    def test_00_00_zeros(self):
        """Test all combinations of options with an image of all zeros"""
        for image in (np.zeros((10, 10)), np.zeros((10, 10, 3))):
            pipeline = cpp.Pipeline()
            pipeline.add_listener(self.error_callback)
            inj_module = inj.InjectImage("MyImage", image)
            inj_module.module_num = 1
            pipeline.add_module(inj_module)
            module = calc.CorrectIlluminationCalculate()
            module.module_num = 2
            pipeline.add_module(module)
            module.image_name.value = "MyImage"
            module.illumination_image_name.value = "OutputImage"
            module.save_average_image.value = True
            module.save_dilated_image.value = True

            for ea in (calc.EA_EACH, calc.EA_ALL_ACROSS, calc.EA_ALL_FIRST):
                module.each_or_all.value = ea
                for intensity_choice in (calc.IC_BACKGROUND, calc.IC_REGULAR):
                    module.intensity_choice.value = intensity_choice
                    for dilate_objects in (True, False):
                        module.dilate_objects.value = dilate_objects
                        for rescale_option in (cps.YES, cps.NO, calc.RE_MEDIAN):
                            module.rescale_option.value = rescale_option
                            for smoothing_method \
                                    in (calc.SM_NONE, calc.SM_FIT_POLYNOMIAL,
                                        calc.SM_GAUSSIAN_FILTER, calc.SM_MEDIAN_FILTER,
                                        calc.SM_TO_AVERAGE, calc.SM_SPLINES,
                                        calc.SM_CONVEX_HULL):
                                module.smoothing_method.value = smoothing_method
                                for ow in (calc.FI_AUTOMATIC, calc.FI_MANUALLY,
                                           calc.FI_OBJECT_SIZE):
                                    module.automatic_object_width.value = ow
                                    measurements = cpm.Measurements()
                                    image_set_list = cpi.ImageSetList()
                                    workspace = cpw.Workspace(
                                            pipeline, None, None, None,
                                            measurements, image_set_list)
                                    pipeline.prepare_run(workspace)
                                    inj_module.prepare_group(workspace, {}, [1])
                                    module.prepare_group(workspace, {}, [1])
                                    image_set = image_set_list.get_image_set(0)
                                    object_set = cpo.ObjectSet()
                                    workspace = cpw.Workspace(pipeline,
                                                              inj_module,
                                                              image_set,
                                                              object_set,
                                                              measurements,
                                                              image_set_list)
                                    inj_module.run(workspace)
                                    module.run(workspace)
                                    image = image_set.get_image("OutputImage")
                                    self.assertTrue(image is not None)
                                    self.assertTrue(np.all(image.pixel_data == 0),
                                                    """Failure case:
                intensity_choice = %(intensity_choice)s
                dilate_objects = %(dilate_objects)s
                rescale_option = %(rescale_option)s
                smoothing_method = %(smoothing_method)s
                automatic_object_width = %(ow)s""" % locals())

    def test_01_01_ones_image(self):
        """The illumination correction of an image of all ones should be uniform

        """
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        for image in (np.ones((10, 10)), np.ones((10, 10, 3))):
            inj_module = inj.InjectImage("MyImage", image)
            inj_module.module_num = 1
            pipeline.add_module(inj_module)
            module = calc.CorrectIlluminationCalculate()
            module.module_num = 2
            pipeline.add_module(module)
            module.image_name.value = "MyImage"
            module.illumination_image_name.value = "OutputImage"
            module.rescale_option.value = cps.YES

            for ea in (calc.EA_EACH, calc.EA_ALL_ACROSS, calc.EA_ALL_FIRST):
                module.each_or_all.value = ea
                for intensity_choice in (calc.IC_BACKGROUND, calc.IC_REGULAR):
                    module.intensity_choice.value = intensity_choice
                    for dilate_objects in (True, False):
                        module.dilate_objects.value = dilate_objects
                        for smoothing_method \
                                in (calc.SM_NONE, calc.SM_FIT_POLYNOMIAL,
                                    calc.SM_GAUSSIAN_FILTER, calc.SM_MEDIAN_FILTER,
                                    calc.SM_TO_AVERAGE, calc.SM_SPLINES,
                                    calc.SM_CONVEX_HULL):
                            module.smoothing_method.value = smoothing_method
                            for ow in (calc.FI_AUTOMATIC, calc.FI_MANUALLY,
                                       calc.FI_OBJECT_SIZE):
                                module.automatic_object_width.value = ow
                                measurements = cpm.Measurements()
                                image_set_list = cpi.ImageSetList()
                                workspace = cpw.Workspace(
                                        pipeline, None, None, None,
                                        measurements, image_set_list)
                                pipeline.prepare_run(workspace)
                                inj_module.prepare_group(workspace, {}, [1])
                                module.prepare_group(workspace, {}, [1])
                                image_set = image_set_list.get_image_set(0)
                                object_set = cpo.ObjectSet()
                                workspace = cpw.Workspace(pipeline,
                                                          inj_module,
                                                          image_set,
                                                          object_set,
                                                          measurements,
                                                          image_set_list)
                                inj_module.run(workspace)
                                module.run(workspace)
                                image = image_set.get_image("OutputImage")
                                self.assertTrue(image is not None)
                                self.assertTrue(np.all(np.std(image.pixel_data) < .00001),
                                                """Failure case:
            each_or_all            = %(ea)s
            intensity_choice       = %(intensity_choice)s
            dilate_objects         = %(dilate_objects)s
            smoothing_method       = %(smoothing_method)s
            automatic_object_width = %(ow)s""" % locals())

    def test_01_02_masked_image(self):
        """A masked image should be insensitive to points outside the mask"""
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        np.random.seed(12)
        for image in (np.random.uniform(size=(10, 10)),
                      np.random.uniform(size=(10, 10, 3))):
            mask = np.zeros((10, 10), bool)
            mask[2:7, 3:8] = True
            image[mask] = 1
            inj_module = inj.InjectImage("MyImage", image, mask)
            inj_module.module_num = 1
            pipeline.add_module(inj_module)
            module = calc.CorrectIlluminationCalculate()
            module.module_num = 2
            pipeline.add_module(module)
            module.image_name.value = "MyImage"
            module.illumination_image_name.value = "OutputImage"
            module.rescale_option.value = cps.YES
            module.dilate_objects.value = False

            for ea in (calc.EA_EACH, calc.EA_ALL_ACROSS, calc.EA_ALL_FIRST):
                module.each_or_all.value = ea
                for intensity_choice in (calc.IC_BACKGROUND, calc.IC_REGULAR):
                    module.intensity_choice.value = intensity_choice
                    for smoothing_method \
                            in (calc.SM_NONE, calc.SM_FIT_POLYNOMIAL,
                                calc.SM_GAUSSIAN_FILTER, calc.SM_MEDIAN_FILTER,
                                calc.SM_TO_AVERAGE, calc.SM_CONVEX_HULL):
                        module.smoothing_method.value = smoothing_method
                        for ow in (calc.FI_AUTOMATIC, calc.FI_MANUALLY,
                                   calc.FI_OBJECT_SIZE):
                            module.automatic_object_width.value = ow
                            measurements = cpm.Measurements()
                            image_set_list = cpi.ImageSetList()
                            workspace = cpw.Workspace(
                                    pipeline, None, None, None,
                                    measurements, image_set_list)
                            pipeline.prepare_run(workspace)
                            inj_module.prepare_group(workspace, {}, [1])
                            module.prepare_group(workspace, {}, [1])
                            image_set = image_set_list.get_image_set(0)
                            object_set = cpo.ObjectSet()
                            workspace = cpw.Workspace(pipeline,
                                                      inj_module,
                                                      image_set,
                                                      object_set,
                                                      measurements,
                                                      image_set_list)
                            inj_module.run(workspace)
                            module.run(workspace)
                            image = image_set.get_image("OutputImage")
                            self.assertTrue(image is not None)
                            self.assertTrue(np.all(abs(image.pixel_data[mask] - 1 < .00001)),
                                            """Failure case:
            each_or_all            = %(ea)s
            intensity_choice       = %(intensity_choice)s
            smoothing_method       = %(smoothing_method)s
            automatic_object_width = %(ow)s""" % locals())

    def test_01_03_filtered(self):
        '''Regression test of issue #310

        post_group should add the composite image to the image set
        if CorrectIllumination_Calculate didn't run because the image
        set was filtered.
        '''
        r = np.random.RandomState()
        r.seed(13)
        i0 = r.uniform(size=(11, 13))
        i1 = r.uniform(size=(11, 13))
        i2 = r.uniform(size=(11, 13))
        workspaces, module = self.make_workspaces((
            (i0, None),
            (i1, None),
            (i2, None)))
        module.each_or_all.value = calc.EA_ALL_ACROSS
        module.smoothing_method.value = calc.SM_TO_AVERAGE
        module.save_average_image.value = True
        module.save_dilated_image.value = True

        module.prepare_group(workspaces[0], None, [1, 2, 3])
        assert isinstance(module, calc.CorrectIlluminationCalculate)
        for workspace in workspaces[:-1]:
            assert isinstance(workspace, cpw.Workspace)
            module.run(workspace)
        image_set = workspaces[-1].image_set
        self.assertNotIn(OUTPUT_IMAGE_NAME, image_set.get_names())
        self.assertNotIn(DILATED_IMAGE_NAME, image_set.get_names())
        self.assertNotIn(AVERAGE_IMAGE_NAME, image_set.get_names())
        module.post_group(workspaces[-1], None)
        self.assertIn(OUTPUT_IMAGE_NAME, image_set.get_names())
        self.assertIn(DILATED_IMAGE_NAME, image_set.get_names())
        self.assertIn(AVERAGE_IMAGE_NAME, image_set.get_names())

    def test_01_04_not_filtered(self):
        '''Regression test of issue #310, negative case

        post_group should not add the composite image to the image set
        if CorrectIllumination_Calculate did run.
        '''
        r = np.random.RandomState()
        r.seed(13)
        i0 = r.uniform(size=(11, 13))
        i1 = r.uniform(size=(11, 13))
        i2 = r.uniform(size=(11, 13))
        workspaces, module = self.make_workspaces((
            (i0, None),
            (i1, None),
            (i2, None)))
        module.each_or_all.value = calc.EA_ALL_ACROSS
        module.smoothing_method.value = calc.SM_TO_AVERAGE
        module.save_average_image.value = True
        module.save_dilated_image.value = True

        module.prepare_group(workspaces[0], None, [1, 2, 3])
        assert isinstance(module, calc.CorrectIlluminationCalculate)
        for workspace in workspaces:
            assert isinstance(workspace, cpw.Workspace)
            module.run(workspace)
        image_set = workspaces[-1].image_set
        self.assertIn(OUTPUT_IMAGE_NAME, image_set.get_names())
        self.assertIn(DILATED_IMAGE_NAME, image_set.get_names())
        self.assertIn(AVERAGE_IMAGE_NAME, image_set.get_names())
        module.post_group(workspaces[-1], None)
        #
        # Make sure it appears only once
        #
        for image_name in (
                OUTPUT_IMAGE_NAME, DILATED_IMAGE_NAME, AVERAGE_IMAGE_NAME):
            self.assertEqual(len(filter(lambda x: x == image_name,
                                        image_set.get_names())), 1)

    def test_02_02_Background(self):
        """Test an image with four distinct backgrounds"""

        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        image = np.ones((40, 40))
        image[10, 10] = .25
        image[10, 30] = .5
        image[30, 10] = .75
        image[30, 30] = .9
        inj_module = inj.InjectImage("MyImage", image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = "MyImage"
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value == calc.EA_EACH
        module.block_size.value = 20
        module.rescale_option.value = cps.NO
        module.dilate_objects.value = False
        module.smoothing_method.value = calc.SM_NONE
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        self.assertTrue(np.all(image.pixel_data[:20, :20] == .25))
        self.assertTrue(np.all(image.pixel_data[:20, 20:] == .5))
        self.assertTrue(np.all(image.pixel_data[20:, :20] == .75))
        self.assertTrue(np.all(image.pixel_data[20:, 20:] == .9))

    def test_03_00_no_smoothing(self):
        """Make sure that no smoothing takes place if smoothing is turned off"""
        input_image = np.random.uniform(size=(10, 10))
        image_name = "InputImage"
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_NONE
        module.rescale_option.value = cps.NO
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        self.assertTrue(np.all(np.abs(image.pixel_data - input_image) < .001),
                        "Failed to fit polynomial to %s" % image_name)

    def test_03_01_FitPolynomial(self):
        """Test fitting a polynomial to different gradients"""

        y, x = (np.mgrid[0:20, 0:20]).astype(float) / 20.0
        image_x = x
        image_y = y
        image_x2 = x ** 2
        image_y2 = y ** 2
        image_xy = x * y
        for input_image, image_name in ((image_x, "XImage"),
                                        (image_y, "YImage"),
                                        (image_x2, "X2Image"),
                                        (image_y2, "Y2Image"),
                                        (image_xy, "XYImage")):
            pipeline = cpp.Pipeline()
            pipeline.add_listener(self.error_callback)
            inj_module = inj.InjectImage(image_name, input_image)
            inj_module.module_num = 1
            pipeline.add_module(inj_module)
            module = calc.CorrectIlluminationCalculate()
            module.module_num = 2
            pipeline.add_module(module)
            module.image_name.value = image_name
            module.illumination_image_name.value = "OutputImage"
            module.intensity_choice.value = calc.IC_REGULAR
            module.each_or_all.value == calc.EA_EACH
            module.smoothing_method.value = calc.SM_FIT_POLYNOMIAL
            module.rescale_option.value = cps.NO
            module.dilate_objects.value = False
            measurements = cpm.Measurements()
            image_set_list = cpi.ImageSetList()
            workspace = cpw.Workspace(pipeline, None, None, None,
                                      measurements, image_set_list)
            pipeline.prepare_run(workspace)
            inj_module.prepare_group(workspace, {}, [1])
            module.prepare_group(workspace, {}, [1])
            image_set = image_set_list.get_image_set(0)
            object_set = cpo.ObjectSet()
            workspace = cpw.Workspace(pipeline,
                                      inj_module,
                                      image_set,
                                      object_set,
                                      measurements,
                                      image_set_list)
            inj_module.run(workspace)
            module.run(workspace)
            image = image_set.get_image("OutputImage")
            self.assertTrue(np.all(np.abs(image.pixel_data - input_image) < .001),
                            "Failed to fit polynomial to %s" % image_name)

    def test_03_02_gaussian_filter(self):
        """Test gaussian filtering a gaussian of a point"""
        input_image = np.zeros((101, 101))
        input_image[50, 50] = 1
        image_name = "InputImage"
        i, j = np.mgrid[-50:51, -50:51]
        expected_image = np.e ** (- (i ** 2 + j ** 2) / (2 * (10.0 / 2.35) ** 2))
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_GAUSSIAN_FILTER
        module.automatic_object_width.value = calc.FI_MANUALLY
        module.size_of_smoothing_filter.value = 10
        module.rescale_option.value = cps.NO
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        ipd = image.pixel_data[40:61, 40:61]
        expected_image = expected_image[40:61, 40:61]
        self.assertTrue(np.all(np.abs(ipd / ipd.mean() -
                                      expected_image / expected_image.mean()) <
                               .001))

    def test_03_03_median_filter(self):
        """Test median filtering of a point"""
        input_image = np.zeros((101, 101))
        input_image[50, 50] = 1
        image_name = "InputImage"
        expected_image = np.zeros((101, 101))
        filter_distance = int(.5 + 10 / 2.35)
        expected_image[-filter_distance:filter_distance + 1,
        -filter_distance:filter_distance + 1] = 1
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_MEDIAN_FILTER
        module.automatic_object_width.value = calc.FI_MANUALLY
        module.size_of_smoothing_filter.value = 10
        module.rescale_option.value = cps.NO
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        self.assertTrue(np.all(image.pixel_data == expected_image))

    def test_03_04_smooth_to_average(self):
        """Test smoothing to an average value"""
        np.random.seed(0)
        input_image = np.random.uniform(size=(10, 10)).astype(np.float32)
        image_name = "InputImage"
        expected_image = np.ones((10, 10)) * input_image.mean()
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_TO_AVERAGE
        module.automatic_object_width.value = calc.FI_MANUALLY
        module.size_of_smoothing_filter.value = 10
        module.rescale_option.value = cps.NO
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        np.testing.assert_almost_equal(image.pixel_data, expected_image)

    def test_03_05_splines(self):
        for automatic, bg_mode, spline_points, threshold, convergence, offset, hi, lo, succeed in (
                (True, calc.MODE_AUTO, 5, 2, .001, 0, True, False, True),
                (True, calc.MODE_AUTO, 5, 2, .001, .7, False, True, True),
                (True, calc.MODE_AUTO, 5, 2, .001, .5, True, True, True),
                (False, calc.MODE_AUTO, 5, 2, .001, 0, True, False, True),
                (False, calc.MODE_AUTO, 5, 2, .001, .7, False, True, True),
                (False, calc.MODE_AUTO, 5, 2, .001, .5, True, True, True),
                (False, calc.MODE_BRIGHT, 5, 2, .001, .7, False, True, True),
                (False, calc.MODE_DARK, 5, 2, .001, 0, True, False, True),
                (False, calc.MODE_GRAY, 5, 2, .001, .5, True, True, True),
                (False, calc.MODE_AUTO, 7, 2, .001, 0, True, False, True),
                (False, calc.MODE_AUTO, 4, 2, .001, 0, True, False, True),
                (False, calc.MODE_DARK, 5, 2, .001, .7, False, True, False),
                (False, calc.MODE_BRIGHT, 5, 2, .001, 0, True, False, False)
        ):

            #
            # Make an image with a random background
            #
            np.random.seed(35)
            image = np.random.uniform(size=(21, 31)) * .05 + offset
            if hi:
                #
                # Add some "foreground" pixels
                #
                fg = np.random.permutation(400)[:100]
                image[fg % image.shape[0], (fg / image.shape[0]).astype(int)] *= 10
            if lo:
                #
                # Add some "background" pixels
                #
                bg = np.random.permutation(400)[:100]
                image[bg % image.shape[0], (bg / image.shape[0]).astype(int)] -= offset

            #
            # Make a background function
            #
            ii, jj = np.mgrid[-10:11, -15:16]
            bg = ((ii.astype(float) / 10) ** 2) * ((jj.astype(float) / 15) ** 2)
            bg *= .2
            image += bg

            workspaces, module = self.make_workspaces(((image, None),))
            self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
            module.intensity_choice.value = calc.IC_BACKGROUND
            module.each_or_all.value = calc.EA_EACH
            module.rescale_option.value = cps.NO
            module.smoothing_method.value = calc.SM_SPLINES
            module.automatic_splines.value = automatic
            module.spline_bg_mode.value = bg_mode
            module.spline_convergence.value = convergence
            module.spline_threshold.value = threshold
            module.spline_points.value = spline_points
            module.spline_rescale.value = 1
            module.prepare_group(workspaces[0], {}, [1])
            module.run(workspaces[0])
            img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
            pixel_data = img.pixel_data
            diff = pixel_data - np.min(pixel_data) - bg
            if succeed:
                self.assertTrue(np.all(diff < .05))
            else:
                self.assertFalse(np.all(diff < .05))

    def test_03_06_splines_scaled(self):
        #
        # Make an image with a random background
        #
        np.random.seed(36)
        image = np.random.uniform(size=(101, 131)) * .05
        #
        # Add some "foreground" pixels
        #
        fg = np.random.permutation(np.prod(image.shape))[:200]
        image[fg % image.shape[0], (fg / image.shape[0]).astype(int)] *= 15
        #
        # Make a background function
        #
        ii, jj = np.mgrid[-50:51, -65:66]
        bg = ((ii.astype(float) / 10) ** 2) * ((jj.astype(float) / 15) ** 2)
        bg *= .2
        image += bg

        workspaces, module = self.make_workspaces(((image, None),))
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value = calc.EA_EACH
        module.rescale_option.value = cps.NO
        module.smoothing_method.value = calc.SM_SPLINES
        module.automatic_splines.value = False
        module.spline_rescale.value = 2
        module.prepare_group(workspaces[0], {}, [1])
        module.run(workspaces[0])
        img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
        pixel_data = img.pixel_data
        diff = pixel_data - np.min(pixel_data) - bg
        np.all(diff < .05)

    def test_03_07_splines_masked(self):
        #
        # Make an image with a random background
        #
        np.random.seed(37)
        image = np.random.uniform(size=(21, 31)) * .05
        #
        # Mask 1/2 of the pixels
        #
        mask = np.random.uniform(size=(21, 31)) < .5
        #
        # Make a background function
        #
        ii, jj = np.mgrid[-10:11, -15:16]
        bg = ((ii.astype(float) / 10) ** 2) * ((jj.astype(float) / 15) ** 2)
        bg *= .2
        image += bg
        #
        # Offset the background within the mask
        #
        image[~mask] += bg[~mask]

        workspaces, module = self.make_workspaces(((image, mask),))
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value = calc.EA_EACH
        module.rescale_option.value = cps.NO
        module.smoothing_method.value = calc.SM_SPLINES
        module.automatic_splines.value = True
        module.prepare_group(workspaces[0], {}, [1])
        module.run(workspaces[0])
        img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
        pixel_data = img.pixel_data
        diff = pixel_data - np.min(pixel_data) - bg
        self.assertTrue(np.all(diff < .05))
        #
        # Make sure test fails w/o mask
        #
        workspaces, module = self.make_workspaces(((image, None),))
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value = calc.EA_EACH
        module.rescale_option.value = cps.NO
        module.smoothing_method.value = calc.SM_SPLINES
        module.automatic_splines.value = True
        module.prepare_group(workspaces[0], {}, [1])
        module.run(workspaces[0])
        img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
        pixel_data = img.pixel_data
        diff = pixel_data - np.min(pixel_data) - bg
        self.assertFalse(np.all(diff < .05))

    def test_03_07_splines_cropped(self):
        #
        # Make an image with a random background
        #
        np.random.seed(37)
        image = np.random.uniform(size=(21, 31)) * .05
        #
        # Mask 1/2 of the pixels
        #
        mask = np.zeros(image.shape, bool)
        mask[4:-4, 6:-6] = True
        #
        # Make a background function
        #
        ii, jj = np.mgrid[-10:11, -15:16]
        bg = ((ii.astype(float) / 10) ** 2) * ((jj.astype(float) / 15) ** 2)
        bg *= .2
        image += bg
        #
        # Offset the background within the mask
        #
        image[~mask] += bg[~mask]

        workspaces, module = self.make_workspaces(((image, mask),))
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value = calc.EA_EACH
        module.rescale_option.value = cps.NO
        module.smoothing_method.value = calc.SM_SPLINES
        module.automatic_splines.value = True
        module.prepare_group(workspaces[0], {}, [1])
        module.run(workspaces[0])
        img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
        pixel_data = img.pixel_data
        diff = pixel_data - np.min(pixel_data) - bg
        self.assertTrue(np.all(diff < .05))
        #
        # Make sure test fails w/o mask
        #
        workspaces, module = self.make_workspaces(((image, None),))
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        module.intensity_choice.value = calc.IC_BACKGROUND
        module.each_or_all.value = calc.EA_EACH
        module.rescale_option.value = cps.NO
        module.smoothing_method.value = calc.SM_SPLINES
        module.automatic_splines.value = True
        module.prepare_group(workspaces[0], {}, [1])
        module.run(workspaces[0])
        img = workspaces[0].image_set.get_image(OUTPUT_IMAGE_NAME)
        pixel_data = img.pixel_data
        diff = pixel_data - np.min(pixel_data) - bg
        self.assertFalse(np.all(diff < .05))

    def test_04_01_intermediate_images(self):
        """Make sure the average and dilated image flags work"""
        for average_flag, dilated_flag in ((False, False),
                                           (False, True),
                                           (True, False),
                                           (True, True)):
            pipeline = cpp.Pipeline()
            pipeline.add_listener(self.error_callback)
            inj_module = inj.InjectImage("InputImage", np.zeros((10, 10)))
            inj_module.module_num = 1
            pipeline.add_module(inj_module)
            module = calc.CorrectIlluminationCalculate()
            module.module_num = 2
            pipeline.add_module(module)
            module.image_name.value = "InputImage"
            module.illumination_image_name.value = "OutputImage"
            module.save_average_image.value = average_flag
            module.average_image_name.value = "AverageImage"
            module.save_dilated_image.value = dilated_flag
            module.dilated_image_name.value = "DilatedImage"
            measurements = cpm.Measurements()
            image_set_list = cpi.ImageSetList()
            workspace = cpw.Workspace(pipeline, None, None, None,
                                      measurements, image_set_list)
            pipeline.prepare_run(workspace)
            inj_module.prepare_group(workspace, {}, [1])
            module.prepare_group(workspace, {}, [1])
            image_set = image_set_list.get_image_set(0)
            object_set = cpo.ObjectSet()
            workspace = cpw.Workspace(pipeline,
                                      inj_module,
                                      image_set,
                                      object_set,
                                      measurements,
                                      image_set_list)
            inj_module.run(workspace)
            module.run(workspace)
            if average_flag:
                img = image_set.get_image("AverageImage")
            else:
                self.assertRaises(AssertionError,
                                  image_set.get_image,
                                  "AverageImage")
            if dilated_flag:
                img = image_set.get_image("DilatedImage")
            else:
                self.assertRaises(AssertionError,
                                  image_set.get_image,
                                  "DilatedImage")

    def test_05_01_rescale(self):
        """Test basic rescaling of an image with two values"""
        input_image = np.ones((10, 10))
        input_image[0:5, :] *= .5
        image_name = "InputImage"
        expected_image = input_image * 2
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_NONE
        module.automatic_object_width.value = calc.FI_MANUALLY
        module.size_of_smoothing_filter.value = 10
        module.rescale_option.value = cps.YES
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        self.assertTrue(np.all(image.pixel_data == expected_image))

    def test_05_02_rescale_outlier(self):
        """Test rescaling with one low outlier"""
        input_image = np.ones((10, 10))
        input_image[0:5, :] *= .5
        input_image[0, 0] = .1
        image_name = "InputImage"
        expected_image = input_image * 2
        expected_image[0, 0] = 1
        pipeline = cpp.Pipeline()
        pipeline.add_listener(self.error_callback)
        inj_module = inj.InjectImage(image_name, input_image)
        inj_module.module_num = 1
        pipeline.add_module(inj_module)
        module = calc.CorrectIlluminationCalculate()
        module.module_num = 2
        pipeline.add_module(module)
        module.image_name.value = image_name
        module.illumination_image_name.value = "OutputImage"
        module.intensity_choice.value = calc.IC_REGULAR
        module.each_or_all.value == calc.EA_EACH
        module.smoothing_method.value = calc.SM_NONE
        module.automatic_object_width.value = calc.FI_MANUALLY
        module.size_of_smoothing_filter.value = 10
        module.rescale_option.value = cps.YES
        module.dilate_objects.value = False
        measurements = cpm.Measurements()
        image_set_list = cpi.ImageSetList()
        workspace = cpw.Workspace(pipeline, None, None, None,
                                  measurements, image_set_list)
        pipeline.prepare_run(workspace)
        inj_module.prepare_group(workspace, {}, [1])
        module.prepare_group(workspace, {}, [1])
        image_set = image_set_list.get_image_set(0)
        object_set = cpo.ObjectSet()
        workspace = cpw.Workspace(pipeline,
                                  inj_module,
                                  image_set,
                                  object_set,
                                  measurements,
                                  image_set_list)
        inj_module.run(workspace)
        module.run(workspace)
        image = image_set.get_image("OutputImage")
        self.assertTrue(np.all(image.pixel_data == expected_image))

    def test_06_01_load_matlab(self):
        data = ('eJzzdQzxcXRSMNUzUPB1DNFNy8xJ1VEIyEksScsvyrVSCHAO9/TTUXAuSk0s'
                'SU1RyM+zUvDNz1PwTSxSMDBUMDSxMrW0MrRQMDIwNFAgGTAwevryMzAwLGdk'
                'YKiYczZkr99hA4F9S17yd4TJRk44dCxC7AiHCBvbrVWuoj4nfO9emSTs3tnL'
                'Jdx/QPmjgA1Df9NltVyhRcda+OaUeL37U3s/Mv13FMOHUPYVJ/Odd/Fpr3bb'
                'OO2DgVziuc5s9lCDBwan6j3klecv4Dya7MLKl5Bb+O/a3I2/xfP3lhxf1vRI'
                'rmhSQqbtQ58N8l/SDQ2j5CawLlP+1KWYa5jTMd/TYYb0R+W/OWWx0z/63J32'
                'xTX1Mrvucv6zLnZH4g+w5T958F3oR5nI/SCtOdo3F7ecq2z0U158uaP0V9Pq'
                'D68l6yT4N+pqfJr+1Zq1Rvfo9WkVovPmPXpZcC3wcWjQHi6bU5uDHkpqzmM0'
                'PzFr+tv3DRUzhMRXz/ns2CZ/zDaNjS+5Rk+e2+Hn7yJNi2IB9bAp4Rdvnn/R'
                '8tHUOPaYr+CD/6s/r3v77e/Tq6p8mza+NX648vUWY6u3U/o872h+i+qs/ft1'
                '9+q/b7ye826b711k1/LD0fHuYp+7Bu+M7h8Xi+8zfXSK+/yd5XqLpskEyRw+'
                'vzNQ+0a73v9ZljZTf5ZFbYrby3J+wpnzj0XfP5xea3ezqV/3XD3zpczepQDs'
                'fe/W')
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.LoadExceptionEvent))

        pipeline.add_listener(callback)
        pipeline.load(StringIO(zlib.decompress(base64.b64decode(data))))
        self.assertEqual(len(pipeline.modules()), 1)
        module = pipeline.modules()[0]
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        self.assertEqual(module.image_name, "IllumBlue")
        self.assertEqual(module.illumination_image_name, "IllumOut")
        self.assertEqual(module.intensity_choice, calc.IC_REGULAR)
        self.assertFalse(module.dilate_objects)
        self.assertEqual(module.rescale_option, cps.YES)
        self.assertEqual(module.each_or_all, calc.EA_EACH)
        self.assertEqual(module.smoothing_method, calc.SM_NONE)
        self.assertEqual(module.automatic_object_width, calc.FI_AUTOMATIC)
        self.assertFalse(module.save_average_image)
        self.assertFalse(module.save_dilated_image)

    def test_06_02_load_v1(self):
        data = r"""CellProfiler Pipeline: http://www.cellprofiler.org
Version:1
SVNRevision:9411

LoadImages:[module_num:1|svn_version:\'Unknown\'|variable_revision_number:4|show_window:True|notes:\x5B\x5D]
    What type of files are you loading?:individual images
    How do you want to load these files?:Text-Exact match
    How many images are there in each group?:3
    Type the text that the excluded images have in common:Do not use
    Analyze all subfolders within the selected folder?:No
    Image location:Default Image Folder
    Enter the full path to the images:.
    Do you want to check image sets for missing or duplicate files?:No
    Do you want to group image sets by metadata?:No
    Do you want to exclude certain files?:No
    What metadata fields do you want to group by?:
    Type the text that these images have in common (case-sensitive):D.TIF
    What do you want to call this image in CellProfiler?:Image1
    What is the position of this image in each group?:D.TIF
    Do you want to extract metadata from the file name, the subfolder path or both?:None
    Type the regular expression that finds metadata in the file name\x3A:None
    Type the regular expression that finds metadata in the subfolder path\x3A:None

CorrectIlluminationCalculate:[module_num:2|svn_version:\'9401\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    Select the input image:Image1
    Name the output image:Illum1
    Select how the illumination function is calculated:Regular
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:Yes
    Calculate function for each image individually, or based on all images?:All
    Smoothing method:No smoothing
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the averaged image:Illum1Average
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the dilated image:Illum1Dilated

CorrectIlluminationCalculate:[module_num:3|svn_version:\'9401\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    Select the input image:Image2
    Name the output image:Illum2
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:Yes
    Dilation radius:2
    Block size:65
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:All\x3A First cycle
    Smoothing method:Median Filter
    Method to calculate smoothing filter size:Manually
    Approximate object size:15
    Smoothing filter size:20
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the averaged image:Illum2Avg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the dilated image:Illum2Dilated

CorrectIlluminationCalculate:[module_num:4|svn_version:\'9401\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    Select the input image:Image3
    Name the output image:Illum3
    Select how the illumination function is calculated:Regular
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:Median
    Calculate function for each image individually, or based on all images?:All\x3A Across cycles
    Smoothing method:Median Filter
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:Illum3Avg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the dilated image:Illum3Dilated

CorrectIlluminationCalculate:[module_num:5|svn_version:\'9401\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    Select the input image:Image4
    Name the output image:Illum4
    Select how the illumination function is calculated:Regular
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:Median
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Gaussian Filter
    Method to calculate smoothing filter size:Object size
    Approximate object size:15
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:Illum4Avg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:Yes
    Name the dilated image:Illum4Dilated

CorrectIlluminationCalculate:[module_num:6|svn_version:\'9401\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    Select the input image:Image5
    Name the output image:Illum5
    Select how the illumination function is calculated:Regular
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:Median
    Calculate function for each image individually, or based on all images?:All
    Smoothing method:Smooth to Average
    Method to calculate smoothing filter size:Object size
    Approximate object size:15
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:Illum5Avg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:Illum5Dilated
"""
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.LoadExceptionEvent))

        pipeline.add_listener(callback)
        pipeline.load(StringIO(data))
        self.assertEqual(len(pipeline.modules()), 6)
        for i, (image_name, illumination_image_name, intensity_choice,
                dilate_objects, object_dilation_radius, block_size,
                rescale_option, each_or_all, smoothing_method,
                automatic_object_width, object_width, size_of_smoothing_filter,
                save_average_image, average_image_name, save_dilated_image,
                dilated_image_name) in enumerate((
                ("Image1", "Illum1", calc.IC_REGULAR, False, 1, 60, cps.YES,
                 calc.EA_ALL_FIRST, calc.SM_NONE, calc.FI_AUTOMATIC, 10, 10, True,
                 "Illum1Average", True, "Illum1Dilated"),
                ("Image2", "Illum2", calc.IC_BACKGROUND, True, 2, 65, cps.NO,
                 calc.EA_ALL_FIRST, calc.SM_MEDIAN_FILTER, calc.FI_MANUALLY, 15, 20,
                 True, "Illum2Avg", True, "Illum2Dilated"),
                ("Image3", "Illum3", calc.IC_REGULAR, False, 1, 60,
                 calc.RE_MEDIAN, calc.EA_ALL_ACROSS, calc.SM_MEDIAN_FILTER,
                 calc.FI_AUTOMATIC, 10, 10, False, "Illum3Avg", True,
                 "Illum3Dilated"),
                ("Image4", "Illum4", calc.IC_REGULAR, cps.NO, 1, 60,
                 calc.RE_MEDIAN, calc.EA_EACH, calc.SM_GAUSSIAN_FILTER,
                 calc.FI_OBJECT_SIZE, 15, 10, False, "Illum4Avg", True,
                 "Illum4Dilated"),
                ("Image5", "Illum5", calc.IC_REGULAR, cps.NO, 1, 60,
                 calc.RE_MEDIAN, calc.EA_ALL_ACROSS, calc.SM_TO_AVERAGE,
                 calc.FI_OBJECT_SIZE, 15, 10, False, "Illum5Avg",
                 False, "Illum5Dilated"))):
            module = pipeline.modules()[i + 1]
            self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
            self.assertEqual(module.image_name, image_name)
            self.assertEqual(module.illumination_image_name, illumination_image_name)
            self.assertEqual(module.intensity_choice, intensity_choice)
            self.assertEqual(module.dilate_objects, dilate_objects)
            self.assertEqual(module.object_dilation_radius, object_dilation_radius)
            self.assertEqual(module.block_size, block_size)
            self.assertEqual(module.rescale_option, rescale_option)
            self.assertEqual(module.each_or_all, each_or_all)
            self.assertEqual(module.smoothing_method, smoothing_method)
            self.assertEqual(module.automatic_object_width, automatic_object_width)
            self.assertEqual(module.object_width, object_width)
            self.assertEqual(module.size_of_smoothing_filter, size_of_smoothing_filter)
            self.assertEqual(module.save_average_image, save_average_image)
            self.assertEqual(module.average_image_name, average_image_name)
            self.assertEqual(module.save_dilated_image, save_dilated_image)
            self.assertEqual(module.dilated_image_name, dilated_image_name)

    def test_06_03_load_v2(self):
        data = r"""CellProfiler Pipeline: http://www.cellprofiler.org
Version:1
SVNRevision:10125

CorrectIlluminationCalculate:[module_num:1|svn_version:\'10063\'|variable_revision_number:2|show_window:True|notes:\x5B\x5D]
    Select the input image:Masked
    Name the output image:Illum
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:No
    Dilation radius:2
    Block size:55
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Splines
    Method to calculate smoothing filter size:Automatic
    Approximate object size:11
    Smoothing filter size:12
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:IllumAverage
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:IllumDilated
    Automatically calculate spline parameters?:No
    Background mode:bright
    # of spline points:4
    Background threshold:2
    Image resampling factor:2
    Max # of iterations:40
    Convergence:0.001

CorrectIlluminationCalculate:[module_num:2|svn_version:\'10063\'|variable_revision_number:2|show_window:True|notes:\x5B\x5D]
    Select the input image:Masked
    Name the output image:Illum
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Splines
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:IllumBlueAvg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:IllumBlueDilated
    Automatically calculate spline parameters?:Yes
    Background mode:auto
    # of spline points:3
    Background threshold:2
    Image resampling factor:2
    Max # of iterations:40
    Convergence:0.001

CorrectIlluminationCalculate:[module_num:3|svn_version:\'10063\'|variable_revision_number:2|show_window:True|notes:\x5B\x5D]
    Select the input image:Masked
    Name the output image:Illum
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Splines
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:IllumBlueAvg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:IllumBlueDilated
    Automatically calculate spline parameters?:Yes
    Background mode:dark
    # of spline points:3
    Background threshold:2
    Image resampling factor:2
    Max # of iterations:40
    Convergence:0.001

CorrectIlluminationCalculate:[module_num:4|svn_version:\'10063\'|variable_revision_number:2|show_window:True|notes:\x5B\x5D]
    Select the input image:Masked
    Name the output image:Illum
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Splines
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:IllumBlueAvg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:IllumBlueDilated
    Automatically calculate spline parameters?:No
    Background mode:gray
    # of spline points:3
    Background threshold:2
    Image resampling factor:2
    Max # of iterations:40
    Convergence:0.001

CorrectIlluminationCalculate:[module_num:5|svn_version:\'10063\'|variable_revision_number:2|show_window:True|notes:\x5B\x5D]
    Select the input image:Masked
    Name the output image:Illum
    Select how the illumination function is calculated:Background
    Dilate objects in the final averaged image?:No
    Dilation radius:1
    Block size:60
    Rescale the illumination function?:No
    Calculate function for each image individually, or based on all images?:Each
    Smoothing method:Convex Hull
    Method to calculate smoothing filter size:Automatic
    Approximate object size:10
    Smoothing filter size:10
    Retain the averaged image for use later in the pipeline (for example, in SaveImages)?:No
    Name the averaged image:IllumBlueAvg
    Retain the dilated image for use later in the pipeline (for example, in SaveImages)?:No
    Name the dilated image:IllumBlueDilated
    Automatically calculate spline parameters?:No
    Background mode:gray
    # of spline points:3
    Background threshold:2
    Image resampling factor:2
    Max # of iterations:40
    Convergence:0.001
"""
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.LoadExceptionEvent))

        pipeline.add_listener(callback)
        pipeline.load(StringIO(data))
        self.assertEqual(len(pipeline.modules()), 5)
        module = pipeline.modules()[0]
        self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
        self.assertEqual(module.image_name, "Masked")
        self.assertEqual(module.illumination_image_name, "Illum")
        self.assertEqual(module.intensity_choice, calc.IC_BACKGROUND)
        self.assertFalse(module.dilate_objects)
        self.assertEqual(module.object_dilation_radius, 2)
        self.assertEqual(module.block_size, 55)
        self.assertEqual(module.rescale_option, cps.NO)
        self.assertEqual(module.each_or_all, calc.EA_EACH)
        self.assertEqual(module.smoothing_method, calc.SM_SPLINES)
        self.assertEqual(module.automatic_object_width, calc.FI_AUTOMATIC)
        self.assertEqual(module.object_width, 11)
        self.assertEqual(module.size_of_smoothing_filter, 12)
        self.assertFalse(module.save_average_image)
        self.assertEqual(module.average_image_name, "IllumAverage")
        self.assertFalse(module.save_dilated_image)
        self.assertEqual(module.dilated_image_name, "IllumDilated")
        self.assertFalse(module.automatic_splines)
        self.assertEqual(module.spline_bg_mode, calc.MODE_BRIGHT)
        self.assertEqual(module.spline_points, 4)
        self.assertEqual(module.spline_threshold, 2)
        self.assertEqual(module.spline_rescale, 2)
        self.assertEqual(module.spline_maximum_iterations, 40)
        self.assertAlmostEqual(module.spline_convergence.value, 0.001)

        self.assertTrue(pipeline.modules()[1].automatic_splines)

        for module, spline_bg_mode in zip(pipeline.modules()[1:4], (
                calc.MODE_AUTO, calc.MODE_DARK, calc.MODE_GRAY)):
            self.assertTrue(isinstance(module, calc.CorrectIlluminationCalculate))
            self.assertEqual(module.spline_bg_mode, spline_bg_mode)

        module = pipeline.modules()[4]
        self.assertEqual(module.smoothing_method, calc.SM_CONVEX_HULL)

'''test_labelimages.py - test the labelimages module
'''

import base64
import os
import unittest
import zlib
from StringIO import StringIO

import numpy as np
import scipy.ndimage

from cellprofiler.preferences import set_headless

set_headless()

import cellprofiler.pipeline as cpp
import cellprofiler.cpmodule as cpm
import cellprofiler.cpimage as cpi
import cellprofiler.measurements as cpmeas
import cellprofiler.objects as cpo
import cellprofiler.workspace as cpw

import cellprofiler.modules.labelimages as L


class TestLabelImages(unittest.TestCase):
    def test_01_00_load_matlab(self):
        data = ('eJzzdQzxcXRSMNUzUPB1DNFNy8xJ1VEIyEksScsvyrVSCHAO9/TTUXAuSk0s'
                'SU1RyM+zUvDNz1PwTaxUMDRXMDS1Mja3MjFQMDIwNFAgGTAwevryMzAwPGdk'
                'YKiY8zbstN8hB5G9uq0PlF0idxps9Bbf55Ac03TQRchl0bEjTxmS5y4p8bBW'
                'fSD+w1Pv7i/2uj7jW4dKFn17mH04N+/E3HvF5+Za9wcxNUxPbWD6r1t0/eHd'
                'T45rPPUeeZ+RKNTR52dbw/rY/876id4B149dP/FtI3sw9/HZ53T3zuJZvv5c'
                'xv3thdK8T1LycusvHF1XXifN1Hxd1ujYN9WdxwW2Grxl6u9wXyH8gLmutGSV'
                '98xF0Y/2hc/46hj8S+FX0LRjrnW8aT+PsddGT/wUzrT/UPK2WiM32zQ+gdd3'
                'VR9GBlW43NkoeFjXZKHd7Kvyh5NdKngyrlj7Nlp3HXwqWisYzrd9FXtjnsy+'
                'FQ9ca94JvZ6RP1Wl9LnSD5kr31d1/tq/4kXXDkffdqPL35t6p7jEd3/ICGhN'
                'eT0/RP5Zjf2h6WWy3+t3zrsssS+01f/IkZeTBV/+PmF2kiHycrTWWZuvHOv5'
                'cn/ttXu75M3lJiN9wWi9WRW2Kcmd6pm2r37ufxj783Tdil/n9LoevC+Krzd/'
                'UlezLDqgYGGrReyhWvH47pnv3e+v3X42Y+cV+Vk13Y8e7vlX+fd/6H/lmrp7'
                'Vz7P+xqXH3gl30u84Nr/TN0rR7e5iJjnb62ba5+qfHtv7MkV069feszzTybP'
                'Zj33HeYl0fPP6n9yPj3rP1fMtVVHANI9Gb4=')
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.LoadExceptionEvent))

        pipeline.add_listener(callback)
        pipeline.load(StringIO(zlib.decompress(base64.b64decode(data))))
        self.assertEqual(len(pipeline.modules()), 2)
        module = pipeline.modules()[-1]
        self.assertTrue(isinstance(module, L.LabelImages))
        self.assertEqual(module.row_count.value, 16)
        self.assertEqual(module.column_count.value, 24)
        self.assertEqual(module.site_count.value, 2)
        self.assertEqual(module.order, L.O_COLUMN)

    def test_01_01_load_v1(self):
        data = r"""CellProfiler Pipeline: http://www.cellprofiler.org
Version:1
SVNRevision:9973

LabelImages:[module_num:1|svn_version:\'9970\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    # sites / well\x3A:3
    # of columns\x3A:48
    # of rows\x3A:32
    Order\x3A:Column

LabelImages:[module_num:2|svn_version:\'9970\'|variable_revision_number:1|show_window:True|notes:\x5B\x5D]
    # sites / well\x3A:1
    # of columns\x3A:12
    # of rows\x3A:8
    Order\x3A:Row
"""
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.LoadExceptionEvent))

        pipeline.add_listener(callback)
        pipeline.load(StringIO(data))
        self.assertEqual(len(pipeline.modules()), 2)
        module = pipeline.modules()[0]
        self.assertTrue(isinstance(module, L.LabelImages))
        self.assertEqual(module.site_count, 3)
        self.assertEqual(module.row_count, 32)
        self.assertEqual(module.column_count, 48)
        self.assertEqual(module.order, L.O_COLUMN)

        module = pipeline.modules()[1]
        self.assertTrue(isinstance(module, L.LabelImages))
        self.assertEqual(module.site_count, 1)
        self.assertEqual(module.row_count, 8)
        self.assertEqual(module.column_count, 12)
        self.assertEqual(module.order, L.O_ROW)

    def make_workspace(self, image_set_count):
        image_set_list = cpi.ImageSetList()
        for i in range(image_set_count):
            image_set = image_set_list.get_image_set(i)
        module = L.LabelImages()
        pipeline = cpp.Pipeline()

        def callback(caller, event):
            self.assertFalse(isinstance(event, cpp.RunExceptionEvent))

        pipeline.add_listener(callback)
        module.module_num = 1
        pipeline.add_module(module)

        workspace = cpw.Workspace(pipeline, module,
                                  image_set_list.get_image_set(0),
                                  cpo.ObjectSet(), cpmeas.Measurements(),
                                  image_set_list)
        return workspace, module

    def test_02_01_label_plate_by_row(self):
        '''Label one complete plate'''
        nsites = 6
        nimagesets = 96 * nsites
        workspace, module = self.make_workspace(nimagesets)
        measurements = workspace.measurements
        self.assertTrue(isinstance(measurements, cpmeas.Measurements))
        self.assertTrue(isinstance(module, L.LabelImages))
        module.row_count.value = 8
        module.column_count.value = 12
        module.order.value = L.O_ROW
        module.site_count.value = nsites
        for i in range(nimagesets):
            if i != 0:
                measurements.next_image_set()
            module.run(workspace)
        sites = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_SITE)
        rows = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_ROW)
        columns = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_COLUMN)
        plates = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_PLATE)
        wells = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_WELL)
        for i in range(nimagesets):
            self.assertEqual(sites[i], (i % 6) + 1)
            this_row = 'ABCDEFGH'[int(i / 6 / 12)]
            this_column = (int(i / 6) % 12) + 1
            self.assertEqual(rows[i], this_row)
            self.assertEqual(columns[i], this_column)
            self.assertEqual(wells[i], '%s%02d' % (this_row, this_column))
            self.assertEqual(plates[i], 1)

    def test_02_02_label_plate_by_column(self):
        '''Label one complete plate'''
        nsites = 6
        nimagesets = 96 * nsites
        workspace, module = self.make_workspace(nimagesets)
        measurements = workspace.measurements
        self.assertTrue(isinstance(measurements, cpmeas.Measurements))
        self.assertTrue(isinstance(module, L.LabelImages))
        module.row_count.value = 8
        module.column_count.value = 12
        module.order.value = L.O_COLUMN
        module.site_count.value = nsites
        for i in range(nimagesets):
            if i != 0:
                measurements.next_image_set()
            module.run(workspace)
        sites = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_SITE)
        rows = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_ROW)
        columns = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_COLUMN)
        plates = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_PLATE)
        wells = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_WELL)
        for i in range(nimagesets):
            self.assertEqual(sites[i], (i % 6) + 1)
            this_row = 'ABCDEFGH'[int(i / 6) % 8]
            this_column = int(i / 6 / 8) + 1
            self.assertEqual(rows[i], this_row)
            self.assertEqual(columns[i], this_column)
            self.assertEqual(wells[i], '%s%02d' % (this_row, this_column))
            self.assertEqual(plates[i], 1)

    def test_02_03_label_many_plates(self):
        nsites = 1
        nplates = 6
        nimagesets = 96 * nsites * nplates
        workspace, module = self.make_workspace(nimagesets)
        measurements = workspace.measurements
        self.assertTrue(isinstance(measurements, cpmeas.Measurements))
        self.assertTrue(isinstance(module, L.LabelImages))
        module.row_count.value = 8
        module.column_count.value = 12
        module.order.value = L.O_ROW
        module.site_count.value = nsites
        for i in range(nimagesets):
            if i != 0:
                measurements.next_image_set()
            module.run(workspace)
        sites = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_SITE)
        rows = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_ROW)
        columns = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_COLUMN)
        plates = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_PLATE)
        wells = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_WELL)
        for i in range(nimagesets):
            self.assertEqual(sites[i], 1)
            this_row = 'ABCDEFGH'[int(i / 12) % 8]
            this_column = (i % 12) + 1
            self.assertEqual(rows[i], this_row)
            self.assertEqual(columns[i], this_column)
            self.assertEqual(wells[i], '%s%02d' % (this_row, this_column))
            self.assertEqual(plates[i], int(i / 8 / 12) + 1)

    def test_02_04_multichar_row_names(self):
        nimagesets = 1000
        workspace, module = self.make_workspace(nimagesets)
        measurements = workspace.measurements
        self.assertTrue(isinstance(measurements, cpmeas.Measurements))
        self.assertTrue(isinstance(module, L.LabelImages))
        module.row_count.value = 1000
        module.column_count.value = 1
        module.order.value = L.O_ROW
        module.site_count.value = 1
        for i in range(nimagesets):
            if i != 0:
                measurements.next_image_set()
            module.run(workspace)
        sites = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_SITE)
        rows = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_ROW)
        columns = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_COLUMN)
        plates = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_PLATE)
        wells = measurements.get_all_measurements(cpmeas.IMAGE, cpmeas.M_WELL)
        abc = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
        for i in range(nimagesets):
            self.assertEqual(sites[i], 1)
            this_row = (abc[int(i / 26 / 26)] +
                        abc[int(i / 26) % 26] +
                        abc[i % 26])
            self.assertEqual(rows[i], this_row)

# -*- coding: utf-8 -*-
##############################################################################
#
# Copyright (c) 2010, 2013, 2degrees Limited.
# All Rights Reserved.
#
# This file is part of django-wsgi <https://github.com/2degrees/django-wsgi/>,
# which is subject to the provisions of the BSD at
# <http://dev.2degreesnetwork.com/p/2degrees-license.html>. A copy of the
# license should accompany this distribution. THIS SOFTWARE IS PROVIDED "AS IS"
# AND ANY AND ALL EXPRESS OR IMPLIED WARRANTIES ARE DISCLAIMED, INCLUDING, BUT
# NOT LIMITED TO, THE IMPLIED WARRANTIES OF TITLE, MERCHANTABILITY, AGAINST
# INFRINGEMENT, AND FITNESS FOR A PARTICULAR PURPOSE.
#
##############################################################################

'''test_rules - test the CPA rules parser
'''

import unittest
from StringIO import StringIO

import numpy as np

import cellprofiler.measurements as cpmeas
import cellprofiler.utilities.rules as R

OBJECT_NAME = "MyObject"
M_FEATURES = ["Measurement%d" % i for i in range(1, 11)]


class TestRules(unittest.TestCase):
    def test_01_01_load_rules(self):
        data = """IF (Nuclei_Intensity_UpperQuartileIntensity_CorrDend > 0.12762499999999999, [0.79607587785712131, -0.79607587785712131], [-0.94024303819690347, 0.94024303819690347])
IF (Nuclei_Intensity_MinIntensity_CorrAxon > 0.026831299999999999, [0.68998040630066426, -0.68998040630066426], [-0.80302016375137986, 0.80302016375137986])
IF (Nuclei_Intensity_UpperQuartileIntensity_CorrDend > 0.19306000000000001, [0.71934712791500899, -0.71934712791500899], [-0.47379648809429048, 0.47379648809429048])
IF (Nuclei_Intensity_UpperQuartileIntensity_CorrDend > 0.100841, [0.24553066971563919, -0.24553066971563919], [-1.0, 1.0])
IF (Nuclei_Location_Center_Y > 299.32499999999999, [-0.61833689824912363, 0.61833689824912363], [0.33384091087462237, -0.33384091087462237])
IF (Nuclei_Intensity_MaxIntensity_CorrNuclei > 0.76509499999999997, [-0.95683305069726121, 0.95683305069726121], [0.22816438860290775, -0.22816438860290775])
IF (Nuclei_Neighbors_NumberOfNeighbors_5 > 2.0, [-0.92848043428127192, 0.92848043428127192], [0.20751332434602923, -0.20751332434602923])
IF (Nuclei_Intensity_MedianIntensity_CorrDend > 0.15327499999999999, [0.79784566567342285, -0.79784566567342285], [-0.35665314560825129, 0.35665314560825129])
IF (Nuclei_Neighbors_SecondClosestXVector_5 > -11.5113, [-0.21859862538067179, 0.21859862538067179], [0.71270785008847592, -0.71270785008847592])
IF (Nuclei_Intensity_StdIntensity_CorrDend > 0.035382299999999998, [0.28838229530755011, -0.28838229530755011], [-0.75312050069265968, 0.75312050069265968])
IF (Nuclei_Intensity_MaxIntensityEdge_CorrNuclei > 0.63182899999999997, [-0.93629855522957672, 0.93629855522957672], [0.1710257492070047, -0.1710257492070047])
IF (Nuclei_Intensity_StdIntensityEdge_CorrNuclei > 0.037909400000000003, [0.28514731668218346, -0.28514731668218346], [-0.60783543053602795, 0.60783543053602795])
IF (Nuclei_Intensity_MedianIntensity_CorrAxon > 0.042631500000000003, [0.20227787378316109, -0.20227787378316109], [-0.78282539096589077, 0.78282539096589077])
IF (Nuclei_Intensity_MinIntensity_CorrDend > 0.042065400000000003, [0.52616744135942872, -0.52616744135942872], [-0.32613209033812068, 0.32613209033812068])
IF (Nuclei_Neighbors_FirstClosestYVector_5 > 3.8226100000000001, [0.69128399165300047, -0.69128399165300047], [-0.34874605597401531, 0.34874605597401531])
IF (Nuclei_Intensity_MeanIntensity_CorrNuclei > 0.283188, [-0.79881507037552979, 0.79881507037552979], [0.24825909570051025, -0.24825909570051025])
IF (Nuclei_Location_Center_Y > 280.154, [-0.50545174099468504, 0.50545174099468504], [0.3297202808867149, -0.3297202808867149])
IF (Nuclei_Intensity_UpperQuartileIntensity_CorrDend > 0.132241, [0.35771841831789791, -0.35771841831789791], [-0.63545019489162846, 0.63545019489162846])
IF (Nuclei_AreaShape_MinorAxisLength > 6.4944899999999999, [0.5755128363506562, -0.5755128363506562], [-0.41737581982462335, 0.41737581982462335])
IF (Nuclei_Intensity_LowerQuartileIntensity_CorrDend > 0.075424000000000005, [0.50557978238660795, -0.50557978238660795], [-0.35606081901385256, 0.35606081901385256])
"""
        fd = StringIO(data)
        rules = R.Rules()
        rules.parse(fd)
        self.assertEqual(len(rules.rules), 20)
        for rule in rules.rules:
            self.assertEqual(rule.object_name, "Nuclei")
            self.assertEqual(rule.comparitor, ">")

        rule = rules.rules[0]
        self.assertEqual(rule.feature, "Intensity_UpperQuartileIntensity_CorrDend")
        self.assertAlmostEqual(rule.threshold, 0.127625)
        self.assertAlmostEqual(rule.weights[0, 0], 0.79607587785712131)
        self.assertAlmostEqual(rule.weights[0, 1], -0.79607587785712131)
        self.assertAlmostEqual(rule.weights[1, 0], -0.94024303819690347)
        self.assertAlmostEqual(rule.weights[1, 1], 0.94024303819690347)

    def test_02_00_no_measurements(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], ">", 0,
                                     np.array([[1.0, -1.0], [-1.0, 1.0]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 0)
        self.assertEqual(score.shape[1], 2)

    def test_02_01_score_one_positive(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([1.5], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], ">", 0,
                                     np.array([[1.0, -0.5], [-2.0, 0.6]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 1)
        self.assertEqual(score.shape[1], 2)
        self.assertAlmostEqual(score[0, 0], 1.0)
        self.assertAlmostEqual(score[0, 1], -0.5)

    def test_02_02_score_one_negative(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([1.5], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], ">", 2.0,
                                     np.array([[1.0, -0.5], [-2.0, 0.6]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 1)
        self.assertEqual(score.shape[1], 2)
        self.assertAlmostEqual(score[0, 0], -2.0)
        self.assertAlmostEqual(score[0, 1], 0.6)

    def test_02_03_score_one_nan(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([np.NaN], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], ">", 2.0,
                                     np.array([[1.0, -0.5], [-2.0, 0.6]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 1)
        self.assertEqual(score.shape[1], 2)
        self.assertTrue(score[0, 0], -2)
        self.assertTrue(score[0, 1], .6)

    def test_03_01_score_two_rules(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([1.5], float))
        m.add_measurement(OBJECT_NAME, M_FEATURES[1], np.array([-1.5], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], ">", 0,
                                     np.array([[1.0, -0.5], [-2.0, 0.6]])),
                        R.Rules.Rule(OBJECT_NAME, M_FEATURES[1], ">", 0,
                                     np.array([[1.5, -0.7], [-2.3, 0.9]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 1)
        self.assertEqual(score.shape[1], 2)
        self.assertAlmostEqual(score[0, 0], 1.0 - 2.3)
        self.assertAlmostEqual(score[0, 1], -0.5 + 0.9)

    def test_03_02_score_two_objects(self):
        m = cpmeas.Measurements()
        m.add_measurement(OBJECT_NAME, M_FEATURES[0], np.array([1.5, 2.5], float))
        rules = R.Rules()
        rules.rules += [R.Rules.Rule(OBJECT_NAME, M_FEATURES[0], "<", 2.0,
                                     np.array([[1.0, -0.5], [-2.0, 0.6]]))]
        score = rules.score(m)
        self.assertEqual(score.shape[0], 2)
        self.assertEqual(score.shape[1], 2)
        self.assertAlmostEqual(score[0, 0], 1.0)
        self.assertAlmostEqual(score[0, 1], -0.5)
        self.assertAlmostEqual(score[1, 0], -2.0)
        self.assertAlmostEqual(score[1, 1], 0.6)

from django.conf.urls import url
from django.contrib.auth.decorators import login_required as login

from . import views

urlpatterns = [
    url(r'^$', login(views.OSFStatisticsListView.as_view()), name='stats_list'),
    url(r'^update/$', login(views.update_metrics), name='update'),
    url(r'^download/$', login(views.download_csv), name='download'),
]

from nose.tools import *  # flake8: noqa

from tests.base import AdminTestCase
from tests.factories import NodeFactory, UserFactory

from admin.nodes.serializers import serialize_simple_user, serialize_node


class TestNodeSerializers(AdminTestCase):
    def test_serialize_node(self):
        node = NodeFactory()
        info = serialize_node(node)
        assert_is_instance(info, dict)
        assert_equal(info['parent'], node.parent_id)
        assert_equal(info['title'], node.title)
        assert_equal(info['children'], [])
        assert_equal(info['id'], node._id)
        assert_equal(info['public'], node.is_public)
        assert_equal(len(info['contributors']), 1)
        assert_false(info['deleted'])

    def test_serialize_deleted(self):
        node = NodeFactory()
        info = serialize_node(node)
        assert_false(info['deleted'])
        node.is_deleted = True
        info = serialize_node(node)
        assert_true(info['deleted'])
        node.is_deleted = False
        info = serialize_node(node)
        assert_false(info['deleted'])

    def test_serialize_simple_user(self):
        user = UserFactory()
        info = serialize_simple_user((user._id, 'admin'))
        assert_is_instance(info, dict)
        assert_equal(info['id'], user._id)
        assert_equal(info['name'], user.fullname)
        assert_equal(info['permission'], 'admin')

# -*- coding: utf-8 -*-
from modularodm import Q
from modularodm.exceptions import NoResultsFound
from rest_framework.exceptions import NotFound
from rest_framework.reverse import reverse
import furl

from website import util as website_util  # noqa
from website import settings as website_settings
from framework.auth import Auth, User
from api.base.exceptions import Gone

# These values are copied from rest_framework.fields.BooleanField
# BooleanField cannot be imported here without raising an
# ImproperlyConfigured error
TRUTHY = set(('t', 'T', 'true', 'True', 'TRUE', '1', 1, True))
FALSY = set(('f', 'F', 'false', 'False', 'FALSE', '0', 0, 0.0, False))

UPDATE_METHODS = ['PUT', 'PATCH']

def decompose_field(field):
    from api.base.serializers import (
        HideIfRetraction, HideIfRegistration,
        HideIfDisabled, AllowMissing
    )
    WRAPPER_FIELDS = (HideIfRetraction, HideIfRegistration, HideIfDisabled, AllowMissing)

    while isinstance(field, WRAPPER_FIELDS):
        try:
            field = getattr(field, 'field')
        except AttributeError:
            break
    return field

def is_bulk_request(request):
    """
    Returns True if bulk request.  Can be called as early as the parser.
    """
    content_type = request.content_type
    return 'ext=bulk' in content_type

def is_truthy(value):
    return value in TRUTHY

def is_falsy(value):
    return value in FALSY

def get_user_auth(request):
    """Given a Django request object, return an ``Auth`` object with the
    authenticated user attached to it.
    """
    user = request.user
    private_key = request.query_params.get('view_only', None)
    if user.is_anonymous():
        auth = Auth(None, private_key=private_key)
    else:
        auth = Auth(user, private_key=private_key)
    return auth


def absolute_reverse(view_name, query_kwargs=None, args=None, kwargs=None):
    """Like django's `reverse`, except returns an absolute URL. Also add query parameters."""
    relative_url = reverse(view_name, kwargs=kwargs)

    url = website_util.api_v2_url(relative_url, params=query_kwargs, base_prefix='')
    return url


def get_object_or_error(model_cls, query_or_pk, display_name=None, **kwargs):
    if isinstance(query_or_pk, basestring):
        obj = model_cls.load(query_or_pk)
        if obj is None:
            raise NotFound
    else:
        try:
            obj = model_cls.find_one(query_or_pk, **kwargs)
        except NoResultsFound:
            raise NotFound

    # For objects that have been disabled (is_active is False), return a 410.
    # The User model is an exception because we still want to allow
    # users who are unconfirmed or unregistered, but not users who have been
    # disabled.
    if model_cls is User and obj.is_disabled:
        raise Gone(detail='The requested user is no longer available.',
                   meta={'full_name': obj.fullname, 'family_name': obj.family_name, 'given_name': obj.given_name,
                         'middle_names': obj.middle_names, 'profile_image': obj.profile_image_url()})
    elif model_cls is not User and not getattr(obj, 'is_active', True) or getattr(obj, 'is_deleted', False):
        if display_name is None:
            raise Gone
        else:
            raise Gone(detail='The requested {name} is no longer available.'.format(name=display_name))
    return obj


def waterbutler_url_for(request_type, provider, path, node_id, token, obj_args=None, **query):
    """Reverse URL lookup for WaterButler routes
    :param str request_type: data or metadata
    :param str provider: The name of the requested provider
    :param str path: The path of the requested file or folder
    :param str node_id: The id of the node being accessed
    :param str token: The cookie to be used or None
    :param dict **query: Addition query parameters to be appended
    """
    url = furl.furl(website_settings.WATERBUTLER_URL)
    url.path.segments.append(request_type)

    url.args.update({
        'path': path,
        'nid': node_id,
        'provider': provider,
    })

    if token is not None:
        url.args['cookie'] = token

    if 'view_only' in obj_args:
        url.args['view_only'] = obj_args['view_only']

    url.args.update(query)
    return url.url

def default_node_list_query():
    return (
        Q('is_deleted', 'ne', True) &
        Q('is_collection', 'ne', True) &
        Q('is_registration', 'ne', True)
    )


def default_node_permission_query(user):
    permission_query = Q('is_public', 'eq', True)
    if not user.is_anonymous():
        permission_query = (permission_query | Q('contributors', 'eq', user._id))

    return permission_query

def extend_querystring_params(url, params):
    return furl.furl(url).add(args=params).url

from rest_framework import serializers as ser
from rest_framework import exceptions

from modularodm import Q
from modularodm.exceptions import ValidationValueError

from framework.auth.core import Auth
from framework.exceptions import PermissionsError

from website.models import Node, User, Comment, Institution
from website.exceptions import NodeStateError, UserNotAffiliatedError
from website.util import permissions as osf_permissions
from website.project.model import NodeUpdateError

from api.base.utils import get_user_auth, get_object_or_error, absolute_reverse
from api.base.serializers import (JSONAPISerializer, WaterbutlerLink, NodeFileHyperLinkField, IDField, TypeField,
                                  TargetTypeField, JSONAPIListField, LinksField, RelationshipField, DevOnly,
                                  HideIfRegistration)
from api.base.exceptions import InvalidModelValueError


class NodeTagField(ser.Field):
    def to_representation(self, obj):
        if obj is not None:
            return obj._id
        return None

    def to_internal_value(self, data):
        return data


class NodeSerializer(JSONAPISerializer):
    # TODO: If we have to redo this implementation in any of the other serializers, subclass ChoiceField and make it
    # handle blank choices properly. Currently DRF ChoiceFields ignore blank options, which is incorrect in this
    # instance
    filterable_fields = frozenset([
        'id',
        'title',
        'description',
        'public',
        'tags',
        'category',
        'date_created',
        'date_modified',
        'root',
        'parent',
        'contributors'
    ])

    non_anonymized_fields = [
        'id',
        'title',
        'description',
        'category',
        'date_created',
        'date_modified',
        'registration',
        'tags',
        'public',
        'links',
        'children',
        'comments',
        'contributors',
        'files',
        'node_links',
        'parent',
        'root',
        'logs',
    ]

    id = IDField(source='_id', read_only=True)
    type = TypeField()

    category_choices = Node.CATEGORY_MAP.items()
    category_choices_string = ', '.join(["'{}'".format(choice[0]) for choice in category_choices])

    title = ser.CharField(required=True)
    description = ser.CharField(required=False, allow_blank=True, allow_null=True)
    category = ser.ChoiceField(choices=category_choices, help_text="Choices: " + category_choices_string)
    date_created = ser.DateTimeField(read_only=True)
    date_modified = ser.DateTimeField(read_only=True)
    registration = ser.BooleanField(read_only=True, source='is_registration')
    fork = ser.BooleanField(read_only=True, source='is_fork')
    collection = ser.BooleanField(read_only=True, source='is_collection')
    tags = JSONAPIListField(child=NodeTagField(), required=False)
    template_from = ser.CharField(required=False, allow_blank=False, allow_null=False,
                                  help_text='Specify a node id for a node you would like to use as a template for the '
                                            'new node. Templating is like forking, except that you do not copy the '
                                            'files, only the project structure. Some information is changed on the top '
                                            'level project by submitting the appropriate fields in the request body, '
                                            'and some information will not change. By default, the description will '
                                            'be cleared and the project will be made private.')
    current_user_permissions = ser.SerializerMethodField(help_text='List of strings representing the permissions '
                                                                   'for the current user on this node.')

    # Public is only write-able by admins--see update method
    public = ser.BooleanField(source='is_public', required=False,
                              help_text='Nodes that are made public will give read-only access '
                                        'to everyone. Private nodes require explicit read '
                                        'permission. Write and admin access are the same for '
                                        'public and private nodes. Administrators on a parent '
                                        'node have implicit read permissions for all child nodes')

    links = LinksField({'html': 'get_absolute_html_url'})
    # TODO: When we have osf_permissions.ADMIN permissions, make this writable for admins

    children = RelationshipField(
        related_view='nodes:node-children',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'count': 'get_node_count'},
    )

    comments = RelationshipField(
        related_view='nodes:node-comments',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'unread': 'get_unread_comments_count'})

    contributors = RelationshipField(
        related_view='nodes:node-contributors',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'count': 'get_contrib_count'},
    )

    files = RelationshipField(
        related_view='nodes:node-providers',
        related_view_kwargs={'node_id': '<pk>'}
    )

    forked_from = RelationshipField(
        related_view='nodes:node-detail',
        related_view_kwargs={'node_id': '<forked_from_id>'}
    )

    node_links = RelationshipField(
        related_view='nodes:node-pointers',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'count': 'get_pointers_count'},
    )

    parent = RelationshipField(
        related_view='nodes:node-detail',
        related_view_kwargs={'node_id': '<parent_node._id>'},
        filter_key='parent_node'
    )

    registrations = DevOnly(HideIfRegistration(RelationshipField(
        related_view='nodes:node-registrations',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'count': 'get_registration_count'}
    )))

    primary_institution = RelationshipField(
        related_view='nodes:node-institution-detail',
        related_view_kwargs={'node_id': '<pk>'},
        self_view='nodes:node-relationships-institution',
        self_view_kwargs={'node_id': '<pk>'}
    )

    root = RelationshipField(
        related_view='nodes:node-detail',
        related_view_kwargs={'node_id': '<root._id>'}
    )

    logs = RelationshipField(
        related_view='nodes:node-logs',
        related_view_kwargs={'node_id': '<pk>'},
        related_meta={'count': 'get_logs_count'}
    )

    def get_current_user_permissions(self, obj):
        user = self.context['request'].user
        if user.is_anonymous():
            return ['read']
        permissions = obj.get_permissions(user=user)
        if not permissions:
            permissions = ['read']
        return permissions

    class Meta:
        type_ = 'nodes'

    def get_absolute_url(self, obj):
        return obj.get_absolute_url()

    # TODO: See if we can get the count filters into the filter rather than the serializer.

    def get_logs_count(self, obj):
        return len(obj.logs)

    def get_node_count(self, obj):
        auth = get_user_auth(self.context['request'])
        nodes = [node for node in obj.nodes if node.can_view(auth) and node.primary and not node.is_deleted]
        return len(nodes)

    def get_contrib_count(self, obj):
        return len(obj.contributors)

    def get_registration_count(self, obj):
        auth = get_user_auth(self.context['request'])
        registrations = [node for node in obj.registrations_all if node.can_view(auth)]
        return len(registrations)

    def get_pointers_count(self, obj):
        return len(obj.nodes_pointer)

    def get_unread_comments_count(self, obj):
        user = get_user_auth(self.context['request']).user
        node_comments = Comment.find_n_unread(user=user, node=obj, page='node')

        return {
            'node': node_comments
        }

    def create(self, validated_data):
        if 'template_from' in validated_data:
            request = self.context['request']
            user = request.user
            template_from = validated_data.pop('template_from')
            template_node = Node.load(key=template_from)
            if template_node is None:
                raise exceptions.NotFound
            if not template_node.has_permission(user, 'read', check_parent=False):
                raise exceptions.PermissionDenied

            validated_data.pop('creator')
            changed_data = {template_from: validated_data}
            node = template_node.use_as_template(auth=get_user_auth(request), changes=changed_data)
        else:
            node = Node(**validated_data)
        try:
            node.save()
        except ValidationValueError as e:
            raise InvalidModelValueError(detail=e.message)
        return node

    def update(self, node, validated_data):
        """Update instance with the validated data. Requires
        the request to be in the serializer context.
        """
        assert isinstance(node, Node), 'node must be a Node'
        auth = get_user_auth(self.context['request'])
        old_tags = set([tag._id for tag in node.tags])
        if 'tags' in validated_data:
            current_tags = set(validated_data.get('tags'))
            del validated_data['tags']
        elif self.partial:
            current_tags = set(old_tags)
        else:
            current_tags = set()

        for new_tag in (current_tags - old_tags):
            node.add_tag(new_tag, auth=auth)
        for deleted_tag in (old_tags - current_tags):
            node.remove_tag(deleted_tag, auth=auth)

        if validated_data:
            try:
                node.update(validated_data, auth=auth)
            except ValidationValueError as e:
                raise InvalidModelValueError(detail=e.message)
            except PermissionsError:
                raise exceptions.PermissionDenied
            except NodeUpdateError as e:
                raise exceptions.ValidationError(detail=e.reason)
            except NodeStateError as e:
                raise InvalidModelValueError(detail=e.message)

        return node


class NodeDetailSerializer(NodeSerializer):
    """
    Overrides NodeSerializer to make id required.
    """
    id = IDField(source='_id', required=True)


class NodeContributorsSerializer(JSONAPISerializer):
    """ Separate from UserSerializer due to necessity to override almost every field as read only
    """
    non_anonymized_fields = ['bibliographic', 'permission']
    filterable_fields = frozenset([
        'id',
        'bibliographic',
        'permission'
    ])

    id = IDField(source='_id', required=True)
    type = TypeField()

    bibliographic = ser.BooleanField(help_text='Whether the user will be included in citations for this node or not.',
                                     default=True)
    permission = ser.ChoiceField(choices=osf_permissions.PERMISSIONS, required=False, allow_null=True,
                                 default=osf_permissions.reduce_permissions(osf_permissions.DEFAULT_CONTRIBUTOR_PERMISSIONS),
                                 help_text='User permission level. Must be "read", "write", or "admin". Defaults to "write".')

    links = LinksField({
        'self': 'get_absolute_url'
    })

    users = RelationshipField(
        related_view='users:user-detail',
        related_view_kwargs={'user_id': '<pk>'},
        always_embed=True
    )

    class Meta:
        type_ = 'contributors'

    def get_absolute_url(self, obj):
        node_id = self.context['request'].parser_context['kwargs']['node_id']
        return absolute_reverse(
            'nodes:node-contributor-detail',
            kwargs={
                'node_id': node_id,
                'user_id': obj._id
            }
        )


class NodeContributorsCreateSerializer(NodeContributorsSerializer):
    """
    Overrides NodeContributorsSerializer to add target_type field
    """
    target_type = TargetTypeField(target_type='users')

    def create(self, validated_data):
        auth = Auth(self.context['request'].user)
        node = self.context['view'].get_node()
        contributor = get_object_or_error(User, validated_data['_id'], display_name='user')
        # Node object checks for contributor existence but can still change permissions anyway
        if contributor in node.contributors:
            raise exceptions.ValidationError('{} is already a contributor'.format(contributor.fullname))

        bibliographic = validated_data['bibliographic']
        permissions = osf_permissions.expand_permissions(validated_data.get('permission')) or osf_permissions.DEFAULT_CONTRIBUTOR_PERMISSIONS
        node.add_contributor(contributor=contributor, auth=auth, visible=bibliographic, permissions=permissions, save=True)
        contributor.permission = osf_permissions.reduce_permissions(node.get_permissions(contributor))
        contributor.bibliographic = node.get_visible(contributor)
        contributor.node_id = node._id
        return contributor


class NodeContributorDetailSerializer(NodeContributorsSerializer):
    """
    Overrides node contributor serializer to add additional methods
    """

    def update(self, instance, validated_data):
        contributor = instance
        auth = Auth(self.context['request'].user)
        node = self.context['view'].get_node()

        visible = validated_data.get('bibliographic')
        permission = validated_data.get('permission')
        try:
            node.update_contributor(contributor, permission, visible, auth, save=True)
        except NodeStateError as e:
            raise exceptions.ValidationError(detail=e.message)
        contributor.permission = osf_permissions.reduce_permissions(node.get_permissions(contributor))
        contributor.bibliographic = node.get_visible(contributor)
        contributor.node_id = node._id
        return contributor


class NodeLinksSerializer(JSONAPISerializer):

    id = IDField(source='_id')
    type = TypeField()
    target_type = TargetTypeField(target_type='nodes')

    # TODO: We don't show the title because the current user may not have access to this node. We may want to conditionally
    # include this field in the future.
    # title = ser.CharField(read_only=True, source='node.title', help_text='The title of the node that this Node Link '
    #                                                                      'points to')

    target_node = RelationshipField(
        related_view='nodes:node-detail',
        related_view_kwargs={'node_id': '<pk>'},
        always_embed=True

    )
    class Meta:
        type_ = 'node_links'

    links = LinksField({
        'self': 'get_absolute_url'
    })

    def get_absolute_url(self, obj):
        node_id = self.context['request'].parser_context['kwargs']['node_id']
        return absolute_reverse(
            'nodes:node-pointer-detail',
            kwargs={
                'node_id': node_id,
                'node_link_id': obj._id
            }
        )

    def create(self, validated_data):
        request = self.context['request']
        user = request.user
        auth = Auth(user)
        node = self.context['view'].get_node()
        target_node_id = validated_data['_id']
        pointer_node = Node.load(target_node_id)
        if not pointer_node or pointer_node.is_collection:
            raise InvalidModelValueError(
                source={'pointer': '/data/relationships/node_links/data/id'},
                detail='Target Node \'{}\' not found.'.format(target_node_id)
            )
        try:
            pointer = node.add_pointer(pointer_node, auth, save=True)
            return pointer
        except ValueError:
            raise InvalidModelValueError(
                source={'pointer': '/data/relationships/node_links/data/id'},
                detail='Target Node \'{}\' already pointed to by \'{}\'.'.format(target_node_id, node._id)
            )

    def update(self, instance, validated_data):
        pass


class NodeProviderSerializer(JSONAPISerializer):
    id = ser.SerializerMethodField(read_only=True)
    kind = ser.CharField(read_only=True)
    name = ser.CharField(read_only=True)
    path = ser.CharField(read_only=True)
    node = ser.CharField(source='node_id', read_only=True)
    provider = ser.CharField(read_only=True)
    files = NodeFileHyperLinkField(
        related_view='nodes:node-files',
        related_view_kwargs={'node_id': '<node_id>', 'path': '<path>', 'provider': '<provider>'},
        kind='folder',
        never_embed=True
    )
    links = LinksField({
        'upload': WaterbutlerLink(),
        'new_folder': WaterbutlerLink(kind='folder')
    })

    class Meta:
        type_ = 'files'

    @staticmethod
    def get_id(obj):
        return '{}:{}'.format(obj.node._id, obj.provider)

    def get_absolute_url(self, obj):
        return absolute_reverse(
            'nodes:node-provider-detail',
            kwargs={
                'node_id': obj.node._id,
                'provider': obj.provider
            }
        )


class NodeInstitutionRelationshipSerializer(ser.Serializer):
    id = ser.CharField(source='institution_id', required=False, allow_null=True)
    type = TypeField(required=False, allow_null=True)

    links = LinksField({
        'self': 'get_self_link',
        'related': 'get_related_link',
    })

    class Meta:
        type_ = 'institutions'

    def get_self_link(self, obj):
        return obj.institution_relationship_url()

    def get_related_link(self, obj):
        return obj.institution_url()

    def update(self, instance, validated_data):
        node = instance
        user = self.context['request'].user

        inst = validated_data.get('institution_id', None)
        if inst:
            inst = Institution.load(inst)
            if not inst:
                raise exceptions.NotFound
            try:
                node.add_primary_institution(inst=inst, user=user)
            except UserNotAffiliatedError:
                raise exceptions.ValidationError(detail='User not affiliated with institution')
            node.save()
            return node
        node.remove_primary_institution(user)
        node.save()
        return node

    def to_representation(self, obj):
        data = {}
        meta = getattr(self, 'Meta', None)
        type_ = getattr(meta, 'type_', None)
        assert type_ is not None, 'Must define Meta.type_'
        relation_id_field = self.fields['id']
        data['data'] = None
        if obj.primary_institution:
            attribute = obj.primary_institution._id
            relationship = relation_id_field.to_representation(attribute)
            data['data'] = {'type': type_, 'id': relationship}
        data['links'] = {key: val for key, val in self.fields.get('links').to_representation(obj).iteritems()}

        return data


class NodeAlternativeCitationSerializer(JSONAPISerializer):

    id = IDField(source="_id", read_only=True)
    type = TypeField()
    name = ser.CharField(required=True)
    text = ser.CharField(required=True)

    class Meta:
        type_ = 'citations'

    def create(self, validated_data):
        errors = self.error_checker(validated_data)
        if len(errors) > 0:
            raise exceptions.ValidationError(detail=errors)
        node = self.context['view'].get_node()
        auth = Auth(self.context['request']._user)
        citation = node.add_citation(auth, save=True, **validated_data)
        return citation

    def update(self, instance, validated_data):
        errors = self.error_checker(validated_data)
        if len(errors) > 0:
            raise exceptions.ValidationError(detail=errors)
        node = self.context['view'].get_node()
        auth = Auth(self.context['request']._user)
        instance = node.edit_citation(auth, instance, save=True, **validated_data)
        return instance

    def error_checker(self, data):
        errors = []
        name = data.get('name', None)
        text = data.get('text', None)
        citations = self.context['view'].get_node().alternative_citations
        if not (self.instance and self.instance.name == name) and citations.find(Q('name', 'eq', name)).count() > 0:
            errors.append("There is already a citation named '{}'".format(name))
        if not (self.instance and self.instance.text == text):
            matching_citations = citations.find(Q('text', 'eq', text))
            if matching_citations.count() > 0:
                names = "', '".join([str(citation.name) for citation in matching_citations])
                errors.append("Citation matches '{}'".format(names))
        return errors

    def get_absolute_url(self, obj):
        #  Citations don't have urls
        raise NotImplementedError

from nose.tools import *  # flake8: noqa
from datetime import datetime

from framework.guid.model import Guid

from api.base.settings.defaults import API_BASE
from api_tests import utils as test_utils
from tests.base import ApiTestCase
from tests.factories import ProjectFactory, AuthUserFactory, CommentFactory, NodeWikiFactory


class CommentReportsMixin(object):

    def setUp(self):
        super(CommentReportsMixin, self).setUp()
        self.user = AuthUserFactory()
        self.contributor = AuthUserFactory()
        self.non_contributor = AuthUserFactory()
        self.payload = {
            'data': {
                'type': 'comment_reports',
                'attributes': {
                    'category': 'spam',
                    'message': 'delicious spam'
                }
            }
        }

    def _set_up_private_project_comment_reports(self):
        raise NotImplementedError

    def _set_up_public_project_comment_reports(self, comment_level='public'):
        raise NotImplementedError

    def test_private_node_logged_out_user_cannot_view_reports(self):
        self._set_up_private_project_comment_reports()
        res = self.app.get(self.private_url, expect_errors=True)
        assert_equal(res.status_code, 401)

    def test_private_node_logged_in_non_contributor_cannot_view_reports(self):
        self._set_up_private_project_comment_reports()
        res = self.app.get(self.private_url, auth=self.non_contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 403)

    def test_private_node_only_reporting_user_can_view_reports(self):
        self._set_up_private_project_comment_reports()
        res = self.app.get(self.private_url, auth=self.user.auth)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 1)
        assert_in(self.user._id, report_ids)

    def test_private_node_reported_user_does_not_see_report(self):
        self._set_up_private_project_comment_reports()
        res = self.app.get(self.private_url, auth=self.contributor.auth)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 0)
        assert_not_in(self.contributor._id, report_ids)

    def test_public_node_only_reporting_contributor_can_view_report(self):
        self._set_up_public_project_comment_reports()
        res = self.app.get(self.public_url, auth=self.user.auth)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 1)
        assert_in(self.user._id, report_ids)

    def test_public_node_reported_user_does_not_see_report(self):
        self._set_up_public_project_comment_reports()
        res = self.app.get(self.public_url, auth=self.contributor.auth)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 0)
        assert_not_in(self.contributor._id, report_ids)

    def test_public_node_non_contributor_does_not_see_other_user_reports(self):
        self._set_up_public_project_comment_reports()
        res = self.app.get(self.public_url, auth=self.non_contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 0)
        assert_not_in(self.non_contributor._id, report_ids)

    def test_public_node_non_contributor_reporter_can_view_own_report(self):
        self._set_up_public_project_comment_reports()
        self.public_comment.reports[self.non_contributor._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.public_comment.save()
        res = self.app.get(self.public_url, auth=self.non_contributor.auth)
        assert_equal(res.status_code, 200)
        report_json = res.json['data']
        report_ids = [report['id'] for report in report_json]
        assert_equal(len(report_json), 1)
        assert_in(self.non_contributor._id, report_ids)

    def test_public_node_logged_out_user_cannot_view_reports(self):
        self._set_up_public_project_comment_reports()
        res = self.app.get(self.public_url, expect_errors=True)
        assert_equal(res.status_code, 401)

    def test_public_node_private_comment_level_non_contributor_cannot_see_reports(self):
        self._set_up_public_project_comment_reports(comment_level='private')
        res = self.app.get(self.public_url, auth=self.non_contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

    def test_report_comment_invalid_type(self):
        self._set_up_private_project_comment_reports()
        payload = {
            'data': {
                'type': 'Not a valid type.',
                'attributes': {
                    'category': 'spam',
                    'message': 'delicious spam'
                }
            }
        }
        res = self.app.post_json_api(self.private_url, payload, auth=self.user.auth, expect_errors=True)
        assert_equal(res.status_code, 409)

    def test_report_comment_no_type(self):
        self._set_up_private_project_comment_reports()
        payload = {
            'data': {
                'type': '',
                'attributes': {
                    'category': 'spam',
                    'message': 'delicious spam'
                }
            }
        }
        res = self.app.post_json_api(self.private_url, payload, auth=self.user.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'This field may not be blank.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/type')

    def test_report_comment_invalid_spam_category(self):
        self._set_up_private_project_comment_reports()
        category = 'Not a valid category'
        payload = {
            'data': {
                'type': 'comment_reports',
                'attributes': {
                    'category': category,
                    'message': 'delicious spam'
                }
            }
        }
        res = self.app.post_json_api(self.private_url, payload, auth=self.user.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], '\"' + category + '\"' + ' is not a valid choice.')

    def test_report_comment_allow_blank_message(self):
        self._set_up_private_project_comment_reports()
        comment = CommentFactory(node=self.private_project, user=self.contributor, target=self.comment.target)
        url = '/{}comments/{}/reports/'.format(API_BASE, comment._id)
        payload = {
            'data': {
                'type': 'comment_reports',
                'attributes': {
                    'category': 'spam',
                    'message': ''
                }
            }
        }
        res = self.app.post_json_api(url, payload, auth=self.user.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.json['data']['id'], self.user._id)
        assert_equal(res.json['data']['attributes']['message'], payload['data']['attributes']['message'])

    def test_private_node_logged_out_user_cannot_report_comment(self):
        self._set_up_private_project_comment_reports()
        res = self.app.post_json_api(self.private_url, self.payload, expect_errors=True)
        assert_equal(res.status_code, 401)

    def test_private_node_logged_in_non_contributor_cannot_report_comment(self):
        self._set_up_private_project_comment_reports()
        res = self.app.post_json_api(self.private_url, self.payload, auth=self.non_contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 403)

    def test_private_node_logged_in_contributor_can_report_comment(self):
        self._set_up_private_project_comment_reports()
        comment = CommentFactory(node=self.private_project, user=self.contributor, target=self.comment.target)
        url = '/{}comments/{}/reports/'.format(API_BASE, comment._id)
        res = self.app.post_json_api(url, self.payload, auth=self.user.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.json['data']['id'], self.user._id)

    def test_user_cannot_report_own_comment(self):
        self._set_up_private_project_comment_reports()
        res = self.app.post_json_api(self.private_url, self.payload, auth=self.contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'You cannot report your own comment.')

    def test_user_cannot_report_comment_twice(self):
        self._set_up_private_project_comment_reports()
        # User cannot report the comment again
        res = self.app.post_json_api(self.private_url, self.payload, auth=self.user.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Comment already reported.')

    def test_public_node_logged_out_user_cannot_report_comment(self):
        self._set_up_public_project_comment_reports()
        res = self.app.post_json_api(self.public_url, self.payload, expect_errors=True)
        assert_equal(res.status_code, 401)

    def test_public_node_contributor_can_report_comment(self):
        self._set_up_public_project_comment_reports()
        comment = CommentFactory(node=self.public_project, user=self.contributor, target=self.public_comment.target)
        url = '/{}comments/{}/reports/'.format(API_BASE, comment._id)

        res = self.app.post_json_api(url, self.payload, auth=self.user.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.json['data']['id'], self.user._id)

    def test_public_node_non_contributor_can_report_comment(self):
        """ Test that when a public project allows any osf user to
            comment (comment_level == 'public), non-contributors
            can also report comments.
        """
        self._set_up_public_project_comment_reports()
        res = self.app.post_json_api(self.public_url, self.payload, auth=self.non_contributor.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.json['data']['id'], self.non_contributor._id)

    def test_public_node_private_comment_level_non_contributor_cannot_report_comment(self):
        self._set_up_public_project_comment_reports(comment_level='private')
        res = self.app.get(self.public_url, auth=self.non_contributor.auth, expect_errors=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')


class TestCommentReportsView(CommentReportsMixin, ApiTestCase):

    def _set_up_private_project_comment_reports(self):
        self.private_project = ProjectFactory.create(is_public=False, creator=self.user)
        self.private_project.add_contributor(contributor=self.contributor, save=True)
        self.comment = CommentFactory.build(node=self.private_project, user=self.contributor)
        self.comment.reports = self.comment.reports or {}
        self.comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.comment.save()
        self.private_url = '/{}comments/{}/reports/'.format(API_BASE, self.comment._id)

    def _set_up_public_project_comment_reports(self, comment_level='public'):
        self.public_project = ProjectFactory.create(is_public=True, creator=self.user, comment_level=comment_level)
        self.public_project.add_contributor(contributor=self.contributor, save=True)
        self.public_comment = CommentFactory.build(node=self.public_project, user=self.contributor)
        self.public_comment.reports = self.public_comment.reports or {}
        self.public_comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.public_comment.save()
        self.public_url = '/{}comments/{}/reports/'.format(API_BASE, self.public_comment._id)


class TestFileCommentReportsView(CommentReportsMixin, ApiTestCase):

    def _set_up_private_project_comment_reports(self):
        self.private_project = ProjectFactory.create(is_public=False, creator=self.user)
        self.private_project.add_contributor(contributor=self.contributor, save=True)
        self.file = test_utils.create_test_file(self.private_project, self.user)
        self.comment = CommentFactory.build(node=self.private_project, target=self.file.get_guid(), user=self.contributor)
        self.comment.reports = self.comment.reports or {}
        self.comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.comment.save()
        self.private_url = '/{}comments/{}/reports/'.format(API_BASE, self.comment._id)

    def _set_up_public_project_comment_reports(self, comment_level='public'):
        self.public_project = ProjectFactory.create(is_public=True, creator=self.user, comment_level=comment_level)
        self.public_project.add_contributor(contributor=self.contributor, save=True)
        self.public_file = test_utils.create_test_file(self.public_project, self.user)
        self.public_comment = CommentFactory.build(node=self.public_project, target=self.public_file.get_guid(), user=self.contributor)
        self.public_comment.reports = self.public_comment.reports or {}
        self.public_comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.public_comment.save()
        self.public_url = '/{}comments/{}/reports/'.format(API_BASE, self.public_comment._id)


class TestWikiCommentReportsView(CommentReportsMixin, ApiTestCase):

    def _set_up_private_project_comment_reports(self):
        self.private_project = ProjectFactory.create(is_public=False, creator=self.user)
        self.private_project.add_contributor(contributor=self.contributor, save=True)
        self.wiki = NodeWikiFactory(node=self.private_project, user=self.user)
        self.comment = CommentFactory.build(node=self.private_project, target=Guid.load(self.wiki._id), user=self.contributor)
        self.comment.reports = self.comment.reports or {}
        self.comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.comment.save()
        self.private_url = '/{}comments/{}/reports/'.format(API_BASE, self.comment._id)

    def _set_up_public_project_comment_reports(self, comment_level='public'):
        self.public_project = ProjectFactory.create(is_public=True, creator=self.user, comment_level=comment_level)
        self.public_project.add_contributor(contributor=self.contributor, save=True)
        self.public_wiki = NodeWikiFactory(node=self.public_project, user=self.user)
        self.public_comment = CommentFactory.build(node=self.public_project, target=Guid.load(self.public_wiki._id), user=self.contributor)
        self.public_comment.reports = self.public_comment.reports or {}
        self.public_comment.reports[self.user._id] = {
            'category': 'spam',
            'text': 'This is spam',
            'date': datetime.utcnow(),
            'retracted': False,
        }
        self.public_comment.save()
        self.public_url = '/{}comments/{}/reports/'.format(API_BASE, self.public_comment._id)

# -*- coding: utf-8 -*-
from nose.tools import *  # flake8: noqa

from modularodm import Q
from framework.auth.core import Auth

from website.models import Node, NodeLog
from website.util import permissions
from website.util.sanitize import strip_html

from api.base.settings.defaults import API_BASE, MAX_PAGE_SIZE

from tests.base import ApiTestCase
from tests.factories import (
    BookmarkCollectionFactory,
    CollectionFactory,
    ProjectFactory,
    RegistrationFactory,
    AuthUserFactory,
    UserFactory,
    RetractedRegistrationFactory
)


class TestNodeList(ApiTestCase):
    def setUp(self):
        super(TestNodeList, self).setUp()
        self.user = AuthUserFactory()

        self.non_contrib = AuthUserFactory()

        self.deleted = ProjectFactory(is_deleted=True)
        self.private = ProjectFactory(is_public=False, creator=self.user)
        self.public = ProjectFactory(is_public=True, creator=self.user)

        self.url = '/{}nodes/'.format(API_BASE)

    def tearDown(self):
        super(TestNodeList, self).tearDown()
        Node.remove()

    def test_only_returns_non_deleted_public_projects(self):
        res = self.app.get(self.url)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.public._id, ids)
        assert_not_in(self.deleted._id, ids)
        assert_not_in(self.private._id, ids)

    def test_return_public_node_list_logged_out_user(self):
        res = self.app.get(self.url, expect_errors=True)
        assert_equal(res.status_code, 200)
        assert_equal(res.content_type, 'application/vnd.api+json')
        ids = [each['id'] for each in res.json['data']]
        assert_in(self.public._id, ids)
        assert_not_in(self.private._id, ids)

    def test_return_public_node_list_logged_in_user(self):
        res = self.app.get(self.url, auth=self.non_contrib)
        assert_equal(res.status_code, 200)
        assert_equal(res.content_type, 'application/vnd.api+json')
        ids = [each['id'] for each in res.json['data']]
        assert_in(self.public._id, ids)
        assert_not_in(self.private._id, ids)

    def test_return_private_node_list_logged_out_user(self):
        res = self.app.get(self.url)
        ids = [each['id'] for each in res.json['data']]
        assert_in(self.public._id, ids)
        assert_not_in(self.private._id, ids)

    def test_return_private_node_list_logged_in_contributor(self):
        res = self.app.get(self.url, auth=self.user.auth)
        assert_equal(res.status_code, 200)
        assert_equal(res.content_type, 'application/vnd.api+json')
        ids = [each['id'] for each in res.json['data']]
        assert_in(self.public._id, ids)
        assert_in(self.private._id, ids)

    def test_return_private_node_list_logged_in_non_contributor(self):
        res = self.app.get(self.url, auth=self.non_contrib.auth)
        ids = [each['id'] for each in res.json['data']]
        assert_in(self.public._id, ids)
        assert_not_in(self.private._id, ids)

    def test_node_list_does_not_returns_registrations(self):
        registration = RegistrationFactory(project=self.public, creator=self.user)
        res = self.app.get(self.url, auth=self.user.auth)
        ids = [each['id'] for each in res.json['data']]
        assert_not_in(registration._id, ids)

    def test_omit_retracted_registration(self):
        registration = RegistrationFactory(creator=self.user, project=self.public)
        res = self.app.get(self.url, auth=self.user.auth)
        assert_equal(len(res.json['data']), 2)
        retraction = RetractedRegistrationFactory(registration=registration, user=registration.creator)
        res = self.app.get(self.url, auth=self.user.auth)
        assert_equal(len(res.json['data']), 2)

    def test_node_list_has_root(self):
        res = self.app.get(self.url, auth=self.user.auth)
        projects_with_root = 0
        for project in res.json['data']:
            if project['relationships'].get('root', None):
                projects_with_root += 1
        assert_not_equal(projects_with_root, 0)
        assert_true(
            all([each['relationships'].get(
                'root'
            ) is not None for each in res.json['data']])
        )


    def test_node_list_has_proper_root(self):
        project_one = ProjectFactory(title="Project One", is_public=True)
        ProjectFactory(parent=project_one, is_public=True)

        res = self.app.get(self.url+'?embed=root&embed=parent', auth=self.user.auth)

        for project_json in res.json['data']:
            project = Node.load(project_json['id'])
            assert_equal(project_json['embeds']['root']['data']['id'], project.root._id)



class TestNodeFiltering(ApiTestCase):

    def setUp(self):
        super(TestNodeFiltering, self).setUp()
        self.user_one = AuthUserFactory()
        self.user_two = AuthUserFactory()
        self.project_one = ProjectFactory(title="Project One", is_public=True)
        self.project_two = ProjectFactory(title="Project Two", description="One Three", is_public=True)
        self.project_three = ProjectFactory(title="Three", is_public=True)
        self.private_project_user_one = ProjectFactory(title="Private Project User One",
                                                       is_public=False,
                                                       creator=self.user_one)
        self.private_project_user_two = ProjectFactory(title="Private Project User Two",
                                                       is_public=False,
                                                       creator=self.user_two)
        self.folder = CollectionFactory()
        self.bookmark_collection = BookmarkCollectionFactory()

        self.url = "/{}nodes/".format(API_BASE)

        self.tag1, self.tag2 = 'tag1', 'tag2'
        self.project_one.add_tag(self.tag1, Auth(self.project_one.creator), save=False)
        self.project_one.add_tag(self.tag2, Auth(self.project_one.creator), save=False)
        self.project_one.save()

        self.project_two.add_tag(self.tag1, Auth(self.project_two.creator), save=True)

    def tearDown(self):
        super(TestNodeFiltering, self).tearDown()
        Node.remove()

    def test_filtering_by_id(self):
        url = '/{}nodes/?filter[id]={}'.format(API_BASE, self.project_one._id)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        ids = [each['id'] for each in res.json['data']]

        assert_in(self.project_one._id, ids)
        assert_equal(len(ids), 1)

    def test_filtering_by_multiple_ids(self):
        url = '/{}nodes/?filter[id]={},{}'.format(API_BASE, self.project_one._id, self.project_two._id)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        ids = [each['id'] for each in res.json['data']]

        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_equal(len(ids), 2)

    def test_filtering_by_multiple_ids_one_private(self):
        url = '/{}nodes/?filter[id]={},{}'.format(API_BASE, self.project_one._id, self.private_project_user_two._id)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        ids = [each['id'] for each in res.json['data']]

        assert_in(self.project_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_equal(len(ids), 1)

    def test_filtering_by_multiple_ids_brackets_in_query_params(self):
        url = '/{}nodes/?filter[id]=[{},   {}]'.format(API_BASE, self.project_one._id, self.project_two._id)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        ids = [each['id'] for each in res.json['data']]

        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_equal(len(ids), 2)

    def test_filtering_by_category(self):
        project = ProjectFactory(creator=self.user_one, category='hypothesis')
        project2 = ProjectFactory(creator=self.user_one, category='procedure')
        url = '/{}nodes/?filter[category]=hypothesis'.format(API_BASE)
        res = self.app.get(url, auth=self.user_one.auth)

        node_json = res.json['data']
        ids = [each['id'] for each in node_json]

        assert_in(project._id, ids)
        assert_not_in(project2._id, ids)

    def test_filtering_by_public(self):
        project = ProjectFactory(creator=self.user_one, is_public=True)
        project2 = ProjectFactory(creator=self.user_one, is_public=False)

        url = '/{}nodes/?filter[public]=false'.format(API_BASE)
        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        # No public projects returned
        assert_false(
            any([each['attributes']['public'] for each in node_json])
        )

        ids = [each['id'] for each in node_json]
        assert_not_in(project._id, ids)
        assert_in(project2._id, ids)

        url = '/{}nodes/?filter[public]=true'.format(API_BASE)
        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        # No private projects returned
        assert_true(
            all([each['attributes']['public'] for each in node_json])
        )

        ids = [each['id'] for each in node_json]
        assert_not_in(project2._id, ids)
        assert_in(project._id, ids)

    def test_filtering_tags(self):
        # both project_one and project_two have tag1
        url = '/{}nodes/?filter[tags]={}'.format(API_BASE, self.tag1)

        res = self.app.get(url, auth=self.project_one.creator.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)

        # filtering two tags
        # project_one has both tags; project_two only has one
        url = '/{}nodes/?filter[tags]={}&filter[tags]={}'.format(API_BASE, self.tag1, self.tag2)

        res = self.app.get(url, auth=self.project_one.creator.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_not_in(self.project_two._id, ids)

    def test_get_all_projects_with_no_filter_logged_in(self):
        res = self.app.get(self.url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_in(self.project_three._id, ids)
        assert_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_all_projects_with_no_filter_not_logged_in(self):
        res = self.app.get(self.url)
        node_json = res.json['data']
        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_one_project_with_exact_filter_logged_in(self):
        url = "/{}nodes/?filter[title]=Project%20One".format(API_BASE)

        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_not_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_one_project_with_exact_filter_not_logged_in(self):
        url = "/{}nodes/?filter[title]=Project%20One".format(API_BASE)

        res = self.app.get(url)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_not_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_some_projects_with_substring_logged_in(self):
        url = "/{}nodes/?filter[title]=Two".format(API_BASE)

        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_not_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_some_projects_with_substring_not_logged_in(self):
        url = "/{}nodes/?filter[title]=Two".format(API_BASE)

        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_not_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_only_public_or_my_projects_with_filter_logged_in(self):
        url = "/{}nodes/?filter[title]=Project".format(API_BASE)

        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_get_only_public_projects_with_filter_not_logged_in(self):
        url = "/{}nodes/?filter[title]=Project".format(API_BASE)

        res = self.app.get(url)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_alternate_filtering_field_logged_in(self):
        url = "/{}nodes/?filter[description]=Three".format(API_BASE)

        res = self.app.get(url, auth=self.user_one.auth)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_not_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_alternate_filtering_field_not_logged_in(self):
        url = "/{}nodes/?filter[description]=Three".format(API_BASE)

        res = self.app.get(url)
        node_json = res.json['data']

        ids = [each['id'] for each in node_json]
        assert_not_in(self.project_one._id, ids)
        assert_in(self.project_two._id, ids)
        assert_not_in(self.project_three._id, ids)
        assert_not_in(self.private_project_user_one._id, ids)
        assert_not_in(self.private_project_user_two._id, ids)
        assert_not_in(self.folder._id, ids)
        assert_not_in(self.bookmark_collection._id, ids)

    def test_incorrect_filtering_field_not_logged_in(self):
        url = '/{}nodes/?filter[notafield]=bogus'.format(API_BASE)

        res = self.app.get(url, expect_errors=True)
        assert_equal(res.status_code, 400)
        errors = res.json['errors']
        assert_equal(len(errors), 1)
        assert_equal(errors[0]['detail'], "'notafield' is not a valid field for this endpoint.")

    def test_filtering_on_root(self):
        root = ProjectFactory(is_public=True)
        child = ProjectFactory(parent=root, is_public=True)
        ProjectFactory(parent=root, is_public=True)
        ProjectFactory(parent=child, is_public=True)
        # create some unrelated projects
        ProjectFactory(title="Road Dogg Jesse James", is_public=True)
        ProjectFactory(title="Badd *** Billy Gunn", is_public=True)

        url = '/{}nodes/?filter[root]={}'.format(API_BASE, root._id)

        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)

        root_nodes = Node.find(Q('is_public', 'eq', True) & Q('root', 'eq', root._id))
        assert_equal(len(res.json['data']), root_nodes.count())

    def test_filtering_on_null_parent(self):
        # add some nodes TO be included
        new_user = AuthUserFactory()
        root = ProjectFactory(is_public=True)
        ProjectFactory(is_public=True)
        # Build up a some of nodes not to be included
        child = ProjectFactory(parent=root, is_public=True)
        ProjectFactory(parent=root, is_public=True)
        ProjectFactory(parent=child, is_public=True)

        url = '/{}nodes/?filter[parent]=null'.format(API_BASE)

        res = self.app.get(url, auth=new_user.auth)
        assert_equal(res.status_code, 200)

        public_root_nodes = Node.find(Q('is_public', 'eq', True) & Q('parent_node', 'eq', None))
        assert_equal(len(res.json['data']), public_root_nodes.count())

    def test_filtering_on_title_not_equal(self):
        url = '/{}nodes/?filter[title][ne]=Project%20One'.format(API_BASE)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        data = res.json['data']
        assert_equal(len(data), 3)

        titles = [each['attributes']['title'] for each in data]

        assert_not_in(self.project_one.title, titles)
        assert_in(self.project_two.title, titles)
        assert_in(self.project_three.title, titles)
        assert_in(self.private_project_user_one.title, titles)

    def test_filtering_on_description_not_equal(self):
        url = '/{}nodes/?filter[description][ne]=One%20Three'.format(API_BASE)
        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        data = res.json['data']
        assert_equal(len(data), 3)

        descriptions = [each['attributes']['description'] for each in data]

        assert_not_in(self.project_two.description, descriptions)
        assert_in(self.project_one.description, descriptions)
        assert_in(self.project_three.description, descriptions)
        assert_in(self.private_project_user_one.description, descriptions)


class TestNodeCreate(ApiTestCase):

    def setUp(self):
        super(TestNodeCreate, self).setUp()
        self.user_one = AuthUserFactory()
        self.url = '/{}nodes/'.format(API_BASE)

        self.title = 'Cool Project'
        self.description = 'A Properly Cool Project'
        self.category = 'data'

        self.user_two = AuthUserFactory()

        self.public_project = {
            'data': {
                'type': 'nodes',
                'attributes':
                    {
                        'title': self.title,
                        'description': self.description,
                        'category': self.category,
                        'public': True,
                    }
            }
        }
        self.private_project = {
            'data': {
                'type': 'nodes',
                'attributes': {
                    'title': self.title,
                    'description': self.description,
                    'category': self.category,
                    'public': False
                }
            }
        }
    def test_node_create_invalid_data(self):
        res = self.app.post_json_api(self.url, "Incorrect data", auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], "Malformed request.")

        res = self.app.post_json_api(self.url, ["Incorrect data"], auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], "Malformed request.")

    def test_creates_public_project_logged_out(self):
        res = self.app.post_json_api(self.url, self.public_project, expect_errors=True)
        assert_equal(res.status_code, 401)
        assert_in('detail', res.json['errors'][0])

    def test_creates_public_project_logged_in(self):
        res = self.app.post_json_api(self.url, self.public_project, expect_errors=True, auth=self.user_one.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.json['data']['attributes']['title'], self.public_project['data']['attributes']['title'])
        assert_equal(res.json['data']['attributes']['description'], self.public_project['data']['attributes']['description'])
        assert_equal(res.json['data']['attributes']['category'], self.public_project['data']['attributes']['category'])
        assert_equal(res.content_type, 'application/vnd.api+json')
        pid = res.json['data']['id']
        project = Node.load(pid)
        assert_equal(project.logs[-1].action, NodeLog.PROJECT_CREATED)

    def test_creates_private_project_logged_out(self):
        res = self.app.post_json_api(self.url, self.private_project, expect_errors=True)
        assert_equal(res.status_code, 401)
        assert_in('detail', res.json['errors'][0])

    def test_creates_private_project_logged_in_contributor(self):
        res = self.app.post_json_api(self.url, self.private_project, auth=self.user_one.auth)
        assert_equal(res.status_code, 201)
        assert_equal(res.content_type, 'application/vnd.api+json')
        assert_equal(res.json['data']['attributes']['title'], self.private_project['data']['attributes']['title'])
        assert_equal(res.json['data']['attributes']['description'], self.private_project['data']['attributes']['description'])
        assert_equal(res.json['data']['attributes']['category'], self.private_project['data']['attributes']['category'])
        pid = res.json['data']['id']
        project = Node.load(pid)
        assert_equal(project.logs[-1].action, NodeLog.PROJECT_CREATED)

    def test_creates_project_from_template(self):
        template_from = ProjectFactory(creator=self.user_one, is_public=True)
        template_component = ProjectFactory(creator=self.user_one, is_public=True, parent=template_from)
        templated_project_title = 'Templated Project'
        templated_project_data = {
            'data': {
                'type': 'nodes',
                'attributes':
                    {
                        'title': templated_project_title,
                        'category': self.category,
                        'template_from': template_from._id,
                    }
            }
        }

        res = self.app.post_json_api(self.url, templated_project_data, auth=self.user_one.auth)
        assert_equal(res.status_code, 201)
        json_data = res.json['data']

        new_project_id = json_data['id']
        new_project = Node.load(new_project_id)
        assert_equal(new_project.title, templated_project_title)
        assert_equal(new_project.description, None)
        assert_false(new_project.is_public)
        assert_equal(len(new_project.nodes), len(template_from.nodes))
        assert_equal(new_project.nodes[0].title, template_component.title)

    def test_404_on_create_from_template_of_nonexistent_project(self):
        template_from_id = 'thisisnotavalidguid'
        templated_project_data = {
            'data': {
                'type': 'nodes',
                'attributes':
                    {
                        'title': 'No title',
                        'category': 'project',
                        'template_from': template_from_id,
                    }
            }
        }
        res = self.app.post_json_api(self.url, templated_project_data, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 404)

    def test_403_on_create_from_template_of_unauthorized_project(self):
        template_from = ProjectFactory(creator=self.user_two, is_public=True)
        templated_project_data = {
            'data': {
                'type': 'nodes',
                'attributes':
                    {
                        'title': 'No permission',
                        'category': 'project',
                        'template_from': template_from._id,
                    }
            }
        }
        res = self.app.post_json_api(self.url, templated_project_data, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 403)

    def test_creates_project_creates_project_and_sanitizes_html(self):
        title = '<em>Cool</em> <strong>Project</strong>'
        description = 'An <script>alert("even cooler")</script> project'

        res = self.app.post_json_api(self.url, {
            'data': {
                'attributes': {
                    'title': title,
                    'description': description,
                    'category': self.category,
                    'public': True
                },
                'type': 'nodes'
            }
        }, auth=self.user_one.auth)
        project_id = res.json['data']['id']
        assert_equal(res.status_code, 201)
        assert_equal(res.content_type, 'application/vnd.api+json')
        url = '/{}nodes/{}/'.format(API_BASE, project_id)

        project = Node.load(project_id)
        assert_equal(project.logs[-1].action, NodeLog.PROJECT_CREATED)

        res = self.app.get(url, auth=self.user_one.auth)
        assert_equal(res.json['data']['attributes']['title'], strip_html(title))
        assert_equal(res.json['data']['attributes']['description'], strip_html(description))
        assert_equal(res.json['data']['attributes']['category'], self.category)

    def test_creates_project_no_type(self):
        project = {
            'data': {
                'attributes': {
                    'title': self.title,
                    'description': self.description,
                    'category': self.category,
                    'public': False
                }
            }
        }
        res = self.app.post_json_api(self.url, project, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'This field may not be null.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/type')

    def test_creates_project_incorrect_type(self):
        project = {
            'data': {
                'attributes': {
                    'title': self.title,
                    'description': self.description,
                    'category': self.category,
                    'public': False
                },
                'type': 'Wrong type.'
            }
        }
        res = self.app.post_json_api(self.url, project, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 409)
        assert_equal(res.json['errors'][0]['detail'], 'Resource identifier does not match server endpoint.')

    def test_creates_project_properties_not_nested(self):
        project = {
            'data': {
                'title': self.title,
                'description': self.description,
                'category': self.category,
                'public': False,
                'type': 'nodes'
            }
        }
        res = self.app.post_json_api(self.url, project, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Request must include /data/attributes.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/attributes')

    def test_create_project_invalid_title(self):
        project = {
            'data': {
                'type': 'nodes',
                'attributes': {
                    'title': 'A' * 201,
                    'description': self.description,
                    'category': self.category,
                    'public': False,
                }
            }
        }
        res = self.app.post_json_api(self.url, project, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Title cannot exceed 200 characters.')


class TestNodeBulkCreate(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkCreate, self).setUp()
        self.user_one = AuthUserFactory()
        self.url = '/{}nodes/'.format(API_BASE)

        self.title = 'Cool Project'
        self.description = 'A Properly Cool Project'
        self.category = 'data'

        self.user_two = AuthUserFactory()

        self.public_project = {
                'type': 'nodes',
                'attributes': {
                    'title': self.title,
                    'description': self.description,
                    'category': self.category,
                    'public': True
                }
        }

        self.private_project = {
                'type': 'nodes',
                'attributes': {
                    'title': self.title,
                    'description': self.description,
                    'category': self.category,
                    'public': False
                }
        }

        self.empty_project = {'type': 'nodes', 'attributes': {'title': "", 'description': "", "category": ""}}

    def test_bulk_create_nodes_blank_request(self):
        res = self.app.post_json_api(self.url, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

    def test_bulk_create_all_or_nothing(self):
        res = self.app.post_json_api(self.url, {'data': [self.public_project, self.empty_project]}, bulk=True, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_logged_out(self):
        res = self.app.post_json_api(self.url, {'data': [self.public_project, self.private_project]}, bulk=True, expect_errors=True)
        assert_equal(res.status_code, 401)

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_error_formatting(self):
        res = self.app.post_json_api(self.url, {'data': [self.empty_project, self.empty_project]}, bulk=True, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 2)
        errors = res.json['errors']
        assert_items_equal([errors[0]['source'], errors[1]['source']],
                           [{'pointer': '/data/0/attributes/title'}, {'pointer': '/data/1/attributes/title'}])
        assert_items_equal([errors[0]['detail'], errors[1]['detail']],
                           ["This field may not be blank.", "This field may not be blank."])

    def test_bulk_create_limits(self):
        node_create_list = {'data': [self.public_project] * 101}
        res = self.app.post_json_api(self.url, node_create_list, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.json['errors'][0]['detail'], 'Bulk operation limit is 100, got 101.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data')

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_no_type(self):
        payload = {'data': [{"attributes": {'category': self.category, 'title': self.title}}]}
        res = self.app.post_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/0/type')

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_incorrect_type(self):
        payload = {'data': [self.public_project, {'type': 'Incorrect type.', "attributes": {'category': self.category, 'title': self.title}}]}
        res = self.app.post_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 409)

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_no_attributes(self):
        payload = {'data': [self.public_project, {'type': 'nodes', }]}
        res = self.app.post_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)

        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/attributes')

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_no_title(self):
        payload = {'data': [self.public_project, {'type': 'nodes', "attributes": {'category': self.category}}]}
        res = self.app.post_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)

        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/1/attributes/title')

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_ugly_payload(self):
        payload = 'sdf;jlasfd'
        res = self.app.post_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 0)

    def test_bulk_create_logged_in(self):
        res = self.app.post_json_api(self.url, {'data': [self.public_project, self.private_project]}, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 201)
        assert_equal(len(res.json['data']), 2)
        assert_equal(res.json['data'][0]['attributes']['title'], self.public_project['attributes']['title'])
        assert_equal(res.json['data'][0]['attributes']['category'], self.public_project['attributes']['category'])
        assert_equal(res.json['data'][0]['attributes']['description'], self.public_project['attributes']['description'])
        assert_equal(res.json['data'][1]['attributes']['title'], self.private_project['attributes']['title'])
        assert_equal(res.json['data'][1]['attributes']['category'], self.public_project['attributes']['category'])
        assert_equal(res.json['data'][1]['attributes']['description'], self.public_project['attributes']['description'])
        assert_equal(res.content_type, 'application/vnd.api+json')

        res = self.app.get(self.url, auth=self.user_one.auth)
        assert_equal(len(res.json['data']), 2)
        id_one = res.json['data'][0]['id']
        id_two = res.json['data'][1]['id']

        res = self.app.delete_json_api(self.url, {'data': [{'id': id_one, 'type': 'nodes'},
                                                           {'id': id_two, 'type': 'nodes'}]},
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 204)


class TestNodeBulkUpdate(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkUpdate, self).setUp()
        self.user = AuthUserFactory()

        self.title = 'Cool Project'
        self.new_title = 'Super Cool Project'
        self.description = 'A Properly Cool Project'
        self.new_description = 'An even cooler project'
        self.category = 'data'

        self.new_category = 'project'

        self.user_two = AuthUserFactory()

        self.public_project = ProjectFactory(title=self.title,
                                             description=self.description,
                                             category=self.category,
                                             is_public=True,
                                             creator=self.user)

        self.public_project_two = ProjectFactory(title=self.title,
                                                description=self.description,
                                                category=self.category,
                                                is_public=True,
                                                creator=self.user)

        self.public_payload = {
            'data': [
                {
                    'id': self.public_project._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                },
                {
                    'id': self.public_project_two._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                }
            ]
        }

        self.url = '/{}nodes/'.format(API_BASE)

        self.private_project = ProjectFactory(title=self.title,
                                              description=self.description,
                                              category=self.category,
                                              is_public=False,
                                              creator=self.user)

        self.private_project_two = ProjectFactory(title=self.title,
                                              description=self.description,
                                              category=self.category,
                                              is_public=False,
                                              creator=self.user)

        self.private_payload = {'data': [
                {
                    'id': self.private_project._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': False
                    }
                },
                {
                    'id': self.private_project_two._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': False
                    }
                }
            ]
        }


        self.empty_payload = {'data': [
            {'id': self.public_project._id, 'type': 'nodes', 'attributes': {'title': "", 'description': "", "category": ""}},
            {'id': self.public_project_two._id, 'type': 'nodes', 'attributes': {'title': "", 'description': "", "category": ""}}
        ]}

    def test_bulk_update_nodes_blank_request(self):
        res = self.app.put_json_api(self.url, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

    def test_bulk_update_blank_but_not_empty_title(self):
        payload = {
            "data": [
                {
                  "id": self.public_project._id,
                  "type": "nodes",
                  "attributes": {
                    "title": "This shouldn't update.",
                    "category": "instrumentation"
                  }
                },
                {
                  "id": self.public_project_two._id,
                  "type": "nodes",
                  "attributes": {
                    "title": " ",
                    "category": "hypothesis"
                  }
                }
              ]
            }
        url = '/{}nodes/{}/'.format(API_BASE, self.public_project._id)
        res = self.app.put_json_api(self.url, payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

        res = self.app.get(url)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_with_tags(self):
        new_payload = {'data': [{'id': self.public_project._id, 'type': 'nodes', 'attributes': {'title': 'New title', 'category': 'project', 'tags': ['new tag']}}]}

        res = self.app.put_json_api(self.url, new_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 200)
        assert_equal(res.json['data'][0]['attributes']['tags'], ['new tag'])

    def test_bulk_update_public_projects_one_not_found(self):
        empty_payload = {'data': [
            {
                'id': 12345,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title,
                    'category': self.new_category
                }
            }, self.public_payload['data'][0]
        ]}

        res = self.app.put_json_api(self.url, empty_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Could not find all objects to update.')


        url = '/{}nodes/{}/'.format(API_BASE, self.public_project._id)
        res = self.app.get(url)
        assert_equal(res.json['data']['attributes']['title'], self.title)


    def test_bulk_update_public_projects_logged_out(self):
        res = self.app.put_json_api(self.url, self.public_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], "Authentication credentials were not provided.")

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_public_projects_logged_in(self):
        res = self.app.put_json_api(self.url, self.public_payload, auth=self.user.auth, bulk=True)
        assert_equal(res.status_code, 200)
        assert_equal({self.public_project._id, self.public_project_two._id},
                     {res.json['data'][0]['id'], res.json['data'][1]['id']})
        assert_equal(res.json['data'][0]['attributes']['title'], self.new_title)
        assert_equal(res.json['data'][1]['attributes']['title'], self.new_title)

    def test_bulk_update_private_projects_logged_out(self):
        res = self.app.put_json_api(self.url, self.private_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], 'Authentication credentials were not provided.')


        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_private_projects_logged_in_contrib(self):
        res = self.app.put_json_api(self.url, self.private_payload, auth=self.user.auth, bulk=True)
        assert_equal(res.status_code, 200)
        assert_equal({self.private_project._id, self.private_project_two._id},
                     {res.json['data'][0]['id'], res.json['data'][1]['id']})
        assert_equal(res.json['data'][0]['attributes']['title'], self.new_title)
        assert_equal(res.json['data'][1]['attributes']['title'], self.new_title)

    def test_bulk_update_private_projects_logged_in_non_contrib(self):
        res = self.app.put_json_api(self.url, self.private_payload, auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_private_projects_logged_in_read_only_contrib(self):
        self.private_project.add_contributor(self.user_two, permissions=[permissions.READ], save=True)
        self.private_project_two.add_contributor(self.user_two, permissions=[permissions.READ], save=True)
        res = self.app.put_json_api(self.url, self.private_payload, auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_projects_send_dictionary_not_list(self):
        res = self.app.put_json_api(self.url, {'data': {'id': self.public_project._id, 'type': 'nodes',
                                                        'attributes': {'title': self.new_title, 'category': "project"}}},
                                    auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Expected a list of items but got type "dict".')

    def test_bulk_update_error_formatting(self):
        res = self.app.put_json_api(self.url, self.empty_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 2)
        errors = res.json['errors']
        assert_items_equal([errors[0]['source'], errors[1]['source']],
                           [{'pointer': '/data/0/attributes/title'}, {'pointer': '/data/1/attributes/title'}])
        assert_items_equal([errors[0]['detail'], errors[1]['detail']],
                           ['This field may not be blank.'] * 2)

    def test_bulk_update_id_not_supplied(self):
        res = self.app.put_json_api(self.url, {'data': [self.public_payload['data'][1], {'type': 'nodes', 'attributes':
            {'title': self.new_title, 'category': self.new_category}}]}, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 1)
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/1/id')
        assert_equal(res.json['errors'][0]['detail'], "This field may not be null.")

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_type_not_supplied(self):
        res = self.app.put_json_api(self.url, {'data': [self.public_payload['data'][1], {'id': self.public_project._id, 'attributes':
            {'title': self.new_title, 'category': self.new_category}}]}, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 1)
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data/1/type')
        assert_equal(res.json['errors'][0]['detail'], "This field may not be null.")

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_incorrect_type(self):
        res = self.app.put_json_api(self.url, {'data': [self.public_payload['data'][1], {'id': self.public_project._id, 'type': 'Incorrect', 'attributes':
            {'title': self.new_title, 'category': self.new_category}}]}, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 409)

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_update_limits(self):
        node_update_list = {'data': [self.public_payload['data'][0]] * 101}
        res = self.app.put_json_api(self.url, node_update_list, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.json['errors'][0]['detail'], 'Bulk operation limit is 100, got 101.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data')

    def test_bulk_update_no_title_or_category(self):
        new_payload = {'id': self.public_project._id, 'type': 'nodes', 'attributes': {}}
        res = self.app.put_json_api(self.url, {'data': [self.public_payload['data'][1], new_payload]}, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)


class TestNodeBulkPartialUpdate(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkPartialUpdate, self).setUp()
        self.user = AuthUserFactory()

        self.title = 'Cool Project'
        self.new_title = 'Super Cool Project'
        self.description = 'A Properly Cool Project'
        self.new_description = 'An even cooler project'
        self.category = 'data'

        self.new_category = 'project'

        self.user_two = AuthUserFactory()

        self.public_project = ProjectFactory(title=self.title,
                                             description=self.description,
                                             category=self.category,
                                             is_public=True,
                                             creator=self.user)

        self.public_project_two = ProjectFactory(title=self.title,
                                                description=self.description,
                                                category=self.category,
                                                is_public=True,
                                                creator=self.user)

        self.public_payload = {'data': [
            {
                'id': self.public_project._id,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title
                }
            },
            {
                'id': self.public_project_two._id,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title
                }
            }
        ]}

        self.url = '/{}nodes/'.format(API_BASE)

        self.private_project = ProjectFactory(title=self.title,
                                              description=self.description,
                                              category=self.category,
                                              is_public=False,
                                              creator=self.user)

        self.private_project_two = ProjectFactory(title=self.title,
                                              description=self.description,
                                              category=self.category,
                                              is_public=False,
                                              creator=self.user)

        self.private_payload = {'data': [
            {
                'id': self.private_project._id,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title
                }
            },
            {
                'id': self.private_project_two._id,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title
                }
            }
        ]}

        self.empty_payload = {'data': [
            {'id': self.public_project._id, 'type': 'nodes', 'attributes': {'title': ""}},
            {'id': self.public_project_two._id, 'type': 'nodes', 'attributes': {'title': ""}}
        ]
        }

    def test_bulk_patch_nodes_blank_request(self):
        res = self.app.patch_json_api(self.url, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

    def test_bulk_partial_update_public_projects_one_not_found(self):
        empty_payload = {'data': [
            {
                'id': 12345,
                'type': 'nodes',
                'attributes': {
                    'title': self.new_title
                }
            },
            self.public_payload['data'][0]
        ]}
        res = self.app.patch_json_api(self.url, empty_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Could not find all objects to update.')

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project._id)
        res = self.app.get(url)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_partial_update_public_projects_logged_out(self):
        res = self.app.patch_json_api(self.url, self.public_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], "Authentication credentials were not provided.")

        url = '/{}nodes/{}/'.format(API_BASE, self.public_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.public_project_two._id)

        res = self.app.get(url)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_partial_update_public_projects_logged_in(self):
        res = self.app.patch_json_api(self.url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 200)
        assert_equal({self.public_project._id, self.public_project_two._id},
                     {res.json['data'][0]['id'], res.json['data'][1]['id']})
        assert_equal(res.json['data'][0]['attributes']['title'], self.new_title)
        assert_equal(res.json['data'][1]['attributes']['title'], self.new_title)

    def test_bulk_partial_update_private_projects_logged_out(self):
        res = self.app.patch_json_api(self.url, self.private_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], 'Authentication credentials were not provided.')


        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_partial_update_private_projects_logged_in_contrib(self):
        res = self.app.patch_json_api(self.url, self.private_payload, auth=self.user.auth, bulk=True)
        assert_equal(res.status_code, 200)
        assert_equal({self.private_project._id, self.private_project_two._id},
                     {res.json['data'][0]['id'], res.json['data'][1]['id']})
        assert_equal(res.json['data'][0]['attributes']['title'], self.new_title)
        assert_equal(res.json['data'][1]['attributes']['title'], self.new_title)

    def test_bulk_partial_update_private_projects_logged_in_non_contrib(self):
        res = self.app.patch_json_api(self.url, self.private_payload, auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_partial_update_private_projects_logged_in_read_only_contrib(self):
        self.private_project.add_contributor(self.user_two, permissions=[permissions.READ], save=True)
        self.private_project_two.add_contributor(self.user_two, permissions=[permissions.READ], save=True)
        res = self.app.patch_json_api(self.url, self.private_payload, auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        url = '/{}nodes/{}/'.format(API_BASE, self.private_project._id)
        url_two = '/{}nodes/{}/'.format(API_BASE, self.private_project_two._id)

        res = self.app.get(url, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

        res = self.app.get(url_two, auth=self.user.auth)
        assert_equal(res.json['data']['attributes']['title'], self.title)

    def test_bulk_partial_update_projects_send_dictionary_not_list(self):
        res = self.app.patch_json_api(self.url, {'data': {'id': self.public_project._id, 'attributes': {'title': self.new_title, 'category': "project"}}},
                                    auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Expected a list of items but got type "dict".')

    def test_bulk_partial_update_error_formatting(self):
        res = self.app.patch_json_api(self.url, self.empty_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 2)
        errors = res.json['errors']
        assert_items_equal([errors[0]['source'], errors[1]['source']],
                           [{'pointer': '/data/0/attributes/title'}, {'pointer': '/data/1/attributes/title'}])
        assert_items_equal([errors[0]['detail'], errors[1]['detail']],
                           ['This field may not be blank.']*2)

    def test_bulk_partial_update_id_not_supplied(self):
        res = self.app.patch_json_api(self.url, {'data': [{'type': 'nodes', 'attributes': {'title': self.new_title}}]},
                                      auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(len(res.json['errors']), 1)
        assert_equal(res.json['errors'][0]['detail'], 'This field may not be null.')

    def test_bulk_partial_update_limits(self):
        node_update_list = {'data': [self.public_payload['data'][0]] * 101 }
        res = self.app.patch_json_api(self.url, node_update_list, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.json['errors'][0]['detail'], 'Bulk operation limit is 100, got 101.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data')

    def test_bulk_partial_update_privacy_has_no_effect_on_tags(self):
        self.public_project.add_tag('tag1', Auth(self.public_project.creator), save=True)
        payload = {'id': self.public_project._id, 'type': 'nodes', 'attributes': {'public': False}}
        res = self.app.patch_json_api(self.url, {'data': [payload]}, auth=self.user.auth, bulk=True)
        assert_equal(res.status_code, 200)
        self.public_project.reload()
        assert_equal(self.public_project.tags, ['tag1'])
        assert_equal(self.public_project.is_public, False)


class TestNodeBulkUpdateSkipUneditable(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkUpdateSkipUneditable, self).setUp()
        self.user = AuthUserFactory()
        self.user_two = AuthUserFactory()

        self.title = 'Cool Project'
        self.new_title = 'Super Cool Project'
        self.description = 'A Properly Cool Project'
        self.new_description = 'An even cooler project'
        self.category = 'data'
        self.new_category = 'project'

        self.public_project = ProjectFactory(title=self.title,
                                             description=self.description,
                                             category=self.category,
                                             is_public=True,
                                             creator=self.user)

        self.public_project_two = ProjectFactory(title=self.title,
                                                description=self.description,
                                                category=self.category,
                                                is_public=True,
                                                creator=self.user)

        self.public_project_three = ProjectFactory(title=self.title,
                                                description=self.description,
                                                category=self.category,
                                                is_public=True,
                                                creator=self.user_two)

        self.public_project_four = ProjectFactory(title=self.title,
                                                description=self.description,
                                                category=self.category,
                                                is_public=True,
                                                creator=self.user_two)

        self.public_payload = {
            'data': [
                {
                    'id': self.public_project._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                },
                {
                    'id': self.public_project_two._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                },
                 {
                    'id': self.public_project_three._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                },
                 {
                    'id': self.public_project_four._id,
                    'type': 'nodes',
                    'attributes': {
                        'title': self.new_title,
                        'description': self.new_description,
                        'category': self.new_category,
                        'public': True
                    }
                }
            ]
        }

        self.url = '/{}nodes/?skip_uneditable=True'.format(API_BASE)

    def test_skip_uneditable_bulk_update(self):
        res = self.app.put_json_api(self.url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 200)
        edited = res.json['data']
        skipped = res.json['errors']
        assert_items_equal([edited[0]['id'], edited[1]['id']],
                           [self.public_project._id, self.public_project_two._id])
        assert_items_equal([skipped[0]['_id'], skipped[1]['_id']],
                           [self.public_project_three._id, self.public_project_four._id])
        self.public_project.reload()
        self.public_project_two.reload()
        self.public_project_three.reload()
        self.public_project_four.reload()

        assert_equal(self.public_project.title, self.new_title)
        assert_equal(self.public_project_two.title, self.new_title)
        assert_equal(self.public_project_three.title, self.title)
        assert_equal(self.public_project_four.title, self.title)


    def test_skip_uneditable_bulk_update_query_param_required(self):
        url = '/{}nodes/'.format(API_BASE)
        res = self.app.put_json_api(url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        self.public_project.reload()
        self.public_project_two.reload()
        self.public_project_three.reload()
        self.public_project_four.reload()

        assert_equal(self.public_project.title, self.title)
        assert_equal(self.public_project_two.title, self.title)
        assert_equal(self.public_project_three.title, self.title)
        assert_equal(self.public_project_four.title, self.title)

    def test_skip_uneditable_equals_false_bulk_update(self):
        url = '/{}nodes/?skip_uneditable=False'.format(API_BASE)
        res = self.app.put_json_api(url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        self.public_project.reload()
        self.public_project_two.reload()
        self.public_project_three.reload()
        self.public_project_four.reload()

        assert_equal(self.public_project.title, self.title)
        assert_equal(self.public_project_two.title, self.title)
        assert_equal(self.public_project_three.title, self.title)
        assert_equal(self.public_project_four.title, self.title)

    def test_skip_uneditable_bulk_partial_update(self):
        res = self.app.patch_json_api(self.url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 200)
        edited = res.json['data']
        skipped = res.json['errors']
        assert_items_equal([edited[0]['id'], edited[1]['id']],
                           [self.public_project._id, self.public_project_two._id])
        assert_items_equal([skipped[0]['_id'], skipped[1]['_id']],
                           [self.public_project_three._id, self.public_project_four._id])
        self.public_project.reload()
        self.public_project_two.reload()
        self.public_project_three.reload()
        self.public_project_four.reload()

        assert_equal(self.public_project.title, self.new_title)
        assert_equal(self.public_project_two.title, self.new_title)
        assert_equal(self.public_project_three.title, self.title)
        assert_equal(self.public_project_four.title, self.title)


    def test_skip_uneditable_bulk_partial_update_query_param_required(self):
        url = '/{}nodes/'.format(API_BASE)
        res = self.app.patch_json_api(url, self.public_payload, auth=self.user.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        self.public_project.reload()
        self.public_project_two.reload()
        self.public_project_three.reload()
        self.public_project_four.reload()

        assert_equal(self.public_project.title, self.title)
        assert_equal(self.public_project_two.title, self.title)
        assert_equal(self.public_project_three.title, self.title)
        assert_equal(self.public_project_four.title, self.title)


class TestNodeBulkDelete(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkDelete, self).setUp()
        self.user_one = AuthUserFactory()
        self.user_two = AuthUserFactory()
        self.project_one = ProjectFactory(title="Project One", is_public=True, creator=self.user_one, category="project")
        self.project_two = ProjectFactory(title="Project Two", description="One Three", is_public=True, creator=self.user_one)
        self.private_project_user_one = ProjectFactory(title="Private Project User One",
                                                       is_public=False,
                                                       creator=self.user_one)
        self.private_project_user_two = ProjectFactory(title="Private Project User Two",
                                                       is_public=False,
                                                       creator=self.user_two)

        self.url = "/{}nodes/".format(API_BASE)
        self.project_one_url = '/{}nodes/{}/'.format(API_BASE, self.project_one._id)
        self.project_two_url = '/{}nodes/{}/'.format(API_BASE, self.project_two._id)
        self.private_project_url = "/{}nodes/{}/".format(API_BASE, self.private_project_user_one._id)

        self.public_payload = {'data': [{'id': self.project_one._id, 'type': 'nodes'}, {'id': self.project_two._id, 'type': 'nodes'}]}
        self.private_payload = {'data': [{'id': self.private_project_user_one._id, 'type': 'nodes'}]}

    def test_bulk_delete_nodes_blank_request(self):
        res = self.app.delete_json_api(self.url, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)

    def test_bulk_delete_no_type(self):
        payload = {'data': [
            {'id': self.project_one._id},
            {'id': self.project_two._id}
        ]}
        res = self.app.delete_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Request must include /type.')

    def test_bulk_delete_no_id(self):
        payload = {'data': [
            {'type': 'nodes'},
            {'id': 'nodes'}
        ]}
        res = self.app.delete_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Request must include /data/id.')

    def test_bulk_delete_dict_inside_data(self):
        res = self.app.delete_json_api(self.url, {'data': {'id': self.project_one._id, 'type': 'nodes'}},
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Expected a list of items but got type "dict".')

    def test_bulk_delete_invalid_type(self):
        res = self.app.delete_json_api(self.url, {'data': [{'type': 'Wrong type', 'id': self.project_one._id}]},
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 409)

    def test_bulk_delete_public_projects_logged_in(self):
        res = self.app.delete_json_api(self.url, self.public_payload, auth=self.user_one.auth, bulk=True)
        assert_equal(res.status_code, 204)

        res = self.app.get(self.project_one_url, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 410)
        self.project_one.reload()
        self.project_two.reload()

    def test_bulk_delete_public_projects_logged_out(self):
        res = self.app.delete_json_api(self.url, self.public_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], 'Authentication credentials were not provided.')

        res = self.app.get(self.project_one_url, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 200)

        res = self.app.get(self.project_two_url, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 200)

    def test_bulk_delete_private_projects_logged_out(self):
        res = self.app.delete_json_api(self.url, self.private_payload, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 401)
        assert_equal(res.json['errors'][0]['detail'], 'Authentication credentials were not provided.')

    def test_bulk_delete_private_projects_logged_in_contributor(self):
        res = self.app.delete_json_api(self.url, self.private_payload,
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 204)

        res = self.app.get(self.private_project_url, auth=self.user_one.auth, expect_errors=True)
        assert_equal(res.status_code, 410)
        self.private_project_user_one.reload()

    def test_bulk_delete_private_projects_logged_in_non_contributor(self):
        res = self.app.delete_json_api(self.url, self.private_payload,
                                       auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        res = self.app.get(self.private_project_url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)

    def test_bulk_delete_private_projects_logged_in_read_only_contributor(self):
        self.private_project_user_one.add_contributor(self.user_two, permissions=[permissions.READ], save=True)
        res = self.app.delete_json_api(self.url, self.private_payload,
                                       auth=self.user_two.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        res = self.app.get(self.private_project_url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)

    def test_bulk_delete_all_or_nothing(self):
        new_payload = {'data': [{'id': self.private_project_user_one._id, 'type': 'nodes'}, {'id': self.private_project_user_two._id, 'type': 'nodes'}]}
        res = self.app.delete_json_api(self.url, new_payload,
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)
        assert_equal(res.json['errors'][0]['detail'], 'You do not have permission to perform this action.')

        res = self.app.get(self.private_project_url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)

        url = "/{}nodes/{}/".format(API_BASE, self.private_project_user_two._id)
        res = self.app.get(url, auth=self.user_two.auth)
        assert_equal(res.status_code, 200)

    def test_bulk_delete_limits(self):
        new_payload = {'data': [{'id': self.private_project_user_one._id, 'type':'nodes'}] * 101 }
        res = self.app.delete_json_api(self.url, new_payload,
                                       auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Bulk operation limit is 100, got 101.')
        assert_equal(res.json['errors'][0]['source']['pointer'], '/data')

    def test_bulk_delete_invalid_payload_one_not_found(self):
        new_payload = {'data': [self.public_payload['data'][0], {'id': '12345', 'type': 'nodes'}]}
        res = self.app.delete_json_api(self.url, new_payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)
        assert_equal(res.json['errors'][0]['detail'], 'Could not find all objects to delete.')

        res = self.app.get(self.project_one_url, auth=self.user_one.auth)
        assert_equal(res.status_code, 200)

    def test_bulk_delete_no_payload(self):
        res = self.app.delete_json_api(self.url, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 400)


class TestNodeBulkDeleteSkipUneditable(ApiTestCase):

    def setUp(self):
        super(TestNodeBulkDeleteSkipUneditable, self).setUp()
        self.user_one = AuthUserFactory()
        self.user_two = AuthUserFactory()
        self.project_one = ProjectFactory(title="Project One", is_public=True, creator=self.user_one)
        self.project_two = ProjectFactory(title="Project Two",  is_public=True, creator=self.user_one)
        self.project_three = ProjectFactory(title="Project Three", is_public=True, creator=self.user_two)
        self.project_four = ProjectFactory(title="Project Four", is_public=True, creator=self.user_two)

        self.payload = {
            'data': [
                {
                    'id': self.project_one._id,
                    'type': 'nodes',
                },
                {
                    'id': self.project_two._id,
                    'type': 'nodes',
                },
                 {
                    'id': self.project_three._id,
                    'type': 'nodes',
                },
                 {
                    'id': self.project_four._id,
                    'type': 'nodes',
                }
            ]
        }



        self.url = "/{}nodes/?skip_uneditable=True".format(API_BASE)

    def tearDown(self):
        super(TestNodeBulkDeleteSkipUneditable, self).tearDown()
        Node.remove()

    def test_skip_uneditable_bulk_delete(self):
        res = self.app.delete_json_api(self.url, self.payload, auth=self.user_one.auth, bulk=True)
        assert_equal(res.status_code, 200)
        skipped = res.json['errors']
        assert_items_equal([skipped[0]['id'], skipped[1]['id']],
                           [self.project_three._id, self.project_four._id])

        res = self.app.get('/{}nodes/'.format(API_BASE), auth=self.user_one.auth)
        assert_items_equal([res.json['data'][0]['id'], res.json['data'][1]['id']],
                           [self.project_three._id, self.project_four._id])

    def test_skip_uneditable_bulk_delete_query_param_required(self):
        url = '/{}nodes/'.format(API_BASE)
        res = self.app.delete_json_api(url, self.payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)

        res = self.app.get('/{}nodes/'.format(API_BASE), auth=self.user_one.auth)
        assert_equal(res.status_code, 200)
        assert_equal(len(res.json['data']), 4)

    def test_skip_uneditable_has_admin_permission_for_all_nodes(self):
        payload = {
            'data': [
                {
                    'id': self.project_one._id,
                    'type': 'nodes',
                },
                {
                    'id': self.project_two._id,
                    'type': 'nodes',
                }
            ]
        }

        res = self.app.delete_json_api(self.url, payload, auth=self.user_one.auth, bulk=True)
        assert_equal(res.status_code, 204)
        self.project_one.reload()
        self.project_two.reload()

        assert_equal(self.project_one.is_deleted, True)
        assert_equal(self.project_two.is_deleted, True)

    def test_skip_uneditable_does_not_have_admin_permission_for_any_nodes(self):
        payload = {
            'data': [
                {
                    'id': self.project_three._id,
                    'type': 'nodes',
                },
                {
                    'id': self.project_four._id,
                    'type': 'nodes',
                }
            ]
        }

        res = self.app.delete_json_api(self.url, payload, auth=self.user_one.auth, expect_errors=True, bulk=True)
        assert_equal(res.status_code, 403)


class TestNodeListPagination(ApiTestCase):

    def setUp(self):
        super(TestNodeListPagination, self).setUp()

        # Ordered by date modified: oldest first
        self.users = [UserFactory() for _ in range(11)]
        self.projects = [ProjectFactory(is_public=True, creator=self.users[0]) for _ in range(11)]
        
        self.url = '/{}nodes/'.format(API_BASE)

    def tearDown(self):
        super(TestNodeListPagination, self).tearDown()
        Node.remove()

    def test_default_pagination_size(self):
        res = self.app.get(self.url, auth=Auth(self.users[0]))
        pids = [e['id'] for e in res.json['data']]
        for project in self.projects[1:]:
            assert_in(project._id, pids)
        assert_not_in(self.projects[0]._id, pids)
        assert_equal(res.json['links']['meta']['per_page'], 10)

    def test_max_page_size_enforced(self):
        url = '{}?page[size]={}'.format(self.url, MAX_PAGE_SIZE+1)
        res = self.app.get(url, auth=Auth(self.users[0]))
        pids = [e['id'] for e in res.json['data']]
        for project in self.projects:
            assert_in(project._id, pids)
        assert_equal(res.json['links']['meta']['per_page'], MAX_PAGE_SIZE)

    def test_embed_page_size_not_affected(self):
        for user in self.users[1:]:
            self.projects[-1].add_contributor(user, auth=Auth(self.users[0]), save=True)

        url = '{}?page[size]={}&embed=contributors'.format(self.url, MAX_PAGE_SIZE+1)
        res = self.app.get(url, auth=Auth(self.users[0]))
        pids = [e['id'] for e in res.json['data']]
        for project in self.projects:
            assert_in(project._id, pids)
        assert_equal(res.json['links']['meta']['per_page'], MAX_PAGE_SIZE)

        uids = [e['id'] for e in res.json['data'][0]['embeds']['contributors']['data']]
        for user in self.users[:9]:
            assert_in(user._id, uids)
        assert_not_in(self.users[10]._id, uids)
        assert_equal(res.json['data'][0]['embeds']['contributors']['links']['meta']['per_page'], 10)

import logging
import threading

from website import settings

_local = threading.local()
logger = logging.getLogger(__name__)

def postcommit_queue():
    if not hasattr(_local, 'postcommit_queue'):
        _local.postcommit_queue = set()
    return _local.postcommit_queue

def postcommit_before_request():
    _local.postcommit_queue = set()

def postcommit_after_request(response, base_status_error_code=500):
    if response.status_code >= base_status_error_code:
        _local.postcommit_queue = set()
        return response
    try:
        if settings.ENABLE_VARNISH and postcommit_queue():
            import gevent
            threads = [gevent.spawn(func, *args) for func, args in postcommit_queue()]
            gevent.joinall(threads)
    except AttributeError:
        if not settings.DEBUG_MODE:
            logger.error('Post commit task queue not initialized')
    return response

def enqueue_postcommit_task(function_and_args):
    postcommit_queue().add(function_and_args)


handlers = {
    'before_request': postcommit_before_request,
    'after_request': postcommit_after_request,
}

# -*- coding: utf-8 -*-

import os
import matplotlib.pyplot as plt

from framework.mongo import database
from website import settings

from .utils import plot_dates, mkdirp


log_collection = database['nodelog']

FIG_PATH = os.path.join(settings.ANALYTICS_PATH, 'figs', 'logs')
mkdirp(FIG_PATH)


def analyze_log_action(action):
    logs = log_collection.find({'action': action})
    dates = [
        log['date']
        for log in logs
        if log['date']
    ]
    if not dates:
        return
    fig = plot_dates(dates)
    plt.title('logged actions for {} ({} total)'.format(action, len(dates)))
    plt.savefig(os.path.join(FIG_PATH, '{}.png'.format(action)))
    plt.close()


def main():
    actions = log_collection.find(
        {},
        {'action': True}
    ).distinct(
        'action'
    )
    for action in actions:
        analyze_log_action(action)


if __name__ == '__main__':
    main()


#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Adds wiki and osffiles addons to nodes that do not have them.

Log:

    Performed on production by sloria on 2014-08-19 at 4:55PM (EST). 2008 projects
    without the OSF File Storage Addon were migrated. 2 projects without the
    OSF Wiki addon were migrated.
"""

import logging

from nose.tools import *  # noqa (PEP8 asserts)

from tests.base import OsfTestCase
from tests.factories import NodeFactory

from website.app import init_app
from website.project.model import Node

from website.addons.wiki.model import AddonWikiNodeSettings
from website.addons.osffiles.model import AddonFilesNodeSettings

logger = logging.getLogger(__name__)

ADDONS = {AddonFilesNodeSettings, AddonWikiNodeSettings}


def main():
    from framework.mongo import db
    init_app(routes=False)
    migrate_nodes(db)


def migrate_addons(node):
    ret = False
    if not node.has_addon('wiki'):
        node.add_addon('wiki', auth=node.creator, log=False)
        ret = True
    if not node.has_addon('osffiles'):
        node.add_addon('osffiles', auth=node.creator, log=False)
        ret = True
    return ret


def migrate_nodes(db):
    for addon_class in ADDONS:
        print('Processing ' + addon_class.__name__)

        for node in get_affected_nodes(db, addon_class):
            print(' - ' + node._id)
            migrate_addons(node)

        print('')

    print('-----\nDone.')


def get_affected_nodes(db, addon_class):
    """Generate affected nodes."""
    query = db['node'].find({
        '.'.join(
            ('__backrefs',
                'addons',
                addon_class.__name__.lower(),
                'owner',
                '0'
            )
        ): {'$exists': False}
    })
    return (Node.load(node['_id']) for node in query)


class TestMigratingAddons(OsfTestCase):

    def test_migrate_wiki(self):
        node = NodeFactory()
        wiki_addon = node.get_addon('wiki')
        AddonWikiNodeSettings.remove_one(wiki_addon)
        assert_false(node.has_addon('wiki'))
        was_migrated = migrate_addons(node)
        assert_true(was_migrated)
        assert_true(node.has_addon('wiki'))

    def test_migrate_osffiles(self):
        node = NodeFactory()
        osf_addon = node.get_addon('osffiles')
        AddonFilesNodeSettings.remove_one(osf_addon)
        assert_false(node.has_addon('osffiles'))
        was_migrated = migrate_addons(node)
        assert_true(was_migrated)
        assert_true(node.has_addon('osffiles'))

    def test_no_migration_if_addon_exists(self):
        node = NodeFactory()
        assert_true(node.has_addon('wiki'))
        assert_true(node.has_addon('osffiles'))
        migrate_nodes(self.db)
        assert_false(migrate_addons(node))

    def test_affected_nodes(self):
        affected_node = NodeFactory()
        AddonWikiNodeSettings.remove_one(affected_node.get_addon('wiki'))
        assert_false(affected_node.has_addon('wiki'))

        unaffected_node = NodeFactory()
        assert_true(unaffected_node.has_addon('wiki'))

        affected_nodes = list(get_affected_nodes(self.db, AddonWikiNodeSettings))

        assert_in(affected_node, affected_nodes)
        assert_not_in(unaffected_node, affected_nodes)


if __name__ == '__main__':
    main()

from __future__ import unicode_literals
import sys
import logging

from website.app import init_app
from website.models import User
from scripts import utils as script_utils
from modularodm import Q
from bson.son import SON
from framework.mongo import database as db
from framework.transactions.context import TokuTransaction

logger = logging.getLogger(__name__)

pipeline = [
    {"$unwind": "$emails"},
    {"$project": {"emails": {"$toLower": "$emails"}}},
    {"$group": {"_id": "$emails", "count": {"$sum": 1}}},
    {"$sort": SON([("count", -1), ("_id", -1)])}
]


def get_duplicate_email():
    duplicate_emails = []
    result = db['user'].aggregate(pipeline)
    for each in result['result']:
        if each['count'] > 1:
            duplicate_emails.append(each['_id'])
    return duplicate_emails


def log_duplicate_acount(dry):
    duplicate_emails = get_duplicate_email()
    count = 0
    if duplicate_emails:
        for email in duplicate_emails:
            users = User.find(Q('emails', 'eq', email) & Q('merged_by', 'eq', None) & Q('username', 'ne', None))
            for user in users:
                count += 1
                logger.info("User {}, username {}, id {}, email {} is a duplicate"
                            .format(user.fullname, user.username, user._id, user.emails))
    logger.info("Found {} users with duplicate emails".format(count))


def main():
    init_app(routes=False)  # Sets the storage backends on all models
    dry = '--dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    with TokuTransaction():
        log_duplicate_acount(dry)


if __name__ == '__main__':
    main()

import sys
import logging

from website.app import init_app
from website.models import User
from scripts import utils as script_utils
from modularodm import Q

logger = logging.getLogger(__name__)


def do_migration(records, dry=False):
    for user in records:
        log_info(user)
        if not dry:
            user.username = None
            user.password = None
            user.email_verifications = {}
            user.verification_key = None
            user.save()
    logger.info('{}Migrated {} users'.format('[dry]'if dry else '', len(records)))


def get_targets():
    return User.find(Q('merged_by', 'ne', None) & Q('username', 'ne', None))


def log_info(user):
    logger.info(
        'Migrating user - {}: merged_by={}, '.format(
            user._id,
            user.merged_by._id,
        )
    )


def main():
    init_app(routes=False)  # Sets the storage backends on all models
    dry = 'dry' in sys.argv
    if not dry:
        script_utils.add_file_logger(logger, __file__)
    do_migration(get_targets(), dry)


if __name__ == '__main__':
    main()

"""Removes User.username from User.emails for unconfirmed users.

"""

import logging
import sys

from modularodm import Q
from nose.tools import *

from website import models
from website.app import init_app
from scripts import utils as scripts_utils


logger = logging.getLogger(__name__)


def main():
    # Set up storage backends
    init_app(routes=False)
    dry_run = 'dry' in sys.argv
    if not dry_run:
        scripts_utils.add_file_logger(logger, __file__)
    logger.info("Iterating users with unconfirmed email"
                "s")
    for user in get_users_with_unconfirmed_emails():
        remove_unconfirmed_emails(user)
        logger.info(repr(user))
        if not dry_run:
            user.save()


def get_users_with_unconfirmed_emails():
    return models.User.find(
        Q('date_confirmed', 'eq', None)
        & Q('emails', 'ne', [])
    )


def remove_unconfirmed_emails(user):
    user.emails = []


if __name__ == '__main__':
    main()

#!/usr/bin/env python
# encoding: utf-8


import logging
import datetime

from modularodm import Q
from dateutil.relativedelta import relativedelta

from framework.celery_tasks import app as celery_app

from scripts import utils as scripts_utils

from website.app import init_app
from website.addons.box.model import Box
from website.oauth.models import ExternalAccount
from website.addons.base.exceptions import AddonError

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


def get_targets(delta):
    # NOTE: expires_at is the  access_token's expiration date,
    # NOT the refresh token's
    return ExternalAccount.find(
        Q('expires_at', 'lt', datetime.datetime.utcnow() - delta) &
        Q('provider', 'eq', 'box')
    )


def main(delta, dry_run):
    for record in get_targets(delta):
        logger.info(
            'Refreshing tokens on record {0}; expires at {1}'.format(
                record._id,
                record.expires_at.strftime('%c')
            )
        )
        if not dry_run:
            try:
                Box(record).refresh_oauth_key(force=True)
            except AddonError as ex:
                logger.error(ex.message)


@celery_app.task(name='scripts.refresh_box_tokens')
def run_main(days=None, dry_run=True):
    init_app(set_backends=True, routes=False)
    try:
        days = int(days)
    except (ValueError, TypeError):
        days = 60 - 7  # refresh tokens that expire this week
    delta = relativedelta(days=days)
    if not dry_run:
        scripts_utils.add_file_logger(logger, __file__)
    main(delta, dry_run=dry_run)


from nose.tools import *

from scripts.googledrive.migrate_to_external_account import do_migration, get_targets

from framework.auth import Auth

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory

from website.addons.googledrive.model import GoogleDriveUserSettings
from website.addons.googledrive.tests.factories import GoogleDriveOAuthSettingsFactory


class TestGoogleDriveMigration(OsfTestCase):
    # Note: GoogleDriveUserSettings.user_settings has to be changed to foreign_user_settings (model and mongo). See migration instructions

    def test_migration_no_project(self):

        user = UserFactory()

        user.add_addon('googledrive')
        user_addon = user.get_addon('googledrive')
        user_addon.oauth_settings = GoogleDriveOAuthSettingsFactory()
        user_addon.save()

        do_migration([user_addon])
        user_addon.reload()

        assert_is_none(user_addon.oauth_settings)
        assert_equal(len(user.external_accounts), 1)

        account = user.external_accounts[0]
        assert_equal(account.provider, 'googledrive')
        assert_equal(account.oauth_key, 'abcdef1')

    def test_migration_removes_targets(self):
        GoogleDriveUserSettings.remove()

        user = UserFactory()
        project = ProjectFactory(creator=user)


        user.add_addon('googledrive', auth=Auth(user))
        user_addon = user.get_addon('googledrive')
        user_addon.oauth_settings = GoogleDriveOAuthSettingsFactory()
        user_addon.save()


        project.add_addon('googledrive', auth=Auth(user))
        node_addon = project.get_addon('googledrive')
        node_addon.foreign_user_settings = user_addon
        node_addon.save()

        assert_equal(get_targets().count(), 1)

        do_migration([user_addon])
        user_addon.reload()

        assert_equal(get_targets().count(), 0)

    def test_migration_multiple_users(self):
        user1 = UserFactory()
        user2 = UserFactory()
        oauth_settings = GoogleDriveOAuthSettingsFactory()

        user1.add_addon('googledrive')
        user1_addon = user1.get_addon('googledrive')
        user1_addon.oauth_settings = oauth_settings
        user1_addon.save()

        user2.add_addon('googledrive')
        user2_addon = user2.get_addon('googledrive')
        user2_addon.oauth_settings = oauth_settings
        user2_addon.save()

        do_migration([user1_addon, user2_addon])
        user1_addon.reload()
        user2_addon.reload()

        assert_equal(
            user1.external_accounts[0],
            user2.external_accounts[0],
        )

    def test_get_targets(self):
        GoogleDriveUserSettings.remove()
        addons = [
            GoogleDriveUserSettings(),
            GoogleDriveUserSettings(oauth_settings=GoogleDriveOAuthSettingsFactory()),
        ]
        for addon in addons:
            addon.save()
        targets = get_targets()
        assert_equal(targets.count(), 1)
        assert_equal(targets[0]._id, addons[-1]._id)

import logging

from datetime import datetime

from modularodm import Q

from framework.auth import User
from framework.celery_tasks import app as celery_app
from framework.transactions.context import TokuTransaction

from website.app import init_app
from website import mails, settings

from scripts.utils import add_file_logger

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


def main(dry_run=True):
    for user in find_inactive_users_with_no_inactivity_email_sent_or_queued():
        if dry_run:
            logger.warn('Dry run mode')
        logger.warn('Email of type no_login queued to {0}'.format(user.username))
        if not dry_run:
            with TokuTransaction():
                mails.queue_mail(
                    to_addr=user.username,
                    mail=mails.NO_LOGIN,
                    send_at=datetime.utcnow(),
                    user=user,
                    fullname=user.fullname,
                )


def find_inactive_users_with_no_inactivity_email_sent_or_queued():
    inactive_users = User.find(
        (Q('date_last_login', 'lt', datetime.utcnow() - settings.NO_LOGIN_WAIT_TIME) & Q('osf4m', 'ne', 'system_tags')) |
        (Q('date_last_login', 'lt', datetime.utcnow() - settings.NO_LOGIN_OSF4M_WAIT_TIME) & Q('osf4m', 'eq', 'system_tags'))
    )
    inactive_emails = mails.QueuedMail.find(Q('email_type', 'eq', mails.NO_LOGIN_TYPE))

    #This is done to prevent User query returns comparison to User, as equality fails
    #on datetime fields due to pymongo rounding. Instead here _id is compared.
    users_sent_id = [email.user._id for email in inactive_emails]
    inactive_ids = [user._id for user in inactive_users if user.is_active]
    users_to_send = [User.load(id) for id in (set(inactive_ids) - set(users_sent_id))]
    return users_to_send


@celery_app.task(name='scripts.triggered_mails')
def run_main(dry_run=True):
    init_app(routes=False)
    if not dry_run:
        add_file_logger(logger, __file__)
    main(dry_run=dry_run)


# -*- coding: utf-8 -*-

import datetime
from nose.tools import *  # noqa

from scripts import parse_citation_styles
from framework.auth.core import Auth
from website.util import api_url_for
from website.citations.utils import datetime_to_csl
from website.models import Node, User
from flask import redirect

from tests.base import OsfTestCase
from tests.factories import ProjectFactory, UserFactory, AuthUserFactory


class CitationsUtilsTestCase(OsfTestCase):
    def test_datetime_to_csl(self):
        # Convert a datetime instance to csl's date-variable schema
        now = datetime.datetime.utcnow()

        assert_equal(
            datetime_to_csl(now),
            {'date-parts': [[now.year, now.month, now.day]]},
        )


class CitationsNodeTestCase(OsfTestCase):
    def setUp(self):
        super(CitationsNodeTestCase, self).setUp()
        self.node = ProjectFactory()

    def tearDown(self):
        super(CitationsNodeTestCase, self).tearDown()
        Node.remove()
        User.remove()

    def test_csl_single_author(self):
        # Nodes with one contributor generate valid CSL-data
        assert_equal(
            self.node.csl,
            {
                'publisher': 'Open Science Framework',
                'author': [{
                    'given': self.node.creator.given_name,
                    'family': self.node.creator.family_name,
                }],
                'URL': self.node.display_absolute_url,
                'issued': datetime_to_csl(self.node.logs[-1].date),
                'title': self.node.title,
                'type': 'webpage',
                'id': self.node._id,
            },
        )

    def test_csl_multiple_authors(self):
        # Nodes with multiple contributors generate valid CSL-data
        user = UserFactory()
        self.node.add_contributor(user)
        self.node.save()

        assert_equal(
            self.node.csl,
            {
                'publisher': 'Open Science Framework',
                'author': [
                    {
                        'given': self.node.creator.given_name,
                        'family': self.node.creator.family_name,
                    },
                    {
                        'given': user.given_name,
                        'family': user.family_name,
                    }
                ],
                'URL': self.node.display_absolute_url,
                'issued': datetime_to_csl(self.node.logs[-1].date),
                'title': self.node.title,
                'type': 'webpage',
                'id': self.node._id,
            },
        )

    def test_non_visible_contributors_arent_included_in_csl(self):
        node = ProjectFactory()
        visible = UserFactory()
        node.add_contributor(visible, auth=Auth(node.creator))
        invisible = UserFactory()
        node.add_contributor(invisible, auth=Auth(node.creator), visible=False)
        node.save()
        assert_equal(len(node.csl['author']), 2)
        expected_authors = [
            contrib.csl_name for contrib in [node.creator, visible]
        ]

        assert_equal(node.csl['author'], expected_authors)

class CitationsUserTestCase(OsfTestCase):
    def setUp(self):
        super(CitationsUserTestCase, self).setUp()
        self.user = UserFactory()

    def tearDown(self):
        super(CitationsUserTestCase, self).tearDown()
        User.remove()

    def test_user_csl(self):
        # Convert a User instance to csl's name-variable schema
        assert_equal(
            self.user.csl_name,
            {
                'given': self.user.given_name,
                'family': self.user.family_name,
            },
        )


class CitationsViewsTestCase(OsfTestCase):
    @classmethod
    def setUpClass(cls):
        super(CitationsViewsTestCase, cls).setUpClass()
        # populate the DB with parsed citation styles
        try:
            parse_citation_styles.main()
        except OSError:
            pass

    def test_list_styles(self):
        # Response includes a list of available citation styles
        response = self.app.get(api_url_for('list_citation_styles'))

        assert_true(response.json)

        assert_equal(
            len(
                [
                    style for style in response.json['styles']
                    if style.get('id') == 'bibtex'
                ]
            ),
            1,
        )

    def test_list_styles_filter(self):
        # Response includes a list of available citation styles
        response = self.app.get(api_url_for('list_citation_styles', q='bibtex'))

        assert_true(response.json)

        assert_equal(
            len(response.json['styles']), 1
        )

        assert_equal(
            response.json['styles'][0]['id'], 'bibtex'
        )

    def test_node_citation_view(self):
        node = ProjectFactory()
        user = AuthUserFactory()
        node.add_contributor(user)
        node.save()
        response = self.app.get("/api/v1" + "/project/" + node._id + "/citation/", auto_follow=True, auth=user.auth)
        assert_true(response.json)


# -*- coding: utf-8 -*-
import httplib as http
import mock
import unittest  # noqa
from nose.tools import *  # noqa (PEP8 asserts)

import datetime
from modularodm import fields, storage, Q

from tests.base import OsfTestCase
from tests import factories
from tests.utils import mock_archive, assert_logs

from framework.auth import Auth
from framework.mongo import handlers

from website.exceptions import NodeStateError
from website.project.model import ensure_schemas, Node, NodeLog
from website.project.sanctions import Sanction, TokenApprovableSanction, EmailApprovableSanction, PreregCallbackMixin

def valid_user():
    return factories.UserFactory(system_tags=['flag'])

class SanctionTestClass(TokenApprovableSanction):

    DISPLAY_NAME = 'test class'

    initiated_by = fields.ForeignField('user', backref='tested')

    def _validate_authorizer(self, user):
        return 'flag' in user.system_tags

    def _get_registration(self):
        return factories.RegistrationFactory()

class EmailApprovableSanctionTestClass(PreregCallbackMixin, EmailApprovableSanction):

    AUTHORIZER_NOTIFY_EMAIL_TEMPLATE = 'authorizer'
    NON_AUTHORIZER_NOTIFY_EMAIL_TEMPLATE = 'non-authorizer'

    def _get_registration(self):
        return factories.RegistrationFactory()


class SanctionsTestCase(OsfTestCase):

    def setUp(self, *args, **kwargs):
        super(SanctionsTestCase, self).setUp(*args, **kwargs)
        handlers.set_up_storage([
            SanctionTestClass,
            EmailApprovableSanctionTestClass
        ], storage.MongoStorage)

class TestSanction(SanctionsTestCase):

    def setUp(self, *args, **kwargs):
        super(TestSanction, self).setUp(*args, **kwargs)
        self.user = valid_user()
        self.invalid_user = factories.UserFactory()
        self.sanction = SanctionTestClass(
            initiated_by=self.user,
            end_date=datetime.datetime.now() + datetime.timedelta(days=2)
        )
        self.registration = factories.RegistrationFactory()
        self.sanction.add_authorizer(self.user, self.registration, save=True)

    def test_pending_approval(self):
        assert_true(self.sanction.is_pending_approval)
        self.sanction.state = Sanction.APPROVED
        assert_false(self.sanction.is_pending_approval)

    def test_validate_authorizer(self):
        assert_false(self.sanction._validate_authorizer(self.invalid_user))
        assert_true(self.sanction._validate_authorizer(self.user))

    def test_add_authorizer(self):
        new_user = valid_user()
        added = self.sanction.add_authorizer(new_user, node=self.registration)
        assert_true(added)
        assert_in(new_user._id, self.sanction.approval_state.keys())
        assert_in('approval_token', self.sanction.approval_state[new_user._id])
        assert_in('rejection_token', self.sanction.approval_state[new_user._id])
        assert_equal(self.sanction.approval_state[new_user._id]['node_id'], self.registration._id)

    def test_add_authorizer_already_added(self):
        added = self.sanction.add_authorizer(self.user, self.registration)
        assert_false(added)
        assert_in(self.user._id, self.sanction.approval_state.keys())

    def test_add_authorizer_invalid(self):
        invalid_user = factories.UserFactory()
        added = self.sanction.add_authorizer(invalid_user, self.registration)
        assert_false(added)
        assert_not_in(invalid_user._id, self.sanction.approval_state.keys())

    def test_remove_authorizer(self):
        removed = self.sanction.remove_authorizer(self.user)
        self.sanction.save()
        assert_true(removed)
        assert_not_in(self.user._id, self.sanction.approval_state.keys())

    def test_remove_authorizer_not_added(self):
        not_added = factories.UserFactory()
        removed = self.sanction.remove_authorizer(not_added)
        self.sanction.save()
        assert_false(removed)
        assert_not_in(not_added, self.sanction.approval_state.keys())

    @mock.patch.object(SanctionTestClass, '_on_complete')
    def test_on_approve_incomplete(self, mock_complete):
        another_user = valid_user()
        self.sanction.add_authorizer(another_user, self.sanction._get_registration(), approved=True)
        self.sanction._on_approve(self.user, '')
        assert_false(mock_complete.called)

    @mock.patch.object(SanctionTestClass, '_on_complete')
    def test_on_approve_complete(self, mock_complete):
        self.sanction.approval_state[self.user._id]['has_approved'] = True
        self.sanction._on_approve(self.user, '')
        assert_true(mock_complete.called)

    def test_on_reject_raises_NotImplementedError(self):
        err = lambda: self.sanction._on_reject(self.user)
        assert_raises(NotImplementedError, err)

    def test_on_complete_raises_NotImplementedError(self):
        err = lambda: self.sanction._on_complete(self.user)
        assert_raises(NotImplementedError, err)

    @mock.patch.object(SanctionTestClass, '_on_approve')
    def test_approve(self, mock_on_approve):
        approval_token = self.sanction.approval_state[self.user._id]['approval_token']
        self.sanction.approve(self.user, approval_token)
        assert_true(self.sanction.approval_state[self.user._id]['has_approved'])
        assert_true(mock_on_approve.called)

    @mock.patch.object(SanctionTestClass, '_on_reject')
    def test_reject(self, mock_on_reject):
        rejection_token = self.sanction.approval_state[self.user._id]['rejection_token']
        self.sanction.reject(self.user, rejection_token)
        assert_false(self.sanction.approval_state[self.user._id]['has_approved'])
        assert_true(mock_on_reject.called)

    @mock.patch.object(SanctionTestClass, '_notify_authorizer')
    @mock.patch.object(SanctionTestClass, '_notify_non_authorizer')
    def test_ask(self, mock_notify_non_authorizer, mock_notify_authorizer):
        other_user = factories.UserFactory()
        p1 = factories.ProjectFactory()
        p2 = factories.ProjectFactory()
        group = [
            (other_user, p1),
            (self.user, p2),
        ]
        self.sanction.ask(group)
        mock_notify_non_authorizer.assert_called_once_with(other_user, p1)
        mock_notify_authorizer.assert_called_once_with(self.user, p2)


class TestEmailApprovableSanction(SanctionsTestCase):

    def setUp(self, *args, **kwargs):
        super(TestEmailApprovableSanction, self).setUp(*args, **kwargs)
        self.user = factories.UserFactory()
        self.sanction = EmailApprovableSanctionTestClass(
            initiated_by=self.user,
            end_date=datetime.datetime.now() + datetime.timedelta(days=2)
        )
        self.sanction.add_authorizer(self.user, self.sanction._get_registration())

    def test_format_or_empty(self):
        context = {
            'key': 'value'
        }
        template = 'What a good {key}'
        assert_equal(EmailApprovableSanctionTestClass._format_or_empty(template, context), 'What a good value')

    def test_format_or_empty_empty(self):
        context = None
        template = 'What a good {key}'
        assert_equal(EmailApprovableSanctionTestClass._format_or_empty(template, context), '')

    @mock.patch.object(EmailApprovableSanctionTestClass, '_send_approval_request_email')
    @mock.patch.object(EmailApprovableSanctionTestClass, '_email_template_context')
    def test_notify_authorizer(self, mock_get_email_template_context, mock_send_approval_email):
        mock_get_email_template_context.return_value = 'context'
        reg  = self.sanction._get_registration()
        self.sanction._notify_authorizer(self.user, reg)
        mock_get_email_template_context.assert_called_once_with(
            self.user,
            reg, 
            is_authorizer=True
        )
        mock_send_approval_email.assert_called_once_with(self.user, 'authorizer', 'context')

    @mock.patch.object(EmailApprovableSanctionTestClass, '_send_approval_request_email')
    @mock.patch.object(EmailApprovableSanctionTestClass, '_email_template_context')
    def test_notify_non_authorizer(self, mock_get_email_template_context, mock_send_approval_email):
        mock_get_email_template_context.return_value = 'context'
        other_user = factories.UserFactory()
        reg = self.sanction._get_registration()
        self.sanction._notify_non_authorizer(other_user, reg)
        mock_get_email_template_context.assert_called_once_with(
            other_user,
            reg
        )
        mock_send_approval_email.assert_called_once_with(other_user, 'non-authorizer', 'context')

    def test_add_authorizer(self):
        assert_is_not_none(self.sanction.stashed_urls.get(self.user._id))

    @mock.patch('website.mails.send_mail')
    def test__notify_authorizer(self, mock_send):
        self.sanction._notify_authorizer(self.user, self.sanction._get_registration())
        assert_true(mock_send.called)
        args, kwargs = mock_send.call_args
        assert_true(self.user.username in args)

    @mock.patch('website.mails.send_mail')
    def test__notify_non_authorizer(self, mock_send):
        self.sanction._notify_non_authorizer(self.user, self.sanction._get_registration())
        assert_true(mock_send.called)
        args, kwargs = mock_send.call_args
        assert_true(self.user.username in args)

    @mock.patch('website.mails.send_mail')
    def test_ask(self, mock_send):
        group = [(self.user, factories.ProjectFactory())]
        for i in range(5):
            u, n = factories.UserFactory(), factories.ProjectFactory()
            group.append((u, n))
        self.sanction.ask(group)
        authorizer = group.pop(0)[0]
        mock_send.assert_any_call(
            authorizer.username,
            self.sanction.AUTHORIZER_NOTIFY_EMAIL_TEMPLATE,
            user=authorizer,
            **{}
        )
        for user, _ in group:
            mock_send.assert_any_call(
                user.username,
                self.sanction.NON_AUTHORIZER_NOTIFY_EMAIL_TEMPLATE,
                user=user,
                **{}
            )

    def test_on_complete_notify_initiator(self):
        sanction = EmailApprovableSanctionTestClass(
            initiated_by=self.user,
            end_date=datetime.datetime.now() + datetime.timedelta(days=2),
            notify_initiator_on_complete=True
        )
        sanction.add_authorizer(self.user, sanction._get_registration())
        sanction.save()
        with mock.patch.object(EmailApprovableSanctionTestClass, '_notify_initiator') as mock_notify:
            sanction._on_complete(self.user)
        assert_equal(mock_notify.call_count, 1)

    def test_notify_initiator_with_PreregCallbackMixin(self):
        sanction = EmailApprovableSanctionTestClass(
            initiated_by=self.user,
            end_date=datetime.datetime.now() + datetime.timedelta(days=2),
            notify_initiator_on_complete=True
        )
        sanction.add_authorizer(self.user, sanction._get_registration())
        sanction.save()
        with mock.patch.object(PreregCallbackMixin, '_notify_initiator') as mock_notify:
            sanction._on_complete(self.user)
        assert_equal(mock_notify.call_count, 1)


class TestRegistrationApproval(OsfTestCase):

    def setUp(self):
        super(TestRegistrationApproval, self).setUp()
        ensure_schemas()
        self.user = factories.AuthUserFactory()
        self.registration = factories.RegistrationFactory(creator=self.user, archive=True)

    @mock.patch('framework.celery_tasks.handlers.enqueue_task')
    def test_non_contributor_GET_approval_returns_HTTPError(self, mock_enqueue):
        non_contributor = factories.AuthUserFactory()

        approval_token = self.registration.registration_approval.approval_state[self.user._id]['approval_token']
        approval_url = self.registration.web_url_for('view_project', token=approval_token)

        res = self.app.get(approval_url, auth=non_contributor.auth, expect_errors=True)
        assert_equal(http.FORBIDDEN, res.status_code)
        assert_true(self.registration.is_pending_registration)
        assert_false(self.registration.is_registration_approved)

    @mock.patch('framework.celery_tasks.handlers.enqueue_task')
    def test_non_contributor_GET_disapproval_returns_HTTPError(self, mock_enqueue):
        non_contributor = factories.AuthUserFactory()

        rejection_token = self.registration.registration_approval.approval_state[self.user._id]['rejection_token']
        rejection_url = self.registration.web_url_for('view_project', token=rejection_token)

        res = self.app.get(rejection_url, auth=non_contributor.auth, expect_errors=True)
        assert_equal(http.FORBIDDEN, res.status_code)
        assert_true(self.registration.is_pending_registration)
        assert_false(self.registration.is_registration_approved)


class TestRegistrationApprovalHooks(OsfTestCase):

    # Regression test for https://openscience.atlassian.net/browse/OSF-4940
    def test_on_complete_sets_state_to_approved(self):
        user = factories.UserFactory()
        registration = factories.RegistrationFactory(creator=user)
        registration.require_approval(user)

        assert_true(registration.registration_approval.is_pending_approval)  # sanity check
        registration.registration_approval._on_complete(None)
        assert_false(registration.registration_approval.is_pending_approval)

class TestNodeSanctionStates(OsfTestCase):

    def test_sanction_none(self):
        node = factories.NodeFactory()
        assert_false(node.sanction)

    def test_sanction_embargo_termination_first(self):
        embargo_termination_approval = factories.EmbargoTerminationApprovalFactory()
        registration = Node.find_one(Q('embargo_termination_approval', 'eq', embargo_termination_approval))
        assert_equal(registration.sanction, embargo_termination_approval)

    def test_sanction_retraction(self):
        retraction = factories.RetractionFactory()
        registration = Node.find_one(Q('retraction', 'eq', retraction))
        assert_equal(registration.sanction, retraction)

    def test_sanction_embargo(self):
        embargo = factories.EmbargoFactory()
        registration = Node.find_one(Q('embargo', 'eq', embargo))
        assert_equal(registration.sanction, embargo)

    def test_sanction_registration_approval(self):
        registration_approval = factories.RegistrationApprovalFactory()
        registration = Node.find_one(Q('registration_approval', 'eq', registration_approval))
        assert_equal(registration.sanction, registration_approval)

    def test_sanction_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node) as registration:
            approval = registration.registration_approval
            sub_reg = registration.nodes[0].nodes[0]
            assert_equal(sub_reg.sanction, approval)

    def test_is_pending_registration(self):
        registration_approval = factories.RegistrationApprovalFactory()
        registration = Node.find_one(Q('registration_approval', 'eq', registration_approval))
        assert_true(registration_approval.is_pending_approval)
        assert_true(registration.is_pending_registration)

    def test_is_pending_registration_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_true(sub_reg.is_pending_registration)

    def test_is_registration_approved(self):
        registration_approval = factories.RegistrationApprovalFactory()
        registration = Node.find_one(Q('registration_approval', 'eq', registration_approval))
        with mock.patch('website.project.sanctions.Sanction.is_approved', mock.Mock(return_value=True)):
            assert_true(registration.is_registration_approved)

    def test_is_registration_approved_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node) as registration:
            with mock.patch('website.project.sanctions.Sanction.is_approved', mock.Mock(return_value=True)):
                sub_reg = registration.nodes[0].nodes[0]
                assert_true(sub_reg.is_registration_approved)

    def test_is_retracted(self):
        retraction = factories.RetractionFactory()
        registration = Node.find_one(Q('retraction', 'eq', retraction))
        with mock.patch('website.project.sanctions.Sanction.is_approved', mock.Mock(return_value=True)):
            assert_true(registration.is_retracted)

    def test_is_retracted_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node, autoapprove=True, retraction=True, autoapprove_retraction=True) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_true(sub_reg.is_retracted)

    def test_is_pending_retraction(self):
        retraction = factories.RetractionFactory()
        registration = Node.find_one(Q('retraction', 'eq', retraction))
        assert_true(retraction.is_pending_approval)
        assert_true(registration.is_pending_retraction)

    def test_is_pending_retraction_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node, autoapprove=True, retraction=True) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_true(sub_reg.is_pending_retraction)

    def test_embargo_end_date(self):
        embargo = factories.EmbargoFactory()
        registration = Node.find_one(Q('embargo', 'eq', embargo))
        assert_equal(registration.embargo_end_date, embargo.end_date)

    def test_embargo_end_date_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node, embargo=True) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_equal(sub_reg.embargo_end_date, registration.embargo_end_date)

    def test_is_pending_embargo(self):
        embargo = factories.EmbargoFactory()
        registration = Node.find_one(Q('embargo', 'eq', embargo))
        assert_true(embargo.is_pending_approval)
        assert_true(registration.is_pending_embargo)

    def test_is_pending_embargo_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node, embargo=True) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_true(sub_reg.is_pending_embargo)

    def test_is_embargoed(self):
        embargo = factories.EmbargoFactory()
        registration = Node.find_one(Q('embargo', 'eq', embargo))
        with mock.patch('website.project.sanctions.Sanction.is_approved', mock.Mock(return_value=True)):
            assert_true(registration.is_embargoed)

    def test_is_embargoed_searches_parents(self):
        user = factories.AuthUserFactory()
        node = factories.NodeFactory(creator=user)
        child = factories.NodeFactory(creator=user, parent=node)
        factories.NodeFactory(creator=user, parent=child)
        with mock_archive(node, embargo=True, autoapprove=True) as registration:
            sub_reg = registration.nodes[0].nodes[0]
            assert_true(sub_reg.is_embargoed)

class TestNodeEmbargoTerminations(OsfTestCase):

    def tearDown(self):
        with mock.patch('framework.celery_tasks.handlers.queue', mock.Mock(return_value=None)):
            super(TestNodeEmbargoTerminations, self).tearDown()

    def setUp(self):
        super(TestNodeEmbargoTerminations, self).setUp()

        self.user = factories.AuthUserFactory()
        self.node = factories.ProjectFactory(creator=self.user)
        with mock_archive(self.node, embargo=True, autoapprove=True) as registration:
            self.registration = registration

        self.not_embargoed = factories.RegistrationFactory()

    def test_request_embargo_termination_not_embargoed(self):
        with assert_raises(NodeStateError):
            self.not_embargoed.request_embargo_termination(Auth(self.user))

    def test_terminate_embargo_makes_registrations_public(self):
        self.registration.terminate_embargo(Auth(self.user))
        for node in self.registration.node_and_primary_descendants():
            assert_true(node.is_public)
            assert_false(node.is_embargoed)

    @assert_logs(NodeLog.EMBARGO_TERMINATED, 'node')
    def test_terminate_embargo_adds_log_to_registered_from(self):
        self.registration.terminate_embargo(Auth(self.user))

    def test_terminate_embargo_log_is_nouser(self):
        self.registration.terminate_embargo(Auth(self.user))
        last_log = self.node.logs[-1]
        assert_equal(last_log.action, NodeLog.EMBARGO_TERMINATED)
        assert_equal(last_log.user, None)

import mock
import random
import string
from nose.tools import *
import website.app
from webtest_plus import TestApp

from website.util import api_url_for, web_url_for
from website.addons.base.testing import AddonTestCase
from website.addons.badges.util import get_node_badges

from tests.factories import AuthUserFactory
from utils import create_mock_badger, create_badge_dict, get_garbage


class TestBadgesViews(AddonTestCase):

    ADDON_SHORT_NAME = 'badges'

    def setUp(self):

        super(TestBadgesViews, self).setUp()

    def set_node_settings(self, settings):
        return settings

    def set_user_settings(self, settings):
        return create_mock_badger(settings)

    def create_app(self):
        return TestApp(app)

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_create_badge(self, img_proc):
        img_proc.return_value = 'temp.png'
        badge = create_badge_dict()
        ret = self.app.post_json(api_url_for('create_badge'), badge, auth=self.user.auth)
        self.user_settings.reload()
        assert_equals(ret.status_int, 201)
        assert_equals(ret.content_type, 'application/json')
        assert_true(ret.json['badgeid'] in [badge._id for badge in self.user_settings.badges])

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_create_badge_no_data(self, img_proc):
        url = api_url_for('create_badge')
        badge = {}
        ret = self.app.post_json(url, badge, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_create_badge_some_data(self, img_proc):
        img_proc.return_value = 'temp.png'
        url = api_url_for('create_badge')
        badge = {
            'badgeName': ''.join(random.choice(string.ascii_lowercase + string.digits) for _ in range(4)),
            'description': 'Just doesn\'t '.join(random.choice(string.ascii_letters + string.digits) for _ in range(6))
        }
        ret = self.app.post_json(url, badge, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_create_badge_empty_data(self, img_proc):
        img_proc.return_value = 'temp.png'
        url = api_url_for('create_badge')
        badge = create_badge_dict()
        badge['imageurl'] = ''
        ret = self.app.post_json(url, badge, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_create_badge_cant_issue(self, img_proc):
        img_proc.return_value = 'temp.png'
        self.user.delete_addon('badges')
        url = api_url_for('create_badge')
        badge = create_badge_dict()
        ret = self.app.post_json(url, badge, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    def test_award_badge(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

    def test_award_badge_bad_badge_id(self):
        badgeid = 'badid67'
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    def test_award_badge_empty_badge_id(self):
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': ''}, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    def test_award_badge_no_badge_id(self):
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {}, auth=self.user.auth, expect_errors=True)
        assert_equals(ret.status_int, 400)

    @mock.patch('website.addons.badges.model.badges.acquire_badge_image')
    def test_badge_html(self, img_proc):
        img_proc.return_value = 'temp.png'
        badge = {
            'badgeName': get_garbage(),
            'description': get_garbage(),
            'imageurl': get_garbage(),
            'criteria': get_garbage()
        }
        ret = self.app.post_json(api_url_for('create_badge'), badge, auth=self.user.auth)
        self.user_settings.reload()
        assert_equals(ret.status_int, 201)
        assert_equals(ret.content_type, 'application/json')
        assert_true(ret.json['badgeid'] in [badge._id for badge in self.user_settings.badges])
        with self.app.app.test_request_context():
            bstr = str(self.user_settings.badges[0].to_openbadge())
        assert_false('>' in bstr)
        assert_false('<' in bstr)

    def test_revoke_badge(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]

        revoke = api_url_for('revoke_badge', pid=self.project._id)
        ret = self.app.post_json(revoke,
            {
                'id': assertion._id,
                'reason': ''
            }, auth=self.user.auth)
        self.project.reload()
        self.user_settings.reload()
        assertion.reload()

        assert_equals(ret.status_int, 200)
        assert_true(get_node_badges(self.project)[0]._id, assertion._id)
        assert_true(assertion.revoked)
        assert_true(assertion._id in self.user_settings.revocation_list)
        assert_equals(len(self.user_settings.revocation_list), 1)

    def test_revoke_badge_reason(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]

        revoke = api_url_for('revoke_badge', pid=self.project._id)
        ret = self.app.post_json(revoke,
            {
                'id': assertion._id,
                'reason': 'Is a loser'
            }, auth=self.user.auth)
        self.project.reload()
        self.user_settings.reload()
        assertion.reload()

        assert_equals(ret.status_int, 200)
        assert_true(get_node_badges(self.project)[0]._id, assertion._id)
        assert_true(assertion._id in self.user_settings.revocation_list)
        assert_equals(len(self.user_settings.revocation_list), 1)
        assert_true(assertion.revoked)
        assert_equals(self.user_settings.revocation_list[assertion._id], 'Is a loser')

    def test_revoke_badge_no_addon(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]

        revoke = api_url_for('revoke_badge', pid=self.project._id)
        self.user.delete_addon('badges')
        self.user.save()
        self.user.reload()

        ret = self.app.post_json(revoke,
            {
                'id': assertion._id,
                'reason': ''
            }, auth=self.user.auth, expect_errors=True)
        self.project.reload()
        self.user_settings.reload()
        assertion.reload()

        assert_equals(ret.status_int, 400)
        assert_false(assertion.revoked)
        assert_true(get_node_badges(self.project)[0]._id, assertion._id)
        assert_false(assertion._id in self.user_settings.revocation_list)

    def test_revoke_didnt_award(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]

        revoke = api_url_for('revoke_badge', pid=self.project._id)

        user2 = AuthUserFactory()
        user2.add_addon('badges', override=True)
        user2.save()
        user2.reload()

        ret = self.app.post_json(revoke,
            {
                'id': assertion._id,
                'reason': ''
            }, auth=user2.auth, expect_errors=True)
        self.project.reload()
        self.user_settings.reload()
        assertion.reload()

        assert_equals(ret.status_int, 400)
        assert_false(assertion.revoked)
        assert_true(get_node_badges(self.project)[0]._id, assertion._id)
        assert_false(assertion._id in self.user_settings.revocation_list)

    def test_issuer_html(self):
        pass

    def test_revoke_bad_aid(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]

        revoke = api_url_for('revoke_badge', pid=self.project._id)

        ret = self.app.post_json(revoke,
            {
                'id': 'Im a bad id :D',
                'reason': ''
            }, auth=self.user.auth, expect_errors=True)
        self.project.reload()
        self.user_settings.reload()
        assertion.reload()

        assert_equals(ret.status_int, 400)
        assert_false(assertion.revoked)
        assert_true(get_node_badges(self.project)[0]._id, assertion._id)
        assert_false(assertion._id in self.user_settings.revocation_list)

    def test_system_badge_awarder(self):
        badgeid = self.user_settings.badges[0]._id
        self.user_settings.badges[0].make_system_badge()
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]
        assert_equals(assertion.awarder._id, self.user_settings._id)

    def test_badge_awarder(self):
        badgeid = self.user_settings.badges[0]._id
        initnum = get_node_badges(self.project).count()
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badgeid}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        assert_equals(initnum + 1, get_node_badges(self.project).count())

        assertion = get_node_badges(self.project)[0]
        assert_equals(assertion.awarder._id, self.user_settings._id)

    def test_award_times(self):
        badge = self.user_settings.badges[0]
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        badge.reload()
        assert_equals(badge.awarded_count, 3)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        badge.reload()
        assert_equals(badge.awarded_count, 5)

    def test_unique_awards(self):
        badge = self.user_settings.badges[0]
        assert_true(self.user_settings.can_award)
        url = api_url_for('award_badge', pid=self.project._id)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        self.project.reload()
        assert_equals(ret.status_int, 200)
        badge.reload()
        assert_equals(badge.unique_awards_count, 1)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        ret = self.app.post_json(url, {'badgeid': badge._id}, auth=self.user.auth)
        badge.reload()
        assert_equals(badge.unique_awards_count, 1)

# OAuth app keys
BOX_KEY = None
BOX_SECRET = None

REFRESH_TIME = 5 * 60  # 5 minutes

BOX_OAUTH_TOKEN_ENDPOINT = 'https://www.box.com/api/oauth2/token'
BOX_OAUTH_AUTH_ENDPOINT = 'https://www.box.com/api/oauth2/authorize'
BOX_OAUTH_REVOKE_ENDPOINT = 'https://api.box.com/oauth2/revoke'

# -*- coding: utf-8 -*-
import httplib as http
import logging
import os

from dropbox.client import DropboxOAuth2Flow, DropboxClient
from dropbox.rest import ErrorResponse
from flask import request
import markupsafe

from modularodm import fields

from framework.auth import Auth
from framework.exceptions import HTTPError
from framework.sessions import session

from website.util import web_url_for
from website.addons.base import exceptions
from website.addons.base import AddonOAuthUserSettingsBase, AddonOAuthNodeSettingsBase
from website.addons.base import StorageAddonBase
from website.oauth.models import ExternalProvider

from website.addons.dropbox import settings
from website.addons.dropbox.serializer import DropboxSerializer

logger = logging.getLogger(__name__)


class DropboxProvider(ExternalProvider):

    name = 'DropBox'
    short_name = 'dropbox'

    client_id = settings.DROPBOX_KEY
    client_secret = settings.DROPBOX_SECRET

    # Explicitly override auth_url_base as None -- DropboxOAuth2Flow handles this for us
    auth_url_base = None
    callback_url = None
    handle_callback = None

    @property
    def oauth_flow(self):
        if 'oauth_states' not in session.data:
            session.data['oauth_states'] = {}
        if self.short_name not in session.data['oauth_states']:
            session.data['oauth_states'][self.short_name] = {
                'state': None
            }
        return DropboxOAuth2Flow(
            self.client_id,
            self.client_secret,
            redirect_uri=web_url_for(
                'oauth_callback',
                service_name=self.short_name,
                _absolute=True
            ),
            session=session.data['oauth_states'][self.short_name], csrf_token_session_key='state'
        )

    @property
    def auth_url(self):
        return self.oauth_flow.start('force_reapprove=true')

    # Overrides ExternalProvider
    def auth_callback(self, user):
        # TODO: consider not using client library during auth flow
        try:
            access_token, dropbox_user_id, url_state = self.oauth_flow.finish(request.values)
        except (DropboxOAuth2Flow.NotApprovedException, DropboxOAuth2Flow.BadStateException):
            # 1) user cancelled and client library raised exc., or
            # 2) the state was manipulated, possibly due to time.
            # Either way, return and display info about how to properly connect.
            return
        except (DropboxOAuth2Flow.ProviderException, DropboxOAuth2Flow.CsrfException):
            raise HTTPError(http.FORBIDDEN)
        except DropboxOAuth2Flow.BadRequestException:
            raise HTTPError(http.BAD_REQUEST)

        self.client = DropboxClient(access_token)

        info = self.client.account_info()
        return self._set_external_account(
            user,
            {
                'key': access_token,
                'provider_id': info['uid'],
                'display_name': info['display_name'],
            }
        )


class DropboxUserSettings(AddonOAuthUserSettingsBase):
    """Stores user-specific dropbox information.
    token.
    """

    oauth_provider = DropboxProvider
    serializer = DropboxSerializer

    def revoke_remote_oauth_access(self, external_account):
        """Overrides default behavior during external_account deactivation.

        Tells DropBox to remove the grant for the OSF associated with this account.
        """
        client = DropboxClient(external_account.oauth_key)
        try:
            client.disable_access_token()
        except ErrorResponse:
            pass

class DropboxNodeSettings(StorageAddonBase, AddonOAuthNodeSettingsBase):

    oauth_provider = DropboxProvider
    serializer = DropboxSerializer

    folder = fields.StringField(default=None)

    #: Information saved at the time of registration
    #: Note: This is unused right now
    registration_data = fields.DictionaryField()

    _folder_data = None

    _api = None

    @property
    def api(self):
        """authenticated ExternalProvider instance"""
        if self._api is None:
            self._api = DropboxProvider(self.external_account)
        return self._api

    @property
    def folder_id(self):
        return self.folder

    @property
    def folder_name(self):
        return os.path.split(self.folder or '')[1]

    @property
    def folder_path(self):
        return self.folder

    @property
    def display_name(self):
        return '{0}: {1}'.format(self.config.full_name, self.folder)

    def clear_settings(self):
        self.folder = None

    def fetch_folder_name(self):
        return self.folder

    def set_folder(self, folder, auth):
        self.folder = folder
        # Add log to node
        self.nodelogger.log(action="folder_selected", save=True)

    # TODO: Is this used? If not, remove this and perhaps remove the 'deleted' field
    def delete(self, save=True):
        self.deauthorize(add_log=False)
        super(DropboxNodeSettings, self).delete(save)

    def deauthorize(self, auth=None, add_log=True):
        """Remove user authorization from this node and log the event."""
        folder = self.folder
        self.clear_settings()

        if add_log:
            extra = {'folder': folder}
            self.nodelogger.log(action="node_deauthorized", extra=extra, save=True)

        self.clear_auth()

    def serialize_waterbutler_credentials(self):
        if not self.has_auth:
            raise exceptions.AddonError('Addon is not authorized')
        return {'token': self.external_account.oauth_key}

    def serialize_waterbutler_settings(self):
        if not self.folder:
            raise exceptions.AddonError('Folder is not configured')
        return {'folder': self.folder}

    def create_waterbutler_log(self, auth, action, metadata):
        url = self.owner.web_url_for('addon_view_or_download_file', path=metadata['path'].strip('/'), provider='dropbox')
        self.owner.add_log(
            'dropbox_{0}'.format(action),
            auth=auth,
            params={
                'project': self.owner.parent_id,
                'node': self.owner._id,
                'path': metadata['path'],
                'folder': self.folder,
                'urls': {
                    'view': url,
                    'download': url + '?action=download'
                },
            },
        )

    def __repr__(self):
        return u'<DropboxNodeSettings(node_id={self.owner._primary_key!r})>'.format(self=self)

    ##### Callback overrides #####

    def before_register_message(self, node, user):
        """Return warning text to display if user auth will be copied to a
        registration.
        """
        category = node.project_or_component
        if self.user_settings and self.user_settings.has_auth:
            return (
                u'The contents of Dropbox add-ons cannot be registered at this time; '
                u'the Dropbox folder linked to this {category} will not be included '
                u'as part of this registration.'
            ).format(category=markupsafe.escape(category))

    # backwards compatibility
    before_register = before_register_message

    def before_remove_contributor_message(self, node, removed):
        """Return warning text to display if removed contributor is the user
        who authorized the Dropbox addon
        """
        if self.user_settings and self.user_settings.owner == removed:
            category = node.project_or_component
            name = removed.fullname
            return (u'The Dropbox add-on for this {category} is authenticated by {name}. '
                    u'Removing this user will also remove write access to Dropbox '
                    u'unless another contributor re-authenticates the add-on.'
                    ).format(category=markupsafe.escape(category),
                             name=markupsafe.escape(name))

    # backwards compatibility
    before_remove_contributor = before_remove_contributor_message

    # Note: Registering Dropbox content is disabled for now; leaving this code
    # here in case we enable registrations later on.
    # @jmcarp
    # def after_register(self, node, registration, user, save=True):
    #     """After registering a node, copy the user settings and save the
    #     chosen folder.
    #
    #     :return: A tuple of the form (cloned_settings, message)
    #     """
    #     clone, message = super(DropboxNodeSettings, self).after_register(
    #         node, registration, user, save=False
    #     )
    #     # Copy user_settings and add registration data
    #     if self.has_auth and self.folder is not None:
    #         clone.user_settings = self.user_settings
    #         clone.registration_data['folder'] = self.folder
    #     if save:
    #         clone.save()
    #     return clone, message

    def after_fork(self, node, fork, user, save=True):
        """After forking, copy user settings if the user is the one who authorized
        the addon.

        :return: A tuple of the form (cloned_settings, message)
        """
        clone, _ = super(DropboxNodeSettings, self).after_fork(
            node=node, fork=fork, user=user, save=False
        )

        if self.user_settings and self.user_settings.owner == user:
            clone.user_settings = self.user_settings
            message = (
                'Dropbox authorization copied to forked {cat}.'
            ).format(
                cat=markupsafe.escape(fork.project_or_component)
            )
        else:
            message = (
                u'Dropbox authorization not copied to forked {cat}. You may '
                u'authorize this fork on the <u><a href="{url}">Settings</a></u> '
                u'page.'
            ).format(
                url=fork.web_url_for('node_setting'),
                cat=markupsafe.escape(fork.project_or_component)
            )
        if save:
            clone.save()
        return clone, message

    def after_remove_contributor(self, node, removed, auth=None):
        """If the removed contributor was the user who authorized the Dropbox
        addon, remove the auth credentials from this node.
        Return the message text that will be displayed to the user.
        """
        if self.user_settings and self.user_settings.owner == removed:
            self.user_settings = None
            self.save()

            message = (
                u'Because the Dropbox add-on for {category} "{title}" was authenticated '
                u'by {user}, authentication information has been deleted.'
            ).format(
                category=markupsafe.escape(node.category_display),
                title=markupsafe.escape(node.title),
                user=markupsafe.escape(removed.fullname)
            )

            if not auth or auth.user != removed:
                url = node.web_url_for('node_setting')
                message += (
                    u' You can re-authenticate on the <u><a href="{url}">Settings</a></u> page.'
                ).format(url=url)
            #
            return message

    def after_delete(self, node, user):
        self.deauthorize(Auth(user=user), add_log=True)
        self.save()

# -*- coding: utf-8 -*-

from website.util import rubeus

from ..api import Figshare

def figshare_hgrid_data(node_settings, auth, parent=None, **kwargs):
    node = node_settings.owner
    if node_settings.figshare_type == 'project':
        item = Figshare.from_settings(node_settings.user_settings).project(node_settings, node_settings.figshare_id)
    else:
        item = Figshare.from_settings(node_settings.user_settings).article(node_settings, node_settings.figshare_id)
    if not node_settings.figshare_id or not node_settings.has_auth or not item:
        return
    #TODO Test me
    #Throw error if neither
    node_settings.figshare_title = item.get('title') or item['items'][0]['title']
    node_settings.save()
    return [
        rubeus.build_addon_root(
            node_settings, u'{0}:{1}'.format(node_settings.figshare_title or "Unnamed {0}".format(node_settings.figshare_type or ''), node_settings.figshare_id), permissions=auth,
            nodeUrl=node.url, nodeApiUrl=node.api_url,
            extra={
                'status': (item.get('articles') or item['items'])[0]['status'].lower()
            }
        )
    ]

"""Views for the node settings page."""
# -*- coding: utf-8 -*-
from dateutil.parser import parse as dateparse
import httplib as http
import logging

from flask import request, make_response

from framework.exceptions import HTTPError

from website.addons.base import generic_views
from website.addons.github.api import GitHubClient, ref_to_params
from website.addons.github.exceptions import NotFoundError, GitHubError
from website.addons.github.serializer import GitHubSerializer
from website.addons.github.utils import (
    get_refs, check_permissions,
    verify_hook_signature, MESSAGES
)

from website.models import NodeLog
from website.project.decorators import (
    must_have_addon, must_be_addon_authorizer,
    must_have_permission, must_not_be_registration,
    must_be_contributor_or_public, must_be_valid_project,
)
from website.util import rubeus

logger = logging.getLogger(__name__)

logging.getLogger('github3').setLevel(logging.WARNING)
logging.getLogger('requests.packages.urllib3.connectionpool').setLevel(logging.WARNING)

SHORT_NAME = 'github'
FULL_NAME = 'GitHub'

############
# Generics #
############

github_account_list = generic_views.account_list(
    SHORT_NAME,
    GitHubSerializer
)

github_import_auth = generic_views.import_auth(
    SHORT_NAME,
    GitHubSerializer
)

def _get_folders(node_addon, folder_id):
    pass

github_folder_list = generic_views.folder_list(
    SHORT_NAME,
    FULL_NAME,
    _get_folders
)

github_get_config = generic_views.get_config(
    SHORT_NAME,
    GitHubSerializer
)

github_deauthorize_node = generic_views.deauthorize_node(
    SHORT_NAME
)

github_root_folder = generic_views.root_folder(
    SHORT_NAME
)

#################
# Special Cased #
#################

@must_not_be_registration
@must_have_addon(SHORT_NAME, 'user')
@must_have_addon(SHORT_NAME, 'node')
@must_be_addon_authorizer(SHORT_NAME)
@must_have_permission('write')
def github_set_config(auth, **kwargs):
    node_settings = kwargs.get('node_addon', None)
    node = kwargs.get('node', None)
    user_settings = kwargs.get('user_addon', None)

    try:
        if not node:
            node = node_settings.owner
        if not user_settings:
            user_settings = node_settings.user_settings
    except AttributeError:
        raise HTTPError(http.BAD_REQUEST)

    # Parse request
    github_user_name = request.json.get('github_user', '')
    github_repo_name = request.json.get('github_repo', '')

    if not github_user_name or not github_repo_name:
        raise HTTPError(http.BAD_REQUEST)

    # Verify that repo exists and that user can access
    connection = GitHubClient(external_account=node_settings.external_account)
    repo = connection.repo(github_user_name, github_repo_name)
    if repo is None:
        if user_settings:
            message = (
                'Cannot access repo. Either the repo does not exist '
                'or your account does not have permission to view it.'
            )
        else:
            message = (
                'Cannot access repo.'
            )
        return {'message': message}, http.BAD_REQUEST

    changed = (
        github_user_name != node_settings.user or
        github_repo_name != node_settings.repo
    )

    # Update hooks
    if changed:

        # Delete existing hook, if any
        node_settings.delete_hook()

        # Update node settings
        node_settings.user = github_user_name
        node_settings.repo = github_repo_name

        # Log repo select
        node.add_log(
            action='github_repo_linked',
            params={
                'project': node.parent_id,
                'node': node._id,
                'github': {
                    'user': github_user_name,
                    'repo': github_repo_name,
                }
            },
            auth=auth,
        )

        # Add new hook
        if node_settings.user and node_settings.repo:
            node_settings.add_hook(save=False)

        node_settings.save()

    return {}

@must_be_contributor_or_public
@must_have_addon('github', 'node')
def github_download_starball(node_addon, **kwargs):

    archive = kwargs.get('archive', 'tar')
    ref = request.args.get('sha', 'master')

    connection = GitHubClient(external_account=node_addon.external_account)
    headers, data = connection.starball(
        node_addon.user, node_addon.repo, archive, ref
    )

    resp = make_response(data)
    for key, value in headers.iteritems():
        resp.headers[key] = value

    return resp

#########
# HGrid #
#########

@must_be_contributor_or_public
@must_have_addon('github', 'node')
def github_root_folder(*args, **kwargs):
    """View function returning the root container for a GitHub repo. In
    contrast to other add-ons, this is exposed via the API for GitHub to
    accommodate switching between branches and commits.

    """
    node_settings = kwargs['node_addon']
    auth = kwargs['auth']
    data = request.args.to_dict()

    return github_hgrid_data(node_settings, auth=auth, **data)

def github_hgrid_data(node_settings, auth, **kwargs):

    # Quit if no repo linked
    if not node_settings.complete:
        return

    connection = GitHubClient(external_account=node_settings.external_account)

    # Initialize repo here in the event that it is set in the privacy check
    # below. This potentially saves an API call in _check_permissions, below.
    repo = None

    # Quit if privacy mismatch and not contributor
    node = node_settings.owner
    if node.is_public and not node.is_contributor(auth.user):
        try:
            repo = connection.repo(node_settings.user, node_settings.repo)
        except NotFoundError:
            # TODO: Test me @jmcarp
            # TODO: Add warning message
            logger.error('Could not access GitHub repo')
            return None
        if repo.private:
            return None

    try:
        branch, sha, branches = get_refs(
            node_settings,
            branch=kwargs.get('branch'),
            sha=kwargs.get('sha'),
            connection=connection,
        )
    except (NotFoundError, GitHubError):
        # TODO: Show an alert or change GitHub configuration?
        logger.error('GitHub repo not found')
        return

    if branch is not None:
        ref = ref_to_params(branch, sha)
        can_edit = check_permissions(
            node_settings, auth, connection, branch, sha, repo=repo,
        )
    else:
        ref = None
        can_edit = False

    name_tpl = '{user}/{repo}'.format(
        user=node_settings.user, repo=node_settings.repo
    )

    permissions = {
        'edit': can_edit,
        'view': True,
        'private': node_settings.is_private
    }
    urls = {
        'upload': node_settings.owner.api_url + 'github/file/' + (ref or ''),
        'fetch': node_settings.owner.api_url + 'github/hgrid/' + (ref or ''),
        'branch': node_settings.owner.api_url + 'github/hgrid/root/',
        'zip': node_settings.owner.api_url + 'github/zipball/' + (ref or ''),
        'repo': "https://github.com/{0}/{1}/tree/{2}".format(node_settings.user, node_settings.repo, branch)
    }

    branch_names = [each.name for each in branches]
    if not branch_names:
        branch_names = [branch]  # if repo un-init-ed then still add default branch to list of branches

    return [rubeus.build_addon_root(
        node_settings,
        name_tpl,
        urls=urls,
        permissions=permissions,
        branches=branch_names,
        defaultBranch=branch,
    )]

#########
# Repos #
#########

@must_have_addon(SHORT_NAME, 'user')
@must_have_addon(SHORT_NAME, 'node')
@must_be_addon_authorizer(SHORT_NAME)
@must_have_permission('write')
def github_create_repo(**kwargs):
    repo_name = request.json.get('name')
    if not repo_name:
        raise HTTPError(http.BAD_REQUEST)

    node_settings = kwargs['node_addon']
    connection = GitHubClient(external_account=node_settings.external_account)

    try:
        repo = connection.create_repo(repo_name, auto_init=True)
    except GitHubError:
        # TODO: Check status code
        raise HTTPError(http.BAD_REQUEST)

    return {
        'user': repo.owner.login,
        'repo': repo.name,
    }

#########
# Hooks #
#########

# TODO: Refactor using NodeLogger
def add_hook_log(node, github, action, path, date, committer, include_urls=False,
                 sha=None, save=False):
    """Add log event for commit from webhook payload.

    :param node: Node to add logs to
    :param github: GitHub node settings record
    :param path: Path to file
    :param date: Date of commit
    :param committer: Committer name
    :param include_urls: Include URLs in `params`
    :param sha: SHA of updated file
    :param save: Save changes

    """
    github_data = {
        'user': github.user,
        'repo': github.repo,
    }

    urls = {}

    if include_urls:
        # TODO: Move to helper function
        url = node.web_url_for('addon_view_or_download_file', path=path, provider=SHORT_NAME)

        urls = {
            'view': '{0}?ref={1}'.format(url, sha),
            'download': '{0}?action=download&ref={1}'.format(url, sha)
        }

    node.add_log(
        action=action,
        params={
            'project': node.parent_id,
            'node': node._id,
            'path': path,
            'github': github_data,
            'urls': urls,
        },
        auth=None,
        foreign_user=committer,
        log_date=date,
        save=save,
    )


@must_be_valid_project
@must_not_be_registration
@must_have_addon('github', 'node')
def github_hook_callback(node_addon, **kwargs):
    """Add logs for commits from outside OSF.

    """
    if request.json is None:
        return {}

    # Fail if hook signature is invalid
    verify_hook_signature(
        node_addon,
        request.data,
        request.headers,
    )

    node = kwargs['node'] or kwargs['project']

    payload = request.json

    for commit in payload.get('commits', []):

        # TODO: Look up OSF user by commit

        # Skip if pushed by OSF
        if commit['message'] and commit['message'] in MESSAGES.values():
            continue

        _id = commit['id']
        date = dateparse(commit['timestamp'])
        committer = commit['committer']['name']

        # Add logs
        for path in commit.get('added', []):
            add_hook_log(
                node, node_addon, 'github_' + NodeLog.FILE_ADDED,
                path, date, committer, include_urls=True, sha=_id,
            )
        for path in commit.get('modified', []):
            add_hook_log(
                node, node_addon, 'github_' + NodeLog.FILE_UPDATED,
                path, date, committer, include_urls=True, sha=_id,
            )
        for path in commit.get('removed', []):
            add_hook_log(
                node, node_addon, 'github_' + NodeLog.FILE_REMOVED,
                path, date, committer,
            )

    node.save()

# -*- coding: utf-8 -*-
import mock

import urlparse

from website.addons.base.testing import views
from website.addons.base.testing.utils import MockFolder

from website.addons.mendeley.model import Mendeley
from website.addons.mendeley.provider import MendeleyCitationsProvider
from website.addons.mendeley.serializer import MendeleySerializer

from website.addons.mendeley.tests.utils import MendeleyTestCase, mock_responses

API_URL = 'https://api.mendeley.com'

class TestAuthViews(MendeleyTestCase, views.OAuthAddonAuthViewsTestCaseMixin):
    pass

class TestConfigViews(MendeleyTestCase, views.OAuthCitationAddonConfigViewsTestCaseMixin):
    folder = MockFolder()
    Serializer = MendeleySerializer
    client = Mendeley
    citationsProvider = MendeleyCitationsProvider
    foldersApiUrl = urlparse.urljoin(API_URL, 'folders')
    documentsApiUrl = urlparse.urljoin(API_URL, 'documents')
    mockResponses = mock_responses

    @mock.patch('website.addons.mendeley.model.MendeleyNodeSettings._fetch_folder_name', mock.PropertyMock(return_value='Fake Name'))
    def test_deauthorize_node(self):
        super(TestConfigViews, self).test_deauthorize_node()

import re
import httplib

from boto import exception
from boto.s3.connection import S3Connection
from boto.s3.connection import OrdinaryCallingFormat

from framework.exceptions import HTTPError
from website.addons.s3.settings import BUCKET_LOCATIONS


def connect_s3(access_key=None, secret_key=None, node_settings=None):
    """Helper to build an S3Connection object
    Can be used to change settings on all S3Connections
    See: CallingFormat
    """
    if node_settings is not None:
        if node_settings.external_account is not None:
            access_key, secret_key = node_settings.external_account.oauth_key, node_settings.external_account.oauth_secret
    connection = S3Connection(access_key, secret_key, calling_format=OrdinaryCallingFormat())
    return connection


def get_bucket_names(node_settings):
    try:
        buckets = connect_s3(node_settings=node_settings).get_all_buckets()
    except exception.NoAuthHandlerFound:
        raise HTTPError(httplib.FORBIDDEN)
    except exception.BotoServerError as e:
        raise HTTPError(e.status)

    return [bucket.name for bucket in buckets]


def validate_bucket_location(location):
    return location in BUCKET_LOCATIONS


def validate_bucket_name(name):
    """Make sure the bucket name conforms to Amazon's expectations as described at:
    http://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html#bucketnamingrules
    The laxer rules for US East (N. Virginia) are not supported.
    """
    label = '[a-z0-9]+(?:[a-z0-9\-]*[a-z0-9])?'
    validate_name = re.compile('^' + label + '(?:\\.' + label + ')*$')
    is_ip_address = re.compile('^[0-9]+(?:\.[0-9]+){3}$')
    return (
        len(name) >= 3 and len(name) <= 63 and bool(validate_name.match(name)) and not bool(is_ip_address.match(name))
    )


def create_bucket(node_settings, bucket_name, location=''):
    return connect_s3(node_settings=node_settings).create_bucket(bucket_name, location=location)


def bucket_exists(access_key, secret_key, bucket_name):
    """Tests for the existance of a bucket and if the user
    can access it with the given keys
    """
    if not bucket_name:
        return False

    connection = connect_s3(access_key, secret_key)

    if bucket_name != bucket_name.lower():
        # Must use ordinary calling format for mIxEdCaSe bucket names
        # otherwise use the default as it handles bucket outside of the US
        connection.calling_format = OrdinaryCallingFormat()

    try:
        # Will raise an exception if bucket_name doesn't exist
        connect_s3(access_key, secret_key).head_bucket(bucket_name)
    except exception.S3ResponseError as e:
        if e.status not in (301, 302):
            return False
    return True


def can_list(access_key, secret_key):
    """Return whether or not a user can list
    all buckets accessable by this keys
    """
    # Bail out early as boto does not handle getting
    # Called with (None, None)
    if not (access_key and secret_key):
        return False

    try:
        connect_s3(access_key, secret_key).get_all_buckets()
    except exception.S3ResponseError:
        return False
    return True

def get_user_info(access_key, secret_key):
    """Returns an S3 User with .display_name and .id, or None
    """
    if not (access_key and secret_key):
        return None

    try:
        return connect_s3(access_key, secret_key).get_all_buckets().owner
    except exception.S3ResponseError:
        return None
    return None

# -*- coding: utf-8 -*-

from factory import SubFactory, Sequence

from tests.factories import ModularOdmFactory, UserFactory, ProjectFactory, ExternalAccountFactory

import datetime

from dateutil.relativedelta import relativedelta

from website.addons.zotero import model


class ZoteroAccountFactory(ExternalAccountFactory):
    provider = 'zotero'
    provider_id = Sequence(lambda n: 'id-{0}'.format(n))
    provider_name = 'Fake Provider'
    oauth_key = Sequence(lambda n: 'key-{0}'.format(n))
    oauth_secret = Sequence(lambda n: 'secret-{0}'.format(n))
    expires_at = datetime.datetime.now() + relativedelta(days=1)


class ZoteroUserSettingsFactory(ModularOdmFactory):
    class Meta:
        model = model.ZoteroUserSettings

    owner = SubFactory(UserFactory)


class ZoteroNodeSettingsFactory(ModularOdmFactory):
    class Meta:
        model = model.ZoteroNodeSettings

    owner = SubFactory(ProjectFactory)

from website.files.models.base import *  # noqa

from website.files.models.s3 import *  # noqa
from website.files.models.box import *  # noqa
from website.files.models.github import *  # noqa
from website.files.models.dropbox import *  # noqa
from website.files.models.figshare import *  # noqa
from website.files.models.dataverse import *  # noqa
from website.files.models.osfstorage import *  # noqa
from website.files.models.googledrive import *  # noqa

# -*- coding: utf-8 -*-
"""Consolidates all necessary models from the framework and website packages.
"""

from framework.auth.core import User
from framework.guid.model import Guid, BlacklistGuid
from framework.sessions.model import Session

from website.project.model import (
    Node, NodeLog,
    Tag, WatchConfig, MetaSchema, Pointer,
    Comment, PrivateLink, MetaData,
    AlternativeCitation,
    DraftRegistration,
)
from website.project.sanctions import (
    DraftRegistrationApproval,
    Embargo,
    EmbargoTerminationApproval,
    RegistrationApproval,
    Retraction,
)
from website.oauth.models import ApiOAuth2Application, ExternalAccount, ApiOAuth2PersonalToken
from website.identifiers.model import Identifier
from website.citations.models import CitationStyle
from website.institutions.model import Institution  # flake8: noqa

from website.mails import QueuedMail
from website.files.models.base import FileVersion
from website.files.models.base import StoredFileNode
from website.files.models.base import TrashedFileNode
from website.conferences.model import Conference, MailRecord
from website.notifications.model import NotificationDigest
from website.notifications.model import NotificationSubscription
from website.archiver.model import ArchiveJob, ArchiveTarget
from website.project.licenses import NodeLicense, NodeLicenseRecord

# All models
MODELS = (
    User,
    ApiOAuth2Application, ApiOAuth2PersonalToken, Node,
    NodeLog, StoredFileNode, TrashedFileNode, FileVersion,
    Tag, WatchConfig, Session, Guid, MetaSchema, Pointer,
    MailRecord, Comment, PrivateLink, MetaData, Conference,
    NotificationSubscription, NotificationDigest, CitationStyle,
    CitationStyle, ExternalAccount, Identifier,
    Embargo, Retraction, RegistrationApproval, EmbargoTerminationApproval,
    ArchiveJob, ArchiveTarget, BlacklistGuid,
    QueuedMail, AlternativeCitation,
    DraftRegistration, DraftRegistrationApproval,
    NodeLicense, NodeLicenseRecord
)

GUID_MODELS = (User, Node, Comment, MetaData)

# -*- coding: utf-8 -*-

from datetime import datetime

import markdown
import pytz
from flask import request

from api.caching.tasks import ban_url
from framework.guid.model import Guid
from framework.postcommit_tasks.handlers import enqueue_postcommit_task
from modularodm import Q
from website import settings
from website.addons.base.signals import file_updated
from website.files.models import FileNode, TrashedFileNode
from website.models import Comment
from website.notifications.constants import PROVIDERS
from website.notifications.emails import notify
from website.project.decorators import must_be_contributor_or_public
from website.project.model import Node
from website.project.signals import comment_added


@file_updated.connect
def update_file_guid_referent(self, node, event_type, payload, user=None):
    if event_type == 'addon_file_moved' or event_type == 'addon_file_renamed':
        source = payload['source']
        destination = payload['destination']
        source_node = Node.load(source['node']['_id'])
        destination_node = node
        file_guids = FileNode.resolve_class(source['provider'], FileNode.ANY).get_file_guids(
            materialized_path=source['materialized'] if source['provider'] != 'osfstorage' else source['path'],
            provider=source['provider'],
            node=source_node)

        if event_type == 'addon_file_renamed' and source['provider'] in settings.ADDONS_BASED_ON_IDS:
            return
        if event_type == 'addon_file_moved' and (source['provider'] == destination['provider'] and
                                                 source['provider'] in settings.ADDONS_BASED_ON_IDS) and source_node == destination_node:
            return

        for guid in file_guids:
            obj = Guid.load(guid)
            if source_node != destination_node and Comment.find(Q('root_target', 'eq', guid)).count() != 0:
                update_comment_node(guid, source_node, destination_node)

            if source['provider'] != destination['provider'] or source['provider'] != 'osfstorage':
                old_file = FileNode.load(obj.referent._id)
                obj.referent = create_new_file(obj, source, destination, destination_node)
                obj.save()
                if old_file and not TrashedFileNode.load(old_file._id):
                    old_file.delete()


def create_new_file(obj, source, destination, destination_node):
    # TODO: Remove when materialized paths are fixed in the payload returned from waterbutler
    if not source['materialized'].startswith('/'):
        source['materialized'] = '/' + source['materialized']
    if not destination['materialized'].startswith('/'):
        destination['materialized'] = '/' + destination['materialized']

    if not source['path'].endswith('/'):
        data = dict(destination)
        new_file = FileNode.resolve_class(destination['provider'], FileNode.FILE).get_or_create(destination_node, destination['path'])
        if destination['provider'] != 'osfstorage':
            new_file.update(revision=None, data=data)
    else:
        new_file = find_and_create_file_from_metadata(destination.get('children', []), source, destination, destination_node, obj)
        if not new_file:
            if source['provider'] == 'box':
                new_path = obj.referent.path
            else:
                new_path = obj.referent.materialized_path.replace(source['materialized'], destination['materialized'])
            new_file = FileNode.resolve_class(destination['provider'], FileNode.FILE).get_or_create(destination_node, new_path)
            new_file.name = new_path.split('/')[-1]
            new_file.materialized_path = new_path
            new_file.save()
    return new_file


def find_and_create_file_from_metadata(children, source, destination, destination_node, obj):
    """ Given a Guid obj, recursively search for the metadata of its referent (a file obj)
    in the waterbutler response. If found, create a new addon FileNode with that metadata
    and return the new file.
    """
    for item in children:
        # TODO: Remove when materialized paths are fixed in the payload returned from waterbutler
        if not item['materialized'].startswith('/'):
            item['materialized'] = '/' + item['materialized']

        if item['kind'] == 'folder':
            return find_and_create_file_from_metadata(item.get('children', []), source, destination, destination_node, obj)
        elif item['kind'] == 'file' and item['materialized'].replace(destination['materialized'], source['materialized']) == obj.referent.materialized_path:
            data = dict(item)
            new_file = FileNode.resolve_class(destination['provider'], FileNode.FILE).get_or_create(destination_node, item['path'])
            if destination['provider'] != 'osfstorage':
                new_file.update(revision=None, data=data)
            return new_file


def update_comment_node(root_target_id, source_node, destination_node):
    Comment.update(Q('root_target', 'eq', root_target_id), data={'node': destination_node})
    source_node.save()
    destination_node.save()


@comment_added.connect
def send_comment_added_notification(comment, auth):
    node = comment.node
    target = comment.target

    context = dict(
        gravatar_url=auth.user.profile_image_url(),
        content=markdown.markdown(comment.content, ['del_ins', 'markdown.extensions.tables', 'markdown.extensions.fenced_code']),
        page_type=comment.get_comment_page_type(),
        page_title=comment.get_comment_page_title(),
        provider=PROVIDERS[comment.root_target.referent.provider] if comment.page == Comment.FILES else '',
        target_user=target.referent.user if is_reply(target) else None,
        parent_comment=target.referent.content if is_reply(target) else "",
        url=comment.get_comment_page_url()
    )
    time_now = datetime.utcnow().replace(tzinfo=pytz.utc)
    sent_subscribers = notify(
        event="comments",
        user=auth.user,
        node=node,
        timestamp=time_now,
        **context
    )

    if is_reply(target):
        if target.referent.user and target.referent.user not in sent_subscribers:
            notify(
                event='comment_replies',
                user=auth.user,
                node=node,
                timestamp=time_now,
                **context
            )


def is_reply(target):
    return isinstance(target.referent, Comment)


def _update_comments_timestamp(auth, node, page=Comment.OVERVIEW, root_id=None):
    if node.is_contributor(auth.user):
        enqueue_postcommit_task((ban_url, (node, )))
        if root_id is not None:
            guid_obj = Guid.load(root_id)
            if guid_obj is not None:
                enqueue_postcommit_task((ban_url, (guid_obj.referent, )))

        # update node timestamp
        if page == Comment.OVERVIEW:
            root_id = node._id
        auth.user.comments_viewed_timestamp[root_id] = datetime.utcnow()
        auth.user.save()
        return {root_id: auth.user.comments_viewed_timestamp[root_id].isoformat()}
    else:
        return {}

@must_be_contributor_or_public
def update_comments_timestamp(auth, node, **kwargs):
    timestamp_info = request.get_json()
    page = timestamp_info.get('page')
    root_id = timestamp_info.get('rootId')
    return _update_comments_timestamp(auth, node, page, root_id)

# -*- coding: utf-8 -*-

import os
import itertools

import furl
import requests

from framework.exceptions import HTTPError


class BaseClient(object):

    @property
    def _auth(self):
        return None

    @property
    def _default_headers(self):
        return {}

    def _make_request(self, method, url, params=None, **kwargs):
        expects = kwargs.pop('expects', None)
        throws = kwargs.pop('throws', None)

        kwargs['headers'] = self._build_headers(**kwargs.get('headers', {}))

        response = requests.request(method, url, params=params, auth=self._auth, **kwargs)
        if expects and response.status_code not in expects:
            raise throws if throws else HTTPError(response.status_code, message=response.content)

        return response

    def _build_headers(self, **kwargs):
        headers = self._default_headers
        headers.update(kwargs)
        return {
            key: value
            for key, value in headers.items()
            if value is not None
        }

    def _build_url(self, base, *segments):
        url = furl.furl(base)
        segments = filter(
            lambda segment: segment,
            map(
                lambda segment: segment.strip('/'),
                itertools.chain(url.path.segments, segments)
            )
        )
        url.path = os.path.join(*segments)
        return url.url

# coding: utf-8
from __future__ import (
    absolute_import,
    print_function,
    unicode_literals,
)

from pydocx.models import XmlModel, XmlChild, XmlAttribute
from pydocx.openxml.drawing.extents import Extents


class Transform2D(XmlModel):
    XML_TAG = 'xfrm'

    extents = XmlChild(type=Extents)
    rotate = XmlAttribute(name='rot', default=None)

# coding: utf-8
from __future__ import (
    absolute_import,
    print_function,
    unicode_literals,
)

from pydocx.models import XmlModel, XmlContent


class FieldCode(XmlModel):
    XML_TAG = 'instrText'

    content = XmlContent()

# coding: utf-8
from __future__ import (
    absolute_import,
    print_function,
    unicode_literals,
)

from collections import defaultdict

from pydocx.models import XmlModel, XmlCollection
from pydocx.openxml.wordprocessing.table_cell import TableCell
from pydocx.openxml.wordprocessing.table_row import TableRow


class Table(XmlModel):
    XML_TAG = 'tbl'

    rows = XmlCollection(
        TableRow,
    )

    def calculate_table_cell_spans(self):
        if not self.rows:
            return

        active_rowspan_cells_by_column = {}
        cell_to_rowspan_count = defaultdict(int)
        for row in self.rows:
            for column_index, cell in enumerate(row.cells):
                properties = cell.properties
                # If this element is omitted, then this cell shall not be
                # part of any vertically merged grouping of cells, and any
                # vertically merged group of preceding cells shall be
                # closed.
                if properties is None or properties.vertical_merge is None:
                    # if properties are missing, this is the same as the
                    # the element being omitted
                    active_rowspan_cells_by_column[column_index] = None
                elif properties:
                    vertical_merge = properties.vertical_merge.get('val', 'continue')  # noqa
                    if vertical_merge == 'restart':
                        active_rowspan_cells_by_column[column_index] = cell
                        cell_to_rowspan_count[cell] += 1
                    elif vertical_merge == 'continue':
                        active_rowspan_for_column = active_rowspan_cells_by_column.get(column_index)  # noqa
                        if active_rowspan_for_column:
                            cell_to_rowspan_count[active_rowspan_for_column] += 1  # noqa
        return dict(cell_to_rowspan_count)


# Python makes defining nested class hierarchies at the global level difficult
TableCell.children.types.add(Table)

# coding: utf-8

from __future__ import (
    absolute_import,
    print_function,
    unicode_literals,
)

from pydocx.test import DocumentGeneratorTestCase
from pydocx.test.utils import WordprocessingDocumentFactory
from pydocx.openxml.packaging import MainDocumentPart, StyleDefinitionsPart


class StyleBasedOnTestCase(DocumentGeneratorTestCase):
    def test_style_chain_ends_when_loop_is_detected(self):
        style_xml = '''
            <style styleId="one">
              <basedOn val="three"/>
              <rPr>
                <b val="on"/>
              </rPr>
            </style>
            <style styleId="two">
              <basedOn val="one"/>
            </style>
            <style styleId="three">
              <basedOn val="two"/>
            </style>
        '''

        document_xml = '''
            <p>
              <pPr>
                <pStyle val="three"/>
              </pPr>
              <r>
                <t>aaa</t>
              </r>
            </p>
        '''

        document = WordprocessingDocumentFactory()
        document.add(StyleDefinitionsPart, style_xml)
        document.add(MainDocumentPart, document_xml)

        expected_html = '<p><strong>aaa</strong></p>'
        self.assert_document_generates_html(document, expected_html)

    def test_styles_are_inherited(self):
        style_xml = '''
            <style styleId="one">
              <rPr>
                <b val="on"/>
              </rPr>
            </style>
            <style styleId="two">
              <basedOn val="one"/>
              <rPr>
                <i val="on"/>
              </rPr>
            </style>
            <style styleId="three">
              <basedOn val="two"/>
              <rPr>
                <u val="single"/>
              </rPr>
            </style>
        '''

        document_xml = '''
            <p>
              <pPr>
                <pStyle val="three"/>
              </pPr>
              <r>
                <t>aaa</t>
              </r>
            </p>
        '''

        document = WordprocessingDocumentFactory()
        document.add(StyleDefinitionsPart, style_xml)
        document.add(MainDocumentPart, document_xml)

        expected_html = '''
            <p>
              <span class="pydocx-underline">
                <em>
                  <strong>aaa</strong>
                </em>
              </span>
            </p>
        '''
        self.assert_document_generates_html(document, expected_html)

    def test_basedon_ignored_for_character_based_on_paragraph(self):
        # character styles may only be based on other character styles
        # otherwise, the based on specification should be ignored
        style_xml = '''
            <style styleId="one" type="paragraph">
              <rPr>
                <b val="on"/>
              </rPr>
            </style>
            <style styleId="two" type="character">
              <basedOn val="one"/>
              <rPr>
                <i val="on"/>
              </rPr>
            </style>
        '''

        document_xml = '''
            <p>
              <r>
                <rPr>
                  <rStyle val="two"/>
                </rPr>
                <t>aaa</t>
              </r>
            </p>
        '''

        document = WordprocessingDocumentFactory()
        document.add(StyleDefinitionsPart, style_xml)
        document.add(MainDocumentPart, document_xml)

        expected_html = '<p><em>aaa</em></p>'
        self.assert_document_generates_html(document, expected_html)

    def test_basedon_ignored_for_paragraph_based_on_character(self):
        # paragraph styles may only be based on other paragraph styles
        # otherwise, the based on specification should be ignored
        style_xml = '''
            <style styleId="one" type="character">
              <rPr>
                <b val="on"/>
              </rPr>
            </style>
            <style styleId="two" type="paragraph">
              <basedOn val="one"/>
              <rPr>
                <i val="on"/>
              </rPr>
            </style>
        '''

        document_xml = '''
            <p>
              <pPr>
                <pStyle val="two"/>
              </pPr>
              <r>
                <t>aaa</t>
              </r>
            </p>
        '''

        document = WordprocessingDocumentFactory()
        document.add(StyleDefinitionsPart, style_xml)
        document.add(MainDocumentPart, document_xml)

        expected_html = '<p><em>aaa</em></p>'
        self.assert_document_generates_html(document, expected_html)

# coding: utf-8
from __future__ import (
    absolute_import,
    print_function,
    unicode_literals,
)

import unittest

from pydocx.packaging import ZipPackage


class ZipPackageTestCase(unittest.TestCase):
    def setUp(self):
        self.package = ZipPackage(
            path='tests/fixtures/no_break_hyphen.docx',
        )

    def test_relationship_uri(self):
        self.assertEqual(
            self.package.relationship_uri,
            '/_rels/.rels',
        )

    def test_relationship_part_exists(self):
        assert self.package.part_exists(self.package.relationship_uri)

    def test_word_document_part_exists(self):
        assert self.package.part_exists('/word/document.xml')

    def test_package_relationship_part_stream(self):
        part = self.package.get_part('/_rels/.rels')
        data = part.stream.read()
        assert data
        assert data.startswith(b'<?xml version="1.0" encoding="UTF-8"?>')

'''
Harvester for the Addis Ababa University Institutional Repository for the SHARE project

Example API call: http://etd.aau.edu.et/oai/request?verb=ListRecords&metadataPrefix=oai_dc
'''
from __future__ import unicode_literals

from scrapi.base import OAIHarvester
from scrapi.base import helpers


def oai_process_uris_addis_ababa(*args):

    identifiers = helpers.gather_identifiers(args)
    provider_uris, object_uris = helpers.seperate_provider_object_uris(
        list(map(lambda x: x.replace('http://hdl.handle.net/123456789/', 'http://etd.aau.edu.et/handle/123456789/'), identifiers))
    )

    potential_uris = (provider_uris + object_uris)

    try:
        canonical_uri = potential_uris[0]
    except IndexError:
        raise ValueError('No Canonical URI was returned for this record.')

    return {
        'canonicalUri': canonical_uri,
        'objectUris': object_uris,
        'providerUris': provider_uris
    }


class AauHarvester(OAIHarvester):
    short_name = 'addis_ababa'
    long_name = 'Addis Ababa University Institutional Repository'
    url = 'http://etd.aau.edu.et'

    @property
    def schema(self):
        return helpers.updated_schema(self._schema, {
            "uris": ('//ns0:header/ns0:identifier/node()', '//dc:identifier/node()', oai_process_uris_addis_ababa)
        })

    base_url = 'http://etd.aau.edu.et/oai/request'
    property_list = ['date', 'type', 'identifier', 'setSpec']
    timezone_granularity = True

"""
harvester for Duke University Libraries for the SHARE NS
"""

from __future__ import unicode_literals

from scrapi.base import OAIHarvester


class DukeUniversityLib(OAIHarvester):
    short_name = 'duke'
    long_name = 'Duke University Libraries'
    url = 'http://dukespace.lib.duke.edu'

    base_url = 'http://dukespace.lib.duke.edu/dspace-oai/request'
    timezone_granularity = True

    property_list = [
        'type', 'source', 'setSpec',
        'format', 'identifier'
    ]

    approved_sets = [
        'hdl_10161_5894',
        'hdl_10161_4935',
        'hdl_10161_841',
        'hdl_10161_5738',
        'hdl_10161_7698',
        'hdl_10161_7413',
        'hdl_10161_840',
        'hdl_10161_5893',
        'hdl_10161_31',
        'hdl_10161_5387',
        'hdl_10161_1561',
        'hdl_10161_7406',
        'hdl_10161_7650',
        'hdl_10161_7606',
        'hdl_10161_7637',
        'hdl_10161_7623',
        'hdl_10161_7419',
        'hdl_10161_8357',
        'hdl_10161_8356',
        'hdl_10161_9203',
        'hdl_10161_410',
        'hdl_10161_7666',
        'hdl_10161_1707',
        'hdl_10161_4',
        'hdl_10161_6217',
        'hdl_10161_2840',
        'hdl_10161_4936',
        'hdl_10161_8127',
        'hdl_10161_875',
        'hdl_10161_9149',
        'hdl_10161_8115',
        'hdl_10161_9217',
        'hdl_10161_8914',
        'hdl_10161_9201',
        'hdl_10161_7409',
        'hdl_10161_2658',
        'hdl_10161_2493',
        'hdl_10161_7773',
        'hdl_10161_7682',
        'hdl_10161_3188',
        'hdl_10161_41',
        'hdl_10161_52',
        'hdl_10161_60',
        'hdl_10161_874',
        'hdl_10161_7417',
        'hdl_10161_8989',
        'hdl_10161_9283',
        'hdl_10161_10201',
        'hdl_10161_10200',
        'hdl_10161_2877',
        'hdl_10161_7410',
        'hdl_10161_5885',
        'hdl_10161_5886',
        'hdl_10161_3396',
        'hdl_10161_2841',
        'hdl_10161_1701',
        'hdl_10161_7415',
        'hdl_10161_9219',
        'hdl_10161_460',
        'hdl_10161_7408',
        'hdl_10161_7407',
        'hdl_10161_9218',
        'hdl_10161_8939',
        'hdl_10161_8164',
        'hdl_10161_1',
        'hdl_10161_6',
        'hdl_10161_4937',
        'hdl_10161_5895',
        'hdl_10161_8929',
        'hdl_10161_8108',
        'hdl_10161_5896'
    ]

'''
Harvester for the New Prairie Press for the SHARE project

Example API call: http://newprairiepress.org/do/oai/?verb=ListRecords&metadataPrefix=oai_dc
'''
from __future__ import unicode_literals

from scrapi.base import OAIHarvester


class Npp_ksuHarvester(OAIHarvester):
    short_name = 'npp_ksu'
    long_name = 'New Prairie Press at Kansas State University'
    url = 'http://newprairiepress.org'

    base_url = 'http://newprairiepress.org/do/oai/'
    property_list = ['identifier', 'source', 'date', 'type', 'format', 'setSpec']
    timezone_granularity = True

'''
Harvester for the Scholar Commons for the SHARE project

Example API call: http://scholarcommons.usf.edu/do/oai/?verb=ListRecords&metadataPrefix=oai_dc
'''
from __future__ import unicode_literals

from scrapi.base import OAIHarvester


class U_south_flHarvester(OAIHarvester):
    short_name = 'u_south_fl'
    long_name = 'University of South Florida - Scholar Commons'
    url = 'http://scholarcommons.usf.edu'

    base_url = 'http://scholarcommons.usf.edu/do/oai/'
    property_list = ['identifier', 'date', 'type', 'source', 'format', 'setSpec']

    approved_sets = [
        u'race_place',
        u'ur_symposium',
        u'abo',
        u'tlar_pub',
        u'tlar',
        u'tlas_pub',
        u'acc_etd',
        u'afa',
        u'afa_etd',
        u'gey_etd',
        u'alambique',
        u'ana',
        u'ana_etd',
        u'ant',
        u'ant_facpub',
        u'ant_etd',
        u'aba',
        u'aba_facpub',
        u'aba_etd',
        u'arc_etd',
        u'art_etd',
        u'cutr_pub',
        u'camprec',
        u'sa_catalyst',
        u'bcm',
        u'bcm_facpub',
        u'bcm_etd',
        u'ocep_cbrr',
        u'clphp',
        u'ech',
        u'ech_etd',
        u'chm',
        u'chm_facpub',
        u'chm_etd',
        u'cfs',
        u'cfs_facpub',
        u'cfs_etd',
        u'egx',
        u'egx_facpub',
        u'egx_etd',
        u'cas',
        u'cbcs',
        u'business',
        u'business_pub',
        u'coedu',
        u'coedu_pub',
        u'coe',
        u'coe_pub',
        u'marine',
        u'marine_pub',
        u'med',
        u'nur',
        u'pharm',
        u'cph',
        u'arts',
        u'arts_pub',
        u'spe_facpub',
        u'csd',
        u'csd_etd',
        u'spe_etd',
        u'cfh',
        u'cfh_facpub',
        u'cfh_etd',
        u'clphp_cbdc',
        u'esb',
        u'esb_etd',
        u'conferences',
        u'couch_stone',
        u'cjp',
        u'cjp_facpub',
        u'cjp_etd',
        u'basgp_data',
        u'dean_cbcs',
        u'deepkarst_2016',
        u'spe',
        u'grad_dsli',
        u'ecn',
        u'ecn_etd',
        u'els',
        u'els_facpub',
        u'els_etd',
        u'edq',
        u'edq_facpub',
        u'edq_etd',
        u'esf',
        u'esf_facpub',
        u'esf_etd',
        u'ege',
        u'ege_etd',
        u'eng',
        u'eng_facpub',
        u'eng_etd',
        u'tles',
        u'tles_pub',
        u'tles_gallery',
        u'tles_oh',
        u'eoh',
        u'eoh_etd',
        u'epb',
        u'epb_etd',
        u'sg_exec_pubs',
        u'camprec_exlib',
        u'fmhi_el',
        u'fmhi_pub',
        u'fs',
        u'fs_pubs',
        u'fin',
        u'fin_etd',
        u'wusf_first',
        u'fl_drive_in',
        u'clphp_fphtc',
        u'ocep_fg',
        u'map_links',
        u'gsp',
        u'gep',
        u'gep_etd',
        u'gly',
        u'gly_facpub',
        u'gly_etd',
        u'glo',
        u'glo_etd',
        u'gia',
        u'gia_facpub',
        u'gia_etd',
        u'honors_gast',
        u'grad',
        u'grad_facpub',
        u'etd',
        u'hpm',
        u'hpm_etd',
        u'las_hhfc',
        u'usfhistinfo_oh',
        u'hty',
        u'hty_facpub',
        u'hty_etd',
        u'honors',
        u'hcs',
        u'hcs_etd',
        u'into_facpub',
        u'into',
        u'iigw',
        u'basgp_images',
        u'egs',
        u'egs_etd',
        u'qmb',
        u'qmb_etd',
        u'las',
        u'ibl',
        u'edk_etd',
        u'bin',
        u'bin_facpub',
        u'bin_etd',
        u'eie',
        u'eie_etd',
        u'interdisc_programs',
        u'clumped_isotope',
        u'iciworkshop_gallery',
        u'ijs',
        u'jacaps',
        u'jea',
        u'jpr',
        u'jpt',
        u'jss',
        u'sg_jud_pubs',
        u'las_facpub',
        u'las_etd',
        u'latcom_2013',
        u'ehe',
        u'ehe_facpub',
        u'ehe_etd',
        u'sg_leg_pubs',
        u'fmhi',
        u'man',
        u'man_etd',
        u'msc_facpub',
        u'msc_etd',
        u'mkt',
        u'mkt_etd',
        u'com',
        u'com_etd',
        u'mth',
        u'mth_facpub',
        u'mth_etd',
        u'egr',
        u'egr_etd',
        u'mhlp',
        u'mhlp_facpub',
        u'mhlp_etd',
        u'mhs_etd',
        u'mca',
        u'tow_gallery',
        u'tow',
        u'mme',
        u'mme_etd',
        u'emu',
        u'emu_etd',
        u'mus_etd',
        u'nckms_2013',
        u'numeracy',
        u'nur_facpub',
        u'nur_etd',
        u'ocep',
        u'ocep_pub',
        u'ocep_workshops',
        u'ur',
        u'onc',
        u'onc_etd',
        u'honors_et',
        u'pcb',
        u'pcb_etd',
        u'pcmr',
        u'pth',
        u'pth_etd',
        u'pharm_facpub',
        u'phi',
        u'phi_etd',
        u'edj_etd',
        u'phy',
        u'phy_etd',
        u'pyb',
        u'pyb_etd',
        u'ocep_pi',
        u'clphp_perlc',
        u'projects',
        u'psy',
        u'psy_facpub',
        u'psy_etd',
        u'phc',
        u'phc_etd',
        u'race_place_gallery',
        u'mhs',
        u'mhs_facpub',
        u'rel',
        u'rel_etd',
        u'basgp_report',
        u'research_matters',
        u'research_matters_gallery',
        u'surcosur',
        u'tlar_scpub',
        u'acc',
        u'gey',
        u'arch',
        u'art',
        u'geo',
        u'geo_facpub',
        u'si',
        u'si_facpub',
        u'si_etd',
        u'mus',
        u'edj',
        u'the',
        u'tles_sealevel',
        u'sla',
        u'sla_etd',
        u'edi',
        u'edi_etd',
        u'sinkhole_2013',
        u'sinkhole_2015',
        u'sbdc',
        u'sok',
        u'sok_etd',
        u'soc',
        u'soc_etd',
        u'tlsdc',
        u'ese',
        u'ese_etd',
        u'lat_sponsor_gallery',
        u'lat_sponsors',
        u'siv',
        u'sa',
        u'sa_facpub',
        u'sa_pub',
        u'sg',
        u'geologia',
        u'subsust',
        u'compaccountability-2013',
        u'basgp',
        u'tlib',
        u'edr',
        u'edr_etd',
        u'oa_textbooks',
        u'cutr',
        u'tci',
        u'fhm_newsletter',
        u'iwic',
        u'the_facpub',
        u'alumni_pubs',
        u'usf_facpub',
        u'usf_gallery',
        u'lib',
        u'usf_gallery_ri',
        u'tloa',
        u'human_trafficking',
        u'ujmm',
        u'wusf',
        u'wst',
        u'wst_etd',
        u'wle',
        u'wle_etd',
        u'wle_facpub'
    ]

    timezone_granularity = True

'''
Harvester for the ASU Digital Repository for the SHARE project

Example API call: https://zenodo.org/oai2d?verb=ListRecords&metadataPrefix=oai_dc
'''

from __future__ import unicode_literals
from scrapi.base import OAIHarvester


class ZenodoHarvester(OAIHarvester):
    short_name = 'zenodo'
    long_name = 'Zenodo'
    url = 'https://zenodo.org/oai2d'

    base_url = 'https://zenodo.org/oai2d'
    property_list = ['language', 'rights', 'source', 'relation', 'date', 'identifier', 'type']
    timezone_granularity = True

import logging

import vcr
import mock
import pytest

from scrapi import base
from scrapi import registry, requests

logger = logging.getLogger(__name__)


@pytest.fixture(autouse=True)
def mock_maybe_load_response(monkeypatch):
    mock_mlr = mock.Mock()
    mock_mlr.return_value = None
    mock_save = lambda x: x

    monkeypatch.setattr(requests, '_maybe_load_response', mock_mlr)
    monkeypatch.setattr(requests.HarvesterResponse, 'save', mock_save)


@pytest.mark.parametrize('harvester_name', filter(lambda x: x != 'test', sorted(map(str, registry.keys()))))
def test_harvester(monkeypatch, harvester_name, *args, **kwargs):
    monkeypatch.setattr(requests.time, 'sleep', lambda *_, **__: None)
    base.settings.RAISE_IN_TRANSFORMER = True

    harvester = registry[harvester_name]

    with vcr.use_cassette('tests/vcr/{}.yaml'.format(harvester_name), match_on=['host'], record_mode='none'):
        harvested = harvester.harvest()
        assert len(harvested) > 0

    normalized = list(filter(lambda x: x is not None, map(harvester.normalize, harvested[:25])))
    assert len(normalized) > 0

__version__ = '0.19.5'
__import__('pkg_resources').declare_namespace(__name__)

import os

from waterbutler.core import metadata


class BaseCloudFilesMetadata(metadata.BaseMetadata):

    @property
    def provider(self):
        return 'cloudfiles'


class CloudFilesFileMetadata(BaseCloudFilesMetadata, metadata.BaseFileMetadata):

    @property
    def name(self):
        return os.path.split(self.raw['name'])[1]

    @property
    def path(self):
        return self.build_path(self.raw['name'])

    @property
    def size(self):
        return self.raw['bytes']

    @property
    def modified(self):
        return self.raw['last_modified']

    @property
    def content_type(self):
        return self.raw['content_type']

    @property
    def etag(self):
        return self.raw['hash']


class CloudFilesHeaderMetadata(BaseCloudFilesMetadata, metadata.BaseFileMetadata):

    def __init__(self, raw, path):
        super().__init__(raw)
        self._path = path

    @property
    def name(self):
        return os.path.split(self._path)[1]

    @property
    def path(self):
        return self.build_path(self._path)

    @property
    def size(self):
        return int(self.raw['Content-Length'])

    @property
    def modified(self):
        return self.raw['Last-Modified']

    @property
    def content_type(self):
        return self.raw['Content-Type']

    @property
    def etag(self):
        return self.raw['etag']


class CloudFilesFolderMetadata(BaseCloudFilesMetadata, metadata.BaseFolderMetadata):

    @property
    def name(self):
        return os.path.split(self.raw['subdir'].rstrip('/'))[1]

    @property
    def path(self):
        return self.build_path(self.raw['subdir'])

try:
    from waterbutler import settings
except ImportError:
    settings = {}

config = settings.get('GOOGLEDRIVE_PROVIDER_CONFIG', {})


BASE_URL = config.get('BASE_URL', 'https://www.googleapis.com/drive/v2')
BASE_UPLOAD_URL = config.get('BASE_UPLOAD_URL', 'https://www.googleapis.com/upload/drive/v2')
DRIVE_IGNORE_VERSION = config.get('DRIVE_IGNORE_VERSION', '0000000000000000000000000000000000000')

import os
import asyncio

import tornado.web
import tornado.httpserver
import tornado.platform.asyncio

from raven.contrib.tornado import AsyncSentryClient

import waterbutler
from waterbutler import settings
from waterbutler.server.api import v0
from waterbutler.server.api import v1
from waterbutler.server import handlers
from waterbutler.server import settings as server_settings


def api_to_handlers(api):
    return [
        (os.path.join('/', api.PREFIX, pattern.lstrip('/')), handler)
        for (pattern, handler) in api.HANDLERS
    ]


def make_app(debug):
    app = tornado.web.Application(
        api_to_handlers(v0) +
        api_to_handlers(v1) +
        [(r'/status', handlers.StatusHandler)],
        debug=debug,
    )
    app.sentry_client = AsyncSentryClient(settings.SENTRY_DSN, release=waterbutler.__version__)
    return app


def serve():
    tornado.platform.asyncio.AsyncIOMainLoop().install()

    app = make_app(server_settings.DEBUG)

    ssl_options = None
    if server_settings.SSL_CERT_FILE and server_settings.SSL_KEY_FILE:
        ssl_options = {
            'certfile': server_settings.SSL_CERT_FILE,
            'keyfile': server_settings.SSL_KEY_FILE,
        }

    app.listen(
        server_settings.PORT,
        address=server_settings.ADDRESS,
        xheaders=server_settings.XHEADERS,
        max_body_size=server_settings.MAX_BODY_SIZE,
        ssl_options=ssl_options,
    )

    asyncio.get_event_loop().set_debug(server_settings.DEBUG)
    asyncio.get_event_loop().run_forever()

# This script is only one file, but distutils only lets us install packages
__version__ = '1.2'

# -*- coding: utf-8 -*-
"""
    flask.session
    ~~~~~~~~~~~~~

    This module used to flask with the session global so we moved it
    over to flask.sessions

    :copyright: (c) 2011 by Armin Ronacher.
    :license: BSD, see LICENSE for more details.
"""

from warnings import warn
warn(DeprecationWarning('please use flask.sessions instead'))

from .sessions import SecureCookieSession, NullSession

Session = SecureCookieSession
_NullSession = NullSession

ext_id = 'oldext_simple'

"""
Flask-GoogleLogin
"""

from base64 import (urlsafe_b64encode as b64encode,
                    urlsafe_b64decode as b64decode)
from urllib import urlencode
from urlparse import parse_qsl
from functools import wraps

from flask import request, redirect, abort, current_app, url_for
from flask_login import LoginManager, make_secure_token

import requests


GOOGLE_OAUTH2_AUTH_URL = 'https://accounts.google.com/o/oauth2/auth'
GOOGLE_OAUTH2_TOKEN_URL = 'https://accounts.google.com/o/oauth2/token'
GOOGLE_OAUTH2_USERINFO_URL = 'https://www.googleapis.com/oauth2/v1/userinfo'
USERINFO_PROFILE_SCOPE = 'https://www.googleapis.com/auth/userinfo.profile'


class GoogleLogin(object):
    """
    Main extension class
    """

    def __init__(self, app=None, login_manager=None):
        if login_manager:
            self.login_manager = login_manager
        else:
            self.login_manager = LoginManager()

        if app:
            self._app = app
            self.init_app(app)

    def init_app(self, app, add_context_processor=True, login_manager=None):
        """
        Initialize with app configuration. Existing
        `flask_login.LoginManager` instance can be passed.
        """

        if login_manager:
            self.login_manager = login_manager
        else:
            self.login_manager = LoginManager()

        # Check if login manager has been init
        if not hasattr(app, 'login_manager'):
            self.login_manager.init_app(
                app,
                add_context_processor=add_context_processor)

        # Clear flashed messages since we redirect to auth immediately
        self.login_manager.login_message = None
        self.login_manager.needs_refresh_message = None

        # Set default unauthorized callback
        self.login_manager.unauthorized_handler(self.unauthorized_callback)

    @property
    def app(self):
        return getattr(self, '_app', current_app)

    @property
    def scopes(self):
        return self.app.config.get('GOOGLE_LOGIN_SCOPES', '')

    @property
    def client_id(self):
        return self.app.config['GOOGLE_LOGIN_CLIENT_ID']

    @property
    def client_secret(self):
        return self.app.config['GOOGLE_LOGIN_CLIENT_SECRET']

    @property
    def redirect_uri(self):
        return self.app.config.get('GOOGLE_LOGIN_REDIRECT_URI')

    @property
    def redirect_scheme(self):
        return self.app.config.get('GOOGLE_LOGIN_REDIRECT_SCHEME', 'http')

    def sign_params(self, params):
        return b64encode(urlencode(dict(sig=make_secure_token(**params),
                                        **params)))

    def parse_state(self, state):
        return dict(parse_qsl(b64decode(str(state))))

    def login_url(self, params=None, **kwargs):
        """
        Return login url with params encoded in state

        Available Google auth server params:
        response_type: code, token
        prompt: none, select_account, consent
        approval_prompt: force, auto
        access_type: online, offline
        scopes: string (separated with commas) or list
        redirect_uri: string
        login_hint: string
        """
        kwargs.setdefault('response_type', 'code')
        kwargs.setdefault('access_type', 'online')

        if 'prompt' not in kwargs:
            kwargs.setdefault('approval_prompt', 'auto')

        scopes = kwargs.pop('scopes', self.scopes.split(','))
        if USERINFO_PROFILE_SCOPE not in scopes:
            scopes.append(USERINFO_PROFILE_SCOPE)

        redirect_uri = kwargs.pop('redirect_uri', self.redirect_uri)
        state = self.sign_params(params or {})

        return GOOGLE_OAUTH2_AUTH_URL + '?' + urlencode(
            dict(client_id=self.client_id,
                 scope=' '.join(scopes),
                 redirect_uri=redirect_uri,
                 state=state,
                 **kwargs))

    def unauthorized_callback(self):
        """
        Redirect to login url with next param set as request.url
        """
        return redirect(self.login_url(params=dict(next=request.url)))

    def exchange_code(self, code, redirect_uri):
        """
        Exchanges code for token/s
        """

        token = requests.post(GOOGLE_OAUTH2_TOKEN_URL, data=dict(
            code=code,
            redirect_uri=redirect_uri,
            grant_type='authorization_code',
            client_id=self.client_id,
            client_secret=self.client_secret,
        )).json
        if not token: # or token.get('error'):
            abort(400)
        return token

    def get_userinfo(self, access_token):
        userinfo = requests.get(GOOGLE_OAUTH2_USERINFO_URL, params=dict(
            access_token=access_token,
        )).json
        if not userinfo: # or userinfo.get('error'):
            abort(400)
        return userinfo

    def get_access_token(self, refresh_token):
        """
        Use a refresh token to obtain a new access token
        """

        token = requests.post(GOOGLE_OAUTH2_TOKEN_URL, data=dict(
            refresh_token=refresh_token,
            grant_type='refresh_token',
            client_id=self.client_id,
            client_secret=self.client_secret,
        )).json

        if not token: # or token.get('error'):
            return

        return token

    def oauth2callback(self, view_func):
        """
        Decorator for OAuth2 callback. Calls `GoogleLogin.login` then
        passes results to `view_func`.
        """

        @wraps(view_func)
        def decorated(*args, **kwargs):
            params = {}

            # Check sig
            if 'state' in request.args:
                params.update(**self.parse_state(request.args.get('state')))
                if params.pop('sig', None) != make_secure_token(**params):
                    return self.login_manager.unauthorized()

            code = request.args.get('code')

            # Web server flow
            
            if code:
		# token = self.exchange_code(code, request.url)
                
                token = self.exchange_code(
                    code,
                    url_for(
                        request.endpoint,
                        _external=True,
                        _scheme=self.redirect_scheme,
                    ),
                )
                
                received = self.get_access_token(token['access_token'])
                userinfo = self.get_userinfo(received)
                params.update(token=token, userinfo=userinfo)

            # Browser flow
            else:
                if params:
                    params.update(dict(request.args.items()))
                else:
                    return '''
                    <script>
                      window.onload = function() {
                        location.href = '?' + window.location.hash.substr(1);
                      };
                    </script>
                    '''

            return view_func(**params)

        return decorated

    def user_loader(self, func):
        """
        Shortcut for `login_manager`'s `flask_login.LoginManager.user_loader`
        """
        self.login_manager.user_loader(func)

__version__ = "1.2"

GOOGLE_AUTH_URI = 'https://accounts.google.com/o/oauth2/auth'
GOOGLE_REVOKE_URI = 'https://accounts.google.com/o/oauth2/revoke'
GOOGLE_TOKEN_URI = 'https://accounts.google.com/o/oauth2/token'

######################## BEGIN LICENSE BLOCK ########################
# The Original Code is Mozilla Communicator client code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from .mbcharsetprober import MultiByteCharSetProber
from .codingstatemachine import CodingStateMachine
from .chardistribution import Big5DistributionAnalysis
from .mbcssm import Big5SMModel


class Big5Prober(MultiByteCharSetProber):
    def __init__(self):
        MultiByteCharSetProber.__init__(self)
        self._mCodingSM = CodingStateMachine(Big5SMModel)
        self._mDistributionAnalyzer = Big5DistributionAnalysis()
        self.reset()

    def get_charset_name(self):
        return "Big5"

######################## BEGIN LICENSE BLOCK ########################
# The Original Code is mozilla.org code.
#
# The Initial Developer of the Original Code is
# Netscape Communications Corporation.
# Portions created by the Initial Developer are Copyright (C) 1998
# the Initial Developer. All Rights Reserved.
#
# Contributor(s):
#   Mark Pilgrim - port to Python
#
# This library is free software; you can redistribute it and/or
# modify it under the terms of the GNU Lesser General Public
# License as published by the Free Software Foundation; either
# version 2.1 of the License, or (at your option) any later version.
#
# This library is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
# Lesser General Public License for more details.
#
# You should have received a copy of the GNU Lesser General Public
# License along with this library; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA
# 02110-1301  USA
######################### END LICENSE BLOCK #########################

from . import constants
from .charsetprober import CharSetProber
from .codingstatemachine import CodingStateMachine
from .mbcssm import UTF8SMModel

ONE_CHAR_PROB = 0.5


class UTF8Prober(CharSetProber):
    def __init__(self):
        CharSetProber.__init__(self)
        self._mCodingSM = CodingStateMachine(UTF8SMModel)
        self.reset()

    def reset(self):
        CharSetProber.reset(self)
        self._mCodingSM.reset()
        self._mNumOfMBChar = 0

    def get_charset_name(self):
        return "utf-8"

    def feed(self, aBuf):
        for c in aBuf:
            codingState = self._mCodingSM.next_state(c)
            if codingState == constants.eError:
                self._mState = constants.eNotMe
                break
            elif codingState == constants.eItsMe:
                self._mState = constants.eFoundIt
                break
            elif codingState == constants.eStart:
                if self._mCodingSM.get_current_charlen() >= 2:
                    self._mNumOfMBChar += 1

        if self.get_state() == constants.eDetecting:
            if self.get_confidence() > constants.SHORTCUT_THRESHOLD:
                self._mState = constants.eFoundIt

        return self.get_state()

    def get_confidence(self):
        unlike = 0.99
        if self._mNumOfMBChar < 6:
            for i in range(0, self._mNumOfMBChar):
                unlike = unlike * ONE_CHAR_PROB
            return 1.0 - unlike
        else:
            return unlike

#!/usr/bin/env python
# -*- coding: utf-8 -*-

try:
    from cStringIO import StringIO
except ImportError:
    from StringIO import StringIO

from .. import KeyValueStore


class RedisStore(KeyValueStore):
    """Uses a redis-database as the backend.

    :param redis: An instance of :py:class:`redis.StrictRedis`.
    """

    def __init__(self, redis):
        self.redis = redis

    def _delete(self, key):
        return self.redis.delete(key)

    def keys(self):
        return self.redis.keys()

    def iter_keys(self):
        return iter(self.keys())

    def _has_key(self, key):
        return self.redis.exists(key)

    def _get(self, key):
        val = self.redis.get(key)

        if val == None:
            raise KeyError(key)
        return val

    def _get_file(self, key, file):
        file.write(self._get(key))

    def _open(self, key):
        return StringIO(self._get(key))

    def _put(self, key, value):
        self.redis.set(key, value)
        return key

    def _put_file(self, key, file):
        self._put(key, file.read())
        return key

from wtforms.fields.core import *

from wtforms.fields.simple import *

# Compatibility imports
from wtforms.fields.core import Label, Field, _unset_value, SelectFieldBase, Flags

from django import forms

from multiupload.fields import MultiFileField


class UploadTestForm(forms.Form):
    attachments = MultiFileField(
        min_num=1,
        max_num=3,
        max_file_size=1024*1024*5
    )

"""
This module defines the BubblyViewer class,
which wraps pyds9 to look at bubble catalogs.

Usage
-----

bv = BubblyViewer()

# open ds9, load l=35 data
bv.load_longitude(35)

# draw bubbles as circles
bv.outline(bubble_params(), color='green')

# pan/zoom to 5th entry
bv.look_at(bubble_params()[5])

#delete annotations
bv.clear()
"""

import ds9
import os
from bubbly.field import Field

__all__ = ['BubblyViewer']

class BubblyViewer(object):
    def __init__(self):
        self.ds9 = None

    def start(self):
        """Start ds9 if needed"""
        if self.ds9 is None:
            self.ds9 = ds9.ds9()

    def load_longitude(self, lon):
        """Load the image data associated with a given longitude"""
        self.start()

        f = Field(lon)
        g = os.path.join(f.path, 'registered', '%3.3i_i4.fits' % lon)
        r = os.path.join(f.path, 'registered', '%3.3i_mips.fits' % lon)
        self.ds9.set('frame delete')
        self.ds9.set('frame new rgb')
        self.ds9.set('rgb red')
        self.ds9.set('file %s' % r)
        self.ds9.set('rgb green')
        self.ds9.set('file %s' % g)

        self._set_zscale()
        self._align_galactic()

    def _set_zscale(self):
        self.ds9.set('rgb red')
        self.ds9.set('scale asinh')
        self.ds9.set('rgb green')
        self.ds9.set('scale asinh')

    def _align_galactic(self):
        self.ds9.set('wcs galactic')
        self.ds9.set('wcs skyformat degrees')

    def look_at(self, params):
        """Center on a specific bubble

        Parameters
        ----------
        params: Tuple
            A stamp description tuple of the form
            (lon_field, lon, lat, radius)

            This is returned by, e.g., bubble_params(),
            Field.all_stamps(), etc.
        """
        if self.ds9 is None:
            self.load_longitude(params[0])

        l, b = params[1:3]
        r = params[-1]

        #this is a hacky guess
        zoom = 2 / 3600. / r * 500

        self.ds9.set('pan to {l} {b} galactic'.format(l=l, b=b))
        self.ds9.set('zoom to %f' % zoom)

    def outline(self, params, color='blue'):
        """
        Display a list of stamps as circular regions

        Parameters
        ----------
        params : tuple, or list of tuples
            Stamp descriptions of the form (lon_field, lon, lat, radius)

        color : string
            A ds9-recognized color to use as the region outline
        """
        if not hasattr(params[0], '__len__'):
            params = [params]

        for i, p in enumerate(params):
            l, b, r = p[1:]
            self.ds9.set('regions',
                         'galactic; circle(%f,%f,%f)#color=%s text="%s"' %
                         (l, b, r, color, i))

    def clear(self):
        """
        Remove all annotations
        """
        self.ds9.set('regions delete all')

from HybridModel import HybridModel

class SynthModel(HybridModel):
    def __init__(self,configModel,utils,strTrial):
        self.tag        = configModel[0]
        self.mode       = configModel[1]
        self.misc       = configModel[2]
        self.trial      = strTrial
        self.masterTest = utils.TEST_IDS_PATH
        self.runTrain   = utils.SYNTH_BOOT_PATH \
                          + 'train_t' + strTrial
        self.runCV      = utils.SYNTH_BOOT_PATH \
                          + 'CV_t'  + strTrial
        self.runTest    = utils.SYNTH_ORIGINAL_PATH \
                          + 'test_t'  + strTrial
        self.predTest   = utils.SYNTH_PREDICT_PATH \
                          + 't' + strTrial
        self.bootCV     = self.runCV    + '_tmp'
        self.predCV     = utils.SYNTH_PREDICT_PATH \
                          + 'CV_t' + strTrial
        self.predTestTmp= self.predTest + '_tmp'
        self.predCVTmp  = self.predCV   + '_tmp'
        self.log        = utils.SYNTH_LOG_PATH + self.tag + '_t' \
                          + strTrial
        self.RMSEPath   = utils.SYNTH_RMSE_PATH+ self.tag + '_t' \
                          + strTrial
        self.setupRVars(utils)

"""
ID3 Helper for :mod:`songdetails.mp3details`.

"""
from tagger import ID3FrameException
from tagger import ID3v1
from tagger import ID3v2

__all__ = ["ID3TagDescriptor"]

# Pylint disable settings:
# ------------------------
# ToDos, DocStrings:
# pylint: disable-msg=W0511,W0105 
#
# Protected member access: 
# pylint: disable-msg=W0212
#
# Too many instance attributes, Too few public methods, Too many init arguments:
# pylint: disable-msg=R0902,R0903,R0913

def _genre_convert(genre):
    """If the genre is number, or something numbery, convert to string.
    
    Tries to lookup from known numeric genre table for the actual genre. Even in
    ID3v2 tags there seems to be numeric genres used by some players, such as:
    :const:`"(17)"` meaning Rock, number comes apparently from ID3v1 specs and
    parentheses for fun.
    
    http://en.wikipedia.org/wiki/ID3 and
    http://www.multimediasoft.com/amp3dj/help/amp3dj_00003e.htm#ss13.3
    
    :param genre: Genre, or genre number.
    :type genre: string, or int
    
    :return: Textual representation of genre number.
    :rtype: string
    :raise KeyError: Raised when genre is not found.
    
    """
    
    # Numeric genre
    genre_number = None
    
    # Even in ID3v2 tags some of them includes the genres such as: "(17)"
    # meaning probably same as in ID3v1 tags ("Rock").
    try:
        genre_number = int(genre.strip("()"))
    except (ValueError, TypeError):
        pass
    
    # We convert number to meaningful string.
    if genre_number is not None:
        try:
            return _NUMERIC_GENRES[genre_number]
        except KeyError:
            pass
    
    return genre


def _track_convert(track):
    """Converts track to number.
    
    :param track: Track number of the song.
    :type track: string, or int
    
    :raise ValueError: Raised if cannot be converted.
    
    :return: Number representing track I{order number} in album.
    :rtype: int
    
    """
    if '/' in track:
        return int(track.split("/")[0])
    return int(track)    


def _force_unicode(bstr, encoding, fallback_encodings=None):
    """Force unicode, ignore unknown.
    
    Forces the given string to unicode with first guessing, then forcing by
    using given encoding and ignoring unknown characters. This is sadly many
    times necessary, since there usually are only pieces of string without 
    proper encoding, such as file system file names where usually any bytes
    are accepted as filenames.
    
    :param bstr: String
    :type bstr: Basestring
    
    :param encoding: Assumed encoding, notice that by giving encoding that can
        decode all 8-bit characters such as ISO-8859-1 you effectively may be
        decoding all string regardless were they in that encoding or not.
    :type encoding: string
    
    :param fallback_encodings: Fallback on trying these encodings if not the
        assumed encoding.
    :type fallback_encodings: list of string
    
    :return: Unicoded given string
    :rtype: unicode string
    
    """
    # We got unicode, we give unicode
    if isinstance(bstr, unicode):
        return bstr
    
    if fallback_encodings is None:
        fallback_encodings = ['UTF-16', 'UTF-8', 'ISO-8859-1']
        
    encodings = [encoding] + fallback_encodings
    
    for enc in encodings:
        try:
            return bstr.decode(enc)
        except UnicodeDecodeError:
            pass
        
    # Finally, force the unicode
    return bstr.decode(encoding, 'ignore')


class ID3TagDescriptor(object):
    """ID3vTag descriptor"""
    def __init__(self, v24fid, v23fid=None, v22fid=None, v1fid=None,
                 converter=None):
        """Create id3v descriptor.
        
        :param v24fid: ID3v2.4 Frame ID, for example "TALB" for album.
        :type v24fid: string, or None
        
        :param v23fid: ID3v2.3 Frame ID, for example "TALB" for album.
        :type v23fid: string, or None
        
        :param v22fidd: ID3v2 Frame ID, for example "TAL" for album.
        :type v22fidd: string, or None
        
        :param v1fid: ID3v1 Frame "ID", for example "album".
        :type v1fid: string, or None
        
        :keyword converter: Value converter, converts the raw value to new type.
        :type converter: lambda oldValue: newValue, or None
        
        :see: L{ID3TagDescriptor.initialize_owner}
        
        """
        self.v1fid = v1fid
        self.v22fid = v22fid
        self.v23fid = v23fid
        self.v24fid = v24fid
        self.converter = converter or (lambda x: x)
    
    def _get_fid_by_version(self, instance):
        """Get FID For this version.
        
        :param instance: Instance of owner of this descriptor
        :return: Returns Frame ID used for setting and getting values. 
        :rtype: string
        
        """
        # Note that there exists two pytagger versions, other has id3v2 frame
        # version number as string, and other as floats.
        #
        # The one with string frame version numbers is most likely the newer
        # codebase.
        if instance._id3v2.version in (2.4, "2.4"):
            return self.v24fid
        elif instance._id3v2.version in (2.3, "2.3"):
            return self.v23fid
        elif instance._id3v2.version in (2.2, "2.2"):
            return self.v22fid
        else:
            return self.v24fid # TODO: CHECK!
    
    def __set__(self, instance, value):
        """Set value.
        
        :param instance: Instance of owner.
        :type instance: object
        
        :param value: New value.
        :type value: unicode string
        
        """
        if not hasattr(instance, "_id3v2"):
            return
        
        fid = self._get_fid_by_version(instance)
        if fid is None:
            return
        
        new_frame = instance._id3v2.new_frame(fid)
        new_frame.set_text(_force_unicode(value, 'utf-16'))
        
        # Remove existing (only first)
        if instance._id3v2_frames.has_key(fid):
            instance._id3v2.frames.remove(instance._id3v2_frames[fid][0])
            del instance._id3v2_frames[fid][0]
        
        instance._id3v2.frames.append(new_frame)
        instance._id3v2_frames.setdefault(fid, [])
        instance._id3v2_frames[fid].append(new_frame)
    
    def __get__(self, instance, instance_class=None): #@UnusedVariable
        """Get value.
        
        :param instance: Instance of owner.
        :type instance: object
        
        :param instance_class: Class of instance?
        :type instance_class: class
         
        """
        
        if instance is None:
            return None
        
        # First priority, get id3v2 frame item if found
        try:
            return self.converter(self._get_id3v2(instance))
        except ValueError:
            pass
        
        # Second priority, get id3v1 frame item if found
        try:
            return self.converter(self._get_id3v1(instance))
        except ValueError:
            pass
        
        # Nothing was found, we still have to return None.
        return None
        
    def _get_id3v1(self, instance):
        """Get ID3v1 value.
        
        :param instance: Instance having descriptor.
        :type instance: object.
        
        :raise ValueError: Raised when value cannot be retrieved.
        
        """
        try:
            return _force_unicode(getattr(instance._id3v1, self.v1fid),
                                    'ISO-8859-1')
        except AttributeError:
            raise ValueError('Cannot find the id3v1 frame, or frame ID.')

    def _get_id3v2(self, instance):
        """Get ID3v2 value.
        
        :param instance: Instance having descriptor.
        :type instance: object.
        
        :raise ValueError: Raised when value cannot be retrieved.
        
        """
        
        id3v2_frames = instance._id3v2_frames
        
        fid = self._get_fid_by_version(instance)
        
        try:
            # Parse field
            first_frame = id3v2_frames[fid][0]
        except KeyError:
            pass
        else:
            first_frame.parse_field()
            first_string = first_frame.strings[0].replace("\x00", "")
            return _force_unicode(first_string, first_frame.encoding)
            
        raise ValueError('Cannot find the id3v2 frame, or frame ID.')
    
    @classmethod
    def initialize_owner(cls, instance, filepath, force=False):
        """Initializes the owner of this descriptor for using the descriptor.
        
        :param instance: Owner instance.
        :type instance: object
        
        :param filepath: File path to MP3.
        :type filepath: string
        
        :param force: Force re-initialization, rewriting the values.
        :type force: bool
        
        """
        if not hasattr(instance, "_id3v1") or force:     
            # Parse ID3v1:
            instance._id3v1 = ID3v1(filepath)
        
        if not hasattr(instance, "_id3v2") or force:
            # Parse ID3v2
            instance._id3v2 = ID3v2(filepath)
        
        if not hasattr(instance, "_id3v2_frames") or force:
            # ID3v2 Frames:
            instance._id3v2_frames = {}
            for frame in instance._id3v2.frames:
                instance._id3v2_frames.setdefault(frame.fid, [])
                instance._id3v2_frames[frame.fid].append(frame)
    
    @classmethod
    def save(cls, instance):
        """Saves the changes in instance.
        
        :param instance: Owner instance.
        :type instance: object
        
        """        
        instance._id3v2.commit()


_NUMERIC_GENRES = {
    0 : "Blues",
    1 : "Classic Rock",
    2 : "Country",
    3 : "Dance",
    4 : "Disco",
    5 : "Funk",
    6 : "Grunge",
    7 : "Hip-Hop",
    8 : "Jazz",
    9 : "Metal",
    10 : "New Age",
    11 : "Oldies",
    12 : "Other",
    13 : "Pop",
    14 : "R&B",
    15 : "Rap",
    16 : "Reggae",
    17 : "Rock",
    18 : "Techno",
    19 : "Industrial",
    20 : "Alternative",
    21 : "Ska",
    22 : "Death Metal",
    23 : "Pranks",
    24 : "Soundtrack",
    25 : "Euro-Techno",
    26 : "Ambient",
    27 : "Trip-Hop",
    28 : "Vocal",
    29 : "Jazz+Funk",
    30 : "Fusion",
    31 : "Trance",
    32 : "Classical",
    33 : "Instrumental",
    34 : "Acid",
    35 : "House",
    36 : "Game",
    37 : "Sound Clip",
    38 : "Gospel",
    39 : "Noise",
    40 : "Alternative Rock",
    41 : "Bass",
    42 : "Soul",
    43 : "Punk",
    44 : "Space",
    45 : "Meditative",
    46 : "Instrumental Pop",
    47 : "Instrumental Rock",
    48 : "Ethnic",
    49 : "Gothic",
    50 : "Darkwave",
    51 : "Techno-Industrial",
    52 : "Electronic",
    53 : "Pop-Folk",
    54 : "Eurodance",
    55 : "Dream",
    56 : "Southern Rock",
    57 : "Comedy",
    58 : "Cult",
    59 : "Gangsta",
    60 : "Top 40",
    61 : "Christian Rap",
    62 : "Pop/Funk",
    63 : "Jungle",
    64 : "Native US",
    65 : "Cabaret",
    66 : "New Wave",
    67 : "Psychadelic",
    68 : "Rave",
    69 : "Showtunes",
    70 : "Trailer",
    71 : "Lo-Fi",
    72 : "Tribal",
    73 : "Acid Punk",
    74 : "Acid Jazz",
    75 : "Polka",
    76 : "Retro",
    77 : "Musical",
    78 : "Rock & Roll",
    79 : "Hard Rock",
    80 : "Folk",
    81 : "Folk-Rock",
    82 : "National Folk",
    83 : "Swing",
    84 : "Fast Fusion",
    85 : "Bebob",
    86 : "Latin",
    87 : "Revival",
    88 : "Celtic",
    89 : "Bluegrass",
    90 : "Avantgarde",
    91 : "Gothic Rock",
    92 : "Progressive Rock",
    93 : "Psychedelic Rock",
    94 : "Symphonic Rock",
    95 : "Slow Rock",
    96 : "Big Band",
    97 : "Chorus",
    98 : "Easy Listening",
    99 : "Acoustic",
    100 : "Humour",
    101 : "Speech",
    102 : "Chanson",
    103 : "Opera",
    104 : "Chamber Music",
    105 : "Sonata",
    106 : "Symphony",
    107 : "Booty Bass",
    108 : "Primus",
    109 : "Porn Groove",
    110 : "Satire",
    111 : "Slow Jam",
    112 : "Club",
    113 : "Tango",
    114 : "Samba",
    115 : "Folklore",
    116 : "Ballad",
    117 : "Power Ballad",
    118 : "Rhythmic Soul",
    119 : "Freestyle",
    120 : "Duet",
    121 : "Punk Rock",
    122 : "Drum Solo",
    123 : "Acapella",
    124 : "Euro-House",
    125 : "Dance Hall",
    126 : "Goa",
    127 : "Drum & Bass",
    128 : "Club - House",
    129 : "Hardcore",
    130 : "Terror",
    131 : "Indie",
    132 : "BritPop",
    133 : "Negerpunk",
    134 : "Polsk Punk",
    135 : "Beat",
    136 : "Christian Gangsta Rap", # My favorite!
    137 : "Heavy Metal",
    138 : "Black Metal",
    139 : "Crossover",
    140 : "Contemporary Christian",
    141 : "Christian Rock",
    142 : "Merengue",
    143 : "Salsa",
    144 : "Thrash Metal",
    145 : "Anime",
    146 : "JPop",
    147 : "Synthpop",
}

import unittest
from JustReleaseNotes.artifacters import GitHubReleases
import requests
import requests_mock
import sys

class GitHubReleases_Test(unittest.TestCase):

  def setUp(self):
    self.__stdoutSaved = sys.stdout
    try:
        from StringIO import StringIO
    except ImportError:
        from io import StringIO
    self.__out = StringIO()
    sys.stdout = self.__out

  def tearDown(self):
    sys.stdout = self.__stdoutSaved

  def test_retrievePromotedVersionsContainsValidVersions(self):
    requests.packages.urllib3.disable_warnings()
    fileContents = '[{"name": "2.0.1.153", "published_at": "2015-04-24T15:24:29Z"},' \
                   '{"name": "1.0.0.15", "published_at": "2015-03-19T23:19:08Z"}]'

    config = { "Authorization" : "token 5dbf862c5197414138e70c4f3fb458c5f5a58f05",
               "Provider" : "GitHubReleases",
               "Url" : "https://api.github.com/repos/cimpress-mcp/PostalCodes.Net/releases"  }

    artifacter = GitHubReleases.GitHubReleases(config);
    with requests_mock.mock() as m:
        m.get('https://api.github.com/repos/cimpress-mcp/PostalCodes.Net/releases', text=fileContents)
        promotedVersion = artifacter.retrievePromotedVersions()
        self.assertIn("1.0.0.15", promotedVersion)
        self.assertIn("2.0.1.153", promotedVersion)
        self.assertTrue(2, len(promotedVersion))
    sys.stdout = self.__stdoutSaved
    self.assertEquals('GitHub Releases: Retrieving promoted from GitHubReleases at https://api.github.com/repos/cimpress-mcp/PostalCodes.Net/releases ...\n'
                      'GitHub Releases: Found 2 promoted versions\n',
                      self.__out.getvalue())

  def test_retrievePromotedVersionsFromEmptyArrayRaises(self):
    requests.packages.urllib3.disable_warnings()
    fileContents = '[]'

    config = { "Authorization" : "token 5dbf862c5197414138e70c4f3fb458c5f5a58f05",
               "Provider" : "GitHubReleases",
               "Url" : "https://api.github.com/repos/cimpress-mcp/PostalCodes.Net/releases"  }

    artifacter = GitHubReleases.GitHubReleases(config);
    with requests_mock.mock() as m:
        m.get('https://api.github.com/repos/cimpress-mcp/PostalCodes.Net/releases', text=fileContents)
        with self.assertRaises(ValueError):
            artifacter.retrievePromotedVersions()

if __name__ == '__main__':
    unittest.main()
#!/usr/bin/env python
# This script updates the allowed address pairs in Neutron with the
# 'neutron port-update' command. This is required by Calico in OpenStack,
# otherwise BGP will not be working. We query OpenStack API directly to prevent
# installing any dependencies such as python-neutronclient.
#
# USAGE: script_name arg1 arg2...argN
# arg1 - Calico network, i.e. 192.168.0.0/24
# arg2...argN - VMs MAC addresses
#
# Script exit codes (for Ansible)
# 0 - port has been updated
# 1 - error
# 2 - no update to port [default]

import json
import os
import requests
import sys

def credentials():
    """Retrieves credentials"""

    username = os.environ.get('OS_USERNAME')
    password = os.environ.get('OS_PASSWORD')
    tenant_name = os.environ.get('OS_TENANT_NAME')
    auth_url = os.environ.get('OS_AUTH_URL')

    if not all((username, password, tenant_name, auth_url)):
        sys.stderr.write("ERROR: Unable to get Keystone credentials\n")
        exit(1)

    return {
        'username': username,
        'password': password,
        'tenant_name': tenant_name,
        'auth_url': auth_url
    }

def get_catalog():
    """Get service catalog from Keystone with token and all endpoints"""

    creds = credentials()
    headers = {'Content-Type': 'application/json'}
    payload = {
                "auth":
                  {
                    "tenantName": creds['tenant_name'],
                    "passwordCredentials": {
                                             "username": creds['username'],
                                             "password": creds['password']
                                           }
                  }
              }
    auth_url = creds['auth_url'] + "/tokens"
    r = requests.post(auth_url, headers=headers, data=json.dumps(payload))

    parsed_json = json.loads(r.text)
    if not parsed_json or 'error' in parsed_json:
        sys.stderr.write("ERROR: Unable to get authentication token\n")
        exit(1)

    return parsed_json

def get_token(catalog):
    """Get Keystone authentication token"""

    return catalog['access']['token']['id']

def neutron_public_url(catalog):
    """Get Neutron publicURL"""

    for i in catalog['access']['serviceCatalog']:
        if i['name'] == 'neutron':
            for endpoint in i['endpoints']:
                return endpoint['publicURL']

def list_ports(token, public_url):
    """List Neutron ports"""

    headers = {'X-Auth-Token': token}
    auth_url = public_url + "v2.0/ports"
    r = requests.get(auth_url, headers=headers)

    if r.text:
        parsed_json = json.loads(r.text)
        return parsed_json['ports']
    else:
        sys.stderr.write("ERROR: Unable to retrieve Neutron ports list\n")
        exit(1)

def update_port(token, public_url, port_id, mac_address, calico_network):
    """Update Neutron port with the allowed address pairs"""

    headers = {'Content-Type': 'application/json', 'X-Auth-Token': token}
    payload = {
                "port": {
                          "allowed_address_pairs": [
                             {
                               "ip_address": calico_network,
                               "mac_address": mac_address
                             }
                          ]
                        }
              }
    auth_url = public_url + "v2.0/ports/" + port_id
    r = requests.put(auth_url, headers=headers, data=json.dumps(payload))

    parsed_json = json.loads(r.text)
    if r.status_code != 200 or 'NeutronError' in parsed_json:
        sys.stderr.write("ERROR: Unable to update port: %s\n" % parsed_json['NeutronError'])
        exit(1)
    else:
        return r.status_code

if __name__ == "__main__":

    if len(sys.argv) < 3:
        sys.stderr.write("ERROR: Please run script with the correct arguments\n")
        exit(1)

    calico_network = sys.argv[1]
    vms_mac_addresses = sys.argv[2:]

    catalog = get_catalog()
    token = get_token(catalog)
    public_url = neutron_public_url(catalog)
    ports = list_ports(token, public_url)

    exit_code = 0 # no update to port

    for port in ports:
        port_id = port['id']
        mac_address = port['mac_address']
        if mac_address in vms_mac_addresses and not port['allowed_address_pairs']:
            status_code = update_port(token, public_url, port_id, mac_address, calico_network)
            if status_code == 200:
                exit_code = 2 # port has been updated

    exit(exit_code)

# Getting started with APIC-EM APIs 
# Follows APIC-EM Basics Learning Lab
# Hello World with JSON and pretty printing

# * THIS SAMPLE APPLICATION AND INFORMATION IS PROVIDED "AS IS" WITHOUT WARRANTY
# * OF ANY KIND BY CISCO, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED
# * TO THE IMPLIED WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR
# * PURPOSE, NONINFRINGEMENT, SATISFACTORY QUALITY OR ARISING FROM A COURSE OF
# * DEALING, LAW, USAGE, OR TRADE PRACTICE. CISCO TAKES NO RESPONSIBILITY
# * REGARDING ITS USAGE IN AN APPLICATION, AND IT IS PRESENTED ONLY AS AN
# * EXAMPLE. THE SAMPLE CODE HAS NOT BEEN THOROUGHLY TESTED AND IS PROVIDED AS AN
# * EXAMPLE ONLY, THEREFORE CISCO DOES NOT GUARANTEE OR MAKE ANY REPRESENTATIONS
# * REGARDING ITS RELIABILITY, SERVICEABILITY, OR FUNCTION. IN NO EVENT DOES
# * CISCO WARRANT THAT THE SOFTWARE IS ERROR FREE OR THAT CUSTOMER WILL BE ABLE
# * TO OPERATE THE SOFTWARE WITHOUT PROBLEMS OR INTERRUPTIONS. NOR DOES CISCO
# * WARRANT THAT THE SOFTWARE OR ANY EQUIPMENT ON WHICH THE SOFTWARE IS USED WILL
# * BE FREE OF VULNERABILITY TO INTRUSION OR ATTACK. THIS SAMPLE APPLICATION IS
# * NOT SUPPORTED BY CISCO IN ANY MANNER. CISCO DOES NOT ASSUME ANY LIABILITY
# * ARISING FROM THE USE OF THE APPLICATION. FURTHERMORE, IN NO EVENT SHALL CISCO
# * OR ITS SUPPLIERS BE LIABLE FOR ANY INCIDENTAL OR CONSEQUENTIAL DAMAGES, LOST
# * PROFITS, OR LOST DATA, OR ANY OTHER INDIRECT DAMAGES EVEN IF CISCO OR ITS
# * SUPPLIERS HAVE BEEN INFORMED OF THE POSSIBILITY THEREOF.-->

# import the requests library so we can use it to make REST calls (http://docs.python-requests.org/en/latest/index.html)
import requests

# import the json library.  This provides many handy features for formatting, displaying
# and manipulating json.  https://docs.python.org/2/library/json.html
import json

# All of our REST calls will use the url for the APIC EM Controller as the base URL
# So lets define a variable for the controller IP or DNS so we don't have to keep typing it
controller_url = "https://sandboxapic.cisco.com"

# Get Devices
# This function allows you to view a list of all the devices in the network(routers and switches).
get_devices_url = controller_url + '/api/v0/network-device/1/3'

#Perform GET on get_devices_url
get_devices_response = requests.get(get_devices_url, verify=False)

# The json method of the response object returned by requests.get returns the request body in json format
get_devices_json = get_devices_response.json()

# json.dumps serializes the json into a string and allows us to
# print the response in a 'pretty' format with indentation etc.
print ("Devices = ")
print (json.dumps(get_devices_json, indent=4, separators=(',', ': ')))


# Coding 205 Example
# This example retrieves a list of network devices using the APIC-EM APIs
# Then we write the network device ID and type for each device out to a file.


# * THIS SAMPLE APPLICATION AND INFORMATION IS PROVIDED "AS IS" WITHOUT WARRANTY
# * OF ANY KIND BY CISCO, EITHER EXPRESSED OR IMPLIED, INCLUDING BUT NOT LIMITED
# * TO THE IMPLIED WARRANTIES OF MERCHANTABILITY FITNESS FOR A PARTICULAR
# * PURPOSE, NONINFRINGEMENT, SATISFACTORY QUALITY OR ARISING FROM A COURSE OF
# * DEALING, LAW, USAGE, OR TRADE PRACTICE. CISCO TAKES NO RESPONSIBILITY
# * REGARDING ITS USAGE IN AN APPLICATION, AND IT IS PRESENTED ONLY AS AN
# * EXAMPLE. THE SAMPLE CODE HAS NOT BEEN THOROUGHLY TESTED AND IS PROVIDED AS AN
# * EXAMPLE ONLY, THEREFORE CISCO DOES NOT GUARANTEE OR MAKE ANY REPRESENTATIONS
# * REGARDING ITS RELIABILITY, SERVICEABILITY, OR FUNCTION. IN NO EVENT DOES
# * CISCO WARRANT THAT THE SOFTWARE IS ERROR FREE OR THAT CUSTOMER WILL BE ABLE
# * TO OPERATE THE SOFTWARE WITHOUT PROBLEMS OR INTERRUPTIONS. NOR DOES CISCO
# * WARRANT THAT THE SOFTWARE OR ANY EQUIPMENT ON WHICH THE SOFTWARE IS USED WILL
# * BE FREE OF VULNERABILITY TO INTRUSION OR ATTACK. THIS SAMPLE APPLICATION IS
# * NOT SUPPORTED BY CISCO IN ANY MANNER. CISCO DOES NOT ASSUME ANY LIABILITY
# * ARISING FROM THE USE OF THE APPLICATION. FURTHERMORE, IN NO EVENT SHALL CISCO
# * OR ITS SUPPLIERS BE LIABLE FOR ANY INCIDENTAL OR CONSEQUENTIAL DAMAGES, LOST
# * PROFITS, OR LOST DATA, OR ANY OTHER INDIRECT DAMAGES EVEN IF CISCO OR ITS
# * SUPPLIERS HAVE BEEN INFORMED OF THE POSSIBILITY THEREOF.-->

# import the requests library so we can use it to make REST calls (http://docs.python-requests.org/en/latest/index.html)
import requests

# import the json library.  This library gives us many handy features for formatting, displaying
# and manipulating json.
import json

# All of our REST calls will use the url for the APIC EM Controller as the base URL
# So lets define a variable for the controller IP or DNS so we don't have to keep typing it
controller_url = "https://sandboxapic.cisco.com:9443"

#the username and password to access the APIC-EM Controller
payload = {"username":"admin","password":"C!sc0123"}

ticket_url = controller_url + "/api/v1/ticket"

#Content type must be included in the header
header = {"content-type": "application/json"}

#Performs a POST on the specified url to get the service ticket
response= requests.post(ticket_url,data=json.dumps(payload), headers=header, verify=False)
	
#convert response to json format
r_json=response.json()

#parse the json to get the service ticket
ticket = r_json["response"]["serviceTicket"]

# Get Devices
# This function allows you to view a list of 3 of the devices in the network(routers and switches).
get_devices_url = controller_url + '/api/v1/network-device/1/3'

#Content type as well as the ticket must be included in the header 
header = {"content-type": "application/json", "X-Auth-Token":ticket}
	
#Perform GET on get_devices_url
get_devices_response = requests.get(get_devices_url, headers=header, verify=False)

# The json method of the response object returned by requests.get returns the request body in json format
get_devices_json = get_devices_response.json()

#Now let's read and display some specific information from the json

# set our parent as the top level response object
parent =  get_devices_json["response"]

print ("Devices = ")
# you can open the file using 'with'.
# 'with' gives you better exception handling and when you use 'with' the file automatically be closed
with open("list-of-devices.txt", "w") as file:
    # for each device returned, write the networkDeviceId and type value to the file
    for item in parent:
        device="id = " + item["id"] + " type = " + item["type"]
        file.write (device + "\n")
        print(device)
		 



"""
Copyright 2015, Cisco Systems, Inc

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

@author: Pravin Gohite, Cisco Systems, Inc.
"""

import os
import logging
import lxml.etree as ET


class Cxml:
    def __init__(self, filename):
        self.filename = filename
        self.modulename = os.path.splitext(os.path.basename(filename))[0]
        if os.path.exists(filename):
            logging.debug('Parsing %s' % filename)
            try:
                self.cxml = ET.parse(filename)
            except:
                self.cxml = None
                logging.error('ET Failed to parse %s' % filename)
        else:
            self.cxml = None
            logging.error('File %s does not exists' % filename)

    def toxpath(self, path):
        path_elems = path.split('/')
        xpath = '[@name="%s"]' % path_elems[0]
        for elem in path_elems[1:]:
            xpath += '/node[@name="%s"]' % elem
        return xpath

    def get_lazy_node_internal(self, cxml_element, base=''):
        node = ET.Element('node')
        add_placeholder = True

        # Add attributes from cxml node
        for attrib in cxml_element.attrib:
            node.set(attrib, cxml_element.attrib[attrib])
            # Terminal nodes does not requires to lazy nodes.
            if (attrib == 'type' and cxml_element.attrib[attrib] in \
                ['leaf', 'leafref', 'leaf-list']):
                add_placeholder = False
        try:
            desc = cxml_element.find('description')
            if desc is not None:
                node.set('description', desc.text.strip())
        except:
            pass

        if base == '':
            node.set('path', self.modulename)
        else:
            base += '/'
            node.set('path', base + cxml_element.get('name'))

        if add_placeholder:
            pnode = ET.Element('node')
            pnode.set('name', 'Loading ..')
            pnode.set('type', '__yang_placeholder')
            node.append(pnode)

        return node

    def get_lazy_node(self, path='', add_ns=True):
        """
        Returns yang explorer compatible lazy node xml. A lazy
        node only returns a cxml node which is requested. All
        other node along the path returned as _placeholder_
        nodes for on-demand loading in client tree.
        """
        logging.debug('get_lazy_node: ' + path)
        root = ET.Element('root')
        if self.cxml is None:
            return root

        cxml_root = self.cxml.getroot()

        if path == '':
            node = self.get_lazy_node_internal(cxml_root)
            nslist = [c.get('prefix') + ',' + c.text for c in cxml_root if c.tag == 'namespace']
            node.set('namespaces', '|'.join(nslist))
            node.set('name', self.modulename)
            root.append(node)
            return root

        # move root node to requested node
        elements = path.split('/')
        for name in elements[1:]:
            for child in cxml_root:
                if child.get('name', '') == name:
                    cxml_root = child
                    break

        for child in cxml_root:
            if child.tag == 'node':
                node = self.get_lazy_node_internal(child, path)
                root.append(node)

            if child.tag == 'namespace' and add_ns:
                if cxml_root.get('prefix', '') == child.get('prefix'):
                    child.set('default', 'true')
                root.append(child)
        return root

    def get_lazy_tree_one(self, path, value):
        """
        Returns yang explorer compatible lazy tree xml. A lazy
        tree  returns a cxml nested tree from root to requested
        node.

        Other node along the path returned as _placeholder_
        nodes for on-demand loading in client tree.
        """

        tree = None
        path_elems = path.split('/')
        subpath = xpath = ''

        for elems in path_elems:
            nodes = self.get_lazy_node(subpath)
            if tree is None:
                tree = nodes.find('node')
                xpath = '[@name="%s"]' % elems
                logging.info(ET.tostring(tree))
            else:
                subpath += '/'

                temp = tree.find(xpath)
                if temp is not None:
                    tree.find(xpath).remove(tree.find(xpath)[0])
                    for child in nodes:
                        if child.get('path') == path:
                            child.set('value', value)
                        tree.find(xpath).append(child)

                xpath += '/node[@name="%s"]' % elems
            subpath += elems

        return tree

    def get_lazy_tree(self, pathvalues):
        """
        Returns yang explorer compatible lazy tree xml. A lazy
        tree  returns a cxml nested tree from root to requested
        node.

        Other node along the path returned as _placeholder_
        nodes for on-demand loading in client tree.
        """

        logging.debug('get_lazy_tree: Building lazy tree..')

        plist = []
        vdict = {}
        for (path, value) in pathvalues:
            plist.append(path.split('/'))
            vdict[path] = value

        level = 0
        logging.info(str(plist))

        tree = self.get_lazy_node()
        tree = tree[0]

        while True:
            pending = []
            for path_elems in plist:
                if level >= len(path_elems):
                    continue

                cxpath = '/'.join(path_elems[:level + 1])
                if cxpath not in pending:
                    pending.append(cxpath)

            if len(pending) == 0:
                break

            for cxpath in pending:
                subtree = self.get_lazy_node(cxpath, False)
                xpath = self.toxpath(cxpath)

                if len(subtree) == 0:
                    continue

                tree.find(xpath).remove(tree.find(xpath)[0])
                for child in subtree:
                    cpath = child.get('path', '')
                    values = vdict.get(cpath, '')
                    if values is not None:
                        for key in values:
                            child.set(key, values[key])
                    tree.find(xpath).append(child)
            level += 1
        # end while

        return tree

    def get_namespaces(self):
        if self.cxml is None:
            return []

        return [(ns.get('prefix', ''), ns.get('module', ''), ns.text)
                for ns in self.cxml.getroot() if ns.tag == 'namespace']

# Copyright 2012 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2012 Nebula, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
"""
Context processors used by Horizon.
"""

from horizon import conf


def horizon(request):
    """The main Horizon context processor. Required for Horizon to function.

    It adds the Horizon config to the context as well as setting the names
    ``True`` and ``False`` in the context to their boolean equivalents
    for convenience.

    .. warning::

        Don't put API calls in context processors; they will be called once
        for each template/template fragment which takes context that is used
        to render the complete output.
    """
    context = {"HORIZON_CONFIG": conf.HORIZON_CONFIG,
               "True": True,
               "False": False}

    return context

# vim: tabstop=4 shiftwidth=4 softtabstop=4

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from django.template import base
from django.template import defaultfilters
from django.utils import safestring

register = base.Library()


@register.filter(is_safe=True)
@defaultfilters.stringfilter
def shellfilter(value):
    """Replace HTML chars for shell usage."""
    replacements = {'\\': '\\\\',
                   '`': '\`',
                   "'": "\\'",
                   '"': '\\"'}
    for search, repl in replacements.items():
        value = value.replace(search, repl)
    return safestring.mark_safe(value)

# Copyright 2012 OpenStack Foundation
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import time

from django.conf import settings

from django.http import HttpResponseRedirect  # noqa

from horizon import exceptions
from horizon import middleware
from horizon.test import helpers as test


class MiddlewareTests(test.TestCase):
    def test_redirect_login_fail_to_login(self):
        url = settings.LOGIN_URL
        request = self.factory.post(url)

        mw = middleware.HorizonMiddleware()
        resp = mw.process_exception(request, exceptions.NotAuthorized())
        resp.client = self.client

        self.assertRedirects(resp, url)

    def test_session_timeout(self):
        requested_url = '/project/instances/'
        request = self.factory.get(requested_url)
        try:
            timeout = settings.SESSION_TIMEOUT
        except AttributeError:
            timeout = 1800
        request.session['last_activity'] = int(time.time()) - (timeout + 10)
        mw = middleware.HorizonMiddleware()
        resp = mw.process_request(request)
        self.assertEqual(302, resp.status_code)
        self.assertEqual(requested_url, resp.get('Location'))

    def test_process_response_redirect_on_ajax_request(self):
        url = settings.LOGIN_URL
        mw = middleware.HorizonMiddleware()

        request = self.factory.post(url,
                                    HTTP_X_REQUESTED_WITH='XMLHttpRequest')
        request.META['HTTP_X_REQUESTED_WITH'] = 'XMLHttpRequest'
        request.horizon = {'async_messages':
                           [('error', 'error_msg', 'extra_tag')]}

        response = HttpResponseRedirect(url)
        response.client = self.client

        resp = mw.process_response(request, response)
        self.assertEqual(200, resp.status_code)
        self.assertEqual(url, resp['X-Horizon-Location'])

# Copyright 2012 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2012 OpenStack Foundation
# Copyright 2012 Nebula, Inc.
# Copyright (c) 2012 X.commerce, a business unit of eBay Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from __future__ import absolute_import

import logging

from django.conf import settings
from django.utils.translation import pgettext_lazy
from django.utils.translation import ugettext_lazy as _

from cinderclient.v2.contrib import list_extensions as cinder_list_extensions

from horizon import exceptions
from horizon.utils.memoized import memoized  # noqa

from openstack_dashboard.api import base
from openstack_dashboard.api import nova

LOG = logging.getLogger(__name__)


# API static values
VOLUME_STATE_AVAILABLE = "available"
DEFAULT_QUOTA_NAME = 'default'

# Available consumer choices associated with QOS Specs
CONSUMER_CHOICES = (
    ('back-end', _('back-end')),
    ('front-end', _('front-end')),
    ('both', pgettext_lazy('Both of front-end and back-end', u'both')),
)

VERSIONS = base.APIVersionManager("volume", preferred_version=2)

try:
    from cinderclient.v2 import client as cinder_client_v2
    VERSIONS.load_supported_version(2, {"client": cinder_client_v2,
                                        "version": 2})
except ImportError:
    pass


class BaseCinderAPIResourceWrapper(base.APIResourceWrapper):

    @property
    def name(self):
        # If a volume doesn't have a name, use its id.
        return (getattr(self._apiresource, 'name', None) or
                getattr(self._apiresource, 'display_name', None) or
                getattr(self._apiresource, 'id', None))

    @property
    def description(self):
        return (getattr(self._apiresource, 'description', None) or
                getattr(self._apiresource, 'display_description', None))


class Volume(BaseCinderAPIResourceWrapper):

    _attrs = ['id', 'name', 'description', 'size', 'status', 'created_at',
              'volume_type', 'availability_zone', 'imageRef', 'bootable',
              'snapshot_id', 'source_volid', 'attachments', 'tenant_name',
              'os-vol-host-attr:host', 'os-vol-tenant-attr:tenant_id',
              'metadata', 'volume_image_metadata', 'encrypted']

    @property
    def is_bootable(self):
        return self.bootable == 'true'


class VolumeSnapshot(BaseCinderAPIResourceWrapper):

    _attrs = ['id', 'name', 'description', 'size', 'status',
              'created_at', 'volume_id',
              'os-extended-snapshot-attributes:project_id']


class VolumeType(BaseCinderAPIResourceWrapper):

    _attrs = ['id', 'name', 'extra_specs', 'created_at',
              'os-extended-snapshot-attributes:project_id']


class VolumeBackup(BaseCinderAPIResourceWrapper):

    _attrs = ['id', 'name', 'description', 'container', 'size', 'status',
              'created_at', 'volume_id', 'availability_zone']
    _volume = None

    @property
    def volume(self):
        return self._volume

    @volume.setter
    def volume(self, value):
        self._volume = value


class VolTypeExtraSpec(object):
    def __init__(self, type_id, key, val):
        self.type_id = type_id
        self.id = key
        self.key = key
        self.value = val


class QosSpec(object):
    def __init__(self, id, key, val):
        self.id = id
        self.key = key
        self.value = val


@memoized
def cinderclient(request):
    api_version = VERSIONS.get_active_version()

    insecure = getattr(settings, 'OPENSTACK_SSL_NO_VERIFY', False)
    cacert = getattr(settings, 'OPENSTACK_SSL_CACERT', None)
    cinder_url = ""
    try:
        # The cinder client assumes that the v2 endpoint type will be
        # 'volumev2'.
        if api_version['version'] == 2:
            try:
                cinder_url = base.url_for(request, 'volumev2')
            except exceptions.ServiceCatalogException:
                LOG.warning("Cinder v2 requested but no 'volumev2' service "
                            "type available in Keystone catalog.")
    except exceptions.ServiceCatalogException:
        LOG.debug('no volume service configured.')
        raise
    c = api_version['client'].Client(request.user.username,
                                     request.user.token.id,
                                     project_id=request.user.tenant_id,
                                     auth_url=cinder_url,
                                     insecure=insecure,
                                     cacert=cacert,
                                     http_log_debug=settings.DEBUG)
    c.client.auth_token = request.user.token.id
    c.client.management_url = cinder_url
    return c


def _replace_v2_parameters(data):
    if VERSIONS.active < 2:
        data['display_name'] = data['name']
        data['display_description'] = data['description']
        del data['name']
        del data['description']
    return data


def version_get():
    api_version = VERSIONS.get_active_version()
    return api_version['version']


def volume_list(request, search_opts=None):
    """To see all volumes in the cloud as an admin you can pass in a special
    search option: {'all_tenants': 1}
    """
    c_client = cinderclient(request)
    if c_client is None:
        return []
    return [Volume(v) for v in c_client.volumes.list(search_opts=search_opts)]


def volume_get(request, volume_id):
    volume_data = cinderclient(request).volumes.get(volume_id)

    for attachment in volume_data.attachments:
        if "server_id" in attachment:
            instance = nova.server_get(request, attachment['server_id'])
            attachment['instance_name'] = instance.name
        else:
            # Nova volume can occasionally send back error'd attachments
            # the lack a server_id property; to work around that we'll
            # give the attached instance a generic name.
            attachment['instance_name'] = _("Unknown instance")
    return Volume(volume_data)


def volume_create(request, size, name, description, volume_type,
                  snapshot_id=None, metadata=None, image_id=None,
                  availability_zone=None, source_volid=None):
    data = {'name': name,
            'description': description,
            'volume_type': volume_type,
            'snapshot_id': snapshot_id,
            'metadata': metadata,
            'imageRef': image_id,
            'availability_zone': availability_zone,
            'source_volid': source_volid}
    data = _replace_v2_parameters(data)

    volume = cinderclient(request).volumes.create(size, **data)
    return Volume(volume)


def volume_extend(request, volume_id, new_size):
    return cinderclient(request).volumes.extend(volume_id, new_size)


def volume_delete(request, volume_id):
    return cinderclient(request).volumes.delete(volume_id)


def volume_retype(request, volume_id, new_type, migration_policy):
    return cinderclient(request).volumes.retype(volume_id,
                                                new_type,
                                                migration_policy)


def volume_update(request, volume_id, name, description):
    vol_data = {'name': name,
                'description': description}
    vol_data = _replace_v2_parameters(vol_data)
    return cinderclient(request).volumes.update(volume_id,
                                                **vol_data)


def volume_reset_state(request, volume_id, state):
    return cinderclient(request).volumes.reset_state(volume_id, state)


def volume_upload_to_image(request, volume_id, force, image_name,
                           container_format, disk_format):
    return cinderclient(request).volumes.upload_to_image(volume_id,
                                                         force,
                                                         image_name,
                                                         container_format,
                                                         disk_format)


def volume_snapshot_get(request, snapshot_id):
    snapshot = cinderclient(request).volume_snapshots.get(snapshot_id)
    return VolumeSnapshot(snapshot)


def volume_snapshot_list(request, search_opts=None):
    c_client = cinderclient(request)
    if c_client is None:
        return []
    return [VolumeSnapshot(s) for s in c_client.volume_snapshots.list(
        search_opts=search_opts)]


def volume_snapshot_create(request, volume_id, name,
                           description=None, force=False):
    data = {'name': name,
            'description': description,
            'force': force}
    data = _replace_v2_parameters(data)

    return VolumeSnapshot(cinderclient(request).volume_snapshots.create(
        volume_id, **data))


def volume_snapshot_delete(request, snapshot_id):
    return cinderclient(request).volume_snapshots.delete(snapshot_id)


def volume_snapshot_update(request, snapshot_id, name, description):
    snapshot_data = {'name': name,
                     'description': description}
    snapshot_data = _replace_v2_parameters(snapshot_data)
    return cinderclient(request).volume_snapshots.update(snapshot_id,
                                                         **snapshot_data)


def volume_snapshot_reset_state(request, snapshot_id, state):
    return cinderclient(request).volume_snapshots.reset_state(
        snapshot_id, state)


@memoized
def volume_backup_supported(request):
    """This method will determine if cinder supports backup.
    """
    # TODO(lcheng) Cinder does not expose the information if cinder
    # backup is configured yet. This is a workaround until that
    # capability is available.
    # https://bugs.launchpad.net/cinder/+bug/1334856
    cinder_config = getattr(settings, 'OPENSTACK_CINDER_FEATURES', {})
    return cinder_config.get('enable_backup', False)


def volume_backup_get(request, backup_id):
    backup = cinderclient(request).backups.get(backup_id)
    return VolumeBackup(backup)


def volume_backup_list(request):
    c_client = cinderclient(request)
    if c_client is None:
        return []
    return [VolumeBackup(b) for b in c_client.backups.list()]


def volume_backup_create(request,
                         volume_id,
                         container_name,
                         name,
                         description):
    backup = cinderclient(request).backups.create(
        volume_id,
        container=container_name,
        name=name,
        description=description)
    return VolumeBackup(backup)


def volume_backup_delete(request, backup_id):
    return cinderclient(request).backups.delete(backup_id)


def volume_backup_restore(request, backup_id, volume_id):
    return cinderclient(request).restores.restore(backup_id=backup_id,
                                                  volume_id=volume_id)


def tenant_quota_get(request, tenant_id):
    c_client = cinderclient(request)
    if c_client is None:
        return base.QuotaSet()
    return base.QuotaSet(c_client.quotas.get(tenant_id))


def tenant_quota_update(request, tenant_id, **kwargs):
    return cinderclient(request).quotas.update(tenant_id, **kwargs)


def default_quota_get(request, tenant_id):
    return base.QuotaSet(cinderclient(request).quotas.defaults(tenant_id))


def volume_type_list_with_qos_associations(request):
    vol_types = volume_type_list(request)
    vol_types_dict = {}

    # initialize and build a dictionary for lookup access below
    for vol_type in vol_types:
        vol_type.associated_qos_spec = ""
        vol_types_dict[vol_type.id] = vol_type

    # get all currently defined qos specs
    qos_specs = qos_spec_list(request)
    for qos_spec in qos_specs:
        # get all volume types this qos spec is associated with
        assoc_vol_types = qos_spec_get_associations(request, qos_spec.id)
        for assoc_vol_type in assoc_vol_types:
            # update volume type to hold this association info
            vol_type = vol_types_dict[assoc_vol_type.id]
            vol_type.associated_qos_spec = qos_spec.name

    return vol_types


def default_quota_update(request, **kwargs):
    cinderclient(request).quota_classes.update(DEFAULT_QUOTA_NAME, **kwargs)


def volume_type_list(request):
    return cinderclient(request).volume_types.list()


def volume_type_create(request, name):
    return cinderclient(request).volume_types.create(name)


def volume_type_delete(request, volume_type_id):
    return cinderclient(request).volume_types.delete(volume_type_id)


def volume_type_get(request, volume_type_id):
    return cinderclient(request).volume_types.get(volume_type_id)


def volume_encryption_type_create(request, volume_type_id, data):
    return cinderclient(request).volume_encryption_types.create(volume_type_id,
                                                                specs=data)


def volume_encryption_type_delete(request, volume_type_id):
    return cinderclient(request).volume_encryption_types.delete(volume_type_id)


def volume_encryption_type_get(request, volume_type_id):
    return cinderclient(request).volume_encryption_types.get(volume_type_id)


def volume_encryption_type_list(request):
    return cinderclient(request).volume_encryption_types.list()


def volume_type_extra_get(request, type_id, raw=False):
    vol_type = volume_type_get(request, type_id)
    extras = vol_type.get_keys()
    if raw:
        return extras
    return [VolTypeExtraSpec(type_id, key, value) for
            key, value in extras.items()]


def volume_type_extra_set(request, type_id, metadata):
    vol_type = volume_type_get(request, type_id)
    if not metadata:
        return None
    return vol_type.set_keys(metadata)


def volume_type_extra_delete(request, type_id, keys):
    vol_type = volume_type_get(request, type_id)
    return vol_type.unset_keys([keys])


def qos_spec_list(request):
    return cinderclient(request).qos_specs.list()


def qos_spec_get(request, qos_spec_id):
    return cinderclient(request).qos_specs.get(qos_spec_id)


def qos_spec_delete(request, qos_spec_id):
    return cinderclient(request).qos_specs.delete(qos_spec_id, force=True)


def qos_spec_create(request, name, specs):
    return cinderclient(request).qos_specs.create(name, specs)


def qos_spec_get_keys(request, qos_spec_id, raw=False):
    spec = qos_spec_get(request, qos_spec_id)
    qos_specs = spec.specs
    if raw:
        return spec
    return [QosSpec(qos_spec_id, key, value) for
            key, value in qos_specs.items()]


def qos_spec_set_keys(request, qos_spec_id, specs):
    return cinderclient(request).qos_specs.set_keys(qos_spec_id, specs)


def qos_spec_unset_keys(request, qos_spec_id, specs):
    return cinderclient(request).qos_specs.unset_keys(qos_spec_id, specs)


def qos_spec_associate(request, qos_specs, vol_type_id):
    return cinderclient(request).qos_specs.associate(qos_specs, vol_type_id)


def qos_spec_disassociate(request, qos_specs, vol_type_id):
    return cinderclient(request).qos_specs.disassociate(qos_specs, vol_type_id)


def qos_spec_get_associations(request, qos_spec_id):
    return cinderclient(request).qos_specs.get_associations(qos_spec_id)


@memoized
def tenant_absolute_limits(request):
    limits = cinderclient(request).limits.get().absolute
    limits_dict = {}
    for limit in limits:
        if limit.value < 0:
            # In some cases, the absolute limits data in Cinder can get
            # out of sync causing the total.*Used limits to return
            # negative values instead of 0. For such cases, replace
            # negative values with 0.
            if limit.name.startswith('total') and limit.name.endswith('Used'):
                limits_dict[limit.name] = 0
            else:
                # -1 is used to represent unlimited quotas
                limits_dict[limit.name] = float("inf")
        else:
            limits_dict[limit.name] = limit.value
    return limits_dict


def service_list(request):
    return cinderclient(request).services.list()


def availability_zone_list(request, detailed=False):
    return cinderclient(request).availability_zones.list(detailed=detailed)


@memoized
def list_extensions(request):
    return cinder_list_extensions.ListExtManager(cinderclient(request))\
        .show_all()


@memoized
def extension_supported(request, extension_name):
    """This method will determine if Cinder supports a given extension name.
    """
    extensions = list_extensions(request)
    for extension in extensions:
        if extension.name == extension_name:
            return True
    return False

# Copyright 2012 Nebula, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from django.utils.translation import ugettext_lazy as _

import horizon


class SystemPanels(horizon.PanelGroup):
    slug = "admin"
    name = _("System")
    panels = ('overview', 'metering', 'hypervisors', 'aggregates',
              'instances', 'volumes', 'flavors', 'images',
              'networks', 'routers', 'defaults', 'info', 'avos')


class Admin(horizon.Dashboard):
    name = _("Admin")
    slug = "admin"
    panels = (SystemPanels,)
    default_panel = 'overview'
    permissions = ('openstack.roles.admin',)


horizon.register(Admin)

# Copyright 2012 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2012 Nebula, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import json

from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon import messages

from openstack_dashboard import api
from openstack_dashboard.dashboards.project.images.images \
    import forms as images_forms


class AdminCreateImageForm(images_forms.CreateImageForm):
    pass


class AdminUpdateImageForm(images_forms.UpdateImageForm):
    pass


class UpdateMetadataForm(forms.SelfHandlingForm):

    def handle(self, request, data):
        id = self.initial['id']
        old_metadata = self.initial['metadata']

        try:
            new_metadata = json.loads(self.data['metadata'])

            metadata = dict(
                (item['key'], str(item['value']))
                for item in new_metadata
            )

            remove_props = [key for key in old_metadata if key not in metadata]

            api.glance.image_update_properties(request,
                                               id,
                                               remove_props,
                                               **metadata)
            message = _('Metadata successfully updated.')
            messages.success(request, message)
        except Exception:
            exceptions.handle(request,
                              _('Unable to update the image metadata.'))
            return False
        return True

# Copyright 2014 OpenStack Foundation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import datetime

from django.forms import ValidationError  # noqa
from django.utils.translation import ugettext_lazy as _

from horizon import forms


class UsageReportForm(forms.SelfHandlingForm):
    PERIOD_CHOICES = (("1", _("Last day")),
                      ("7", _("Last week")),
                      (str(datetime.date.today().day), _("Month to date")),
                      ("15", _("Last 15 days")),
                      ("30", _("Last 30 days")),
                      ("365", _("Last year")),
                      ("other", _("Other")),
                      )
    period = forms.ChoiceField(label=_("Period"),
                               required=True,
                               choices=PERIOD_CHOICES)
    date_from = forms.DateField(label=_("From"), required=False,
                                widget=forms.TextInput(
                                attrs={'data-line-chart-command':
                                       'date_picker_change'}))
    date_to = forms.DateField(label=_("To"), required=False,
                              widget=forms.TextInput(
                              attrs={'data-line-chart-command':
                                     'date_picker_change'}))

    def clean_date_from(self):
        period = self.cleaned_data['period']
        date_from = self.cleaned_data['date_from']
        if period == 'other' and date_from is None:
            raise ValidationError(_('Must specify start of period'))
        return date_from

    def clean_date_to(self):
        data = super(UsageReportForm, self).clean()
        date_from = data.get('date_from')
        date_to = data.get('date_to')
        period = data.get('period')
        if (period == 'other' and date_to is not None
                and date_from is not None and date_to < date_from):
            raise ValidationError(_("Start must be earlier "
                                    "than end of period."))
        else:
            return date_to

    def handle(self, request, data):
        if hasattr(request, 'session'):
            request.session['date_from'] = data['date_from']
            request.session['date_to'] = data['date_to']
            request.session['period'] = data['period']
        return data

# Copyright 2012 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2012 Nebula, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from django.conf import settings
from django.template.defaultfilters import floatformat  # noqa
from django.utils import translation
from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon.utils import csvbase

from openstack_dashboard import api
from openstack_dashboard import usage


class GlobalUsageCsvRenderer(csvbase.BaseCsvResponse):

    columns = [_("Project Name"), _("VCPUs"), _("RAM (MB)"),
               _("Disk (GB)"), _("Usage (Hours)")]

    def get_row_data(self):

        for u in self.context['usage'].usage_list:
            yield (u.project_name or u.tenant_id,
                   u.vcpus,
                   u.memory_mb,
                   u.local_gb,
                   floatformat(u.vcpu_hours, 2))


class GlobalOverview(usage.UsageView):
    table_class = usage.GlobalUsageTable
    usage_class = usage.GlobalUsage
    template_name = 'admin/overview/usage.html'
    csv_response_class = GlobalUsageCsvRenderer

    def get_context_data(self, **kwargs):
        context = super(GlobalOverview, self).get_context_data(**kwargs)
        context['monitoring'] = getattr(settings, 'EXTERNAL_MONITORING', [])
        return context

    def get_data(self):
        data = super(GlobalOverview, self).get_data()
        # Pre-fill project names
        try:
            projects, has_more = api.keystone.tenant_list(self.request)
        except Exception:
            projects = []
            exceptions.handle(self.request,
                              _('Unable to retrieve project list.'))
        for instance in data:
            project = filter(lambda t: t.id == instance.tenant_id, projects)
            # If we could not get the project name, show the tenant_id with
            # a 'Deleted' identifier instead.
            if project:
                instance.project_name = getattr(project[0], "name", None)
            else:
                deleted = _("Deleted")
                instance.project_name = translation.string_concat(
                    instance.tenant_id, " (", deleted, ")")
        return data

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

from django.core.urlresolvers import reverse
from django import http

from mox import IsA  # noqa

from openstack_dashboard import api
from openstack_dashboard.test import helpers as test


class VolTypeExtrasTests(test.BaseAdminViewTests):

    @test.create_stubs({api.cinder: ('volume_type_extra_get',
                                     'volume_type_get'), })
    def test_list_extras_when_none_exists(self):
        vol_type = self.cinder_volume_types.first()
        extras = [api.cinder.VolTypeExtraSpec(vol_type.id, 'k1', 'v1')]

        api.cinder.volume_type_get(IsA(http.HttpRequest),
                                   vol_type.id).AndReturn(vol_type)
        api.cinder.volume_type_extra_get(IsA(http.HttpRequest),
                                         vol_type.id).AndReturn(extras)
        self.mox.ReplayAll()
        url = reverse('horizon:admin:volumes:volume_types:extras:index',
                      args=[vol_type.id])
        resp = self.client.get(url)
        self.assertEqual(resp.status_code, 200)
        self.assertTemplateUsed(resp,
                                "admin/volumes/volume_types/extras/index.html")

    @test.create_stubs({api.cinder: ('volume_type_extra_get',
                                     'volume_type_get'), })
    def test_extras_view_with_exception(self):
        vol_type = self.cinder_volume_types.first()

        api.cinder.volume_type_get(IsA(http.HttpRequest),
                                   vol_type.id).AndReturn(vol_type)
        api.cinder.volume_type_extra_get(IsA(http.HttpRequest),
                                         vol_type.id) \
            .AndRaise(self.exceptions.cinder)
        self.mox.ReplayAll()
        url = reverse('horizon:admin:volumes:volume_types:extras:index',
                      args=[vol_type.id])
        resp = self.client.get(url)
        self.assertEqual(len(resp.context['extras_table'].data), 0)
        self.assertMessageCount(resp, error=1)

    @test.create_stubs({api.cinder: ('volume_type_extra_set', ), })
    def test_extra_create_post(self):
        vol_type = self.cinder_volume_types.first()
        create_url = reverse(
            'horizon:admin:volumes:volume_types:extras:create',
            args=[vol_type.id])
        index_url = reverse(
            'horizon:admin:volumes:volume_types:extras:index',
            args=[vol_type.id])

        data = {'key': u'k1',
                'value': u'v1'}

        api.cinder.volume_type_extra_set(IsA(http.HttpRequest),
                                         vol_type.id,
                                         {data['key']: data['value']})
        self.mox.ReplayAll()

        resp = self.client.post(create_url, data)
        self.assertNoFormErrors(resp)
        self.assertMessageCount(success=1)
        self.assertRedirectsNoFollow(resp, index_url)

    @test.create_stubs({api.cinder: ('volume_type_get', ), })
    def test_extra_create_get(self):
        vol_type = self.cinder_volume_types.first()
        create_url = reverse(
            'horizon:admin:volumes:volume_types:extras:create',
            args=[vol_type.id])

        api.cinder.volume_type_get(IsA(http.HttpRequest),
                                   vol_type.id).AndReturn(vol_type)
        self.mox.ReplayAll()

        resp = self.client.get(create_url)
        self.assertEqual(resp.status_code, 200)
        self.assertTemplateUsed(
            resp, 'admin/volumes/volume_types/extras/create.html')

    @test.create_stubs({api.cinder: ('volume_type_extra_get',
                                     'volume_type_extra_set',), })
    def test_extra_edit(self):
        vol_type = self.cinder_volume_types.first()
        key = 'foo'
        edit_url = reverse('horizon:admin:volumes:volume_types:extras:edit',
                           args=[vol_type.id, key])
        index_url = reverse('horizon:admin:volumes:volume_types:extras:index',
                            args=[vol_type.id])

        data = {'value': u'v1'}
        extras = {key: data['value']}

        api.cinder.volume_type_extra_get(IsA(http.HttpRequest),
                                         vol_type.id,
                                         raw=True).AndReturn(extras)
        api.cinder.volume_type_extra_set(IsA(http.HttpRequest),
                                         vol_type.id,
                                         extras)
        self.mox.ReplayAll()

        resp = self.client.post(edit_url, data)
        self.assertNoFormErrors(resp)
        self.assertMessageCount(success=1)
        self.assertRedirectsNoFollow(resp, index_url)

    @test.create_stubs({api.cinder: ('volume_type_extra_get',
                                     'volume_type_extra_delete'), })
    def test_extra_delete(self):
        vol_type = self.cinder_volume_types.first()
        extras = [api.cinder.VolTypeExtraSpec(vol_type.id, 'k1', 'v1')]
        formData = {'action': 'extras__delete__k1'}
        index_url = reverse('horizon:admin:volumes:volume_types:extras:index',
                            args=[vol_type.id])

        api.cinder.volume_type_extra_get(IsA(http.HttpRequest),
                                         vol_type.id).AndReturn(extras)
        api.cinder.volume_type_extra_delete(IsA(http.HttpRequest),
                                            vol_type.id,
                                            'k1').AndReturn(vol_type)
        self.mox.ReplayAll()

        res = self.client.post(index_url, formData)
        self.assertNoFormErrors(res)
        self.assertRedirectsNoFollow(res, index_url)

# Copyright 2013 Hewlett-Packard Development Company, L.P.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging

from django.core.urlresolvers import reverse
from django.template import defaultfilters
from django.utils.translation import ugettext_lazy as _
from django.utils.translation import ungettext_lazy

from horizon import tables

from openstack_dashboard import api

from openstack_dashboard.dashboards.identity.groups import constants


LOG = logging.getLogger(__name__)
LOGOUT_URL = 'logout'
STATUS_CHOICES = (
    ("true", True),
    ("false", False)
)


class CreateGroupLink(tables.LinkAction):
    name = "create"
    verbose_name = _("Create Group")
    url = constants.GROUPS_CREATE_URL
    classes = ("ajax-modal",)
    icon = "plus"
    policy_rules = (("identity", "identity:create_group"),)

    def allowed(self, request, group):
        return api.keystone.keystone_can_edit_group()


class EditGroupLink(tables.LinkAction):
    name = "edit"
    verbose_name = _("Edit Group")
    url = constants.GROUPS_UPDATE_URL
    classes = ("ajax-modal",)
    icon = "pencil"
    policy_rules = (("identity", "identity:update_group"),)

    def allowed(self, request, group):
        return api.keystone.keystone_can_edit_group()


class DeleteGroupsAction(tables.DeleteAction):
    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Delete Group",
            u"Delete Groups",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Deleted Group",
            u"Deleted Groups",
            count
        )

    name = "delete"
    policy_rules = (("identity", "identity:delete_group"),)

    def allowed(self, request, datum):
        return api.keystone.keystone_can_edit_group()

    def delete(self, request, obj_id):
        LOG.info('Deleting group "%s".' % obj_id)
        api.keystone.group_delete(request, obj_id)


class ManageUsersLink(tables.LinkAction):
    name = "users"
    verbose_name = _("Manage Members")
    url = constants.GROUPS_MANAGE_URL
    icon = "pencil"
    policy_rules = (("identity", "identity:get_group"),
                    ("identity", "identity:list_users"),)

    def allowed(self, request, datum):
        return api.keystone.keystone_can_edit_group()


class GroupFilterAction(tables.FilterAction):
    def filter(self, table, groups, filter_string):
        """Naive case-insensitive search."""
        q = filter_string.lower()

        def comp(group):
            if q in group.name.lower():
                return True
            return False

        return filter(comp, groups)


class GroupsTable(tables.DataTable):
    name = tables.Column('name', verbose_name=_('Name'))
    description = tables.Column(lambda obj: getattr(obj, 'description', None),
                                verbose_name=_('Description'))
    id = tables.Column('id', verbose_name=_('Group ID'))

    class Meta:
        name = "groups"
        verbose_name = _("Groups")
        row_actions = (ManageUsersLink, EditGroupLink, DeleteGroupsAction)
        table_actions = (GroupFilterAction, CreateGroupLink,
                         DeleteGroupsAction)


class UserFilterAction(tables.FilterAction):
    def filter(self, table, users, filter_string):
        """Naive case-insensitive search."""
        q = filter_string.lower()
        return [user for user in users
                if q in user.name.lower()
                or q in (getattr(user, 'email', None) or '').lower()]


class RemoveMembers(tables.DeleteAction):
    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Remove User",
            u"Remove Users",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Removed User",
            u"Removed Users",
            count
        )

    name = "removeGroupMember"
    policy_rules = (("identity", "identity:remove_user_from_group"),)

    def allowed(self, request, user=None):
        return api.keystone.keystone_can_edit_group()

    def action(self, request, obj_id):
        user_obj = self.table.get_object_by_id(obj_id)
        group_id = self.table.kwargs['group_id']
        LOG.info('Removing user %s from group %s.' % (user_obj.id,
                                                      group_id))
        api.keystone.remove_group_user(request,
                                       group_id=group_id,
                                       user_id=user_obj.id)
        # TODO(lin-hua-cheng): Fix the bug when removing current user
        # Keystone revokes the token of the user removed from the group.
        # If the logon user was removed, redirect the user to logout.


class AddMembersLink(tables.LinkAction):
    name = "add_user_link"
    verbose_name = _("Add...")
    classes = ("ajax-modal",)
    icon = "plus"
    url = constants.GROUPS_ADD_MEMBER_URL
    policy_rules = (("identity", "identity:list_users"),
                    ("identity", "identity:add_user_to_group"),)

    def allowed(self, request, user=None):
        return api.keystone.keystone_can_edit_group()

    def get_link_url(self, datum=None):
        return reverse(self.url, kwargs=self.table.kwargs)


class UsersTable(tables.DataTable):
    name = tables.Column('name', verbose_name=_('User Name'))
    email = tables.Column('email', verbose_name=_('Email'),
                          filters=[defaultfilters.escape,
                                   defaultfilters.urlize])
    id = tables.Column('id', verbose_name=_('User ID'))
    enabled = tables.Column('enabled', verbose_name=_('Enabled'),
                            status=True,
                            status_choices=STATUS_CHOICES,
                            empty_value="False")


class GroupMembersTable(UsersTable):
    class Meta:
        name = "group_members"
        verbose_name = _("Group Members")
        table_actions = (UserFilterAction, AddMembersLink, RemoveMembers)


class AddMembers(tables.BatchAction):
    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Add User",
            u"Add Users",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Added User",
            u"Added Users",
            count
        )

    name = "addMember"
    icon = "plus"
    requires_input = True
    success_url = constants.GROUPS_MANAGE_URL
    policy_rules = (("identity", "identity:add_user_to_group"),)

    def allowed(self, request, user=None):
        return api.keystone.keystone_can_edit_group()

    def action(self, request, obj_id):
        user_obj = self.table.get_object_by_id(obj_id)
        group_id = self.table.kwargs['group_id']
        LOG.info('Adding user %s to group %s.' % (user_obj.id,
                                                  group_id))
        api.keystone.add_group_user(request,
                                    group_id=group_id,
                                    user_id=user_obj.id)
        # TODO(lin-hua-cheng): Fix the bug when adding current user
        # Keystone revokes the token of the user added to the group.
        # If the logon user was added, redirect the user to logout.

    def get_success_url(self, request=None):
        group_id = self.table.kwargs.get('group_id', None)
        return reverse(self.success_url, args=[group_id])


class GroupNonMembersTable(UsersTable):
    class Meta:
        name = "group_non_members"
        verbose_name = _("Non-Members")
        table_actions = (UserFilterAction, AddMembers)

# Copyright 2012 United States Government as represented by the
# Administrator of the National Aeronautics and Space Administration.
# All Rights Reserved.
#
# Copyright 2012 Nebula, Inc.
# Copyright (c) 2012 X.commerce, a business unit of eBay Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from django.core.urlresolvers import reverse
from django import http
from django.utils.http import urlencode

from mox import IsA  # noqa

from openstack_dashboard import api
from openstack_dashboard.dashboards.project.access_and_security \
    .floating_ips import tables
from openstack_dashboard.test import helpers as test
from openstack_dashboard.usage import quotas

from horizon.workflows import views


INDEX_URL = reverse('horizon:project:access_and_security:index')
NAMESPACE = "horizon:project:access_and_security:floating_ips"


class FloatingIpViewTests(test.TestCase):
    @test.create_stubs({api.network: ('floating_ip_target_list',
                                      'tenant_floating_ip_list',)})
    def test_associate(self):
        api.network.floating_ip_target_list(IsA(http.HttpRequest)) \
            .AndReturn(self.servers.list())
        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        self.mox.ReplayAll()

        url = reverse('%s:associate' % NAMESPACE)
        res = self.client.get(url)
        self.assertTemplateUsed(res, views.WorkflowView.template_name)
        workflow = res.context['workflow']
        choices = dict(workflow.steps[0].action.fields['ip_id'].choices)
        # Verify that our "associated" floating IP isn't in the choices list.
        self.assertTrue(self.floating_ips.first() not in choices)

    @test.create_stubs({api.network: ('floating_ip_target_list',
                                      'floating_ip_target_get_by_instance',
                                      'tenant_floating_ip_list',)})
    def test_associate_with_instance_id(self):
        api.network.floating_ip_target_list(IsA(http.HttpRequest)) \
            .AndReturn(self.servers.list())
        api.network.floating_ip_target_get_by_instance(
            IsA(http.HttpRequest), 'TEST-ID', self.servers.list()) \
            .AndReturn('TEST-ID')
        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        self.mox.ReplayAll()

        base_url = reverse('%s:associate' % NAMESPACE)
        params = urlencode({'instance_id': 'TEST-ID'})
        url = '?'.join([base_url, params])
        res = self.client.get(url)
        self.assertTemplateUsed(res, views.WorkflowView.template_name)
        workflow = res.context['workflow']
        choices = dict(workflow.steps[0].action.fields['ip_id'].choices)
        # Verify that our "associated" floating IP isn't in the choices list.
        self.assertTrue(self.floating_ips.first() not in choices)

    @test.create_stubs({api.network: ('floating_ip_associate',
                                      'floating_ip_target_list',
                                      'tenant_floating_ip_list',)})
    def test_associate_post(self):
        floating_ip = self.floating_ips.list()[1]
        server = self.servers.first()

        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        api.network.floating_ip_target_list(IsA(http.HttpRequest)) \
            .AndReturn(self.servers.list())
        api.network.floating_ip_associate(IsA(http.HttpRequest),
                                          floating_ip.id,
                                          server.id)
        self.mox.ReplayAll()

        form_data = {'instance_id': server.id,
                     'ip_id': floating_ip.id}
        url = reverse('%s:associate' % NAMESPACE)
        res = self.client.post(url, form_data)
        self.assertRedirectsNoFollow(res, INDEX_URL)

    @test.create_stubs({api.network: ('floating_ip_associate',
                                      'floating_ip_target_list',
                                      'tenant_floating_ip_list',)})
    def test_associate_post_with_redirect(self):
        floating_ip = self.floating_ips.list()[1]
        server = self.servers.first()

        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        api.network.floating_ip_target_list(IsA(http.HttpRequest)) \
            .AndReturn(self.servers.list())
        api.network.floating_ip_associate(IsA(http.HttpRequest),
                                          floating_ip.id,
                                          server.id)
        self.mox.ReplayAll()

        form_data = {'instance_id': server.id,
                     'ip_id': floating_ip.id}
        url = reverse('%s:associate' % NAMESPACE)
        next = reverse("horizon:project:instances:index")
        res = self.client.post("%s?next=%s" % (url, next), form_data)
        self.assertRedirectsNoFollow(res, next)

    @test.create_stubs({api.network: ('floating_ip_associate',
                                      'floating_ip_target_list',
                                      'tenant_floating_ip_list',)})
    def test_associate_post_with_exception(self):
        floating_ip = self.floating_ips.list()[1]
        server = self.servers.first()

        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        api.network.floating_ip_target_list(IsA(http.HttpRequest)) \
            .AndReturn(self.servers.list())
        api.network.floating_ip_associate(IsA(http.HttpRequest),
                                          floating_ip.id,
                                          server.id) \
            .AndRaise(self.exceptions.nova)
        self.mox.ReplayAll()

        form_data = {'instance_id': server.id,
                     'ip_id': floating_ip.id}
        url = reverse('%s:associate' % NAMESPACE)
        res = self.client.post(url, form_data)
        self.assertRedirectsNoFollow(res, INDEX_URL)

    @test.create_stubs({api.nova: ('server_list',),
                        api.network: ('floating_ip_disassociate',
                                      'floating_ip_supported',
                                      'tenant_floating_ip_get',
                                      'tenant_floating_ip_list',)})
    def test_disassociate_post(self):
        floating_ip = self.floating_ips.first()

        api.nova.server_list(IsA(http.HttpRequest)) \
            .AndReturn([self.servers.list(), False])
        api.network.floating_ip_supported(IsA(http.HttpRequest)) \
            .AndReturn(True)
        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())
        api.network.floating_ip_disassociate(IsA(http.HttpRequest),
                                             floating_ip.id)
        self.mox.ReplayAll()

        action = "floating_ips__disassociate__%s" % floating_ip.id
        res = self.client.post(INDEX_URL, {"action": action})
        self.assertMessageCount(success=1)
        self.assertRedirectsNoFollow(res, INDEX_URL)

    @test.create_stubs({api.nova: ('server_list',),
                        api.network: ('floating_ip_disassociate',
                                      'floating_ip_supported',
                                      'tenant_floating_ip_get',
                                      'tenant_floating_ip_list',)})
    def test_disassociate_post_with_exception(self):
        floating_ip = self.floating_ips.first()

        api.nova.server_list(IsA(http.HttpRequest)) \
            .AndReturn([self.servers.list(), False])
        api.network.floating_ip_supported(IsA(http.HttpRequest)) \
            .AndReturn(True)
        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .AndReturn(self.floating_ips.list())

        api.network.floating_ip_disassociate(IsA(http.HttpRequest),
                                             floating_ip.id) \
            .AndRaise(self.exceptions.nova)
        self.mox.ReplayAll()

        action = "floating_ips__disassociate__%s" % floating_ip.id
        res = self.client.post(INDEX_URL, {"action": action})
        self.assertRedirectsNoFollow(res, INDEX_URL)

    @test.create_stubs({api.network: ('floating_ip_supported',
                                      'tenant_floating_ip_list',
                                      'security_group_list',
                                      'floating_ip_pools_list',),
                        api.nova: ('keypair_list',
                                   'server_list',),
                        quotas: ('tenant_quota_usages',),
                        api.base: ('is_service_enabled',)})
    def test_allocate_button_disabled_when_quota_exceeded(self):
        keypairs = self.keypairs.list()
        floating_ips = self.floating_ips.list()
        floating_pools = self.pools.list()
        quota_data = self.quota_usages.first()
        quota_data['floating_ips']['available'] = 0
        sec_groups = self.security_groups.list()

        api.network.floating_ip_supported(
            IsA(http.HttpRequest)) \
            .AndReturn(True)
        api.network.tenant_floating_ip_list(
            IsA(http.HttpRequest)) \
            .AndReturn(floating_ips)
        api.network.security_group_list(
            IsA(http.HttpRequest)).MultipleTimes()\
            .AndReturn(sec_groups)
        api.network.floating_ip_pools_list(
            IsA(http.HttpRequest)) \
            .AndReturn(floating_pools)
        api.nova.keypair_list(
            IsA(http.HttpRequest)) \
            .AndReturn(keypairs)
        api.nova.server_list(
            IsA(http.HttpRequest)) \
            .AndReturn([self.servers.list(), False])
        quotas.tenant_quota_usages(
            IsA(http.HttpRequest)).MultipleTimes() \
            .AndReturn(quota_data)

        api.base.is_service_enabled(
            IsA(http.HttpRequest),
            'network').MultipleTimes() \
            .AndReturn(True)
        api.base.is_service_enabled(
            IsA(http.HttpRequest),
            'ec2').MultipleTimes() \
            .AndReturn(False)

        self.mox.ReplayAll()

        res = self.client.get(INDEX_URL +
                              "?tab=access_security_tabs__floating_ips_tab")

        allocate_link = tables.AllocateIP()
        url = allocate_link.get_link_url()
        classes = (list(allocate_link.get_default_classes())
                   + list(allocate_link.classes))
        link_name = "%s (%s)" % (unicode(allocate_link.verbose_name),
                                 "Quota exceeded")
        expected_string = ("<a href='%s' title='%s' class='%s disabled' "
                           "id='floating_ips__action_allocate'>"
                           "<span class='fa fa-link'>"
                           "</span>%s</a>"
                           % (url, link_name, " ".join(classes), link_name))
        self.assertContains(res, expected_string, html=True,
                            msg_prefix="The create button is not disabled")


class FloatingIpNeutronViewTests(FloatingIpViewTests):
    def setUp(self):
        super(FloatingIpViewTests, self).setUp()
        self._floating_ips_orig = self.floating_ips
        self.floating_ips = self.floating_ips_uuid

    def tearDown(self):
        self.floating_ips = self._floating_ips_orig
        super(FloatingIpViewTests, self).tearDown()

    @test.create_stubs({api.nova: ('tenant_quota_get', 'flavor_list',
                                   'server_list'),
                        api.network: ('floating_ip_pools_list',
                                      'floating_ip_supported',
                                      'security_group_list',
                                      'tenant_floating_ip_list'),
                        api.neutron: ('is_extension_supported',
                                      'tenant_quota_get',
                                      'network_list',
                                      'router_list',
                                      'subnet_list'),
                        api.base: ('is_service_enabled',)})
    @test.update_settings(OPENSTACK_NEUTRON_NETWORK={'enable_quotas': True})
    def test_correct_quotas_displayed(self):
        servers = [s for s in self.servers.list()
                   if s.tenant_id == self.request.user.tenant_id]

        api.base.is_service_enabled(IsA(http.HttpRequest), 'volume') \
            .AndReturn(False)
        api.base.is_service_enabled(IsA(http.HttpRequest), 'network') \
            .MultipleTimes().AndReturn(True)
        api.nova.tenant_quota_get(IsA(http.HttpRequest), '1') \
            .AndReturn(self.quotas.first())
        api.nova.flavor_list(IsA(http.HttpRequest)) \
            .AndReturn(self.flavors.list())
        search_opts = {'tenant_id': self.request.user.tenant_id}
        api.nova.server_list(IsA(http.HttpRequest), search_opts=search_opts,
                             all_tenants=True) \
            .AndReturn([servers, False])
        api.neutron.is_extension_supported(
            IsA(http.HttpRequest), 'security-group').AndReturn(True)
        api.neutron.is_extension_supported(IsA(http.HttpRequest), 'quotas') \
            .AndReturn(True)
        api.neutron.tenant_quota_get(IsA(http.HttpRequest), self.tenant.id) \
            .AndReturn(self.neutron_quotas.first())
        api.neutron.router_list(IsA(http.HttpRequest)) \
            .AndReturn(self.routers.list())
        api.neutron.subnet_list(IsA(http.HttpRequest)) \
            .AndReturn(self.subnets.list())
        api.neutron.network_list(IsA(http.HttpRequest), shared=False) \
            .AndReturn(self.networks.list())
        api.network.floating_ip_supported(IsA(http.HttpRequest)) \
            .AndReturn(True)
        api.network.tenant_floating_ip_list(IsA(http.HttpRequest)) \
            .MultipleTimes().AndReturn(self.floating_ips.list())
        api.network.floating_ip_pools_list(IsA(http.HttpRequest)) \
            .AndReturn(self.pools.list())
        api.network.security_group_list(IsA(http.HttpRequest)) \
            .AndReturn(self.security_groups.list())
        self.mox.ReplayAll()

        url = reverse('%s:allocate' % NAMESPACE)
        res = self.client.get(url)
        self.assertEqual(res.context['usages']['floating_ips']['quota'],
                         self.neutron_quotas.first().get('floatingip').limit)

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging

from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms

from openstack_dashboard.api import sahara as saharaclient
from openstack_dashboard.dashboards.project.data_processing. \
    utils import workflow_helpers

LOG = logging.getLogger(__name__)


class UploadFileForm(forms.SelfHandlingForm,
                     workflow_helpers.PluginAndVersionMixin):
    template_name = forms.CharField(max_length=80,
                                    label=_("Cluster Template Name"))

    def __init__(self, request, *args, **kwargs):
        super(UploadFileForm, self).__init__(request, *args, **kwargs)

        sahara = saharaclient.client(request)
        self._generate_plugin_version_fields(sahara)

        self.fields['template_file'] = forms.FileField(label=_("Template"))

    def handle(self, request, data):
        try:
            # we can set a limit on file size, but should we?
            filecontent = self.files['template_file'].read()

            plugin_name = data['plugin_name']
            hadoop_version = data.get(plugin_name + "_version")

            saharaclient.plugin_convert_to_template(request,
                                                    plugin_name,
                                                    hadoop_version,
                                                    data['template_name'],
                                                    filecontent)
            return True
        except Exception:
            exceptions.handle(request,
                              _("Unable to upload cluster template file"))
            return False

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging

from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import tables
from horizon import tabs

from openstack_dashboard.api import sahara as saharaclient
import openstack_dashboard.dashboards.project.data_processing. \
    data_plugins.tables as p_tables
import openstack_dashboard.dashboards.project.data_processing. \
    data_plugins.tabs as p_tabs

LOG = logging.getLogger(__name__)


class PluginsView(tables.DataTableView):
    table_class = p_tables.PluginsTable
    template_name = 'project/data_processing.data_plugins/plugins.html'

    def get_data(self):
        try:
            plugins = saharaclient.plugin_list(self.request)
        except Exception:
            plugins = []
            msg = _('Unable to retrieve data processing plugins.')
            exceptions.handle(self.request, msg)
        return plugins


class PluginDetailsView(tabs.TabView):
    tab_group_class = p_tabs.PluginDetailsTabs
    template_name = 'project/data_processing.data_plugins/details.html'

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import json
import logging

from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon import workflows

from openstack_dashboard.api import sahara as saharaclient
import openstack_dashboard.dashboards.project.data_processing. \
    cluster_templates.workflows.create as t_flows
import openstack_dashboard.dashboards.project.data_processing. \
    clusters.workflows.create as c_flow
import openstack_dashboard.dashboards.project.data_processing. \
    utils.workflow_helpers as whelpers


LOG = logging.getLogger(__name__)

DATA_SOURCE_CREATE_URL = ("horizon:project:data_processing.data_sources"
                          ":create-data-source")


class JobExecutionGeneralConfigAction(workflows.Action):
    job_input = forms.DynamicChoiceField(
        label=_("Input"),
        initial=(None, "None"),
        add_item_link=DATA_SOURCE_CREATE_URL)

    job_output = forms.DynamicChoiceField(
        label=_("Output"),
        initial=(None, "None"),
        add_item_link=DATA_SOURCE_CREATE_URL)

    def __init__(self, request, *args, **kwargs):
        super(JobExecutionGeneralConfigAction, self).__init__(request,
                                                              *args,
                                                              **kwargs)

        if request.REQUEST.get("job_id", None) is None:
            self.fields["job"] = forms.ChoiceField(
                label=_("Job"))
            self.fields["job"].choices = self.populate_job_choices(request)
        else:
            self.fields["job"] = forms.CharField(
                widget=forms.HiddenInput(),
                initial=request.REQUEST.get("job_id", None))

    def populate_job_input_choices(self, request, context):
        return self.get_data_source_choices(request, context)

    def populate_job_output_choices(self, request, context):
        return self.get_data_source_choices(request, context)

    def get_data_source_choices(self, request, context):
        try:
            data_sources = saharaclient.data_source_list(request)
        except Exception:
            data_sources = []
            exceptions.handle(request,
                              _("Unable to fetch data sources."))

        choices = [(data_source.id, data_source.name)
                   for data_source in data_sources]
        choices.insert(0, (None, 'None'))

        return choices

    def populate_job_choices(self, request):
        try:
            jobs = saharaclient.job_list(request)
        except Exception:
            jobs = []
            exceptions.handle(request,
                              _("Unable to fetch jobs."))

        choices = [(job.id, job.name)
                   for job in jobs]

        return choices

    class Meta:
        name = _("Job")
        help_text_template = (
            "project/data_processing.jobs/_launch_job_help.html")


class JobExecutionExistingGeneralConfigAction(JobExecutionGeneralConfigAction):
    cluster = forms.ChoiceField(
        label=_("Cluster"),
        initial=(None, "None"),
        widget=forms.Select(attrs={"class": "cluster_choice"}))

    def populate_cluster_choices(self, request, context):
        try:
            clusters = saharaclient.cluster_list(request)
        except Exception:
            clusters = []
            exceptions.handle(request,
                              _("Unable to fetch clusters."))

        choices = [(cluster.id, cluster.name)
                   for cluster in clusters]

        return choices

    class Meta:
        name = _("Job")
        help_text_template = (
            "project/data_processing.jobs/_launch_job_help.html")


class JobConfigAction(workflows.Action):
    MAIN_CLASS = "edp.java.main_class"
    JAVA_OPTS = "edp.java.java_opts"
    EDP_MAPPER = "edp.streaming.mapper"
    EDP_REDUCER = "edp.streaming.reducer"
    EDP_PREFIX = "edp."

    property_name = forms.ChoiceField(
        required=False,
    )

    job_configs = forms.CharField(
        required=False,
        widget=forms.HiddenInput())

    job_params = forms.CharField(
        required=False,
        widget=forms.HiddenInput())

    job_args_array = forms.CharField(
        required=False,
        widget=forms.HiddenInput())

    job_type = forms.CharField(
        required=False,
        widget=forms.HiddenInput())

    main_class = forms.CharField(label=_("Main Class"),
                                 required=False)

    java_opts = forms.CharField(label=_("Java Opts"),
                                required=False)

    streaming_mapper = forms.CharField(label=_("Mapper"))

    streaming_reducer = forms.CharField(label=_("Reducer"))

    def __init__(self, request, *args, **kwargs):
        super(JobConfigAction, self).__init__(request, *args, **kwargs)
        job_ex_id = request.REQUEST.get("job_execution_id")
        if job_ex_id is not None:
            job_ex_id = request.REQUEST.get("job_execution_id")
            job_ex = saharaclient.job_execution_get(request, job_ex_id)
            job_configs = job_ex.job_configs
            edp_configs = {}

            if 'configs' in job_configs:
                configs, edp_configs = (
                    self.clean_edp_configs(job_configs['configs']))
                self.fields['job_configs'].initial = (
                    json.dumps(configs))

            if 'params' in job_configs:
                self.fields['job_params'].initial = (
                    json.dumps(job_configs['params']))
            job_args = json.dumps(job_configs['args'])
            self.fields['job_args_array'].initial = job_args

            if self.MAIN_CLASS in edp_configs:
                self.fields['main_class'].initial = (
                    edp_configs[self.MAIN_CLASS])
            if self.JAVA_OPTS in edp_configs:
                self.fields['java_opts'].initial = (
                    edp_configs[self.JAVA_OPTS])

            if self.EDP_MAPPER in edp_configs:
                self.fields['streaming_mapper'].initial = (
                    edp_configs[self.EDP_MAPPER])
            if self.EDP_REDUCER in edp_configs:
                self.fields['streaming_reducer'].initial = (
                    edp_configs[self.EDP_REDUCER])

    def clean(self):
        cleaned_data = super(workflows.Action, self).clean()
        job_type = cleaned_data.get("job_type", None)

        if job_type != "MapReduce.Streaming":
            if "streaming_mapper" in self._errors:
                del self._errors["streaming_mapper"]
            if "streaming_reducer" in self._errors:
                del self._errors["streaming_reducer"]

        return cleaned_data

    def populate_property_name_choices(self, request, context):
        job_id = request.REQUEST.get("job_id") or request.REQUEST.get("job")
        job_type = saharaclient.job_get(request, job_id).type
        job_configs = (
            saharaclient.job_get_configs(request, job_type).job_config)
        choices = [(param['value'], param['name'])
                   for param in job_configs['configs']]
        return choices

    def clean_edp_configs(self, configs):
        edp_configs = {}
        for key, value in configs.iteritems():
            if key.startswith(self.EDP_PREFIX):
                edp_configs[key] = value
        for rmkey in edp_configs.keys():
            del configs[rmkey]
        return (configs, edp_configs)

    class Meta:
        name = _("Configure")
        help_text_template = (
            "project/data_processing.jobs/_launch_job_configure_help.html")


class JobExecutionGeneralConfig(workflows.Step):
    action_class = JobExecutionGeneralConfigAction

    def contribute(self, data, context):
        for k, v in data.items():
            if k in ["job_input", "job_output"]:
                context["job_general_" + k] = None if v == "None" else v
            else:
                context["job_general_" + k] = v

        return context


class JobExecutionExistingGeneralConfig(workflows.Step):
    action_class = JobExecutionExistingGeneralConfigAction

    def contribute(self, data, context):
        for k, v in data.items():
            if k in ["job_input", "job_output"]:
                context["job_general_" + k] = None if v == "None" else v
            else:
                context["job_general_" + k] = v

        return context


class JobConfig(workflows.Step):
    action_class = JobConfigAction
    template_name = 'project/data_processing.jobs/config_template.html'

    def contribute(self, data, context):
        job_config = self.clean_configs(
            json.loads(data.get("job_configs", '{}')))
        job_params = self.clean_configs(
            json.loads(data.get("job_params", '{}')))
        job_args_array = self.clean_configs(
            json.loads(data.get("job_args_array", '[]')))
        job_type = data.get("job_type", '')

        context["job_type"] = job_type
        context["job_config"] = {"configs": job_config}
        context["job_config"]["args"] = job_args_array

        if job_type in ["Java", "Spark"]:
            context["job_config"]["configs"][JobConfigAction.MAIN_CLASS] = (
                data.get("main_class", ""))
            context["job_config"]["configs"][JobConfigAction.JAVA_OPTS] = (
                data.get("java_opts", ""))
        elif job_type == "MapReduce.Streaming":
            context["job_config"]["configs"][JobConfigAction.EDP_MAPPER] = (
                data.get("streaming_mapper", ""))
            context["job_config"]["configs"][JobConfigAction.EDP_REDUCER] = (
                data.get("streaming_reducer", ""))
        else:
            context["job_config"]["params"] = job_params

        return context

    @staticmethod
    def clean_configs(configs):
        cleaned_conf = None
        if isinstance(configs, dict):
            cleaned_conf = dict([(k.strip(), v.strip())
                                 for k, v in configs.items()
                                 if len(v.strip()) > 0 and len(k.strip()) > 0])
        elif isinstance(configs, list):
            cleaned_conf = list([v.strip() for v in configs
                                 if len(v.strip()) > 0])
        return cleaned_conf


class NewClusterConfigAction(c_flow.GeneralConfigAction):
    persist_cluster = forms.BooleanField(
        label=_("Persist cluster after job exit"),
        required=False)

    class Meta:
        name = _("Configure Cluster")
        help_text_template = (
            "project/data_processing.clusters/_configure_general_help.html")


class ClusterGeneralConfig(workflows.Step):
    action_class = NewClusterConfigAction
    contributes = ("hidden_configure_field", )

    def contribute(self, data, context):
        for k, v in data.items():
            context["cluster_general_" + k] = v

        return context


class LaunchJob(workflows.Workflow):
    slug = "launch_job"
    name = _("Launch Job")
    finalize_button_name = _("Launch")
    success_message = _("Job launched")
    failure_message = _("Could not launch job")
    success_url = "horizon:project:data_processing.job_executions:index"
    default_steps = (JobExecutionExistingGeneralConfig, JobConfig)

    def handle(self, request, context):
        saharaclient.job_execution_create(
            request,
            context["job_general_job"],
            context["job_general_cluster"],
            context["job_general_job_input"],
            context["job_general_job_output"],
            context["job_config"])
        return True


class SelectHadoopPluginAction(t_flows.SelectPluginAction):
    def __init__(self, request, *args, **kwargs):
        super(SelectHadoopPluginAction, self).__init__(request,
                                                       *args,
                                                       **kwargs)
        self.fields["job_id"] = forms.ChoiceField(
            label=_("Plugin name"),
            initial=request.GET.get("job_id") or request.POST.get("job_id"),
            widget=forms.HiddenInput(attrs={"class": "hidden_create_field"}))

        self.fields["job_configs"] = forms.ChoiceField(
            label=_("Job configs"),
            widget=forms.HiddenInput(attrs={"class": "hidden_create_field"}))

        self.fields["job_args"] = forms.ChoiceField(
            label=_("Job args"),
            widget=forms.HiddenInput(attrs={"class": "hidden_create_field"}))

        self.fields["job_params"] = forms.ChoiceField(
            label=_("Job params"),
            widget=forms.HiddenInput(attrs={"class": "hidden_create_field"}))

        job_ex_id = request.REQUEST.get("job_execution_id")
        if job_ex_id is not None:
            self.fields["job_execution_id"] = forms.ChoiceField(
                label=_("Job Execution ID"),
                initial=request.REQUEST.get("job_execution_id"),
                widget=forms.HiddenInput(
                    attrs={"class": "hidden_create_field"}))

            job_ex_id = request.REQUEST.get("job_execution_id")
            job_configs = (
                saharaclient.job_execution_get(request,
                                               job_ex_id).job_configs)

            if "configs" in job_configs:
                self.fields["job_configs"].initial = (
                    json.dumps(job_configs["configs"]))
            if "params" in job_configs:
                self.fields["job_params"].initial = (
                    json.dumps(job_configs["params"]))
            if "args" in job_configs:
                self.fields["job_args"].initial = (
                    json.dumps(job_configs["args"]))

    class Meta:
        name = _("Select plugin and hadoop version for cluster")
        help_text_template = ("project/data_processing.clusters/"
                              "_create_general_help.html")


class SelectHadoopPlugin(workflows.Step):
    action_class = SelectHadoopPluginAction


class ChosePluginVersion(workflows.Workflow):
    slug = "lunch_job"
    name = _("Launch Job")
    finalize_button_name = _("Create")
    success_message = _("Created")
    failure_message = _("Could not create")
    success_url = "horizon:project:data_processing.cluster_templates:index"
    default_steps = (SelectHadoopPlugin,)


class LaunchJobNewCluster(workflows.Workflow):
    slug = "launch_job"
    name = _("Launch Job")
    finalize_button_name = _("Launch")
    success_message = _("Job launched")
    failure_message = _("Could not launch job")
    success_url = "horizon:project:data_processing.jobs:index"
    default_steps = (ClusterGeneralConfig,
                     JobExecutionGeneralConfig,
                     JobConfig)

    def handle(self, request, context):
        node_groups = None

        plugin, hadoop_version = (
            whelpers.get_plugin_and_hadoop_version(request))

        ct_id = context["cluster_general_cluster_template"] or None
        user_keypair = context["cluster_general_keypair"] or None

        try:
            cluster = saharaclient.cluster_create(
                request,
                context["cluster_general_cluster_name"],
                plugin, hadoop_version,
                cluster_template_id=ct_id,
                default_image_id=context["cluster_general_image"],
                description=context["cluster_general_description"],
                node_groups=node_groups,
                user_keypair_id=user_keypair,
                is_transient=not(context["cluster_general_persist_cluster"]),
                net_id=context.get(
                    "cluster_general_neutron_management_network",
                    None))
        except Exception:
            exceptions.handle(request,
                              _("Unable to create new cluster for job."))
            return False

        try:
            saharaclient.job_execution_create(
                request,
                context["job_general_job"],
                cluster.id,
                context["job_general_job_input"],
                context["job_general_job_output"],
                context["job_config"])
        except Exception:
            exceptions.handle(request,
                              _("Unable to launch job."))
            return False
        return True

# Copyright 2013 Rackspace Hosting
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging

from django.conf import settings
from django.core.urlresolvers import reverse
from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon.utils import memoized
from horizon import workflows
from openstack_dashboard import api

from openstack_dashboard.dashboards.project.instances \
    import utils as instance_utils


LOG = logging.getLogger(__name__)


class SetInstanceDetailsAction(workflows.Action):
    name = forms.CharField(max_length=80, label=_("Instance Name"))
    flavor = forms.ChoiceField(label=_("Flavor"),
                               help_text=_("Size of image to launch."))
    volume = forms.IntegerField(label=_("Volume Size"),
                                min_value=0,
                                initial=1,
                                help_text=_("Size of the volume in GB."))
    datastore = forms.ChoiceField(label=_("Datastore"),
                                  help_text=_(
                                      "Type and version of datastore."))

    class Meta:
        name = _("Details")
        help_text_template = "project/databases/_launch_details_help.html"

    def clean(self):
        if self.data.get("datastore", None) == "select_datastore_type_version":
            msg = _("You must select a datastore type and version.")
            self._errors["datastore"] = self.error_class([msg])
        return self.cleaned_data

    @memoized.memoized_method
    def flavors(self, request):
        try:
            return api.trove.flavor_list(request)
        except Exception:
            LOG.exception("Exception while obtaining flavors list")
            redirect = reverse("horizon:project:databases:index")
            exceptions.handle(request,
                              _('Unable to obtain flavors.'),
                              redirect=redirect)

    def populate_flavor_choices(self, request, context):
        flavors = self.flavors(request)
        if flavors:
            return instance_utils.sort_flavor_list(request, flavors)
        return []

    @memoized.memoized_method
    def datastores(self, request):
        try:
            return api.trove.datastore_list(request)
        except Exception:
            LOG.exception("Exception while obtaining datastores list")
            self._datastores = []

    @memoized.memoized_method
    def datastore_versions(self, request, datastore):
        try:
            return api.trove.datastore_version_list(request, datastore)
        except Exception:
            LOG.exception("Exception while obtaining datastore version list")
            self._datastore_versions = []

    def populate_datastore_choices(self, request, context):
        choices = ()
        set_initial = False
        datastores = self.datastores(request)
        if datastores is not None:
            num_datastores_with_one_version = 0
            for ds in datastores:
                versions = self.datastore_versions(request, ds.name)
                if not set_initial:
                    if len(versions) >= 2:
                        set_initial = True
                    elif len(versions) == 1:
                        num_datastores_with_one_version += 1
                        if num_datastores_with_one_version > 1:
                            set_initial = True
                if len(versions) > 0:
                    # only add to choices if datastore has at least one version
                    version_choices = ()
                    for v in versions:
                        version_choices = (version_choices +
                                           ((ds.name + ',' + v.name, v.name),))
                    datastore_choices = (ds.name, version_choices)
                    choices = choices + (datastore_choices,)
            if set_initial:
                # prepend choice to force user to choose
                initial = (('select_datastore_type_version',
                            _('Select datastore type and version')))
                choices = (initial,) + choices
        return choices


TROVE_ADD_USER_PERMS = getattr(settings, 'TROVE_ADD_USER_PERMS', [])
TROVE_ADD_DATABASE_PERMS = getattr(settings, 'TROVE_ADD_DATABASE_PERMS', [])
TROVE_ADD_PERMS = TROVE_ADD_USER_PERMS + TROVE_ADD_DATABASE_PERMS


class SetInstanceDetails(workflows.Step):
    action_class = SetInstanceDetailsAction
    contributes = ("name", "volume", "flavor", "datastore")


class SetNetworkAction(workflows.Action):
    network = forms.MultipleChoiceField(label=_("Networks"),
                                        widget=forms.CheckboxSelectMultiple(),
                                        error_messages={
                                            'required': _(
                                                "At least one network must"
                                                " be specified.")},
                                        help_text=_("Launch instance with"
                                                    " these networks"))

    def __init__(self, request, *args, **kwargs):
        super(SetNetworkAction, self).__init__(request, *args, **kwargs)
        network_list = self.fields["network"].choices
        if len(network_list) == 1:
            self.fields['network'].initial = [network_list[0][0]]

    class Meta:
        name = _("Networking")
        permissions = ('openstack.services.network',)
        help_text = _("Select networks for your instance.")

    def populate_network_choices(self, request, context):
        try:
            tenant_id = self.request.user.tenant_id
            networks = api.neutron.network_list_for_tenant(request, tenant_id)
            network_list = [(network.id, network.name_or_id)
                            for network in networks]
        except Exception:
            network_list = []
            exceptions.handle(request,
                              _('Unable to retrieve networks.'))
        return network_list


class SetNetwork(workflows.Step):
    action_class = SetNetworkAction
    template_name = "project/databases/_launch_networks.html"
    contributes = ("network_id",)

    def contribute(self, data, context):
        if data:
            networks = self.workflow.request.POST.getlist("network")
            # If no networks are explicitly specified, network list
            # contains an empty string, so remove it.
            networks = [n for n in networks if n != '']
            if networks:
                context['network_id'] = networks

        return context


class AddDatabasesAction(workflows.Action):
    """Initialize the database with users/databases. This tab will honor
    the settings which should be a list of permissions required:

    * TROVE_ADD_USER_PERMS = []
    * TROVE_ADD_DATABASE_PERMS = []
    """
    databases = forms.CharField(label=_('Initial Databases'),
                                required=False,
                                help_text=_('Comma separated list of '
                                            'databases to create'))
    user = forms.CharField(label=_('Initial Admin User'),
                           required=False,
                           help_text=_("Initial admin user to add"))
    password = forms.CharField(widget=forms.PasswordInput(),
                               label=_("Password"),
                               required=False)
    host = forms.CharField(label=_("Allowed Host (optional)"),
                           required=False,
                           help_text=_("Host or IP that the user is allowed "
                                       "to connect through."))

    class Meta:
        name = _("Initialize Databases")
        permissions = TROVE_ADD_PERMS
        help_text_template = "project/databases/_launch_initialize_help.html"

    def clean(self):
        cleaned_data = super(AddDatabasesAction, self).clean()
        if cleaned_data.get('user'):
            if not cleaned_data.get('password'):
                msg = _('You must specify a password if you create a user.')
                self._errors["password"] = self.error_class([msg])
            if not cleaned_data.get('databases'):
                msg = _('You must specify at least one database if '
                        'you create a user.')
                self._errors["databases"] = self.error_class([msg])
        return cleaned_data


class InitializeDatabase(workflows.Step):
    action_class = AddDatabasesAction
    contributes = ["databases", 'user', 'password', 'host']


class RestoreAction(workflows.Action):
    backup = forms.ChoiceField(label=_("Backup"),
                               required=False,
                               help_text=_('Select a backup to restore'))

    class Meta:
        name = _("Restore From Backup")
        permissions = ('openstack.services.object-store',)
        help_text_template = "project/databases/_launch_restore_help.html"

    def populate_backup_choices(self, request, context):
        try:
            backups = api.trove.backup_list(request)
            choices = [(b.id, b.name) for b in backups
                       if b.status == 'COMPLETED']
        except Exception:
            choices = []

        if choices:
            choices.insert(0, ("", _("Select backup")))
        else:
            choices.insert(0, ("", _("No backups available")))
        return choices

    def clean_backup(self):
        backup = self.cleaned_data['backup']
        if backup:
            try:
                # Make sure the user is not "hacking" the form
                # and that they have access to this backup_id
                LOG.debug("Obtaining backups")
                bkup = api.trove.backup_get(self.request, backup)
                self.cleaned_data['backup'] = bkup.id
            except Exception:
                raise forms.ValidationError(_("Unable to find backup!"))
        return backup


class RestoreBackup(workflows.Step):
    action_class = RestoreAction
    contributes = ['backup']


class LaunchInstance(workflows.Workflow):
    slug = "launch_instance"
    name = _("Launch Instance")
    finalize_button_name = _("Launch")
    success_message = _('Launched %(count)s named "%(name)s".')
    failure_message = _('Unable to launch %(count)s named "%(name)s".')
    success_url = "horizon:project:databases:index"
    default_steps = (SetInstanceDetails,
                     SetNetwork,
                     InitializeDatabase,
                     RestoreBackup)

    def __init__(self, request=None, context_seed=None, entry_point=None,
                 *args, **kwargs):
        super(LaunchInstance, self).__init__(request, context_seed,
                                             entry_point, *args, **kwargs)
        self.attrs['autocomplete'] = (
            settings.HORIZON_CONFIG.get('password_autocomplete'))

    def format_status_message(self, message):
        name = self.context.get('name', 'unknown instance')
        return message % {"count": _("instance"), "name": name}

    def _get_databases(self, context):
        """Returns the initial databases for this instance."""
        databases = None
        if context.get('databases'):
            dbs = context['databases']
            databases = [{'name': d.strip()} for d in dbs.split(',')]
        return databases

    def _get_users(self, context):
        users = None
        if context.get('user'):
            user = {
                'name': context['user'],
                'password': context['password'],
                'databases': self._get_databases(context),
            }
            if context['host']:
                user['host'] = context['host']
            users = [user]
        return users

    def _get_backup(self, context):
        backup = None
        if context.get('backup'):
            backup = {'backupRef': context['backup']}
        return backup

    def _get_nics(self, context):
        netids = context.get('network_id', None)
        if netids:
            return [{"net-id": netid, "v4-fixed-ip": ""}
                    for netid in netids]
        else:
            return None

    def handle(self, request, context):
        try:
            datastore = self.context['datastore'].split(',')[0]
            datastore_version = self.context['datastore'].split(',')[1]
            LOG.info("Launching database instance with parameters "
                     "{name=%s, volume=%s, flavor=%s, "
                     "datastore=%s, datastore_version=%s, "
                     "dbs=%s, users=%s, "
                     "backups=%s, nics=%s}",
                     context['name'], context['volume'], context['flavor'],
                     datastore, datastore_version,
                     self._get_databases(context), self._get_users(context),
                     self._get_backup(context), self._get_nics(context))
            api.trove.instance_create(request,
                                      context['name'],
                                      context['volume'],
                                      context['flavor'],
                                      datastore=datastore,
                                      datastore_version=datastore_version,
                                      databases=self._get_databases(context),
                                      users=self._get_users(context),
                                      restore_point=self._get_backup(context),
                                      nics=self._get_nics(context))
            return True
        except Exception:
            exceptions.handle(request)
            return False

# Copyright 2012 Nebula, Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.


import logging

from django.conf import settings
from django.core import urlresolvers
from django.http import HttpResponse  # noqa
from django import shortcuts
from django import template
from django.template.defaultfilters import title  # noqa
from django.utils.http import urlencode
from django.utils.translation import npgettext_lazy
from django.utils.translation import pgettext_lazy
from django.utils.translation import string_concat  # noqa
from django.utils.translation import ugettext_lazy as _
from django.utils.translation import ungettext_lazy

from horizon import conf
from horizon import exceptions
from horizon import messages
from horizon import tables
from horizon.templatetags import sizeformat
from horizon.utils import filters

from openstack_dashboard import api
from openstack_dashboard.dashboards.project.access_and_security.floating_ips \
    import workflows
from openstack_dashboard.dashboards.project.instances import tabs
from openstack_dashboard.dashboards.project.instances.workflows \
    import resize_instance
from openstack_dashboard.dashboards.project.instances.workflows \
    import update_instance
from openstack_dashboard import policy


LOG = logging.getLogger(__name__)

ACTIVE_STATES = ("ACTIVE",)
VOLUME_ATTACH_READY_STATES = ("ACTIVE", "SHUTOFF")
SNAPSHOT_READY_STATES = ("ACTIVE", "SHUTOFF", "PAUSED", "SUSPENDED")

POWER_STATES = {
    0: "NO STATE",
    1: "RUNNING",
    2: "BLOCKED",
    3: "PAUSED",
    4: "SHUTDOWN",
    5: "SHUTOFF",
    6: "CRASHED",
    7: "SUSPENDED",
    8: "FAILED",
    9: "BUILDING",
}

PAUSE = 0
UNPAUSE = 1
SUSPEND = 0
RESUME = 1


def is_deleting(instance):
    task_state = getattr(instance, "OS-EXT-STS:task_state", None)
    if not task_state:
        return False
    return task_state.lower() == "deleting"


class TerminateInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "terminate"
    classes = ("btn-danger",)
    icon = "off"
    policy_rules = (("compute", "compute:delete"),)

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Terminate Instance",
            u"Terminate Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Scheduled termination of Instance",
            u"Scheduled termination of Instances",
            count
        )

    def allowed(self, request, instance=None):
        """Allow terminate action if instance not currently being deleted."""
        return not is_deleting(instance)

    def action(self, request, obj_id):
        api.nova.server_delete(request, obj_id)


class RebootInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "reboot"
    classes = ('btn-danger', 'btn-reboot')
    policy_rules = (("compute", "compute:reboot"),)

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Hard Reboot Instance",
            u"Hard Reboot Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Hard Rebooted Instance",
            u"Hard Rebooted Instances",
            count
        )

    def allowed(self, request, instance=None):
        if instance is not None:
            return ((instance.status in ACTIVE_STATES
                     or instance.status == 'SHUTOFF')
                    and not is_deleting(instance))
        else:
            return True

    def action(self, request, obj_id):
        api.nova.server_reboot(request, obj_id, soft_reboot=False)


class SoftRebootInstance(RebootInstance):
    name = "soft_reboot"

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Soft Reboot Instance",
            u"Soft Reboot Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Soft Rebooted Instance",
            u"Soft Rebooted Instances",
            count
        )

    def action(self, request, obj_id):
        api.nova.server_reboot(request, obj_id, soft_reboot=True)


class TogglePause(tables.BatchAction):
    name = "pause"
    icon = "pause"

    @staticmethod
    def action_present(count):
        return (
            ungettext_lazy(
                u"Pause Instance",
                u"Pause Instances",
                count
            ),
            ungettext_lazy(
                u"Resume Instance",
                u"Resume Instances",
                count
            ),
        )

    @staticmethod
    def action_past(count):
        return (
            ungettext_lazy(
                u"Paused Instance",
                u"Paused Instances",
                count
            ),
            ungettext_lazy(
                u"Resumed Instance",
                u"Resumed Instances",
                count
            ),
        )

    def allowed(self, request, instance=None):
        if not api.nova.extension_supported('AdminActions',
                                            request):
            return False
        if not instance:
            return False
        self.paused = instance.status == "PAUSED"
        if self.paused:
            self.current_present_action = UNPAUSE
            policy = (("compute", "compute_extension:admin_actions:unpause"),)
        else:
            self.current_present_action = PAUSE
            policy = (("compute", "compute_extension:admin_actions:pause"),)

        has_permission = True
        policy_check = getattr(settings, "POLICY_CHECK_FUNCTION", None)
        if policy_check:
            has_permission = policy_check(
                policy, request,
                target={'project_id': getattr(instance, 'tenant_id', None)})

        return (has_permission
                and (instance.status in ACTIVE_STATES or self.paused)
                and not is_deleting(instance))

    def action(self, request, obj_id):
        if self.paused:
            api.nova.server_unpause(request, obj_id)
            self.current_past_action = UNPAUSE
        else:
            api.nova.server_pause(request, obj_id)
            self.current_past_action = PAUSE


class ToggleSuspend(tables.BatchAction):
    name = "suspend"
    classes = ("btn-suspend",)

    @staticmethod
    def action_present(count):
        return (
            ungettext_lazy(
                u"Suspend Instance",
                u"Suspend Instances",
                count
            ),
            ungettext_lazy(
                u"Resume Instance",
                u"Resume Instances",
                count
            ),
        )

    @staticmethod
    def action_past(count):
        return (
            ungettext_lazy(
                u"Suspended Instance",
                u"Suspended Instances",
                count
            ),
            ungettext_lazy(
                u"Resumed Instance",
                u"Resumed Instances",
                count
            ),
        )

    def allowed(self, request, instance=None):
        if not api.nova.extension_supported('AdminActions',
                                            request):
            return False
        if not instance:
            return False
        self.suspended = instance.status == "SUSPENDED"
        if self.suspended:
            self.current_present_action = RESUME
            policy = (("compute", "compute_extension:admin_actions:resume"),)
        else:
            self.current_present_action = SUSPEND
            policy = (("compute", "compute_extension:admin_actions:suspend"),)

        has_permission = True
        policy_check = getattr(settings, "POLICY_CHECK_FUNCTION", None)
        if policy_check:
            has_permission = policy_check(
                policy, request,
                target={'project_id': getattr(instance, 'tenant_id', None)})

        return (has_permission
                and (instance.status in ACTIVE_STATES or self.suspended)
                and not is_deleting(instance))

    def action(self, request, obj_id):
        if self.suspended:
            api.nova.server_resume(request, obj_id)
            self.current_past_action = RESUME
        else:
            api.nova.server_suspend(request, obj_id)
            self.current_past_action = SUSPEND


class LaunchLink(tables.LinkAction):
    name = "launch"
    verbose_name = _("Launch Instance")
    url = "horizon:project:instances:launch"
    classes = ("ajax-modal", "btn-launch")
    icon = "cloud-upload"
    policy_rules = (("compute", "compute:create"),)
    ajax = True

    def __init__(self, attrs=None, **kwargs):
        kwargs['preempt'] = True
        super(LaunchLink, self).__init__(attrs, **kwargs)

    def allowed(self, request, datum):
        try:
            limits = api.nova.tenant_absolute_limits(request, reserved=True)

            instances_available = limits['maxTotalInstances'] \
                - limits['totalInstancesUsed']
            cores_available = limits['maxTotalCores'] \
                - limits['totalCoresUsed']
            ram_available = limits['maxTotalRAMSize'] - limits['totalRAMUsed']

            if instances_available <= 0 or cores_available <= 0 \
                    or ram_available <= 0:
                if "disabled" not in self.classes:
                    self.classes = [c for c in self.classes] + ['disabled']
                    self.verbose_name = string_concat(self.verbose_name, ' ',
                                                      _("(Quota exceeded)"))
            else:
                self.verbose_name = _("Launch Instance")
                classes = [c for c in self.classes if c != "disabled"]
                self.classes = classes
        except Exception:
            LOG.exception("Failed to retrieve quota information")
            # If we can't get the quota information, leave it to the
            # API to check when launching
        return True  # The action should always be displayed

    def single(self, table, request, object_id=None):
        self.allowed(request, None)
        return HttpResponse(self.render())


class EditInstance(policy.PolicyTargetMixin, tables.LinkAction):
    name = "edit"
    verbose_name = _("Edit Instance")
    url = "horizon:project:instances:update"
    classes = ("ajax-modal",)
    icon = "pencil"
    policy_rules = (("compute", "compute:update"),)

    def get_link_url(self, project):
        return self._get_link_url(project, 'instance_info')

    def _get_link_url(self, project, step_slug):
        base_url = urlresolvers.reverse(self.url, args=[project.id])
        next_url = self.table.get_full_url()
        params = {"step": step_slug,
                  update_instance.UpdateInstance.redirect_param_name: next_url}
        param = urlencode(params)
        return "?".join([base_url, param])

    def allowed(self, request, instance):
        return not is_deleting(instance)


class EditInstanceSecurityGroups(EditInstance):
    name = "edit_secgroups"
    verbose_name = _("Edit Security Groups")

    def get_link_url(self, project):
        return self._get_link_url(project, 'update_security_groups')

    def allowed(self, request, instance=None):
        return (instance.status in ACTIVE_STATES and
                not is_deleting(instance) and
                request.user.tenant_id == instance.tenant_id)


class CreateSnapshot(policy.PolicyTargetMixin, tables.LinkAction):
    name = "snapshot"
    verbose_name = _("Create Snapshot")
    url = "horizon:project:images:snapshots:create"
    classes = ("ajax-modal",)
    icon = "camera"
    policy_rules = (("compute", "compute:snapshot"),)

    def allowed(self, request, instance=None):
        return instance.status in SNAPSHOT_READY_STATES \
            and not is_deleting(instance)


class ConsoleLink(policy.PolicyTargetMixin, tables.LinkAction):
    name = "console"
    verbose_name = _("Console")
    url = "horizon:project:instances:detail"
    classes = ("btn-console",)
    policy_rules = (("compute", "compute_extension:consoles"),)

    def allowed(self, request, instance=None):
        # We check if ConsoleLink is allowed only if settings.CONSOLE_TYPE is
        # not set at all, or if it's set to any value other than None or False.
        return bool(getattr(settings, 'CONSOLE_TYPE', True)) and \
            instance.status in ACTIVE_STATES and not is_deleting(instance)

    def get_link_url(self, datum):
        base_url = super(ConsoleLink, self).get_link_url(datum)
        tab_query_string = tabs.ConsoleTab(
            tabs.InstanceDetailTabs).get_query_string()
        return "?".join([base_url, tab_query_string])


class LogLink(policy.PolicyTargetMixin, tables.LinkAction):
    name = "log"
    verbose_name = _("View Log")
    url = "horizon:project:instances:detail"
    classes = ("btn-log",)
    policy_rules = (("compute", "compute_extension:console_output"),)

    def allowed(self, request, instance=None):
        return instance.status in ACTIVE_STATES and not is_deleting(instance)

    def get_link_url(self, datum):
        base_url = super(LogLink, self).get_link_url(datum)
        tab_query_string = tabs.LogTab(
            tabs.InstanceDetailTabs).get_query_string()
        return "?".join([base_url, tab_query_string])


class ResizeLink(policy.PolicyTargetMixin, tables.LinkAction):
    name = "resize"
    verbose_name = _("Resize Instance")
    url = "horizon:project:instances:resize"
    classes = ("ajax-modal", "btn-resize")
    policy_rules = (("compute", "compute:resize"),)

    def get_link_url(self, project):
        return self._get_link_url(project, 'flavor_choice')

    def _get_link_url(self, project, step_slug):
        base_url = urlresolvers.reverse(self.url, args=[project.id])
        next_url = self.table.get_full_url()
        params = {"step": step_slug,
                  resize_instance.ResizeInstance.redirect_param_name: next_url}
        param = urlencode(params)
        return "?".join([base_url, param])

    def allowed(self, request, instance):
        return ((instance.status in ACTIVE_STATES
                 or instance.status == 'SHUTOFF')
                and not is_deleting(instance))


class ConfirmResize(policy.PolicyTargetMixin, tables.Action):
    name = "confirm"
    verbose_name = _("Confirm Resize/Migrate")
    classes = ("btn-confirm", "btn-action-required")
    policy_rules = (("compute", "compute:confirm_resize"),)

    def allowed(self, request, instance):
        return instance.status == 'VERIFY_RESIZE'

    def single(self, table, request, instance):
        api.nova.server_confirm_resize(request, instance)


class RevertResize(policy.PolicyTargetMixin, tables.Action):
    name = "revert"
    verbose_name = _("Revert Resize/Migrate")
    classes = ("btn-revert", "btn-action-required")
    policy_rules = (("compute", "compute:revert_resize"),)

    def allowed(self, request, instance):
        return instance.status == 'VERIFY_RESIZE'

    def single(self, table, request, instance):
        api.nova.server_revert_resize(request, instance)


class RebuildInstance(policy.PolicyTargetMixin, tables.LinkAction):
    name = "rebuild"
    verbose_name = _("Rebuild Instance")
    classes = ("btn-rebuild", "ajax-modal")
    url = "horizon:project:instances:rebuild"
    policy_rules = (("compute", "compute:rebuild"),)

    def allowed(self, request, instance):
        return ((instance.status in ACTIVE_STATES
                 or instance.status == 'SHUTOFF')
                and not is_deleting(instance))

    def get_link_url(self, datum):
        instance_id = self.table.get_object_id(datum)
        return urlresolvers.reverse(self.url, args=[instance_id])


class DecryptInstancePassword(tables.LinkAction):
    name = "decryptpassword"
    verbose_name = _("Retrieve Password")
    classes = ("btn-decrypt", "ajax-modal")
    url = "horizon:project:instances:decryptpassword"

    def allowed(self, request, instance):
        enable = getattr(settings,
                         'OPENSTACK_ENABLE_PASSWORD_RETRIEVE',
                         False)
        return (enable
                and (instance.status in ACTIVE_STATES
                     or instance.status == 'SHUTOFF')
                and not is_deleting(instance)
                and get_keyname(instance) is not None)

    def get_link_url(self, datum):
        instance_id = self.table.get_object_id(datum)
        keypair_name = get_keyname(datum)
        return urlresolvers.reverse(self.url, args=[instance_id,
                                                    keypair_name])


class AssociateIP(policy.PolicyTargetMixin, tables.LinkAction):
    name = "associate"
    verbose_name = _("Associate Floating IP")
    url = "horizon:project:access_and_security:floating_ips:associate"
    classes = ("ajax-modal",)
    icon = "link"
    policy_rules = (("compute", "network:associate_floating_ip"),)

    def allowed(self, request, instance):
        if not api.network.floating_ip_supported(request):
            return False
        if api.network.floating_ip_simple_associate_supported(request):
            return False
        return not is_deleting(instance)

    def get_link_url(self, datum):
        base_url = urlresolvers.reverse(self.url)
        next_url = self.table.get_full_url()
        params = {
            "instance_id": self.table.get_object_id(datum),
            workflows.IPAssociationWorkflow.redirect_param_name: next_url}
        params = urlencode(params)
        return "?".join([base_url, params])


class SimpleAssociateIP(policy.PolicyTargetMixin, tables.Action):
    name = "associate-simple"
    verbose_name = _("Associate Floating IP")
    icon = "link"
    policy_rules = (("compute", "network:associate_floating_ip"),)

    def allowed(self, request, instance):
        if not api.network.floating_ip_simple_associate_supported(request):
            return False
        return not is_deleting(instance)

    def single(self, table, request, instance_id):
        try:
            # target_id is port_id for Neutron and instance_id for Nova Network
            # (Neutron API wrapper returns a 'portid_fixedip' string)
            target_id = api.network.floating_ip_target_get_by_instance(
                request, instance_id).split('_')[0]

            fip = api.network.tenant_floating_ip_allocate(request)
            api.network.floating_ip_associate(request, fip.id, target_id)
            messages.success(request,
                             _("Successfully associated floating IP: %s")
                             % fip.ip)
        except Exception:
            exceptions.handle(request,
                              _("Unable to associate floating IP."))
        return shortcuts.redirect(request.get_full_path())


class SimpleDisassociateIP(policy.PolicyTargetMixin, tables.Action):
    name = "disassociate"
    verbose_name = _("Disassociate Floating IP")
    classes = ("btn-danger", "btn-disassociate",)
    policy_rules = (("compute", "network:disassociate_floating_ip"),)

    def allowed(self, request, instance):
        if not api.network.floating_ip_supported(request):
            return False
        if not conf.HORIZON_CONFIG["simple_ip_management"]:
            return False
        return not is_deleting(instance)

    def single(self, table, request, instance_id):
        try:
            # target_id is port_id for Neutron and instance_id for Nova Network
            # (Neutron API wrapper returns a 'portid_fixedip' string)
            targets = api.network.floating_ip_target_list_by_instance(
                request, instance_id)

            target_ids = [t.split('_')[0] for t in targets]

            fips = [fip for fip in api.network.tenant_floating_ip_list(request)
                    if fip.port_id in target_ids]
            # Removing multiple floating IPs at once doesn't work, so this pops
            # off the first one.
            if fips:
                fip = fips.pop()
                api.network.floating_ip_disassociate(request, fip.id)
                messages.success(request,
                                 _("Successfully disassociated "
                                   "floating IP: %s") % fip.ip)
            else:
                messages.info(request, _("No floating IPs to disassociate."))
        except Exception:
            exceptions.handle(request,
                              _("Unable to disassociate floating IP."))
        return shortcuts.redirect(request.get_full_path())


def instance_fault_to_friendly_message(instance):
    fault = getattr(instance, 'fault', {})
    message = fault.get('message', _("Unknown"))
    default_message = _("Please try again later [Error: %s].") % message
    fault_map = {
        'NoValidHost': _("There is not enough capacity for this "
                         "flavor in the selected availability zone. "
                         "Try again later or select a different availability "
                         "zone.")
    }
    return fault_map.get(message, default_message)


def get_instance_error(instance):
    if instance.status.lower() != 'error':
        return None
    message = instance_fault_to_friendly_message(instance)
    preamble = _('Failed to launch instance "%s"'
                 ) % instance.name or instance.id
    message = string_concat(preamble, ': ', message)
    return message


class UpdateRow(tables.Row):
    ajax = True

    def get_data(self, request, instance_id):
        instance = api.nova.server_get(request, instance_id)
        try:
            instance.full_flavor = api.nova.flavor_get(request,
                                                       instance.flavor["id"])
        except Exception:
            exceptions.handle(request,
                              _('Unable to retrieve flavor information '
                                'for instance "%s".') % instance_id,
                              ignore=True)
        error = get_instance_error(instance)
        if error:
            messages.error(request, error)
        return instance


class StartInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "start"
    classes = ('btn-confirm',)
    policy_rules = (("compute", "compute:start"),)

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Start Instance",
            u"Start Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Started Instance",
            u"Started Instances",
            count
        )

    def allowed(self, request, instance):
        return ((instance is None) or
                (instance.status in ("SHUTDOWN", "SHUTOFF", "CRASHED")))

    def action(self, request, obj_id):
        api.nova.server_start(request, obj_id)


class StopInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "stop"
    classes = ('btn-danger',)
    policy_rules = (("compute", "compute:stop"),)

    @staticmethod
    def action_present(count):
        return npgettext_lazy(
            "Action to perform (the instance is currently running)",
            u"Shut Off Instance",
            u"Shut Off Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return npgettext_lazy(
            "Past action (the instance is currently already Shut Off)",
            u"Shut Off Instance",
            u"Shut Off Instances",
            count
        )

    def allowed(self, request, instance):
        return ((instance is None)
                or ((get_power_state(instance) in ("RUNNING", "SUSPENDED"))
                    and not is_deleting(instance)))

    def action(self, request, obj_id):
        api.nova.server_stop(request, obj_id)


class LockInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "lock"
    policy_rules = (("compute", "compute_extension:admin_actions:lock"),)

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Lock Instance",
            u"Lock Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Locked Instance",
            u"Locked Instances",
            count
        )

    # TODO(akrivoka): When the lock status is added to nova, revisit this
    # to only allow unlocked instances to be locked
    def allowed(self, request, instance):
        if not api.nova.extension_supported('AdminActions', request):
            return False
        return True

    def action(self, request, obj_id):
        api.nova.server_lock(request, obj_id)


class UnlockInstance(policy.PolicyTargetMixin, tables.BatchAction):
    name = "unlock"
    policy_rules = (("compute", "compute_extension:admin_actions:unlock"),)

    @staticmethod
    def action_present(count):
        return ungettext_lazy(
            u"Unlock Instance",
            u"Unlock Instances",
            count
        )

    @staticmethod
    def action_past(count):
        return ungettext_lazy(
            u"Unlocked Instance",
            u"Unlocked Instances",
            count
        )

    # TODO(akrivoka): When the lock status is added to nova, revisit this
    # to only allow locked instances to be unlocked
    def allowed(self, request, instance):
        if not api.nova.extension_supported('AdminActions', request):
            return False
        return True

    def action(self, request, obj_id):
        api.nova.server_unlock(request, obj_id)


def get_ips(instance):
    template_name = 'project/instances/_instance_ips.html'
    context = {"instance": instance}
    return template.loader.render_to_string(template_name, context)


def get_size(instance):
    if hasattr(instance, "full_flavor"):
        template_name = 'project/instances/_instance_flavor.html'
        size_ram = sizeformat.mb_float_format(instance.full_flavor.ram)
        if instance.full_flavor.disk > 0:
            size_disk = sizeformat.diskgbformat(instance.full_flavor.disk)
        else:
            size_disk = _("%s GB") % "0"
        context = {
            "name": instance.full_flavor.name,
            "id": instance.id,
            "size_disk": size_disk,
            "size_ram": size_ram,
            "vcpus": instance.full_flavor.vcpus
        }
        return template.loader.render_to_string(template_name, context)
    return _("Not available")


def get_keyname(instance):
    if hasattr(instance, "key_name"):
        keyname = instance.key_name
        return keyname
    return _("Not available")


def get_power_state(instance):
    return POWER_STATES.get(getattr(instance, "OS-EXT-STS:power_state", 0), '')


STATUS_DISPLAY_CHOICES = (
    ("deleted", pgettext_lazy("Current status of an Instance", u"Deleted")),
    ("active", pgettext_lazy("Current status of an Instance", u"Active")),
    ("shutoff", pgettext_lazy("Current status of an Instance", u"Shutoff")),
    ("suspended", pgettext_lazy("Current status of an Instance",
                                u"Suspended")),
    ("paused", pgettext_lazy("Current status of an Instance", u"Paused")),
    ("error", pgettext_lazy("Current status of an Instance", u"Error")),
    ("resize", pgettext_lazy("Current status of an Instance",
                             u"Resize/Migrate")),
    ("verify_resize", pgettext_lazy("Current status of an Instance",
                                    u"Confirm or Revert Resize/Migrate")),
    ("revert_resize", pgettext_lazy(
        "Current status of an Instance", u"Revert Resize/Migrate")),
    ("reboot", pgettext_lazy("Current status of an Instance", u"Reboot")),
    ("hard_reboot", pgettext_lazy("Current status of an Instance",
                                  u"Hard Reboot")),
    ("password", pgettext_lazy("Current status of an Instance", u"Password")),
    ("rebuild", pgettext_lazy("Current status of an Instance", u"Rebuild")),
    ("migrating", pgettext_lazy("Current status of an Instance",
                                u"Migrating")),
    ("build", pgettext_lazy("Current status of an Instance", u"Build")),
    ("rescue", pgettext_lazy("Current status of an Instance", u"Rescue")),
    ("deleted", pgettext_lazy("Current status of an Instance", u"Deleted")),
    ("soft_deleted", pgettext_lazy("Current status of an Instance",
                                   u"Soft Deleted")),
    ("shelved", pgettext_lazy("Current status of an Instance", u"Shelved")),
    ("shelved_offloaded", pgettext_lazy("Current status of an Instance",
                                        u"Shelved Offloaded")),
)

TASK_DISPLAY_NONE = pgettext_lazy("Task status of an Instance", u"None")

# Mapping of task states taken from Nova's nova/compute/task_states.py
TASK_DISPLAY_CHOICES = (
    ("scheduling", pgettext_lazy("Task status of an Instance",
                                 u"Scheduling")),
    ("block_device_mapping", pgettext_lazy("Task status of an Instance",
                                           u"Block Device Mapping")),
    ("networking", pgettext_lazy("Task status of an Instance",
                                 u"Networking")),
    ("spawning", pgettext_lazy("Task status of an Instance", u"Spawning")),
    ("image_snapshot", pgettext_lazy("Task status of an Instance",
                                     u"Snapshotting")),
    ("image_snapshot_pending", pgettext_lazy("Task status of an Instance",
                                             u"Image Snapshot Pending")),
    ("image_pending_upload", pgettext_lazy("Task status of an Instance",
                                           u"Image Pending Upload")),
    ("image_uploading", pgettext_lazy("Task status of an Instance",
                                      u"Image Uploading")),
    ("image_backup", pgettext_lazy("Task status of an Instance",
                                   u"Image Backup")),
    ("updating_password", pgettext_lazy("Task status of an Instance",
                                        u"Updating Password")),
    ("resize_prep", pgettext_lazy("Task status of an Instance",
                                  u"Preparing Resize or Migrate")),
    ("resize_migrating", pgettext_lazy("Task status of an Instance",
                                       u"Resizing or Migrating")),
    ("resize_migrated", pgettext_lazy("Task status of an Instance",
                                      u"Resized or Migrated")),
    ("resize_finish", pgettext_lazy("Task status of an Instance",
                                    u"Finishing Resize or Migrate")),
    ("resize_reverting", pgettext_lazy("Task status of an Instance",
                                       u"Reverting Resize or Migrate")),
    ("resize_confirming", pgettext_lazy("Task status of an Instance",
                                        u"Confirming Resize or Migrate")),
    ("rebooting", pgettext_lazy("Task status of an Instance", u"Rebooting")),
    ("reboot_pending", pgettext_lazy("Task status of an Instance",
                                     u"Reboot Pending")),
    ("reboot_started", pgettext_lazy("Task status of an Instance",
                                     u"Reboot Started")),
    ("rebooting_hard", pgettext_lazy("Task status of an Instance",
                                     u"Rebooting Hard")),
    ("reboot_pending_hard", pgettext_lazy("Task status of an Instance",
                                          u"Reboot Pending Hard")),
    ("reboot_started_hard", pgettext_lazy("Task status of an Instance",
                                          u"Reboot Started Hard")),
    ("pausing", pgettext_lazy("Task status of an Instance", u"Pausing")),
    ("unpausing", pgettext_lazy("Task status of an Instance", u"Resuming")),
    ("suspending", pgettext_lazy("Task status of an Instance",
                                 u"Suspending")),
    ("resuming", pgettext_lazy("Task status of an Instance", u"Resuming")),
    ("powering-off", pgettext_lazy("Task status of an Instance",
                                   u"Powering Off")),
    ("powering-on", pgettext_lazy("Task status of an Instance",
                                  u"Powering On")),
    ("rescuing", pgettext_lazy("Task status of an Instance", u"Rescuing")),
    ("unrescuing", pgettext_lazy("Task status of an Instance",
                                 u"Unrescuing")),
    ("rebuilding", pgettext_lazy("Task status of an Instance",
                                 u"Rebuilding")),
    ("rebuild_block_device_mapping", pgettext_lazy(
        "Task status of an Instance", u"Rebuild Block Device Mapping")),
    ("rebuild_spawning", pgettext_lazy("Task status of an Instance",
                                       u"Rebuild Spawning")),
    ("migrating", pgettext_lazy("Task status of an Instance", u"Migrating")),
    ("deleting", pgettext_lazy("Task status of an Instance", u"Deleting")),
    ("soft-deleting", pgettext_lazy("Task status of an Instance",
                                    u"Soft Deleting")),
    ("restoring", pgettext_lazy("Task status of an Instance", u"Restoring")),
    ("shelving", pgettext_lazy("Task status of an Instance", u"Shelving")),
    ("shelving_image_pending_upload", pgettext_lazy(
        "Task status of an Instance", u"Shelving Image Pending Upload")),
    ("shelving_image_uploading", pgettext_lazy("Task status of an Instance",
                                               u"Shelving Image Uploading")),
    ("shelving_offloading", pgettext_lazy("Task status of an Instance",
                                          u"Shelving Offloading")),
    ("unshelving", pgettext_lazy("Task status of an Instance",
                                 u"Unshelving")),
)

POWER_DISPLAY_CHOICES = (
    ("NO STATE", pgettext_lazy("Power state of an Instance", u"No State")),
    ("RUNNING", pgettext_lazy("Power state of an Instance", u"Running")),
    ("BLOCKED", pgettext_lazy("Power state of an Instance", u"Blocked")),
    ("PAUSED", pgettext_lazy("Power state of an Instance", u"Paused")),
    ("SHUTDOWN", pgettext_lazy("Power state of an Instance", u"Shut Down")),
    ("SHUTOFF", pgettext_lazy("Power state of an Instance", u"Shut Off")),
    ("CRASHED", pgettext_lazy("Power state of an Instance", u"Crashed")),
    ("SUSPENDED", pgettext_lazy("Power state of an Instance", u"Suspended")),
    ("FAILED", pgettext_lazy("Power state of an Instance", u"Failed")),
    ("BUILDING", pgettext_lazy("Power state of an Instance", u"Building")),
)


class InstancesFilterAction(tables.FilterAction):
    filter_type = "server"
    filter_choices = (('name', _("Instance Name"), True),
                      ('status', _("Status ="), True),
                      ('image', _("Image ID ="), True),
                      ('flavor', _("Flavor ID ="), True))


class InstancesTable(tables.DataTable):
    TASK_STATUS_CHOICES = (
        (None, True),
        ("none", True)
    )
    STATUS_CHOICES = (
        ("active", True),
        ("shutoff", True),
        ("suspended", True),
        ("paused", True),
        ("error", False),
        ("rescue", True),
        ("shelved_offloaded", True),
    )
    name = tables.Column("name",
                         link=("horizon:project:instances:detail"),
                         verbose_name=_("Instance Name"))
    image_name = tables.Column("image_name",
                               verbose_name=_("Image Name"))
    ip = tables.Column(get_ips,
                       verbose_name=_("IP Address"),
                       attrs={'data-type': "ip"})
    size = tables.Column(get_size,
                         verbose_name=_("Size"),
                         attrs={'data-type': 'size'})
    keypair = tables.Column(get_keyname, verbose_name=_("Key Pair"))
    status = tables.Column("status",
                           filters=(title, filters.replace_underscores),
                           verbose_name=_("Status"),
                           status=True,
                           status_choices=STATUS_CHOICES,
                           display_choices=STATUS_DISPLAY_CHOICES)
    az = tables.Column("availability_zone",
                       verbose_name=_("Availability Zone"))
    task = tables.Column("OS-EXT-STS:task_state",
                         verbose_name=_("Task"),
                         empty_value=TASK_DISPLAY_NONE,
                         status=True,
                         status_choices=TASK_STATUS_CHOICES,
                         display_choices=TASK_DISPLAY_CHOICES)
    state = tables.Column(get_power_state,
                          filters=(title, filters.replace_underscores),
                          verbose_name=_("Power State"),
                          display_choices=POWER_DISPLAY_CHOICES)
    created = tables.Column("created",
                            verbose_name=_("Time since created"),
                            filters=(filters.parse_isotime,
                                     filters.timesince_sortable),
                            attrs={'data-type': 'timesince'})

    class Meta:
        name = "instances"
        verbose_name = _("Instances")
        status_columns = ["status", "task"]
        row_class = UpdateRow
        table_actions_menu = (StartInstance, StopInstance, SoftRebootInstance)
        table_actions = (LaunchLink, TerminateInstance, InstancesFilterAction)
        row_actions = (StartInstance, ConfirmResize, RevertResize,
                       CreateSnapshot, SimpleAssociateIP, AssociateIP,
                       SimpleDisassociateIP, EditInstance,
                       DecryptInstancePassword, EditInstanceSecurityGroups,
                       ConsoleLink, LogLink, TogglePause, ToggleSuspend,
                       ResizeLink, LockInstance, UnlockInstance,
                       SoftRebootInstance, RebootInstance,
                       StopInstance, RebuildInstance, TerminateInstance)

# Copyright 2012 NEC Corporation
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging

from django.core.urlresolvers import reverse
from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon import messages

from openstack_dashboard import api


LOG = logging.getLogger(__name__)


class UpdatePort(forms.SelfHandlingForm):
    network_id = forms.CharField(widget=forms.HiddenInput())
    port_id = forms.CharField(widget=forms.HiddenInput())
    name = forms.CharField(max_length=255,
                           label=_("Name"),
                           required=False)
    admin_state = forms.ChoiceField(choices=[(True, _('UP')),
                                             (False, _('DOWN'))],
                                    label=_("Admin State"))
    failure_url = 'horizon:project:networks:detail'

    def __init__(self, request, *args, **kwargs):
        super(UpdatePort, self).__init__(request, *args, **kwargs)
        if api.neutron.is_extension_supported(request, 'mac-learning'):
            self.fields['mac_state'] = forms.BooleanField(
                label=_("Mac Learning State"), required=False)

    def handle(self, request, data):
        data['admin_state'] = (data['admin_state'] == 'True')
        try:
            LOG.debug('params = %s' % data)
            extension_kwargs = {}
            if 'mac_state' in data:
                extension_kwargs['mac_learning_enabled'] = data['mac_state']
            port = api.neutron.port_update(request, data['port_id'],
                                           name=data['name'],
                                           admin_state_up=data['admin_state'],
                                           **extension_kwargs)
            msg = _('Port %s was successfully updated.') % data['port_id']
            LOG.debug(msg)
            messages.success(request, msg)
            return port
        except Exception:
            msg = _('Failed to update port %s') % data['port_id']
            LOG.info(msg)
            redirect = reverse(self.failure_url,
                               args=[data['network_id']])
            exceptions.handle(request, msg, redirect=redirect)

# Copyright 2012,  Nachi Ueno,  NTT MCL,  Inc.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

import logging

from django.core.urlresolvers import reverse
from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon import messages
from openstack_dashboard import api

LOG = logging.getLogger(__name__)


class AddInterface(forms.SelfHandlingForm):
    subnet_id = forms.ChoiceField(label=_("Subnet"))
    ip_address = forms.IPField(
        label=_("IP Address (optional)"), required=False, initial="",
        help_text=_("Specify an IP address for the interface "
                    "created (e.g. 192.168.0.254)."),
        version=forms.IPv4 | forms.IPv6, mask=False)
    router_name = forms.CharField(label=_("Router Name"),
                                  widget=forms.TextInput(
                                      attrs={'readonly': 'readonly'}))
    router_id = forms.CharField(label=_("Router ID"),
                                widget=forms.TextInput(
                                    attrs={'readonly': 'readonly'}))
    failure_url = 'horizon:project:routers:detail'

    def __init__(self, request, *args, **kwargs):
        super(AddInterface, self).__init__(request, *args, **kwargs)
        c = self.populate_subnet_id_choices(request)
        self.fields['subnet_id'].choices = c

    def populate_subnet_id_choices(self, request):
        tenant_id = self.request.user.tenant_id
        networks = []
        try:
            networks = api.neutron.network_list_for_tenant(request, tenant_id)
        except Exception as e:
            msg = _('Failed to get network list %s') % e
            LOG.info(msg)
            messages.error(request, msg)
            router_id = request.REQUEST.get('router_id',
                                            self.initial.get('router_id'))
            if router_id:
                redirect = reverse(self.failure_url, args=[router_id])
            else:
                redirect = reverse('horizon:project:routers:index')
            exceptions.handle(request, msg, redirect=redirect)
            return

        choices = []
        for n in networks:
            net_name = n.name + ': ' if n.name else ''
            choices += [(subnet.id,
                         '%s%s (%s)' % (net_name, subnet.cidr,
                                        subnet.name or subnet.id))
                        for subnet in n['subnets']]
        if choices:
            choices.insert(0, ("", _("Select Subnet")))
        else:
            choices.insert(0, ("", _("No subnets available")))
        return choices

    def handle(self, request, data):
        if data['ip_address']:
            port = self._add_interface_by_port(request, data)
        else:
            port = self._add_interface_by_subnet(request, data)
        msg = _('Interface added')
        if port:
            msg += ' ' + port.fixed_ips[0]['ip_address']
        LOG.debug(msg)
        messages.success(request, msg)
        return True

    def _add_interface_by_subnet(self, request, data):
        router_id = data['router_id']
        try:
            router_inf = api.neutron.router_add_interface(
                request, router_id, subnet_id=data['subnet_id'])
        except Exception as e:
            self._handle_error(request, router_id, e)
        try:
            port = api.neutron.port_get(request, router_inf['port_id'])
        except Exception:
            # Ignore an error when port_get() since it is just
            # to get an IP address for the interface.
            port = None
        return port

    def _add_interface_by_port(self, request, data):
        router_id = data['router_id']
        subnet_id = data['subnet_id']
        try:
            subnet = api.neutron.subnet_get(request, subnet_id)
        except Exception:
            msg = _('Unable to get subnet "%s"') % subnet_id
            self._handle_error(request, router_id, msg)
        try:
            ip_address = data['ip_address']
            body = {'network_id': subnet.network_id,
                    'fixed_ips': [{'subnet_id': subnet.id,
                                   'ip_address': ip_address}]}
            port = api.neutron.port_create(request, **body)
        except Exception as e:
            self._handle_error(request, router_id, e)
        try:
            api.neutron.router_add_interface(request, router_id,
                                             port_id=port.id)
        except Exception as e:
            self._delete_port(request, port)
            self._handle_error(request, router_id, e)
        return port

    def _handle_error(self, request, router_id, reason):
        msg = _('Failed to add_interface: %s') % reason
        LOG.info(msg)
        redirect = reverse(self.failure_url, args=[router_id])
        exceptions.handle(request, msg, redirect=redirect)

    def _delete_port(self, request, port):
        try:
            api.neutron.port_delete(request, port.id)
        except Exception:
            msg = _('Failed to delete port %s') % port.id
            LOG.info(msg)
            exceptions.handle(request, msg)


class SetGatewayForm(forms.SelfHandlingForm):
    network_id = forms.ChoiceField(label=_("External Network"))
    router_name = forms.CharField(label=_("Router Name"),
                                  widget=forms.TextInput(
                                      attrs={'readonly': 'readonly'}))
    router_id = forms.CharField(label=_("Router ID"),
                                widget=forms.TextInput(
                                    attrs={'readonly': 'readonly'}))
    failure_url = 'horizon:project:routers:index'

    def __init__(self, request, *args, **kwargs):
        super(SetGatewayForm, self).__init__(request, *args, **kwargs)
        c = self.populate_network_id_choices(request)
        self.fields['network_id'].choices = c

    def populate_network_id_choices(self, request):
        search_opts = {'router:external': True}
        try:
            networks = api.neutron.network_list(request, **search_opts)
        except Exception as e:
            msg = _('Failed to get network list %s') % e
            LOG.info(msg)
            messages.error(request, msg)
            redirect = reverse(self.failure_url)
            exceptions.handle(request, msg, redirect=redirect)
            return
        choices = [(network.id, network.name or network.id)
                   for network in networks]
        if choices:
            choices.insert(0, ("", _("Select network")))
        else:
            choices.insert(0, ("", _("No networks available")))
        return choices

    def handle(self, request, data):
        try:
            api.neutron.router_add_gateway(request,
                                           data['router_id'],
                                           data['network_id'])
            msg = _('Gateway interface is added')
            LOG.debug(msg)
            messages.success(request, msg)
            return True
        except Exception as e:
            msg = _('Failed to set gateway %s') % e
            LOG.info(msg)
            redirect = reverse(self.failure_url)
            exceptions.handle(request, msg, redirect=redirect)

# Licensed under the Apache License, Version 2.0 (the "License"); you may
# not use this file except in compliance with the License. You may obtain
# a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.


from django.core.urlresolvers import reverse
from django.utils.translation import ugettext_lazy as _

from horizon import exceptions
from horizon import forms
from horizon import messages

from openstack_dashboard.api import cinder


class UpdateForm(forms.SelfHandlingForm):
    name = forms.CharField(max_length=255, label=_("Snapshot Name"))
    description = forms.CharField(max_length=255,
                                  widget=forms.Textarea(attrs={'rows': 4}),
                                  label=_("Description"),
                                  required=False)

    def handle(self, request, data):
        snapshot_id = self.initial['snapshot_id']
        try:
            cinder.volume_snapshot_update(request,
                                          snapshot_id,
                                          data['name'],
                                          data['description'])

            message = _('Updating volume snapshot "%s"') % data['name']
            messages.info(request, message)
            return True
        except Exception:
            redirect = reverse("horizon:project:volumes:index")
            exceptions.handle(request,
                              _('Unable to update volume snapshot.'),
                              redirect=redirect)

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from django.utils.translation import ugettext_lazy as _

from horizon import tabs


class NetworkProfileTab(tabs.Tab):
    name = _("Network Profile")
    slug = "network_profile"
    template_name = 'router/nexus1000v/network_profile/index.html'

    def get_context_data(self, request):
        return None


class PolicyProfileTab(tabs.Tab):
    name = _("Policy Profile")
    slug = "policy_profile"
    template_name = 'router/nexus1000v/policy_profile/index.html'
    preload = False


class IndexTabs(tabs.TabGroup):
    slug = "indextabs"
    tabs = (NetworkProfileTab, PolicyProfileTab)

# Copyright (c) 2012 OpenStack Foundation.
# All Rights Reserved.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

"""
Common Policy Engine Implementation

Policies can be expressed in one of two forms: A list of lists, or a
string written in the new policy language.

In the list-of-lists representation, each check inside the innermost
list is combined as with an "and" conjunction--for that check to pass,
all the specified checks must pass.  These innermost lists are then
combined as with an "or" conjunction.  This is the original way of
expressing policies, but there now exists a new way: the policy
language.

In the policy language, each check is specified the same way as in the
list-of-lists representation: a simple "a:b" pair that is matched to
the correct code to perform that check.  However, conjunction
operators are available, allowing for more expressiveness in crafting
policies.

As an example, take the following rule, expressed in the list-of-lists
representation::

    [["role:admin"], ["project_id:%(project_id)s", "role:projectadmin"]]

In the policy language, this becomes::

    role:admin or (project_id:%(project_id)s and role:projectadmin)

The policy language also has the "not" operator, allowing a richer
policy rule::

    project_id:%(project_id)s and not role:dunce

It is possible to perform policy checks on the following user
attributes (obtained through the token): user_id, domain_id or
project_id::

    domain_id:<some_value>

Attributes sent along with API calls can be used by the policy engine
(on the right side of the expression), by using the following syntax::

    <some_value>:user.id

Contextual attributes of objects identified by their IDs are loaded
from the database. They are also available to the policy engine and
can be checked through the `target` keyword::

    <some_value>:target.role.name

All these attributes (related to users, API calls, and context) can be
checked against each other or against constants, be it literals (True,
<a_number>) or strings.

Finally, two special policy checks should be mentioned; the policy
check "@" will always accept an access, and the policy check "!" will
always reject an access.  (Note that if a rule is either the empty
list ("[]") or the empty string, this is equivalent to the "@" policy
check.)  Of these, the "!" policy check is probably the most useful,
as it allows particular rules to be explicitly disabled.
"""

import abc
import ast
import os
import re

from oslo.config import cfg
from oslo.serialization import jsonutils
import six
import six.moves.urllib.parse as urlparse
import six.moves.urllib.request as urlrequest

from openstack_dashboard.openstack.common import fileutils
from openstack_dashboard.openstack.common._i18n import _, _LE, _LW
from openstack_dashboard.openstack.common import log as logging


policy_opts = [
    cfg.StrOpt('policy_file',
               default='policy.json',
               help=_('The JSON file that defines policies.')),
    cfg.StrOpt('policy_default_rule',
               default='default',
               help=_('Default rule. Enforced when a requested rule is not '
                      'found.')),
    cfg.MultiStrOpt('policy_dirs',
                    default=['policy.d'],
                    help=_('The directories of policy configuration files is '
                           'stored')),
]

CONF = cfg.CONF
CONF.register_opts(policy_opts)

LOG = logging.getLogger(__name__)

_checks = {}


class PolicyNotAuthorized(Exception):

    def __init__(self, rule):
        msg = _("Policy doesn't allow %s to be performed.") % rule
        super(PolicyNotAuthorized, self).__init__(msg)


class Rules(dict):
    """A store for rules. Handles the default_rule setting directly."""

    @classmethod
    def load_json(cls, data, default_rule=None):
        """Allow loading of JSON rule data."""

        # Suck in the JSON data and parse the rules
        rules = dict((k, parse_rule(v)) for k, v in
                     jsonutils.loads(data).items())

        return cls(rules, default_rule)

    def __init__(self, rules=None, default_rule=None):
        """Initialize the Rules store."""

        super(Rules, self).__init__(rules or {})
        self.default_rule = default_rule

    def __missing__(self, key):
        """Implements the default rule handling."""

        if isinstance(self.default_rule, dict):
            raise KeyError(key)

        # If the default rule isn't actually defined, do something
        # reasonably intelligent
        if not self.default_rule:
            raise KeyError(key)

        if isinstance(self.default_rule, BaseCheck):
            return self.default_rule

        # We need to check this or we can get infinite recursion
        if self.default_rule not in self:
            raise KeyError(key)

        elif isinstance(self.default_rule, six.string_types):
            return self[self.default_rule]

    def __str__(self):
        """Dumps a string representation of the rules."""

        # Start by building the canonical strings for the rules
        out_rules = {}
        for key, value in self.items():
            # Use empty string for singleton TrueCheck instances
            if isinstance(value, TrueCheck):
                out_rules[key] = ''
            else:
                out_rules[key] = str(value)

        # Dump a pretty-printed JSON representation
        return jsonutils.dumps(out_rules, indent=4)


class Enforcer(object):
    """Responsible for loading and enforcing rules.

    :param policy_file: Custom policy file to use, if none is
                        specified, `CONF.policy_file` will be
                        used.
    :param rules: Default dictionary / Rules to use. It will be
                  considered just in the first instantiation. If
                  `load_rules(True)`, `clear()` or `set_rules(True)`
                  is called this will be overwritten.
    :param default_rule: Default rule to use, CONF.default_rule will
                         be used if none is specified.
    :param use_conf: Whether to load rules from cache or config file.
    """

    def __init__(self, policy_file=None, rules=None,
                 default_rule=None, use_conf=True):
        self.rules = Rules(rules, default_rule)
        self.default_rule = default_rule or CONF.policy_default_rule

        self.policy_path = None
        self.policy_file = policy_file or CONF.policy_file
        self.use_conf = use_conf

    def set_rules(self, rules, overwrite=True, use_conf=False):
        """Create a new Rules object based on the provided dict of rules.

        :param rules: New rules to use. It should be an instance of dict.
        :param overwrite: Whether to overwrite current rules or update them
                          with the new rules.
        :param use_conf: Whether to reload rules from cache or config file.
        """

        if not isinstance(rules, dict):
            raise TypeError(_("Rules must be an instance of dict or Rules, "
                            "got %s instead") % type(rules))
        self.use_conf = use_conf
        if overwrite:
            self.rules = Rules(rules, self.default_rule)
        else:
            self.rules.update(rules)

    def clear(self):
        """Clears Enforcer rules, policy's cache and policy's path."""
        self.set_rules({})
        fileutils.delete_cached_file(self.policy_path)
        self.default_rule = None
        self.policy_path = None

    def load_rules(self, force_reload=False):
        """Loads policy_path's rules.

        Policy file is cached and will be reloaded if modified.

        :param force_reload: Whether to overwrite current rules.
        """

        if force_reload:
            self.use_conf = force_reload

        if self.use_conf:
            if not self.policy_path:
                self.policy_path = self._get_policy_path(self.policy_file)

            self._load_policy_file(self.policy_path, force_reload)
            for path in CONF.policy_dirs:
                try:
                    path = self._get_policy_path(path)
                except cfg.ConfigFilesNotFoundError:
                    LOG.warn(_LW("Can not find policy directories %s"), path)
                    continue
                self._walk_through_policy_directory(path,
                                                    self._load_policy_file,
                                                    force_reload, False)

    def _walk_through_policy_directory(self, path, func, *args):
        # We do not iterate over sub-directories.
        policy_files = next(os.walk(path))[2]
        policy_files.sort()
        for policy_file in [p for p in policy_files if not p.startswith('.')]:
            func(os.path.join(path, policy_file), *args)

    def _load_policy_file(self, path, force_reload, overwrite=True):
            reloaded, data = fileutils.read_cached_file(
                path, force_reload=force_reload)
            if reloaded or not self.rules:
                rules = Rules.load_json(data, self.default_rule)
                self.set_rules(rules, overwrite)
                LOG.debug("Rules successfully reloaded")

    def _get_policy_path(self, path):
        """Locate the policy json data file/path.

        :param path: It's value can be a full path or related path. When
                     full path specified, this function just returns the full
                     path. When related path specified, this function will
                     search configuration directories to find one that exists.

        :returns: The policy path

        :raises: ConfigFilesNotFoundError if the file/path couldn't
                 be located.
        """
        policy_path = CONF.find_file(path)

        if policy_path:
            return policy_path

        raise cfg.ConfigFilesNotFoundError((path,))

    def enforce(self, rule, target, creds, do_raise=False,
                exc=None, *args, **kwargs):
        """Checks authorization of a rule against the target and credentials.

        :param rule: A string or BaseCheck instance specifying the rule
                    to evaluate.
        :param target: As much information about the object being operated
                    on as possible, as a dictionary.
        :param creds: As much information about the user performing the
                    action as possible, as a dictionary.
        :param do_raise: Whether to raise an exception or not if check
                        fails.
        :param exc: Class of the exception to raise if the check fails.
                    Any remaining arguments passed to check() (both
                    positional and keyword arguments) will be passed to
                    the exception class. If not specified, PolicyNotAuthorized
                    will be used.

        :return: Returns False if the policy does not allow the action and
                exc is not provided; otherwise, returns a value that
                evaluates to True.  Note: for rules using the "case"
                expression, this True value will be the specified string
                from the expression.
        """

        self.load_rules()

        # Allow the rule to be a Check tree
        if isinstance(rule, BaseCheck):
            result = rule(target, creds, self)
        elif not self.rules:
            # No rules to reference means we're going to fail closed
            result = False
        else:
            try:
                # Evaluate the rule
                result = self.rules[rule](target, creds, self)
            except KeyError:
                LOG.debug("Rule [%s] doesn't exist" % rule)
                # If the rule doesn't exist, fail closed
                result = False

        # If it is False, raise the exception if requested
        if do_raise and not result:
            if exc:
                raise exc(*args, **kwargs)

            raise PolicyNotAuthorized(rule)

        return result


@six.add_metaclass(abc.ABCMeta)
class BaseCheck(object):
    """Abstract base class for Check classes."""

    @abc.abstractmethod
    def __str__(self):
        """String representation of the Check tree rooted at this node."""

        pass

    @abc.abstractmethod
    def __call__(self, target, cred, enforcer):
        """Triggers if instance of the class is called.

        Performs the check. Returns False to reject the access or a
        true value (not necessary True) to accept the access.
        """

        pass


class FalseCheck(BaseCheck):
    """A policy check that always returns False (disallow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "!"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return False


class TrueCheck(BaseCheck):
    """A policy check that always returns True (allow)."""

    def __str__(self):
        """Return a string representation of this check."""

        return "@"

    def __call__(self, target, cred, enforcer):
        """Check the policy."""

        return True


class Check(BaseCheck):
    """A base class to allow for user-defined policy checks."""

    def __init__(self, kind, match):
        """Initiates Check instance.

        :param kind: The kind of the check, i.e., the field before the
                     ':'.
        :param match: The match of the check, i.e., the field after
                      the ':'.
        """

        self.kind = kind
        self.match = match

    def __str__(self):
        """Return a string representation of this check."""

        return "%s:%s" % (self.kind, self.match)


class NotCheck(BaseCheck):
    """Implements the "not" logical operator.

    A policy check that inverts the result of another policy check.
    """

    def __init__(self, rule):
        """Initialize the 'not' check.

        :param rule: The rule to negate.  Must be a Check.
        """

        self.rule = rule

    def __str__(self):
        """Return a string representation of this check."""

        return "not %s" % self.rule

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Returns the logical inverse of the wrapped check.
        """

        return not self.rule(target, cred, enforcer)


class AndCheck(BaseCheck):
    """Implements the "and" logical operator.

    A policy check that requires that a list of other checks all return True.
    """

    def __init__(self, rules):
        """Initialize the 'and' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' and '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that all rules accept in order to return True.
        """

        for rule in self.rules:
            if not rule(target, cred, enforcer):
                return False

        return True

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the AndCheck object for convenience.
        """

        self.rules.append(rule)
        return self


class OrCheck(BaseCheck):
    """Implements the "or" operator.

    A policy check that requires that at least one of a list of other
    checks returns True.
    """

    def __init__(self, rules):
        """Initialize the 'or' check.

        :param rules: A list of rules that will be tested.
        """

        self.rules = rules

    def __str__(self):
        """Return a string representation of this check."""

        return "(%s)" % ' or '.join(str(r) for r in self.rules)

    def __call__(self, target, cred, enforcer):
        """Check the policy.

        Requires that at least one rule accept in order to return True.
        """

        for rule in self.rules:
            if rule(target, cred, enforcer):
                return True
        return False

    def add_check(self, rule):
        """Adds rule to be tested.

        Allows addition of another rule to the list of rules that will
        be tested.  Returns the OrCheck object for convenience.
        """

        self.rules.append(rule)
        return self


def _parse_check(rule):
    """Parse a single base check rule into an appropriate Check object."""

    # Handle the special checks
    if rule == '!':
        return FalseCheck()
    elif rule == '@':
        return TrueCheck()

    try:
        kind, match = rule.split(':', 1)
    except Exception:
        LOG.exception(_LE("Failed to understand rule %s") % rule)
        # If the rule is invalid, we'll fail closed
        return FalseCheck()

    # Find what implements the check
    if kind in _checks:
        return _checks[kind](kind, match)
    elif None in _checks:
        return _checks[None](kind, match)
    else:
        LOG.error(_LE("No handler for matches of kind %s") % kind)
        return FalseCheck()


def _parse_list_rule(rule):
    """Translates the old list-of-lists syntax into a tree of Check objects.

    Provided for backwards compatibility.
    """

    # Empty rule defaults to True
    if not rule:
        return TrueCheck()

    # Outer list is joined by "or"; inner list by "and"
    or_list = []
    for inner_rule in rule:
        # Elide empty inner lists
        if not inner_rule:
            continue

        # Handle bare strings
        if isinstance(inner_rule, six.string_types):
            inner_rule = [inner_rule]

        # Parse the inner rules into Check objects
        and_list = [_parse_check(r) for r in inner_rule]

        # Append the appropriate check to the or_list
        if len(and_list) == 1:
            or_list.append(and_list[0])
        else:
            or_list.append(AndCheck(and_list))

    # If we have only one check, omit the "or"
    if not or_list:
        return FalseCheck()
    elif len(or_list) == 1:
        return or_list[0]

    return OrCheck(or_list)


# Used for tokenizing the policy language
_tokenize_re = re.compile(r'\s+')


def _parse_tokenize(rule):
    """Tokenizer for the policy language.

    Most of the single-character tokens are specified in the
    _tokenize_re; however, parentheses need to be handled specially,
    because they can appear inside a check string.  Thankfully, those
    parentheses that appear inside a check string can never occur at
    the very beginning or end ("%(variable)s" is the correct syntax).
    """

    for tok in _tokenize_re.split(rule):
        # Skip empty tokens
        if not tok or tok.isspace():
            continue

        # Handle leading parens on the token
        clean = tok.lstrip('(')
        for i in range(len(tok) - len(clean)):
            yield '(', '('

        # If it was only parentheses, continue
        if not clean:
            continue
        else:
            tok = clean

        # Handle trailing parens on the token
        clean = tok.rstrip(')')
        trail = len(tok) - len(clean)

        # Yield the cleaned token
        lowered = clean.lower()
        if lowered in ('and', 'or', 'not'):
            # Special tokens
            yield lowered, clean
        elif clean:
            # Not a special token, but not composed solely of ')'
            if len(tok) >= 2 and ((tok[0], tok[-1]) in
                                  [('"', '"'), ("'", "'")]):
                # It's a quoted string
                yield 'string', tok[1:-1]
            else:
                yield 'check', _parse_check(clean)

        # Yield the trailing parens
        for i in range(trail):
            yield ')', ')'


class ParseStateMeta(type):
    """Metaclass for the ParseState class.

    Facilitates identifying reduction methods.
    """

    def __new__(mcs, name, bases, cls_dict):
        """Create the class.

        Injects the 'reducers' list, a list of tuples matching token sequences
        to the names of the corresponding reduction methods.
        """

        reducers = []

        for key, value in cls_dict.items():
            if not hasattr(value, 'reducers'):
                continue
            for reduction in value.reducers:
                reducers.append((reduction, key))

        cls_dict['reducers'] = reducers

        return super(ParseStateMeta, mcs).__new__(mcs, name, bases, cls_dict)


def reducer(*tokens):
    """Decorator for reduction methods.

    Arguments are a sequence of tokens, in order, which should trigger running
    this reduction method.
    """

    def decorator(func):
        # Make sure we have a list of reducer sequences
        if not hasattr(func, 'reducers'):
            func.reducers = []

        # Add the tokens to the list of reducer sequences
        func.reducers.append(list(tokens))

        return func

    return decorator


@six.add_metaclass(ParseStateMeta)
class ParseState(object):
    """Implement the core of parsing the policy language.

    Uses a greedy reduction algorithm to reduce a sequence of tokens into
    a single terminal, the value of which will be the root of the Check tree.

    Note: error reporting is rather lacking.  The best we can get with
    this parser formulation is an overall "parse failed" error.
    Fortunately, the policy language is simple enough that this
    shouldn't be that big a problem.
    """

    def __init__(self):
        """Initialize the ParseState."""

        self.tokens = []
        self.values = []

    def reduce(self):
        """Perform a greedy reduction of the token stream.

        If a reducer method matches, it will be executed, then the
        reduce() method will be called recursively to search for any more
        possible reductions.
        """

        for reduction, methname in self.reducers:
            if (len(self.tokens) >= len(reduction) and
                    self.tokens[-len(reduction):] == reduction):
                # Get the reduction method
                meth = getattr(self, methname)

                # Reduce the token stream
                results = meth(*self.values[-len(reduction):])

                # Update the tokens and values
                self.tokens[-len(reduction):] = [r[0] for r in results]
                self.values[-len(reduction):] = [r[1] for r in results]

                # Check for any more reductions
                return self.reduce()

    def shift(self, tok, value):
        """Adds one more token to the state.  Calls reduce()."""

        self.tokens.append(tok)
        self.values.append(value)

        # Do a greedy reduce...
        self.reduce()

    @property
    def result(self):
        """Obtain the final result of the parse.

        Raises ValueError if the parse failed to reduce to a single result.
        """

        if len(self.values) != 1:
            raise ValueError("Could not parse rule")
        return self.values[0]

    @reducer('(', 'check', ')')
    @reducer('(', 'and_expr', ')')
    @reducer('(', 'or_expr', ')')
    def _wrap_check(self, _p1, check, _p2):
        """Turn parenthesized expressions into a 'check' token."""

        return [('check', check)]

    @reducer('check', 'and', 'check')
    def _make_and_expr(self, check1, _and, check2):
        """Create an 'and_expr'.

        Join two checks by the 'and' operator.
        """

        return [('and_expr', AndCheck([check1, check2]))]

    @reducer('and_expr', 'and', 'check')
    def _extend_and_expr(self, and_expr, _and, check):
        """Extend an 'and_expr' by adding one more check."""

        return [('and_expr', and_expr.add_check(check))]

    @reducer('check', 'or', 'check')
    def _make_or_expr(self, check1, _or, check2):
        """Create an 'or_expr'.

        Join two checks by the 'or' operator.
        """

        return [('or_expr', OrCheck([check1, check2]))]

    @reducer('or_expr', 'or', 'check')
    def _extend_or_expr(self, or_expr, _or, check):
        """Extend an 'or_expr' by adding one more check."""

        return [('or_expr', or_expr.add_check(check))]

    @reducer('not', 'check')
    def _make_not_expr(self, _not, check):
        """Invert the result of another check."""

        return [('check', NotCheck(check))]


def _parse_text_rule(rule):
    """Parses policy to the tree.

    Translates a policy written in the policy language into a tree of
    Check objects.
    """

    # Empty rule means always accept
    if not rule:
        return TrueCheck()

    # Parse the token stream
    state = ParseState()
    for tok, value in _parse_tokenize(rule):
        state.shift(tok, value)

    try:
        return state.result
    except ValueError:
        # Couldn't parse the rule
        LOG.exception(_LE("Failed to understand rule %s") % rule)

        # Fail closed
        return FalseCheck()


def parse_rule(rule):
    """Parses a policy rule into a tree of Check objects."""

    # If the rule is a string, it's in the policy language
    if isinstance(rule, six.string_types):
        return _parse_text_rule(rule)
    return _parse_list_rule(rule)


def register(name, func=None):
    """Register a function or Check class as a policy check.

    :param name: Gives the name of the check type, e.g., 'rule',
                 'role', etc.  If name is None, a default check type
                 will be registered.
    :param func: If given, provides the function or class to register.
                 If not given, returns a function taking one argument
                 to specify the function or class to register,
                 allowing use as a decorator.
    """

    # Perform the actual decoration by registering the function or
    # class.  Returns the function or class for compliance with the
    # decorator interface.
    def decorator(func):
        _checks[name] = func
        return func

    # If the function or class is given, do the registration
    if func:
        return decorator(func)

    return decorator


@register("rule")
class RuleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Recursively checks credentials based on the defined rules."""

        try:
            return enforcer.rules[self.match](target, creds, enforcer)
        except KeyError:
            # We don't have any matching rule; fail closed
            return False


@register("role")
class RoleCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check that there is a matching role in the cred dict."""

        return self.match.lower() in [x.lower() for x in creds['roles']]


@register('http')
class HttpCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check http: rules by calling to a remote server.

        This example implementation simply verifies that the response
        is exactly 'True'.
        """

        url = ('http:' + self.match) % target
        data = {'target': jsonutils.dumps(target),
                'credentials': jsonutils.dumps(creds)}
        post_data = urlparse.urlencode(data)
        f = urlrequest.urlopen(url, post_data)
        return f.read() == "True"


@register(None)
class GenericCheck(Check):
    def __call__(self, target, creds, enforcer):
        """Check an individual match.

        Matches look like:

            tenant:%(tenant_id)s
            role:compute:admin
            True:%(user.enabled)s
            'Member':%(role.name)s
        """

        try:
            match = self.match % target
        except KeyError:
            # While doing GenericCheck if key not
            # present in Target return false
            return False

        try:
            # Try to interpret self.kind as a literal
            leftval = ast.literal_eval(self.kind)
        except ValueError:
            try:
                kind_parts = self.kind.split('.')
                leftval = creds
                for kind_part in kind_parts:
                    leftval = leftval[kind_part]
            except KeyError:
                return False
        return match == six.text_type(leftval)

#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.
from selenium.webdriver.common import by

from openstack_dashboard.test.integration_tests.pages import basepage
from openstack_dashboard.test.integration_tests.regions import forms
from openstack_dashboard.test.integration_tests.regions import tables


class OverviewPage(basepage.BaseNavigationPage):
    _usage_table_locator = (by.By.CSS_SELECTOR, 'table#project_usage')
    _date_form_locator = (by.By.CSS_SELECTOR, 'form#date_form')

    USAGE_TABLE_ACTIONS = ("download_csv",)

    def __init__(self, driver, conf):
        super(OverviewPage, self).__init__(driver, conf)
        self._page_title = 'Instance Overview'

    @property
    def usage_table(self):
        src_elem = self._get_element(*self._usage_table_locator)
        return tables.ActionsTableRegion(self.driver, self.conf, src_elem,
                                         self.USAGE_TABLE_ACTIONS)

    @property
    def date_form(self):
        src_elem = self._get_element(*self._date_form_locator)
        return forms.DateFormRegion(self.driver, self.conf, src_elem)

# Copyright 2013 Rackspace Hosting.
#
#    Licensed under the Apache License, Version 2.0 (the "License"); you may
#    not use this file except in compliance with the License. You may obtain
#    a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
#    Unless required by applicable law or agreed to in writing, software
#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
#    License for the specific language governing permissions and limitations
#    under the License.

from troveclient.v1 import backups
from troveclient.v1 import databases
from troveclient.v1 import datastores
from troveclient.v1 import flavors
from troveclient.v1 import instances
from troveclient.v1 import users

from openstack_dashboard.test.test_data import utils


DATABASE_DATA_ONE = {
    "status": "ACTIVE",
    "updated": "2013-08-12T22:00:09",
    "name": "Test Database",
    "links": [],
    "created": "2013-08-12T22:00:03",
    "ip": [
        "10.0.0.3",
    ],
    "volume": {
        "used": 0.13,
        "size": 1,
    },
    "flavor": {
        "id": "1",
        "links": [],
    },
    "datastore": {
        "type": "mysql",
        "version": "5.5"
    },
    "id": "6ddc36d9-73db-4e23-b52e-368937d72719",
}

DATABASE_DATA_TWO = {
    "status": "ACTIVE",
    "updated": "2013-08-12T22:00:09",
    "name": "Test Database With DNS",
    "links": [],
    "created": "2013-08-12T22:00:03",
    "hostname": "trove.instance-2.com",
    "volume": {
        "used": 0.13,
        "size": 1,
    },
    "flavor": {
        "id": "1",
        "links": [],
    },
    "datastore": {
        "type": "mysql",
        "version": "5.6"
    },
    "id": "4d7b3f57-44f5-41d2-8e86-36b88cad572a",
}

BACKUP_ONE = {
    "instance_id": "6ddc36d9-73db-4e23-b52e-368937d72719",
    "status": "COMPLETED",
    "updated": "2013-08-13T19:39:38",
    "locationRef": "http://swift/v1/AUTH/database_backups/0edb.tar.gz",
    "name": "backup1",
    "created": "2013-08-15T18:10:14",
    "size": 0.13,
    "id": "0edb3c14-8919-4583-9add-00df9e524081",
    "description": "Long description of backup",
}


BACKUP_TWO = {
    "instance_id": "4d7b3f57-44f5-41d2-8e86-36b88cad572a",
    "status": "COMPLETED",
    "updated": "2013-08-10T20:20:44",
    "locationRef": "http://swift/v1/AUTH/database_backups/e460.tar.gz",
    "name": "backup2",
    "created": "2013-08-10T20:20:37",
    "size": 0.13,
    "id": "e4602a3c-2bca-478f-b059-b6c215510fb4",
    "description": "Longer description of backup",
}


BACKUP_TWO_INC = {
    "instance_id": "4d7b3f57-44f5-41d2-8e86-36b88cad572a",
    "status": "COMPLETED",
    "updated": "2013-08-10T20:20:55",
    "locationRef": "http://swift/v1/AUTH/database_backups/f145.tar.gz",
    "name": "backup2-Incr",
    "created": "2013-08-10T20:20:37",
    "size": 0.13,
    "id": "e4602a3c-2bca-478f-b059-b6c215510fb5",
    "description": "Longer description of backup",
    "parent_id": "e4602a3c-2bca-478f-b059-b6c215510fb4",
}

USER_ONE = {
    "name": "Test_User",
    "host": "%",
    "databases": [DATABASE_DATA_ONE["name"]],
}


USER_DB_ONE = {
    "name": "db1",
}

DATASTORE_ONE = {
    "id": "537fb940-b5eb-40d9-bdbd-91a3dcb9c17d",
    "links": [],
    "name": "mysql"
}

DATASTORE_TWO = {
    "id": "ccb31517-c472-409d-89b4-1a13db6bdd36",
    "links": [],
    "name": "mysql"
}

VERSION_ONE = {
    "name": "5.5",
    "links": [],
    "image": "b7956bb5-920e-4299-b68e-2347d830d939",
    "active": 1,
    "datastore": "537fb940-b5eb-40d9-bdbd-91a3dcb9c17d",
    "packages": "5.5",
    "id": "390a6d52-8347-4e00-8e4c-f4fa9cf96ae9"
}

VERSION_TWO = {
    "name": "5.6",
    "links": [],
    "image": "c7956bb5-920e-4299-b68e-2347d830d938",
    "active": 1,
    "datastore": "537fb940-b5eb-40d9-bdbd-91a3dcb9c17d",
    "packages": "5.6",
    "id": "500a6d52-8347-4e00-8e4c-f4fa9cf96ae9"
}

FLAVOR_ONE = {
    "ram": 512,
    "id": "1",
    "links": [],
    "name": "m1.tiny"
}

FLAVOR_TWO = {
    "ram": 768,
    "id": "10",
    "links": [],
    "name": "eph.rd-smaller"
}

FLAVOR_THREE = {
    "ram": 800,
    "id": "100",
    "links": [],
    "name": "test.1"
}


def data(TEST):
    database1 = instances.Instance(instances.Instances(None),
                                   DATABASE_DATA_ONE)
    database2 = instances.Instance(instances.Instances(None),
                                   DATABASE_DATA_TWO)
    bkup1 = backups.Backup(backups.Backups(None), BACKUP_ONE)
    bkup2 = backups.Backup(backups.Backups(None), BACKUP_TWO)
    bkup3 = backups.Backup(backups.Backups(None), BACKUP_TWO_INC)
    user1 = users.User(users.Users(None), USER_ONE)
    user_db1 = databases.Database(databases.Databases(None),
                                  USER_DB_ONE)

    datastore1 = datastores.Datastore(datastores.Datastores(None),
                                      DATASTORE_ONE)

    version1 = datastores.\
        DatastoreVersion(datastores.DatastoreVersions(None),
                         VERSION_ONE)
    version2 = datastores.\
        DatastoreVersion(datastores.DatastoreVersions(None),
                         VERSION_TWO)

    flavor1 = flavors.Flavor(flavors.Flavors(None), FLAVOR_ONE)
    flavor2 = flavors.Flavor(flavors.Flavors(None), FLAVOR_TWO)
    flavor3 = flavors.Flavor(flavors.Flavors(None), FLAVOR_THREE)

    TEST.databases = utils.TestDataContainer()
    TEST.database_backups = utils.TestDataContainer()
    TEST.database_users = utils.TestDataContainer()
    TEST.database_user_dbs = utils.TestDataContainer()
    TEST.database_flavors = utils.TestDataContainer()

    TEST.databases.add(database1)
    TEST.databases.add(database2)
    TEST.database_backups.add(bkup1)
    TEST.database_backups.add(bkup2)
    TEST.database_backups.add(bkup3)
    TEST.database_users.add(user1)
    TEST.database_user_dbs.add(user_db1)
    TEST.datastores = utils.TestDataContainer()
    TEST.datastores.add(datastore1)
    TEST.datastore_versions = utils.TestDataContainer()
    TEST.datastore_versions.add(version1)
    TEST.datastore_versions.add(version2)
    TEST.database_flavors.add(flavor1, flavor2, flavor3)

#!/usr/bin/env python
# Copyright (c) 2013 Hewlett-Packard Development Company, L.P.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# THIS FILE IS MANAGED BY THE GLOBAL REQUIREMENTS REPO - DO NOT EDIT
import setuptools

# In python < 2.7.4, a lazy loading of package `pbr` will break
# setuptools if some other modules registered functions in `atexit`.
# solution from: http://bugs.python.org/issue15881#msg170215
try:
    import multiprocessing  # noqa
except ImportError:
    pass

setuptools.setup(
    setup_requires=['pbr'],
    pbr=True)

import arcpy, os, shutil
from arcpy import AddMessage, AddWarning, AddError
from export import Export
from esri2open import esri2open


class Convert(object):
    def __init__(self):
        self.label = 'Convert'
        self.description = 'Convert an ArcGIS feature class to open formats'
        self.canRunInBackground = False

    def getParameterInfo(self):
        """Define the parameters of the tool"""
        feature_class = arcpy.Parameter(
            name = 'in_features',
            displayName = 'In Features',
            direction = 'Input',
            datatype = 'GPFeatureLayer',
            parameterType = 'Required')

        field_mappings = arcpy.Parameter(
            name = 'in_fields',
            displayName = 'In Fields',
            direction = 'Input',
            datatype = 'GPFieldInfo',
            parameterType = 'Required')

        field_mappings.parameterDependencies = [feature_class.name]

        output_dir = arcpy.Parameter(
            name = 'output_dir',
            displayName = 'Output folder',
            direction = 'Input',
            datatype = 'DEFolder',
            parameterType = 'Required')

        output_name = arcpy.Parameter(
            name = 'output_name',
            displayName = 'Output filename',
            direction = 'Input',
            datatype = 'GPString',
            parameterType = 'Required')

        convert_4326 = arcpy.Parameter(
            name = 'convert_4326',
            displayName = 'Convert to WGS84?',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')
        convert_4326.value = 'True'

        convert_geojson = arcpy.Parameter(
            name = 'convert_geojson',
            displayName = 'Convert to GeoJSON?',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')
        convert_geojson.value = 'True'

        convert_kmz = arcpy.Parameter(
            name = 'convert_kmz',
            displayName = 'Convert to KMZ?',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')
        convert_kmz.value = 'True'

        convert_csv = arcpy.Parameter(
            name = 'convert_csv',
            displayName = 'Convert to CSV?',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')

        convert_metadata = arcpy.Parameter(
            name = 'convert_metadata',
            displayName = 'Convert metadata to markdown?',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')

        debug = arcpy.Parameter(
            name = 'debug',
            displayName = 'Debug',
            direction = 'Input',
            datatype = 'GPBoolean',
            parameterType = 'Optional')

        return [feature_class, field_mappings, output_dir, output_name,
                convert_4326, convert_geojson, convert_kmz, convert_csv,
                convert_metadata, debug]

    def isLicensed(self):
        return True

    def updateParameters(self, params):
        """Validate user input"""

        """
        If the input feature class is not point features, disable
        CSV export
        """
        if params[0].valueAsText:
            fc_type = arcpy.Describe(params[0].valueAsText).shapeType
            if fc_type in ['Point', 'MultiPoint']:
                params[7].enabled = 1
            else:
                params[7].enabled = 0

        return

    def checkFieldMappings(self, param):
        """
        Display warning message if any visible field is over 10 characters

        Args:
            param: the parameter that holds the field mappings
        """
        field_mappings = param.value
        over_fields = []
        fields_warning = ('The following visible field name(s) are' +
                         ' over 10 characters and will be shortened' +
                         ' automatically by ArcGIS: ')
        for idx, val in enumerate(range(field_mappings.count)):
            if field_mappings.getVisible(idx) == 'VISIBLE':
                field = field_mappings.getNewName(idx)
                if len(field) > 10:
                    over_fields.append(field)
        if over_fields:
            param.setWarningMessage(fields_warning + ", ".join(over_fields))
        else:
            param.clearMessage()

    def checkShapefileExists(self, dir, name):
        """Display error message if shapefile already exists.

        Args:
            dir: the output directory
            name: the output name
        """
        shapefile = dir.valueAsText + '\\shapefile\\' + name.valueAsText + '.shp'
        exists_error = ('A shapefile with this name already exists' +
                        ' in this directory. Either change the name ' +
                        'or directory or delete the previously created ' +
                        'shapefile.')
        if arcpy.Exists(shapefile):
            name.setErrorMessage(exists_error)
        else:
            name.clearMessage()


    def updateMessages(self, params):
        """Called after internal validation"""

        """
        Throws an error if a shapefile exists at the specified
        directory and file name
        """
        if params[2].value and params[2].altered:
            if params[3].value and params[3].altered:
                self.checkShapefileExists(params[2], params[3])

        """
        Throws a warning, not an error, if there is one or more visible
        output column names longer than 10 characters. ArcGIS will abbreviate
        these columns if they aren't changed or hidden. This behavior may be
        ok with the user, thus why we are only warning.
        """
        if params[1].value:
            self.checkFieldMappings(params[1])

        return

    def toBool(self, value):
            """Casts the user's input to a boolean type"""
            if value == 'true':
                return True
            else:
                return False

    def execute(self, parameters, messages):
        """Runs the script"""

        # Get the user's input
        fc = parameters[0].valueAsText
        field_mappings = parameters[1].valueAsText
        fields = parameters[1].valueAsText.split(';')
        fields.append('SHAPE@XY')
        output_dir = parameters[2].valueAsText
        output_name = parameters[3].valueAsText
        convert_to_wgs84 = self.toBool(parameters[4].valueAsText)
        convert_to_geojson = self.toBool(parameters[5].valueAsText)
        convert_to_kmz = self.toBool(parameters[6].valueAsText)
        convert_to_csv = self.toBool(parameters[7].valueAsText)
        convert_metadata = self.toBool(parameters[8].valueAsText)
        debug = self.toBool(parameters[9].valueAsText)

        # Setup vars
        output_path = output_dir + '\\' + output_name
        shp_output_path = output_dir + '\\shapefile'
        shp_temp_output_path = output_dir + '\\shapefile\\temp\\'
        shapefile = shp_output_path + '\\' + output_name + '.shp'
        temp_shapefile = shp_output_path + '\\temp\\' + output_name + '.shp'

        if debug:
            AddMessage('Field infos:')
            AddMessage(field_mappings)

        try:
            arcpy.Delete_management('temp_layer')
        except:
            if debug:
                AddMessage('Did not have a temp_layer feature ' +
                                    'class to delete')

        if not os.path.exists(shp_output_path):
            os.makedirs(shp_output_path)
            if debug:
                AddMessage('Created directory ' + shp_output_path)

        if not os.path.exists(shp_temp_output_path):
            os.makedirs(shp_temp_output_path)
        else:
            for file in os.listdir(shp_temp_output_path):
                file_path = os.path.join(shp_temp_output_path, file)
                try:
                    if os.path.isfile(file_path):
                        os.unlink(file_path)
                except:
                    AddWarning('Unable to delete ' + file +
                                      'from the temp folder. This ' +
                                      'may become a problem later')
                    pass

        arcpy.MakeFeatureLayer_management(fc, 'temp_layer', '', '',
                                          field_mappings)
        arcpy.CopyFeatures_management('temp_layer', temp_shapefile)

        if convert_to_wgs84:
            AddMessage('Converting spatial reference to WGS84...')
            arcpy.Project_management(temp_shapefile, shapefile, "GEOGCS['GCS_WGS_1984',DATUM['D_WGS_1984',SPHEROID['WGS_1984',6378137.0,298.257223563]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433],METADATA['World',-180.0,-90.0,180.0,90.0,0.0,0.0174532925199433,0.0,1262]]", "WGS_1984_(ITRF00)_To_NAD_1983", "PROJCS['NAD_1983_StatePlane_Pennsylvania_South_FIPS_3702_Feet',GEOGCS['GCS_North_American_1983',DATUM['D_North_American_1983',SPHEROID['GRS_1980',6378137.0,298.257222101]],PRIMEM['Greenwich',0.0],UNIT['Degree',0.0174532925199433]],PROJECTION['Lambert_Conformal_Conic'],PARAMETER['False_Easting',1968500.0],PARAMETER['False_Northing',0.0],PARAMETER['Central_Meridian',-77.75],PARAMETER['Standard_Parallel_1',39.93333333333333],PARAMETER['Standard_Parallel_2',40.96666666666667],PARAMETER['Latitude_Of_Origin',39.33333333333334],UNIT['Foot_US',0.3048006096012192]]")
            AddMessage('Projection conversion completed.')
        else:
            AddMessage('Exporting shapefile already in WGS84...')
            arcpy.FeatureClassToShapefile_conversion(temp_shapefile,
                                                     shp_output_path)

        try:
            arcpy.Delete_management('temp_layer')
        except:
            AddError('Unable to delete in_memory feature class')

        AddMessage('Compressing the shapefile to a .zip file...')

        export = Export(output_dir, output_name, debug)

        zip = export.zip()
        if zip:
            AddMessage('Finished creating ZIP archive')

        if convert_to_geojson:
            AddMessage('Converting to GeoJSON...')
            output = output_path + '.geojson'
            geojson = esri2open.toOpen(shapefile, output,
                                       includeGeometry='geojson')
            if geojson:
                AddMessage('Finished converting to GeoJSON')

        if convert_to_kmz:
            AddMessage('Converting to KML...')
            kmz = export.kmz()
            if kmz:
                AddMessage('Finished converting to KMZ')

        if convert_to_csv:
            AddMessage('Converting to CSV...')
            csv = export.csv()
            if csv:
                AddMessage('Finished converting to CSV')

        if convert_metadata:
            AddMessage('Converting metadata to Markdown ' +
                                'README.md file...')
            md = export.md()
            if md:
                AddMessage('Finished converting metadata to ' +
                                    'Markdown README.md file')

        # Delete the /temp directory because we're done with it
        shutil.rmtree(shp_output_path + '\\temp')
        if (debug):
            AddMessage('Deleted the /temp folder because we don\'t' +
                                ' need it anymore')

        return
try:
    import configparser
except:
    import ConfigParser as configparser
import copy
try:
    from io import StringIO
except:
    from StringIO import StringIO
import os
import shutil
import tempfile
import unittest

from green import config
from green.output import GreenStream



class ParseArguments(unittest.TestCase):


    def test_target(self):
        """
        The specified target gets parsed
        """
        config.sys.argv = ['', 'target1', 'target2']
        args = config.parseArguments()
        self.assertEqual(args.targets, ['target1', 'target2'])


    def test_absent(self):
        """
        Arguments not specified on the command-line are not present in the args
        object.
        """
        config.sys.argv = ['', '--debug']
        args = config.parseArguments()
        self.assertEqual(getattr(args, 'debug', 'not there'), True)
        self.assertEqual(getattr(args, 'verbose', 'not there'), 'not there')
        self.assertEqual(getattr(args, 'targets', 'not there'), 'not there')
        self.assertEqual(getattr(args, 'file_pattern', 'not there'), 'not there')



class ModifiedEnvironment(object):
    """
    I am a context manager that sets up environment variables for a test case.
    """


    def __init__(self, **kwargs):
        self.prev = {}
        self.excur = kwargs
        for k in kwargs:
            self.prev[k] = os.getenv(k)


    def __enter__(self):
        self.update_environment(self.excur)


    def __exit__(self, type, value, traceback):
        self.update_environment(self.prev)


    def update_environment(self, d):
        for k in d:
            if d[k] is None:
                if k in os.environ:
                    del os.environ[k]
            else:
                os.environ[k] = d[k]



class ConfigBase(unittest.TestCase):
    """
    I am an abstract base class that creates and destroys configuration files
    in a temporary directory with known values attached to self.
    """


    def _write_file(self, path, lines):
        f = open(path, 'w')
        f.writelines([x + "\n" for x in lines])
        f.close()


    def setUp(self):
        self.tmpd = tempfile.mkdtemp()
        self.default_filename = os.path.join(self.tmpd, ".green")
        self.default_logging = False
        self.default_version = False
        self.default_failfast = True
        self.default_termcolor = True
        self._write_file(self.default_filename,
                        ["# this is a test config file for green",
                         "logging = {}".format(str(self.default_logging)),
                         "version = {}".format(str(self.default_version)),
                         "omit-patterns = {}".format(self.default_filename),
                         "failfast = {}".format(str(self.default_failfast)),
                         "termcolor = {}".format(str(self.default_termcolor)),
                         ])
        self.env_filename = os.path.join(self.tmpd, "green.env")
        self.env_logging = True
        self.env_no_skip_report = False
        self._write_file(self.env_filename,
                        ["# this is a test config file for green",
                         "logging = {}".format(str(self.env_logging)),
                         "omit-patterns = {}".format(self.env_filename),
                         "no-skip-report = {}".format(self.env_no_skip_report),
                         ])
        self.cmd_filename = os.path.join(self.tmpd, "green.cmd")
        self.cmd_logging = False
        self.cmd_run_coverage = False
        self._write_file(self.cmd_filename,
                        ["# this is a test config file for green",
                         "logging = {}".format(str(self.cmd_logging)),
                         "omit-patterns = {}".format(self.cmd_filename),
                         "run-coverage = {}".format(self.cmd_run_coverage),
                         ])


    def tearDown(self):
        shutil.rmtree(self.tmpd)



class TestConfig(ConfigBase):
    """
    All variations of config file parsing works as expected.
    """


    def test_cmd_env_def(self):
        """
        Setup: --config on cmd, $GREEN_CONFIG is set, $HOME/.green exists
        Result: load --config
        """
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=self.tmpd):
            cfg = config.getConfig(self.cmd_filename)
            ae = self.assertEqual
            ae(["green"],               cfg.sections())
            ae(self.cmd_filename,       cfg.get("green", "omit-patterns"))
            ae(self.cmd_run_coverage,   cfg.getboolean("green", "run-coverage"))
            ae(self.cmd_logging,        cfg.getboolean("green", "logging"))
            ae(self.env_no_skip_report, cfg.getboolean("green", "no-skip-report"))
            ae(self.default_version,    cfg.getboolean("green", "version"))


    def test_cmd_env_nodef(self):
        """
        Setup: --config on cmd, $GREEN_CONFIG is set, $HOME/.green does not
            exist
        Result: load --config
        """
        os.unlink(self.default_filename)
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=self.tmpd):
            cfg = config.getConfig(self.cmd_filename)
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.cmd_filename,          cfg.get("green", "omit-patterns"))
            ae(self.cmd_run_coverage,      cfg.getboolean("green", "run-coverage"))
            ae(self.cmd_logging,           cfg.getboolean("green", "logging"))
            ae(self.env_no_skip_report,    cfg.getboolean("green", "no-skip-report"))
            ar(configparser.NoOptionError, cfg.getboolean, "green", "version")


    def test_cmd_noenv_def(self):
        """
        Setup: --config on cmd, $GREEN_CONFIG unset, $HOME/.green exists
        Result: load --config
        """
        os.unlink(self.env_filename)
        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):
            cfg = config.getConfig(self.cmd_filename)
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.cmd_filename,          cfg.get("green", "omit-patterns"))
            ae(self.cmd_run_coverage,      cfg.getboolean("green", "run-coverage"))
            ae(self.cmd_logging,           cfg.getboolean("green", "logging"))
            ar(configparser.NoOptionError, cfg.getboolean, "green", "no-skip-report")
            ae(self.default_version,       cfg.getboolean("green", "version"))


    def test_cmd_noenv_nodef(self):
        """
        Setup: --config on cmd, $GREEN_CONFIG unset, $HOME/.green does not exist
        Result: load --config
        """
        os.unlink(self.env_filename)
        os.unlink(self.default_filename)
        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):
            cfg = config.getConfig(self.cmd_filename)
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.cmd_filename,          cfg.get("green", "omit-patterns"))
            ae(self.cmd_run_coverage,      cfg.getboolean("green", "run-coverage"))
            ae(self.cmd_logging,           cfg.getboolean("green", "logging"))
            ar(configparser.NoOptionError, cfg.getboolean, "green", "no-skip-report")
            ar(configparser.NoOptionError, cfg.getboolean, "green", "version")


    def test_nocmd_env_def(self):
        """
        Setup: no --config option, $GREEN_CONFIG is set, $HOME/.green exists
        Result: load $GREEN_CONFIG
        """
        os.unlink(self.cmd_filename)
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=self.tmpd):
            cfg = config.getConfig()
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.env_filename,          cfg.get("green", "omit-patterns"))
            ar(configparser.NoOptionError, cfg.get, "green", "run-coverage")
            ae(self.env_logging,           cfg.getboolean("green", "logging"))
            ae(self.env_no_skip_report,    cfg.getboolean("green", "no-skip-report"))
            ae(self.default_version,       cfg.getboolean("green", "version"))


    def test_nocmd_env_nodef(self):
        """
        Setup: no --config option, $GREEN_CONFIG is set, $HOME/.green does not
            exist
        Result: load $GREEN_CONFIG
        """
        os.unlink(self.cmd_filename)
        os.unlink(self.default_filename)
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=self.tmpd):
            cfg = config.getConfig()
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.env_filename,          cfg.get("green", "omit-patterns"))
            ar(configparser.NoOptionError, cfg.get, "green", "run-coverage")
            ae(self.env_logging,           cfg.getboolean("green", "logging"))
            ae(self.env_no_skip_report,    cfg.getboolean("green", "no-skip-report"))
            ar(configparser.NoOptionError, cfg.getboolean, "green", "version")


    def test_nocmd_noenv_def(self):
        """
        Setup: no --config option, $GREEN_CONFIG unset, $HOME/.green exists
        Result: load $HOME/.green
        """
        os.unlink(self.cmd_filename)
        os.unlink(self.env_filename)
        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):
            cfg = config.getConfig()
            ae = self.assertEqual
            ar = self.assertRaises
            ae(["green"],                  cfg.sections())
            ae(self.default_filename,      cfg.get("green", "omit-patterns"))
            ar(configparser.NoOptionError, cfg.get, "green", "run-coverage")
            ae(self.default_logging,       cfg.getboolean("green", "logging"))
            ar(configparser.NoOptionError, cfg.getboolean, "green", "no-skip-report")
            ae(self.default_version,       cfg.getboolean("green", "version"))


    def test_nocmd_noenv_nodef(self):
        """
        Setup: no --config option, $GREEN_CONFIG unset, no $HOME/.green
        Result: empty config
        """
        os.unlink(self.default_filename)
        os.unlink(self.env_filename)
        os.unlink(self.cmd_filename)
        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):
            cfg = config.getConfig()
            ae = self.assertEqual
            ar = self.assertRaises
            ae([], cfg.sections())
            ar(configparser.NoSectionError, cfg.get, "green", "omit-patterns")
            ar(configparser.NoSectionError, cfg.get, "green", "run-coverage")
            ar(configparser.NoSectionError, cfg.get, "green", "logging")
            ar(configparser.NoSectionError, cfg.get, "green", "no-skip-report")
            ar(configparser.NoSectionError, cfg.get, "green", "version")



class TestMergeConfig(ConfigBase):
    """
    Merging config files and command-line arguments works as expected.
    """


    def test_overwrite(self):
        """
        Non-default command-line argument values overwrite config values.
        """
        # This config environment should set the values we look at to False and
        # a filename in omit-patterns
        s = StringIO()
        gs = GreenStream(s)
        saved_stdout = config.sys.stdout
        config.sys.stdout = gs
        self.addCleanup(setattr, config.sys, 'stdout', saved_stdout)
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=self.tmpd):
            new_args = copy.deepcopy(config.default_args)

            new_args.omit_patterns  = 'omitstuff'
            new_args.run_coverage   = True
            new_args.logging        = True
            new_args.no_skip_report = True
            new_args.version        = True

            new_args.config = self.cmd_filename
            computed_args = config.mergeConfig(new_args, testing=True)

            self.assertEqual(computed_args.omit_patterns,  'omitstuff')
            self.assertEqual(computed_args.run_coverage,   new_args.run_coverage)
            self.assertEqual(computed_args.logging,        new_args.logging)
            self.assertEqual(computed_args.no_skip_report, new_args.no_skip_report)
            self.assertEqual(computed_args.version,        new_args.version)


    def test_no_overwrite(self):
        """
        Default unspecified command-line args do not overwrite config values.
        """
        # This config environment should set logging to True
        with ModifiedEnvironment(GREEN_CONFIG=self.env_filename, HOME=""):
            # The default for logging in arguments is False
            da = copy.deepcopy(config.default_args)
            del(da.logging)
            computed_args = config.mergeConfig(da, testing=True)
            self.assertEqual(computed_args.logging, True)


    def test_specified_command_line(self):
        """
        Specified command-line arguments always overwrite config file values
        """
        with ModifiedEnvironment(HOME=self.tmpd):
            new_args = copy.deepcopy(config.default_args)
            new_args.failfast = True # same as config, for sanity
            new_args.logging = True # different than config, not default
            del(new_args.version) # Not in arguments, should get config value
            new_args.termcolor = False # override config, set back to default
            computed_args = config.mergeConfig(new_args, testing=True)
            self.assertEqual(computed_args.failfast, True)
            self.assertEqual(computed_args.logging, True)
            self.assertEqual(computed_args.version, False)
            self.assertEqual(computed_args.termcolor, False)


    def test_targets(self):
        """
        The targets passed in make it through mergeConfig, and the specified
        target gets parsed
        """
        config.sys.argv = ['', 'target1', 'target2']
        args = config.parseArguments()
        args = config.mergeConfig(args)
        self.assertEqual(args.targets, ['target1', 'target2'])


    def test_forgotToUpdateMerge(self):
         """
         mergeConfig raises an exception for unknown cmdline args
         """
         orig_args = copy.deepcopy(config.default_args)
         self.addCleanup(setattr, config, 'default_args', orig_args)
         config.default_args.new_option = True

         new_args = copy.deepcopy(config.default_args)

         self.assertRaises(NotImplementedError, config.mergeConfig, new_args,
                 testing=True)

"""
TAI64N encoding and decoding.

TAI64N encodes nanosecond-accuracy timestamps and is supported by logstash.

@see: U{http://cr.yp.to/libtai/tai64.html}.
"""

from __future__ import unicode_literals

import struct
from binascii import b2a_hex, a2b_hex

_STRUCTURE = b">QI"
_OFFSET = (2 ** 62) + 10 # last 10 are leap seconds


def encode(timestamp):
    """
    Convert seconds since epoch to TAI64N string.

    @param timestamp: Seconds since UTC Unix epoch as C{float}.

    @return: TAI64N-encoded time, as C{unicode}.
    """
    seconds = int(timestamp)
    nanoseconds = int((timestamp - seconds) * 1000000000)
    seconds = seconds + _OFFSET
    encoded = b2a_hex(struct.pack(_STRUCTURE, seconds, nanoseconds))
    return "@" + encoded.decode("ascii")



def decode(tai64n):
    """
    Convert TAI64N string to seconds since epoch.

    Note that dates before 2013 may not decode accurately due to leap second
    issues. If you need correct decoding for earlier dates you can try the
    tai64n package available from PyPI (U{https://pypi.python.org/pypi/tai64n}).

    @param tai64n: TAI64N-encoded time, as C{unicode}.

    @return: Seconds since UTC Unix epoch as C{float}.
    """
    seconds, nanoseconds = struct.unpack(_STRUCTURE, a2b_hex(tai64n[1:]))
    seconds -= _OFFSET
    return seconds + (nanoseconds / 1000000000.0)


from sys import stdout
from eliot import start_action, start_task, to_file
to_file(stdout)


class Place(object):
    def __init__(self, name, contained=()):
        self.name = name
        self.contained = contained

    def visited(self, people):
        # No need to repetitively log people, since caller will:
        with start_action(action_type="visited", place=self.name):
            for thing in self.contained:
                thing.visited(people)


def honeymoon(family, destination):
    with start_task(action_type="honeymoon", people=family):
        destination.visited(family)


honeymoon(["Mrs. Casaubon", "Mr. Casaubon"],
          Place("Rome, Italy",
                [Place("Vatican Museum",
                       [Place("Statue #1"), Place("Statue #2")])]))

# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Tests for ``admin.packaging``.
"""

from glob import glob
import platform
from subprocess import check_output
from textwrap import dedent
from unittest import skipIf
from StringIO import StringIO

from twisted.python.filepath import FilePath
from twisted.python.procutils import which
from twisted.python.usage import UsageError

from virtualenv import REQUIRED_MODULES as VIRTUALENV_REQUIRED_MODULES

from flocker.testtools import TestCase, FakeSysModule

from .. import packaging
from ..packaging import (
    omnibus_package_builder, InstallVirtualEnv, InstallApplication,
    BuildPackage, BuildSequence, BuildOptions, BuildScript, DockerBuildOptions,
    DockerBuildScript, GetPackageVersion, DelayedRpmVersion, CreateLinks,
    PythonPackage, create_virtualenv, VirtualEnv, PackageTypes, Distribution,
    Dependency, build_in_docker, DockerBuild, DockerRun,
    PACKAGE, PACKAGE_PYTHON, PACKAGE_CLI, PACKAGE_NODE, PACKAGE_DOCKER_PLUGIN,
    make_dependencies, available_distributions,
    LintPackage,
)
from flocker.common.version import RPMVersion

FLOCKER_PATH = FilePath(__file__).parent().parent().parent()

require_fpm = skipIf(not which('fpm'), "Tests require the ``fpm`` command.")
require_rpm = skipIf(not which('rpm'), "Tests require the ``rpm`` command.")
require_rpmlint = skipIf(not which('rpmlint'),
                         "Tests require the ``rpmlint`` command.")
require_dpkg = skipIf(not which('dpkg'), "Tests require the ``dpkg`` command.")
require_lintian = skipIf(not which('lintian'),
                         "Tests require the ``lintian`` command.")
require_not_ubuntu = skipIf(
    platform.linux_distribution()[0] == 'Ubuntu',
    "rpmlint returns spurious results on Ubuntu: FLOC-3564.")

DOCKER_SOCK = '/var/run/docker.sock'


def assert_equal_steps(test_case, expected, actual):
    """
    Assert that the list of provided steps are the same.
    If they are not, display the differences intelligently.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param expected: The expected build step instance.
    :param actual: The actual build step instance.
    :raises: ``TestFailure`` if the build steps are not equal, showing the
        unequal or missing steps.
    """
    expected_steps = getattr(expected, 'steps')
    actual_steps = getattr(actual, 'steps')
    if None in (expected_steps, actual_steps):
        test_case.assertEqual(expected, actual)
    else:
        mismatch_steps = []
        missing_steps = []
        index = 0
        for index, expected_step in enumerate(expected_steps):
            try:
                actual_step = actual_steps[index]
            except IndexError:
                missing_steps = expected_steps[index:]
                break
            if expected_step != actual_step:
                mismatch_steps.append(
                    '* expected: {} !=\n'
                    '  actual:   {}'.format(
                        expected_step, actual_step))
        extra_steps = actual_steps[index+1:]
        if mismatch_steps or missing_steps or extra_steps:
            test_case.fail(
                'Step Mismatch\n'
                'Mismatch:\n{}\n'
                'Missing:\n{}\n'
                'Extra:\n{}'.format(
                    '\n'.join(mismatch_steps), missing_steps, extra_steps)
            )


def assert_dict_contains(test_case, expected, actual, message=''):
    """
    Fail unless the supplied ``actual`` ``dict`` contains all the items in
    ``expected``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param expected: The expected build step instance.
    :param actual: The actual build step instance.
    """
    missing_items = []
    mismatch_items = []
    no_value = object()
    for key, expected_value in expected.items():
        actual_value = actual.get(key, no_value)
        if actual_value is no_value:
            missing_items.append(key)
        elif actual_value != expected_value:
            mismatch_items.append(
                '{}: {} != {}'.format(key, expected_value, actual_value)
            )
    if missing_items or mismatch_items:
        test_case.fail(
            '{}\n'
            'Missing items: {}\n'
            'Mismatch items:  {}\n'
            'Actual items: {}'.format(
                message, missing_items, mismatch_items, actual)
        )


def parse_colon_dict(data):
    """
    Parse colon seperated values into a dictionary, treating lines
    lacking a colon as continutation lines.

    Any leading lines without a colon will be associated with the key
    ``None``.

    This is the format output by ``rpm --query`` and ``dpkg --info``.

    :param bytes data: Data to parse
    :return: A ``dict`` containing the parsed data.
    """
    result = {}
    key = None
    for line in data.splitlines():
        parts = [value.strip() for value in line.split(':', 1)]
        if len(parts) == 2:
            key, val = parts
            result[key] = val
        else:
            result.setdefault(key, '')
            result[key] += parts[0]
    return result


def assert_rpm_headers(test_case, expected_headers, rpm_path):
    """
    Fail unless the ``RPM`` file at ``rpm_path`` contains all the
    ``expected_headers``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param dict expected_headers: A dictionary of header key / value pairs.
    :param FilePath rpm_path: The path to the RPM file under test.
    """
    output = check_output(
        ['rpm', '--query', '--info', '--package', rpm_path.path]
    )
    actual_headers = parse_colon_dict(output)

    assert_dict_contains(
        test_case, expected_headers, actual_headers, 'Missing RPM Headers: '
    )


def assert_rpm_content(test_case, expected_paths, package_path):
    """
    Fail unless the ``RPM`` file at ``rpm_path`` contains all the
    ``expected_paths``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param set expected_paths: A set of ``FilePath`` s
    :param FilePath package_path: The path to the package under test.
    """
    output = check_output(
        ['rpm', '--query', '--list', '--package', package_path.path]
    )
    actual_paths = set(map(FilePath, output.splitlines()))
    test_case.assertEqual(expected_paths, actual_paths)


def assert_deb_content(test_case, expected_paths, package_path):
    """
    Fail unless the ``deb`` file at ``package_path`` contains all the
    ``expected_paths``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param set expected_paths: A set of ``FilePath`` s
    :param FilePath package_path: The path to the package under test.
    """
    output_dir = FilePath(test_case.mktemp())
    output_dir.makedirs()
    check_output(['dpkg', '--extract', package_path.path, output_dir.path])

    actual_paths = set()
    for f in output_dir.walk():
        if f.isdir():
            continue
        actual_paths.add(FilePath('/').descendant(f.segmentsFrom(output_dir)))

    test_case.assertEqual(expected_paths, actual_paths)


def assert_deb_headers(test_case, expected_headers, package_path):
    """
    Fail unless the ``deb`` file at ``package_path`` contains all the
    ``expected_headers``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param dict expected_headers: A dictionary of header key / value pairs.
    :param FilePath package_path: The path to the deb file under test.
    """
    output = check_output(
        ['dpkg', '--info', package_path.path]
    )
    actual_headers = parse_colon_dict(output)

    assert_dict_contains(
        test_case, expected_headers, actual_headers, 'Missing dpkg Headers: '
    )


def assert_rpm_requires(test_case, expected_requirements, rpm_path):
    """
    Fail unless the ``RPM`` file at ``rpm_path`` has all the
    ``expected_requirements``.

    :param test_case: The ``TestCase`` whose assert methods will be called.
    :param list expected_requirements: A list of requirement strings.
    :param FilePath rpm_path: The path to the RPM file under test.
    """
    output = check_output(
        ['rpm', '--query', '--requires', '--package', rpm_path.path]
    )
    actual_requirements = set(line.strip() for line in output.splitlines())
    expected_requirements = set(expected_requirements)
    missing_requirements = expected_requirements - actual_requirements
    if missing_requirements:
        test_case.fail('Missing requirements: {} in {}'.format(
            missing_requirements, rpm_path.path))


class SpyVirtualEnv(object):
    """
    A ``VirtualEnv`` like class which records the ``package_uri``s which are
    supplied to its ``install`` method.
    """
    def __init__(self):
        self._installed_packages = []

    def install(self, package_uri):
        self._installed_packages.append(package_uri)


class SpyStep(object):
    """
    A build step which records the fact that it has been run.

    :ivar bool ran: ``False`` by default.
    """
    ran = False

    def run(self):
        self.ran = True


class BuildSequenceTests(TestCase):
    """
    Tests for ``BuildSequence``.
    """
    def test_run(self):
        """
        ``BuildSequence`` calls the ``run`` method of each of its ``steps``.
        """
        step1 = SpyStep()
        step2 = SpyStep()

        BuildSequence(steps=(step1, step2)).run()

        self.assertEqual((True, True), (step1.ran, step2.ran))


def assert_has_paths(test_case, expected_paths, parent_path):
    """
    Fail if any of the ``expected_paths`` are not existing relative paths of
    ``parent_path``.

    :param TestCase test_case: The ``TestCase`` with which to make assertions.
    :param list expected_paths: A ``list`` of ``bytes`` relative path names
        which are expected to exist beneath ``parent_path``.
    :param FilePath parent_path: The root ``FilePath`` in which to search for
        ``expected_paths``.
    """
    missing_paths = []
    for path in expected_paths:
        if not parent_path.preauthChild(path).exists():
            missing_paths.append(path)
        if missing_paths:
            test_case.fail('Missing paths: {}'.format(missing_paths))


class InstallVirtualEnvTests(TestCase):
    """
    Tests for ``InstallVirtualEnv``.
    """
    def test_run(self):
        """
        ``InstallVirtualEnv.run`` installs a virtual python environment using
        create_virtualenv passing ``target_path`` as ``root``.
        """
        virtualenv = VirtualEnv(root=FilePath(self.mktemp()))
        step = InstallVirtualEnv(virtualenv=virtualenv)
        calls = []
        self.patch(
            step, '_create_virtualenv', lambda **kwargs: calls.append(kwargs))
        step.run()
        self.assertEqual([dict(root=virtualenv.root)], calls)


class CreateVirtualenvTests(TestCase):
    """
    """
    def test_bin(self):
        """
        ``create_virtualenv`` installs a virtual python environment in its
        ``target_path``.
        """
        virtualenv = VirtualEnv(root=FilePath(self.mktemp()))
        InstallVirtualEnv(virtualenv=virtualenv).run()
        expected_paths = ['bin/pip', 'bin/python']
        assert_has_paths(self, expected_paths, virtualenv.root)

    def test_pythonpath(self):
        """
        ``create_virtualenv`` installs a virtual python whose path does not
        include the system python libraries.
        """
        target_path = FilePath(self.mktemp())
        create_virtualenv(root=target_path)
        output = check_output([
            target_path.descendant(['bin', 'python']).path,
            '-c', r'import sys; sys.stdout.write("\n".join(sys.path))'
        ])
        # We should probably check for lib64 as well here.
        self.assertNotIn(
            '/usr/lib/python2.7/site-packages', output.splitlines())

    def test_bootstrap_pyc(self):
        """
        ``create_virtualenv`` creates links to the pyc files for all the
        modules required for the virtualenv bootstrap process.
        """
        target_path = FilePath(self.mktemp())
        create_virtualenv(root=target_path)

        py_files = []
        for module_name in VIRTUALENV_REQUIRED_MODULES:
            py_base = target_path.descendant(['lib', 'python2.7', module_name])
            py = py_base.siblingExtension('.py')
            pyc = py_base.siblingExtension('.pyc')
            if py.exists() and False in (py.islink(), pyc.islink()):
                py_files.append('PY: {} > {}\nPYC: {} > {}\n'.format(
                    '/'.join(py.segmentsFrom(target_path)),
                    py.realpath().path,
                    '/'.join(pyc.segmentsFrom(target_path)),
                    pyc.islink() and pyc.realpath().path or 'NOT A SYMLINK'
                ))

        if py_files:
            self.fail(
                'Non-linked bootstrap pyc files in {}: \n{}'.format(
                    target_path, '\n'.join(py_files)
                )
            )

    def test_internal_symlinks_only(self):
        """
        The resulting ``virtualenv`` only contains symlinks to files inside the
        virtualenv and to /usr on the host OS.
        """
        target_path = FilePath(self.mktemp())
        create_virtualenv(root=target_path)
        allowed_targets = (target_path, FilePath('/usr'),)
        bad_links = []
        for path in target_path.walk():
            if path.islink():
                realpath = path.realpath()
                for allowed_target in allowed_targets:
                    try:
                        realpath.segmentsFrom(allowed_target)
                    except ValueError:
                        pass
                    else:
                        # The target is a descendent of an allowed_target.
                        break
                else:
                    bad_links.append(path)
        if bad_links:
            self.fail(
                "Symlinks outside of virtualenv detected:" +
                '\n'.join(
                    '/'.join(
                        path.segmentsFrom(target_path)
                    ) + ' -> ' + path.realpath().path
                    for path in bad_links
                )
            )


class VirtualEnvTests(TestCase):
    """
    Tests for ``VirtualEnv``.
    """
    def test_install(self):
        """
        ``VirtualEnv.install`` accepts a ``PythonPackage`` instance and
        installs it.
        """
        virtualenv_dir = FilePath(self.mktemp())
        virtualenv = create_virtualenv(root=virtualenv_dir)
        package_dir = FilePath(self.mktemp())
        package = canned_package(package_dir)
        virtualenv.install(package_dir.path)
        self.assertIn(
            '{}-{}-py2.7.egg-info'.format(package.name, package.version),
            [f.basename() for f in virtualenv_dir.descendant(
                ['lib', 'python2.7', 'site-packages']).children()]
        )


class InstallApplicationTests(TestCase):
    """
    Tests for ``InstallApplication``.
    """
    def test_run(self):
        """
        ``InstallApplication.run`` installs the supplied application in the
        ``target_path``.
        """
        package_uri = 'http://www.example.com/Bar-1.2.3.whl'
        fake_env = SpyVirtualEnv()
        InstallApplication(
            virtualenv=fake_env,
            package_uri=package_uri
        ).run()

        self.assertEqual(
            [package_uri], fake_env._installed_packages)


class CreateLinksTests(TestCase):
    """
    Tests for ``CreateLinks``.
    """
    def test_run(self):
        """
        ``CreateLinks.run`` generates symlinks in ``destination_path`` for all
        the supplied ``links``.
        """
        root = FilePath(self.mktemp())
        bin_dir = root.descendant(['usr', 'bin'])
        bin_dir.makedirs()

        CreateLinks(
            links=frozenset([
                (FilePath('/opt/flocker/bin/flocker-foo'), bin_dir),
                (FilePath('/opt/flocker/bin/flocker-bar'), bin_dir),
            ])
        ).run()

        self.assertEqual(
            set(FilePath('/opt/flocker/bin').child(script)
                for script in ('flocker-foo', 'flocker-bar')),
            set(child.realpath() for child in bin_dir.children())
        )


def canned_package(root, version=b'0.3.2'):
    """
    Create a directory containing an empty Python package which can be
    installed and with a name and version which can later be tested.

    :param FilePath root: The top-level directory of the canned package.
    :param test_case: The ``TestCase`` whose mktemp method will be called.
    :param version: The version of the created package.
    :return: A ``PythonPackage`` instance.
    """
    name = 'FooBar'
    root.makedirs()
    setup_py = root.child('setup.py')
    setup_py.setContent(
        dedent("""
        from setuptools import setup

        setup(
            name="{package_name}",
            version="{package_version}",
            py_modules=["{package_name}"],
        )
        """).format(package_name=name, package_version=version)
    )
    package_module = root.child(name + ".py")
    package_module.setContent(
        dedent("""
        __version__ = "{package_version}"
        """).format(package_version=version)
    )

    return PythonPackage(name=name, version=version)


class GetPackageVersionTests(TestCase):
    """
    Tests for ``GetPackageVersion``.
    """
    def test_version_default(self):
        """
        ``GetPackageVersion.version`` is ``None`` by default.
        """
        step = GetPackageVersion(virtualenv=None, package_name=None)
        self.assertIs(None, step.version)

    def assert_version_found(self, version):
        """
        ``GetPackageVersion`` assigns the exact version of a found package to
        its ``version`` attribute.

        :param version: The version of the package to test package.
        """
        test_env = FilePath(self.mktemp())
        virtualenv = VirtualEnv(root=test_env)
        InstallVirtualEnv(virtualenv=virtualenv).run()
        package_root = FilePath(self.mktemp())
        test_package = canned_package(root=package_root, version=version)
        InstallApplication(
            virtualenv=virtualenv, package_uri=package_root.path).run()

        step = GetPackageVersion(
            virtualenv=virtualenv, package_name=test_package.name)
        step.run()
        self.assertEqual(test_package.version, step.version)

    def test_version_found(self):
        """
        ``GetPackageVersion`` assigns the exact version of a found package to
        its ``version`` attribute.
        """
        versions = [
            '0.3.2',
            '0.3.3.dev5',
            '0.3.2.post1',
            '0.3.2+1.gf661a6a',
            '0.3.2.post1+1.gf661a6a',
            '0.3.2rc1',
            '0.3.2+1.gf661a6a.dirty'
            '0.3.2.post1+1.gf661a6a.dirty'
        ]
        for version in versions:
            self.assert_version_found(version=version)

    def test_version_not_found(self):
        """
        ``GetPackageVersion.run`` raises an exception if the supplied
        ``package_name`` is not installed in the supplied ``virtual_env``.
        """
        test_env = FilePath(self.mktemp())
        virtualenv = VirtualEnv(root=test_env)
        InstallVirtualEnv(virtualenv=virtualenv).run()

        step = GetPackageVersion(
            virtualenv=virtualenv,
            package_name='PackageWhichIsNotInstalled'
        )
        self.assertRaises(Exception, step.run)


class BuildPackageTests(TestCase):
    """
    Tests for `BuildPackage`.
    """
    @require_fpm
    def setUp(self):
        super(BuildPackageTests, self).setUp()

    @require_rpm
    def test_rpm(self):
        """
        ``BuildPackage.run`` creates an RPM from the supplied ``source_path``.
        """
        destination_path = FilePath(self.mktemp())
        destination_path.makedirs()
        source_path = FilePath(self.mktemp())
        source_path.makedirs()
        source_path.child('Foo').touch()
        source_path.child('Bar').touch()
        expected_prefix = FilePath('/foo/bar')
        expected_paths = set([
            expected_prefix.child('Foo'),
            expected_prefix.child('Bar'),
            FilePath('/other/file'),
        ])
        expected_name = 'FooBar'
        expected_epoch = b'3'
        expected_rpm_version = RPMVersion(version='0.3', release='0.dev.1')
        expected_license = 'My Test License'
        expected_url = 'https://www.example.com/foo/bar'
        expected_vendor = 'Acme Corporation'
        expected_maintainer = 'noreply@example.com'
        expected_architecture = 'i386'
        expected_description = 'Explosive Tennis Balls'
        expected_dependencies = ['test-dep', 'version-dep >= 42']
        BuildPackage(
            package_type=PackageTypes.RPM,
            destination_path=destination_path,
            source_paths={
                source_path: FilePath('/foo/bar'),
                source_path.child('Foo'): FilePath('/other/file'),
            },
            name=expected_name,
            prefix=FilePath('/'),
            epoch=expected_epoch,
            rpm_version=expected_rpm_version,
            license=expected_license,
            url=expected_url,
            vendor=expected_vendor,
            maintainer=expected_maintainer,
            architecture=expected_architecture,
            description=expected_description,
            category="Applications/System",
            dependencies=[
                Dependency(package='test-dep'),
                Dependency(package='version-dep', compare='>=', version='42')],
        ).run()
        rpms = glob('{}*.rpm'.format(
            destination_path.child(expected_name).path))
        self.assertEqual(1, len(rpms))

        expected_headers = dict(
            Name=expected_name,
            Epoch=expected_epoch,
            Version=expected_rpm_version.version,
            Release=expected_rpm_version.release,
            License=expected_license,
            URL=expected_url,
            Vendor=expected_vendor,
            Packager=expected_maintainer,
            Architecture=expected_architecture,
            Group="Applications/System",
        )
        rpm_path = FilePath(rpms[0])
        assert_rpm_requires(self, expected_dependencies, rpm_path)
        assert_rpm_headers(self, expected_headers, rpm_path)
        assert_rpm_content(self, expected_paths, rpm_path)

    @require_dpkg
    def test_deb(self):
        """
        ``BuildPackage.run`` creates a .deb package from the supplied
        ``source_path``.
        """
        destination_path = FilePath(self.mktemp())
        destination_path.makedirs()
        source_path = FilePath(self.mktemp())
        source_path.makedirs()
        source_path.child('Foo').touch()
        source_path.child('Bar').touch()
        expected_prefix = FilePath('/foo/bar')
        expected_paths = set([
            expected_prefix.child('Foo'),
            expected_prefix.child('Bar'),
            FilePath('/other/file'),
            # This is added automatically by fpm despite not supplying the
            # --deb-changelog option
            FilePath('/usr/share/doc/foobar/changelog.Debian.gz'),
        ])
        expected_name = 'FooBar'.lower()
        expected_epoch = b'3'
        expected_rpm_version = RPMVersion(version='0.3', release='0.dev.1')
        expected_license = 'My Test License'
        expected_url = 'https://www.example.com/foo/bar'
        expected_vendor = 'Acme Corporation'
        expected_maintainer = 'noreply@example.com'
        expected_architecture = 'i386'
        expected_description = 'Explosive Tennis Balls'
        BuildPackage(
            package_type=PackageTypes.DEB,
            destination_path=destination_path,
            source_paths={
                source_path: FilePath('/foo/bar'),
                source_path.child('Foo'): FilePath('/other/file'),
            },
            name=expected_name,
            prefix=FilePath("/"),
            epoch=expected_epoch,
            rpm_version=expected_rpm_version,
            license=expected_license,
            url=expected_url,
            vendor=expected_vendor,
            maintainer=expected_maintainer,
            architecture=expected_architecture,
            description=expected_description,
            category="admin",
            dependencies=[
                Dependency(package='test-dep'),
                Dependency(package='version-dep', compare='>=', version='42')],
        ).run()
        packages = glob('{}*.deb'.format(
            destination_path.child(expected_name.lower()).path))
        self.assertEqual(1, len(packages))

        expected_headers = dict(
            Package=expected_name,
            Version=(
                expected_epoch +
                b':' +
                expected_rpm_version.version +
                '-' +
                expected_rpm_version.release
            ),
            License=expected_license,
            Vendor=expected_vendor,
            Architecture=expected_architecture,
            Maintainer=expected_maintainer,
            Homepage=expected_url,
            Depends=', '.join(['test-dep', 'version-dep (>= 42)']),
            Section="admin",
        )
        assert_deb_headers(self, expected_headers, FilePath(packages[0]))
        assert_deb_content(self, expected_paths, FilePath(packages[0]))


class LintPackageTests(TestCase):
    """
    Tests for ``LintPackage``.
    """

    @require_fpm
    def setUp(self):
        super(LintPackageTests, self).setUp()

    def assert_lint(self, package_type, expected_output):
        """
        ``LintPackage.run`` reports only unfiltered errors and raises
        ``SystemExit``.

        :param PackageTypes package_type: The type of package to test.
        :param bytes expected_output: The expected output of the linting.
        """
        destination_path = FilePath(self.mktemp())
        destination_path.makedirs()
        source_path = FilePath(self.mktemp())
        source_path.makedirs()
        source_path.child('Foo').touch()
        source_path.child('Bar').touch()
        BuildPackage(
            package_type=package_type,
            destination_path=destination_path,
            source_paths={
                source_path: FilePath('/foo/bar'),
                source_path.child('Foo'): FilePath('/opt/file'),
            },
            name="package-name",
            prefix=FilePath('/'),
            epoch=b'3',
            rpm_version=RPMVersion(version='0.3', release='0.dev.1'),
            license="Example",
            url="https://package.example/",
            vendor="Acme Corporation",
            maintainer='Someone <noreply@example.com>',
            architecture="all",
            description="Description\n\nExtended",
            category="none",
            dependencies=[]
        ).run()

        step = LintPackage(
            package_type=package_type,
            destination_path=destination_path,
            epoch=b'3',
            rpm_version=RPMVersion(version='0.3', release='0.dev.1'),
            package='package-name',
            architecture='all'
        )
        step.output = StringIO()
        self.assertRaises(SystemExit, step.run)
        self.assertEqual(step.output.getvalue(), expected_output)

    @require_not_ubuntu
    @require_rpmlint
    def test_rpm(self):
        """
        rpmlint doesn't report filtered errors.
        """
        # The following warnings and errors are filtered.
        # - E: no-changelogname-tag
        # - W: no-documentation
        # - E: zero-length
        self.assert_lint(PackageTypes.RPM, b"""\
Package errors (package-name):
package-name.noarch: W: non-standard-group default
package-name.noarch: W: invalid-license Example
package-name.noarch: W: invalid-url URL: https://package.example/ \
<urlopen error [Errno -2] Name or service not known>
package-name.noarch: W: cross-directory-hard-link /foo/bar/Foo /opt/file
""")

    @require_lintian
    def test_deb(self):
        """
        lintian doesn't report filtered errors.
        """
        # The following warnings and errors are filtered.
        # - E: package-name: no-copyright-file
        # - E: package-name: dir-or-file-in-opt
        # - W: package-name: file-missing-in-md5sums .../changelog.Debian.gz
        self.assert_lint(PackageTypes.DEB, b"""\
Package errors (package-name):
W: package-name: unknown-section default
E: package-name: non-standard-toplevel-dir foo/
W: package-name: file-in-unusual-dir foo/bar/Bar
W: package-name: file-in-unusual-dir foo/bar/Foo
W: package-name: package-contains-hardlink foo/bar/Foo -> opt/file
""")


class OmnibusPackageBuilderTests(TestCase):
    """
    Tests for ``omnibus_package_builder``.
    """
    def test_centos_7(self):
        self.assert_omnibus_steps(
            distribution=Distribution(name='centos', version='7'),
            expected_category='Applications/System',
            expected_package_type=PackageTypes.RPM,
        )

    def test_ubuntu_14_04(self):
        self.assert_omnibus_steps(
            distribution=Distribution(name='ubuntu', version='14.04'),
            expected_category='admin',
            expected_package_type=PackageTypes.DEB,
        )

    def assert_omnibus_steps(
            self,
            distribution=Distribution(name='centos', version='7'),
            expected_category='Applications/System',
            expected_package_type=PackageTypes.RPM,
            ):
        """
        A sequence of build steps is returned.
        """
        self.patch(packaging, 'CURRENT_DISTRIBUTION', distribution)

        fake_dependencies = {
            'python': [Dependency(package='python-dep')],
            'node': [Dependency(package='node-dep')],
            'docker-plugin': [Dependency(package='docker-plugin-dep')],
            'cli': [Dependency(package='cli-dep')],
        }

        def fake_make_dependencies(
                package_name, package_version, distribution):
            return fake_dependencies[package_name]

        self.patch(packaging, 'make_dependencies', fake_make_dependencies)

        expected_destination_path = FilePath(self.mktemp())

        target_path = FilePath(self.mktemp())
        flocker_cli_path = target_path.child('flocker-cli')
        flocker_node_path = target_path.child('flocker-node')
        flocker_docker_plugin_path = target_path.child('flocker-docker-plugin')
        flocker_shared_path = target_path.child('flocker-shared')
        empty_path = target_path.child('empty')

        expected_virtualenv_path = FilePath('/opt/flocker')
        expected_prefix = FilePath('/')
        expected_epoch = PACKAGE.EPOCH.value
        expected_package_uri = b'https://www.example.com/foo/Bar-1.2.3.whl'
        expected_package_version_step = GetPackageVersion(
            virtualenv=VirtualEnv(root=expected_virtualenv_path),
            package_name='flocker'
        )
        expected_version = DelayedRpmVersion(
            package_version_step=expected_package_version_step
        )
        expected_license = PACKAGE.LICENSE.value
        expected_url = PACKAGE.URL.value
        expected_vendor = PACKAGE.VENDOR.value
        expected_maintainer = PACKAGE.MAINTAINER.value

        package_files = FilePath('/package-files')

        virtualenv = VirtualEnv(root=expected_virtualenv_path)
        expected = BuildSequence(
            steps=(
                # clusterhq-python-flocker steps
                InstallVirtualEnv(virtualenv=virtualenv),
                InstallApplication(virtualenv=virtualenv,
                                   package_uri='pip==8.1.1'),
                InstallApplication(virtualenv=virtualenv,
                                   package_uri='-r/flocker/requirements.txt'),
                InstallApplication(
                    virtualenv=VirtualEnv(root=expected_virtualenv_path),
                    package_uri=b'https://www.example.com/foo/Bar-1.2.3.whl',
                ),
                expected_package_version_step,
                CreateLinks(
                    links=[
                        (FilePath('/opt/flocker/bin/eliot-prettyprint'),
                         flocker_shared_path),
                        (FilePath('/opt/flocker/bin/eliot-tree'),
                         flocker_shared_path),
                    ],
                ),
                BuildPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    source_paths={
                        expected_virtualenv_path: expected_virtualenv_path,
                        flocker_shared_path: FilePath("/usr/bin"),
                    },
                    name='clusterhq-python-flocker',
                    prefix=expected_prefix,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    license=expected_license,
                    url=expected_url,
                    vendor=expected_vendor,
                    maintainer=expected_maintainer,
                    architecture='native',
                    description=PACKAGE_PYTHON.DESCRIPTION.value,
                    category=expected_category,
                    directories=[expected_virtualenv_path],
                    dependencies=[Dependency(package='python-dep')],
                ),
                LintPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    package='clusterhq-python-flocker',
                    architecture="native",
                ),

                # clusterhq-flocker-cli steps
                CreateLinks(
                    links=[
                        (FilePath('/opt/flocker/bin/flocker-deploy'),
                         flocker_cli_path),
                        (FilePath('/opt/flocker/bin/flocker'),
                         flocker_cli_path),
                        (FilePath('/opt/flocker/bin/flocker-ca'),
                         flocker_cli_path),
                    ]
                ),
                BuildPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    source_paths={flocker_cli_path: FilePath("/usr/bin")},
                    name='clusterhq-flocker-cli',
                    prefix=expected_prefix,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    license=expected_license,
                    url=expected_url,
                    vendor=expected_vendor,
                    maintainer=expected_maintainer,
                    architecture='all',
                    description=PACKAGE_CLI.DESCRIPTION.value,
                    category=expected_category,
                    dependencies=[Dependency(package='cli-dep')],
                ),
                LintPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    package='clusterhq-flocker-cli',
                    architecture="all",
                ),

                # clusterhq-flocker-node steps
                CreateLinks(
                    links=[
                        (FilePath('/opt/flocker/bin/flocker-volume'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-control'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-container-agent'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-dataset-agent'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-diagnostics'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-benchmark'),
                         flocker_node_path),
                        (FilePath('/opt/flocker/bin/flocker-node-era'),
                         flocker_node_path),
                    ]
                ),
                BuildPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    source_paths={
                        flocker_node_path: FilePath("/usr/sbin"),
                        package_files.child('firewalld-services'):
                            FilePath("/usr/lib/firewalld/services/"),
                        # Ubuntu firewall configuration
                        package_files.child('ufw-applications.d'):
                            FilePath("/etc/ufw/applications.d/"),
                        # Systemd configuration
                        package_files.child('systemd'):
                            FilePath("/usr/lib/systemd/system/"),
                        # Upstart configuration
                        package_files.child('upstart'):
                            FilePath('/etc/init'),
                        # rsyslog configuration
                        package_files.child(b'rsyslog'):
                            FilePath(b"/etc/rsyslog.d"),
                        # Flocker Control State dir
                        empty_path: FilePath('/var/lib/flocker/'),
                    },
                    name='clusterhq-flocker-node',
                    prefix=expected_prefix,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    license=expected_license,
                    url=expected_url,
                    vendor=expected_vendor,
                    maintainer=expected_maintainer,
                    architecture='all',
                    description=PACKAGE_NODE.DESCRIPTION.value,
                    category=expected_category,
                    dependencies=[Dependency(package='node-dep')],
                    after_install=package_files.child('after-install.sh'),
                    directories=[FilePath('/var/lib/flocker/')],
                ),
                LintPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    package='clusterhq-flocker-node',
                    architecture="all",
                ),
                CreateLinks(
                    links=[
                        (FilePath('/opt/flocker/bin/flocker-docker-plugin'),
                         flocker_docker_plugin_path),
                    ]
                ),
                BuildPackage(
                    package_type=expected_package_type,
                    destination_path=expected_destination_path,
                    source_paths={
                        flocker_docker_plugin_path: FilePath("/usr/sbin"),
                        # SystemD configuration
                        package_files.child('docker-plugin').child('systemd'):
                            FilePath('/usr/lib/systemd/system'),
                        # Upstart configuration
                        package_files.child('docker-plugin').child('upstart'):
                            FilePath('/etc/init'),
                    },
                    name='clusterhq-flocker-docker-plugin',
                    prefix=FilePath('/'),
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    license=PACKAGE.LICENSE.value,
                    url=PACKAGE.URL.value,
                    vendor=PACKAGE.VENDOR.value,
                    maintainer=PACKAGE.MAINTAINER.value,
                    architecture="all",
                    description=PACKAGE_DOCKER_PLUGIN.DESCRIPTION.value,
                    category=expected_category,
                    dependencies=[Dependency(package='docker-plugin-dep')],
                ),
                LintPackage(
                    package_type=distribution.package_type(),
                    destination_path=expected_destination_path,
                    epoch=expected_epoch,
                    rpm_version=expected_version,
                    package='clusterhq-flocker-docker-plugin',
                    architecture="all",
                ),

            )
        )
        assert_equal_steps(
            self,
            expected,
            omnibus_package_builder(distribution=distribution,
                                    destination_path=expected_destination_path,
                                    package_uri=expected_package_uri,
                                    target_dir=target_path,
                                    package_files=FilePath('/package-files'),
                                    ))


class DockerBuildOptionsTests(TestCase):
    """
    Tests for ``DockerBuildOptions``.
    """

    native_package_type = object()

    def setUp(self):
        """
        Patch ``admin.packaging._native_package_type`` to return a fixed value.
        """
        super(DockerBuildOptionsTests, self).setUp()
        self.patch(
            packaging, '_native_package_type',
            lambda: self.native_package_type)

    def test_defaults(self):
        """
        ``DockerBuildOptions`` destination path defaults to the current working
        directory.
        """
        expected_defaults = {
            'destination-path': '.',
        }
        self.assertEqual(expected_defaults, DockerBuildOptions())

    def test_package_uri_missing(self):
        """
        ``DockerBuildOptions`` requires a single positional argument containing
        the URI of the Python package which is being packaged.
        """
        exception = self.assertRaises(
            UsageError, DockerBuildOptions().parseOptions, [])
        self.assertEqual('Wrong number of arguments.', str(exception))

    def test_package_uri_supplied(self):
        """
        ``DockerBuildOptions`` saves the supplied ``package-uri``.
        """
        expected_uri = 'http://www.example.com/foo-bar.whl'

        options = DockerBuildOptions()
        options.parseOptions([expected_uri])

        self.assertEqual(expected_uri, options['package-uri'])


class DockerBuildScriptTests(TestCase):
    """
    Tests for ``DockerBuildScript``.
    """
    def test_usage_error_status(self):
        """
        ``DockerBuildScript.main`` raises ``SystemExit`` if there are missing
        command line options.
        """
        fake_sys_module = FakeSysModule(argv=[])
        script = DockerBuildScript(sys_module=fake_sys_module)
        exception = self.assertRaises(SystemExit, script.main)
        self.assertEqual(1, exception.code)

    def test_usage_error_message(self):
        """
        ``DockerBuildScript.main`` prints a usage error to ``stderr`` if there
        are missing command line options.
        """
        fake_sys_module = FakeSysModule(argv=[])
        script = DockerBuildScript(sys_module=fake_sys_module)
        try:
            script.main()
        except SystemExit:
            pass
        self.assertEqual(
            'Wrong number of arguments.',
            fake_sys_module.stderr.getvalue().splitlines()[-1]
        )

    def test_build_command(self):
        """
        ``DockerBuildScript.build_command`` is ``omnibus_package_builder`` by
        default.
        """
        self.assertIs(omnibus_package_builder, DockerBuildScript.build_command)

    def test_run(self):
        """
        ``DockerBuildScript.main`` calls ``run`` on the instance returned by
        ``build_command``.
        """
        expected_destination_path = FilePath(self.mktemp())
        expected_package_uri = 'http://www.example.com/foo/bar.whl'
        fake_sys_module = FakeSysModule(
            argv=[
                'build-command-name',
                '--destination-path=%s' % (expected_destination_path.path,),
                expected_package_uri]
        )
        distribution = Distribution(name='test-distro', version='30')
        self.patch(packaging, 'CURRENT_DISTRIBUTION', distribution)
        script = DockerBuildScript(sys_module=fake_sys_module)
        build_step = SpyStep()
        arguments = []

        def record_arguments(*args, **kwargs):
            arguments.append((args, kwargs))
            return build_step
        script.build_command = record_arguments
        script.main(top_level=FilePath('/top-level'))
        expected_build_arguments = [(
            (),
            dict(destination_path=expected_destination_path,
                 package_uri=expected_package_uri,
                 distribution=distribution,
                 package_files=FilePath('/top-level/admin/package-files'))
        )]
        self.assertEqual(expected_build_arguments, arguments)
        self.assertTrue(build_step.ran)


class BuildOptionsTests(TestCase):
    """
    Tests for ``BuildOptions``.
    """

    DISTROS = [u"greatos"]

    def test_defaults(self):
        """
        ``BuildOptions`` destination path defaults to the current working
        directory.
        """
        expected_defaults = {
            'destination-path': '.',
            'distribution': None,
        }
        self.assertEqual(expected_defaults, BuildOptions([]))

    def test_possible_distributions(self):
        """
        ``BuildOptions`` offers as possible distributions all of the names
        passed to its initializer.
        """
        options = BuildOptions([b"greatos", b"betteros"])
        description = options.docs["distribution"]
        self.assertNotIn(
            -1,
            (description.find(b"greatos"), description.find(b"betteros")),
            "Supplied distribution names, greatos and betteros, not found in "
            "--distribution parameter definition: {}".format(description)
        )

    def test_distribution_missing(self):
        """
        ``BuildOptions.parseOptions`` raises ``UsageError`` if
        ``--distribution`` is not supplied.
        """
        options = BuildOptions(self.DISTROS)
        self.assertRaises(
            UsageError,
            options.parseOptions,
            ['http://example.com/fake/uri'])

    def test_package_uri_missing(self):
        """
        ``DockerBuildOptions`` requires a single positional argument containing
        the URI of the Python package which is being packaged.
        """
        exception = self.assertRaises(
            UsageError, BuildOptions(self.DISTROS).parseOptions, [])
        self.assertEqual('Wrong number of arguments.', str(exception))

    def test_package_options_supplied(self):
        """
        ``BuildOptions`` saves the supplied options.
        """
        expected_uri = 'http://www.example.com/foo-bar.whl'
        expected_distribution = 'ubuntu1404'
        options = BuildOptions(self.DISTROS + [expected_distribution])
        options.parseOptions(
            ['--distribution', expected_distribution, expected_uri])

        self.assertEqual(
            (expected_distribution, expected_uri),
            (options['distribution'], options['package-uri'])
        )


class AvailableDistributionTests(TestCase):
    """
    Tests for ``available_distributions``.
    """
    def test_dockerfiles(self):
        """
        Directories in the ``admin/build_targets/`` sub-directory of the path
        passed to ``available_distributions`` which themselves contain a
        ``Dockerfile`` are considered distributions and included in the result.
        """
        root = FilePath(self.mktemp())
        build_targets = root.descendant([b"admin", b"build_targets"])
        build_targets.makedirs()
        build_targets.child(b"foo").setContent(b"bar")
        greatos = build_targets.child(b"greatos")
        greatos.makedirs()
        greatos.child(b"Dockerfile").setContent(
            b"MAINTAINER example@example.invalid\n"
        )
        nothing = build_targets.child(b"nothing")
        nothing.makedirs()

        self.assertEqual(
            {b"greatos"},
            available_distributions(root),
        )


class BuildScriptTests(TestCase):
    """
    Tests for ``BuildScript``.
    """
    def test_usage_error_status(self):
        """
        ``BuildScript.main`` raises ``SystemExit`` if there are missing command
        line options.
        """
        fake_sys_module = FakeSysModule(argv=[])
        script = BuildScript(sys_module=fake_sys_module)
        exception = self.assertRaises(
            SystemExit,
            script.main, top_level=FLOCKER_PATH)
        self.assertEqual(1, exception.code)

    def test_usage_error_message(self):
        """
        ``BuildScript.main`` prints a usage error to ``stderr`` if there are
        missing command line options.
        """
        fake_sys_module = FakeSysModule(argv=[])
        script = BuildScript(sys_module=fake_sys_module)

        try:
            script.main(top_level=FLOCKER_PATH)
        except SystemExit:
            pass
        self.assertEqual(
            'Wrong number of arguments.',
            fake_sys_module.stderr.getvalue().splitlines()[-1]
        )

    def test_build_command(self):
        """
        ``BuildScript.build_command`` is ``build_in_docker`` by default.
        """
        self.assertIs(build_in_docker, BuildScript.build_command)

    def test_run(self):
        """
        ``BuildScript.main`` calls ``run`` on the instance returned by
        ``build_command``.
        """
        expected_destination_path = FilePath(self.mktemp())
        expected_distribution = 'centos7'
        expected_package_uri = 'http://www.example.com/foo/bar.whl'
        fake_sys_module = FakeSysModule(
            argv=[
                'build-command-name',
                '--destination-path', expected_destination_path.path,
                '--distribution=%s' % (expected_distribution,),
                expected_package_uri]
        )
        script = BuildScript(sys_module=fake_sys_module)
        build_step = SpyStep()
        arguments = []

        def record_arguments(*args, **kwargs):
            arguments.append((args, kwargs))
            return build_step
        script.build_command = record_arguments
        script.main(top_level=FLOCKER_PATH)
        expected_build_arguments = [(
            (),
            dict(destination_path=expected_destination_path,
                 distribution=expected_distribution,
                 package_uri=expected_package_uri,
                 top_level=FLOCKER_PATH)
        )]
        self.assertEqual(expected_build_arguments, arguments)
        self.assertTrue(build_step.ran)


class BuildInDockerFunctionTests(TestCase):
    """
    Tests for ``build_in_docker``.
    """
    def test_steps(self):
        """
        ``build_in_docker`` returns a ``BuildSequence`` comprising
        ``DockerBuild`` and ``DockerRun`` instances.
        """
        supplied_distribution = 'Foo'
        expected_tag = 'clusterhq/build-%s' % (supplied_distribution,)
        supplied_top_level = FilePath(self.mktemp())
        expected_build_directory = supplied_top_level.descendant(
            ['admin', 'build_targets', supplied_distribution])
        expected_build_directory.makedirs()
        expected_build_directory.sibling('requirements.txt').setContent('')
        supplied_destination_path = FilePath('/baz/qux')
        expected_volumes = {
            FilePath('/output'): supplied_destination_path,
            FilePath('/flocker'): supplied_top_level,
        }
        expected_package_uri = 'http://www.example.com/foo/bar/whl'

        assert_equal_steps(
            test_case=self,
            expected=BuildSequence(
                steps=[
                    DockerBuild(
                        tag=expected_tag,
                        build_directory=expected_build_directory
                    ),
                    DockerRun(
                        tag=expected_tag,
                        volumes=expected_volumes,
                        command=[expected_package_uri]
                    ),
                ]
            ),
            actual=build_in_docker(
                destination_path=supplied_destination_path,
                distribution=supplied_distribution,
                top_level=supplied_top_level,
                package_uri=expected_package_uri,
            )
        )

    def test_copies_requirements(self):
        """
        A requirements file is copied into the build directory.
        """
        supplied_distribution = 'Foo'
        supplied_top_level = FilePath(self.mktemp())
        expected_build_directory = supplied_top_level.descendant(
            ['admin', 'build_targets', supplied_distribution])
        expected_build_directory.makedirs()
        requirements = 'some_requirement'
        expected_build_directory.sibling('requirements.txt').setContent(
            requirements)
        supplied_destination_path = FilePath('/baz/qux')
        expected_package_uri = 'http://www.example.com/foo/bar/whl'
        build_in_docker(
            destination_path=supplied_destination_path,
            distribution=supplied_distribution,
            top_level=supplied_top_level,
            package_uri=expected_package_uri
        )

        self.assertEqual(
            requirements,
            expected_build_directory.child('requirements.txt').getContent()
        )


class MakeDependenciesTests(TestCase):
    """
    Tests for ``make_dependencies``.
    """
    def test_node(self):
        """
        ``make_dependencies`` includes the supplied ``version`` of
        ``clusterhq-python-flocker`` for ``clusterhq-flocker-node``.
        """
        expected_version = '1.2.3'
        self.assertIn(
            Dependency(
                package='clusterhq-python-flocker',
                compare='=',
                version=expected_version
            ),
            make_dependencies('node', expected_version,
                              Distribution(name='centos', version='7'))
        )

    def test_cli(self):
        """
        ``make_dependencies`` includes the supplied ``version`` of
        ``clusterhq-python-flocker`` for ``clusterhq-flocker-cli``.
        """
        expected_version = '1.2.3'
        self.assertIn(
            Dependency(
                package='clusterhq-python-flocker',
                compare='=',
                version=expected_version
            ),
            make_dependencies('cli', expected_version,
                              Distribution(name='centos', version='7'))
        )

# Copyright 2015 ClusterHQ Inc.  See LICENSE file for details.
"""
Wait operation for the control service benchmarks.
"""

from zope.interface import implementer

from twisted.internet.defer import Deferred, succeed

from benchmark._interfaces import IProbe, IOperation


@implementer(IProbe)
class WaitProbe(object):
    """
    A probe to wait for a specified time period.
    """

    def __init__(self, reactor, wait_seconds):
        self.reactor = reactor
        self.wait_seconds = wait_seconds

    def run(self):
        d = Deferred()
        self.reactor.callLater(self.wait_seconds, d.callback, None)
        return d

    def cleanup(self):
        return succeed(None)


@implementer(IOperation)
class Wait(object):
    """
    An operation to wait for a number of seconds.
    """

    def __init__(self, reactor, cluster, wait_seconds=10):
        self.reactor = reactor
        self.wait_seconds = wait_seconds

    def get_probe(self):
        return WaitProbe(self.reactor, self.wait_seconds)

# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Tests for the datasets REST API.
"""
import os

from datetime import timedelta
from uuid import UUID, uuid4
from unittest import SkipTest, skipIf

from testtools import run_test_with
from testtools.matchers import MatchesListwise, AfterPreprocessing, Equals
from twisted.internet import reactor


from flocker import __version__ as HEAD_FLOCKER_VERSION
from flocker.common.version import get_installable_version
from ...common import loop_until
from ...testtools import AsyncTestCase, flaky, async_runner
from ...node.agents.blockdevice import ICloudAPI

from ...provision import PackageSource

from ...node import backends

from ..testtools import (
    require_cluster, require_moving_backend, create_dataset,
    skip_backend, get_backend_api, verify_socket,
    get_default_volume_size, ACCEPTANCE_TEST_TIMEOUT
)


class DatasetAPITests(AsyncTestCase):
    """
    Tests for the dataset API.
    """

    run_tests_with = async_runner(timeout=ACCEPTANCE_TEST_TIMEOUT)

    @flaky(u'FLOC-3207')
    @require_cluster(1)
    def test_dataset_creation(self, cluster):
        """
        A dataset can be created on a specific node.
        """
        return create_dataset(self, cluster)

    def _get_package_source(self, default_version=None):
        """
        Get the package source for the flocker version under test from
        environment variables.

        The environment variables that will be read are as follows. Note that
        if any of them are not specified the test will be skipped.

        FLOCKER_ACCEPTANCE_PACKAGE_BRANCH:
            The branch to build from or an empty string to use the default.
        FLOCKER_ACCEPTANCE_PACKAGE_VERSION:
            The version of the package of flocker under test or the empty
            string to use the default.
        FLOCKER_ACCEPTANCE_PACKAGE_BUILD_SERVER:
            The build server from which to download the flocker package under
            test.

        :param unicode default_version: The version of flocker to use
            if the ``FLOCKER_ACCEPTANCE_PACKAGE_VERSION`` specifies to use the
            default.

        :return: A ``PackageSource`` that can be used to install the version of
            flocker under test.
        """
        env_vars = ['FLOCKER_ACCEPTANCE_PACKAGE_BRANCH',
                    'FLOCKER_ACCEPTANCE_PACKAGE_VERSION',
                    'FLOCKER_ACCEPTANCE_PACKAGE_BUILD_SERVER']
        defaultable = frozenset(['FLOCKER_ACCEPTANCE_PACKAGE_BRANCH',
                                 'FLOCKER_ACCEPTANCE_PACKAGE_VERSION'])
        missing_vars = list(var for var in env_vars if var not in os.environ)
        if missing_vars:
            message = ('Missing environment variables for upgrade test: %s.' %
                       ', '.join(missing_vars))
            missing_defaultable = list(var for var in missing_vars
                                       if var in defaultable)
            if missing_defaultable:
                message += (' Note that (%s) can be set to an empty string to '
                            'use a default value' %
                            ', '.join(missing_defaultable))
            raise SkipTest(message)
        version = (os.environ['FLOCKER_ACCEPTANCE_PACKAGE_VERSION'] or
                   default_version)
        return PackageSource(
            version=version,
            branch=os.environ['FLOCKER_ACCEPTANCE_PACKAGE_BRANCH'],
            build_server=os.environ['FLOCKER_ACCEPTANCE_PACKAGE_BUILD_SERVER'])

    @skip_backend(
        unsupported={backends.LOOPBACK},
        reason="Does not maintain compute_instance_id across restarting "
               "flocker (and didn't as of most recent release).")
    @skip_backend(
        unsupported={backends.GCE},
        # XXX: FLOC-4297: Enable this after the next marketing release.
        reason="GCE was not available during the most recent release.")
    @run_test_with(async_runner(timeout=timedelta(minutes=6)))
    @require_cluster(1)
    def test_upgrade(self, cluster):
        """
        Given a dataset created and used with the previously installable
        version of flocker, uninstalling the previous version of flocker and
        installing HEAD does not destroy the data on the dataset.
        """
        node = cluster.nodes[0]
        SAMPLE_STR = '123456' * 100

        upgrade_from_version = get_installable_version(HEAD_FLOCKER_VERSION)

        # Get the initial flocker version and setup a cleanup call to restore
        # flocker to that version when the test is done.
        d = cluster.client.version()
        original_package_source = [None]

        def setup_restore_original_flocker(version):
            version_bytes = version.get('flocker', u'').encode('ascii')
            original_package_source[0] = (
                self._get_package_source(
                    default_version=version_bytes or None)
            )
            self.addCleanup(
                lambda: cluster.install_flocker_version(
                    original_package_source[0]))
            return version

        d.addCallback(setup_restore_original_flocker)

        # Double check that the nodes are clean before we destroy the persisted
        # state.
        d.addCallback(lambda _: cluster.clean_nodes())

        # Downgrade flocker to the most recent released version.
        d.addCallback(
            lambda _: cluster.install_flocker_version(
                PackageSource(version=upgrade_from_version),
                destroy_persisted_state=True
            )
        )

        # Create a dataset with the code from the most recent release.
        d.addCallback(lambda _: create_dataset(self, cluster, node=node))
        first_dataset = [None]

        # Write some data to a file in the dataset.
        def write_to_file(dataset):
            first_dataset[0] = dataset
            return node.run_as_root(
                ['bash', '-c', 'echo "%s" > %s' % (
                    SAMPLE_STR, os.path.join(dataset.path.path, 'test.txt'))])
        d.addCallback(write_to_file)

        # Upgrade flocker to the code under test.
        d.addCallback(lambda _: cluster.install_flocker_version(
            original_package_source[0]))

        # Create a new dataset to convince ourselves that the new code is
        # running.
        d.addCallback(lambda _: create_dataset(self, cluster, node=node))

        # Wait for the first dataset to be mounted again.
        d.addCallback(lambda _: cluster.wait_for_dataset(first_dataset[0]))

        # Verify that the file still has its contents.
        def cat_and_verify_file(dataset):
            output = []

            file_catting = node.run_as_root(
                ['bash', '-c', 'cat %s' % (
                    os.path.join(dataset.path.path, 'test.txt'))],
                handle_stdout=output.append)

            def verify_file(_):
                file_contents = ''.join(output)
                self.assertEqual(file_contents, SAMPLE_STR)

            file_catting.addCallback(verify_file)
            return file_catting
        d.addCallback(cat_and_verify_file)
        return d

    @require_cluster(1, required_backend=backends.AWS)
    def test_dataset_creation_with_gold_profile(self, cluster, backend):
        """
        A dataset created with the gold profile as specified in metadata on EBS
        has EBS volume type 'io1'.

        This is verified by constructing an EBS backend in this process, purely
        for the sake of using it as a wrapper on the cloud API.
        """
        waiting_for_create = create_dataset(
            self, cluster, maximum_size=4*1024*1024*1024,
            metadata={u"clusterhq:flocker:profile": u"gold"})

        def confirm_gold(dataset):
            volumes = backend.list_volumes()
            matching = [
                v for v in volumes if v.dataset_id == dataset.dataset_id]
            volume_types = [
                backend._get_ebs_volume(v.blockdevice_id).volume_type
                for v in matching]
            self.assertEqual(volume_types, ['io1'])

        waiting_for_create.addCallback(confirm_gold)
        return waiting_for_create

    @flaky(u'FLOC-3341')
    @require_moving_backend
    @require_cluster(2)
    def test_dataset_move(self, cluster):
        """
        A dataset can be moved from one node to another.

        All attributes, including the maximum size, are preserved.
        """
        waiting_for_create = create_dataset(self, cluster)

        # Once created, request to move the dataset to node2
        def move_dataset(dataset):
            dataset_moving = cluster.client.move_dataset(
                UUID(cluster.nodes[1].uuid), dataset.dataset_id)

            # Wait for the dataset to be moved; we expect the state to
            # match that of the originally created dataset in all ways
            # other than the location.
            moved_dataset = dataset.set(
                primary=UUID(cluster.nodes[1].uuid))
            dataset_moving.addCallback(
                lambda dataset: cluster.wait_for_dataset(moved_dataset))
            return dataset_moving

        waiting_for_create.addCallback(move_dataset)
        return waiting_for_create

    @flaky(u'FLOC-3196')
    @require_cluster(1)
    def test_dataset_deletion(self, cluster):
        """
        A dataset can be deleted, resulting in its removal from the node.
        """
        created = create_dataset(self, cluster)

        def delete_dataset(dataset):
            deleted = cluster.client.delete_dataset(dataset.dataset_id)

            def not_exists():
                request = cluster.client.list_datasets_state()
                request.addCallback(
                    lambda actual_datasets: dataset.dataset_id not in
                    (d.dataset_id for d in actual_datasets))
                return request
            deleted.addCallback(lambda _: loop_until(reactor, not_exists))
            return deleted
        created.addCallback(delete_dataset)
        return created

    @skipIf(True,
            "Shutting down a node invalidates a public IP, which breaks all "
            "kinds of things. So skip for now.")
    @require_moving_backend
    @run_test_with(async_runner(timeout=timedelta(minutes=6)))
    @require_cluster(2)
    def test_dataset_move_from_dead_node(self, cluster):
        """
        A dataset can be moved from a dead node to a live node.

        All attributes, including the maximum size, are preserved.
        """
        api = get_backend_api(cluster.cluster_uuid)
        if not ICloudAPI.providedBy(api):
            raise SkipTest(
                "Backend doesn't support ICloudAPI; therefore it might support"
                " moving from dead node but as first pass we assume it "
                "doesn't.")

        # Find a node which is not running the control service.
        # If the control node is shut down we won't be able to move anything!
        node = list(node for node in cluster.nodes
                    if node.public_address !=
                    cluster.control_node.public_address)[0]
        other_node = list(other_node for other_node in cluster.nodes
                          if other_node != node)[0]
        waiting_for_create = create_dataset(self, cluster, node=node)

        def startup_node(node_id):
            api.start_node(node_id)
            # Wait for node to boot up:; we presume Flocker getting going after
            # SSH is available will be pretty quick:
            return loop_until(reactor, verify_socket(node.public_address, 22))

        # Once created, shut down origin node and then request to move the
        # dataset to node2:
        def shutdown(dataset):
            live_node_ids = set(api.list_live_nodes())
            d = node.shutdown()
            # Wait for shutdown to be far enough long that node is down:
            d.addCallback(
                lambda _:
                loop_until(reactor, lambda:
                           set(api.list_live_nodes()) != live_node_ids))
            # Schedule node start up:
            d.addCallback(
                lambda _: self.addCleanup(
                    startup_node,
                    (live_node_ids - set(api.list_live_nodes())).pop()))
            d.addCallback(lambda _: dataset)
            return d
        waiting_for_shutdown = waiting_for_create.addCallback(shutdown)

        def move_dataset(dataset):
            dataset_moving = cluster.client.move_dataset(
                UUID(other_node.uuid), dataset.dataset_id)

            # Wait for the dataset to be moved; we expect the state to
            # match that of the originally created dataset in all ways
            # other than the location.
            moved_dataset = dataset.set(
                primary=UUID(other_node.uuid))
            dataset_moving.addCallback(
                lambda dataset: cluster.wait_for_dataset(moved_dataset))
            return dataset_moving

        waiting_for_shutdown.addCallback(move_dataset)
        return waiting_for_shutdown

    @require_cluster(1)
    def test_unregistered_volume(self, cluster):
        """
        If there is already a backend volume for a dataset when it is created,
        that volume is used for that dataset.
        """
        api = get_backend_api(cluster.cluster_uuid)

        # Create a volume for a dataset
        dataset_id = uuid4()
        volume = api.create_volume(dataset_id, size=get_default_volume_size())

        # Then create the coresponding dataset.
        wait_for_dataset = create_dataset(self, cluster, dataset_id=dataset_id)

        def check_volumes(dataset):
            new_volumes = api.list_volumes()
            # That volume should be the only dataset in the cluster.
            # Clear `.attached_to` on the new volume, since we expect it to be
            # attached.
            self.assertThat(
                new_volumes,
                MatchesListwise([
                    AfterPreprocessing(
                        lambda new_volume: new_volume.set('attached_to', None),
                        Equals(volume)
                    ),
                ])
            )
        wait_for_dataset.addCallback(check_volumes)
        return wait_for_dataset

    @skip_backend(
        unsupported={backends.GCE},
        reason="The GCE backend does not let you create two volumes with the "
               "same dataset id. When this test is run with GCE the test "
               "fails to create the extra volume, and we do not test the "
               "functionality this test was designed to test.")
    @require_cluster(2)
    def test_extra_volume(self, cluster):
        """
        If an extra volume is created for a dataset, that volume isn't used.

        .. note:
           This test will be flaky if flocker doesn't correctly ignore extra
           volumes that claim to belong to a dataset, since the dataset picked
           will be random.
        """
        api = get_backend_api(cluster.cluster_uuid)

        # Create the dataset
        wait_for_dataset = create_dataset(self, cluster)

        created_volume = []

        # Create an extra volume claiming to belong to that dataset
        def create_extra(dataset):
            # Create a second volume for that dataset
            volume = api.create_volume(dataset.dataset_id,
                                       size=get_default_volume_size())
            created_volume.append(volume)
            return dataset

        wait_for_extra_volume = wait_for_dataset.addCallback(create_extra)

        # Once created, request to move the dataset to node2
        def move_dataset(dataset):
            dataset_moving = cluster.client.move_dataset(
                UUID(cluster.nodes[1].uuid), dataset.dataset_id)

            # Wait for the dataset to be moved; we expect the state to
            # match that of the originally created dataset in all ways
            # other than the location.
            moved_dataset = dataset.set(
                primary=UUID(cluster.nodes[1].uuid))
            dataset_moving.addCallback(
                lambda dataset: cluster.wait_for_dataset(moved_dataset))
            return dataset_moving
        wait_for_move = wait_for_extra_volume.addCallback(move_dataset)

        # Check that the extra volume isn't attached to a node.
        # This indicates that the originally created volume is attached.
        def check_attached(dataset):
            blockdevice_id = created_volume[0].blockdevice_id
            [volume] = [volume for volume in api.list_volumes()
                        if volume.blockdevice_id == blockdevice_id]

            self.assertEqual(volume.attached_to, None)

        return wait_for_move.addCallback(check_attached)

        return wait_for_dataset

# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Client for the Flocker REST API.

This may eventually be a standalone package.
"""

from ._client import (
    IFlockerAPIV1Client, FakeFlockerClient, Dataset, DatasetState,
    DatasetAlreadyExists, FlockerClient, Lease, LeaseAlreadyHeld,
    conditional_create, DatasetsConfiguration, Node, MountedDataset,
)

__all__ = ["IFlockerAPIV1Client", "FakeFlockerClient", "Dataset",
           "DatasetState", "DatasetAlreadyExists", "FlockerClient",
           "Lease", "LeaseAlreadyHeld", "conditional_create",
           "DatasetsConfiguration", "Node", "MountedDataset", ]

# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Helpers for :py:class:`~twisted.python.filepath.FilePath`.
"""


def make_file(path, content='', permissions=None):
    """
    Create a file with given content and permissions.

    Don't use this for sensitive content, as the permissions are applied
    *after* the data is written to the filesystem.

    :param FilePath path: Path to create the file.
    :param str content: Content to write to the file. If not specified,
    :param int permissions: Unix file permissions to be passed to ``chmod``.

    :return: ``path``, unmodified.
    :rtype: :py:class:`twisted.python.filepath.FilePath`
    """
    path.setContent(content)
    if permissions is not None:
        path.chmod(permissions)
    return path


def make_directory(path):
    """
    Create a directory at ``path``.

    :param FilePath path: The place to create a directory.

    :raise OSError: If path already exists and is not a directory.
    :return: ``path``, unmodified.
    :rtype: :py:class:`twisted.python.filepath.FilePath`
    """
    if not path.isdir():
        path.makedirs()
    return path

# Copyright ClusterHQ Inc.  See LICENSE file for details.

"""
Tests for :py:class:`flocker.control._registry`.
"""

from ..testtools import make_istatepersister_tests, InMemoryStatePersister


def make_inmemorystatepersister(test_case):
    """
    Create a ``InMemoryStatePersister`` for use in tests.

    :return: ``tuple`` of ``IStatePersiter`` and 0-argument callable returning
    a ``PersistentState``.
    """
    state_persister = InMemoryStatePersister()
    return state_persister, state_persister.get_state


class InMemoryStatePersisterTests(
    make_istatepersister_tests(make_inmemorystatepersister)
):
    """
    Tests for ``InMemoryStatePersister``.
    """

"""
This package contains backend-specific agent implementations.
"""

